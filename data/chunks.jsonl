{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 0, "text": "Foundations\nand Concepts\nChristopher M. Bishop\nwith Hugh Bishop\n Deep Learning\nDeep Learning\n\nChristopher M. Bishop - Hugh Bishop\nDeep Learning\nFoundations and Concepts\n\nISBN 978-3-031-45467-7 ISBN 978-3-031-45468-4 (eBook)\nhttps://doi.org/10.1007/978-3-031-45468-4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1, "text": "© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nThis work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole or part of the\nmaterial is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2, "text": "microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software,\nor by similar or dissimilar methodology now known or hereafter developed.\nThe use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the\nabsence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for\ngeneral use."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3, "text": "general use.\nThe publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and\naccurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express ed or implied, with respect\nto the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to\njurisdictional claims in published maps and institutional affiliations."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 4, "text": "Cover illustration: maksimee / Alamy Stock Photo\nThis Springer imprint is published by the registered company Springer Nature Switzerland AG\nThe registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 5, "text": "Paper in this product is recyclable.\nChristopher M. Bishop\nMicrosoft Research\nCambridge, UK\nHugh Bishop\nWayve Technologies Ltd\nLondon, UK\nPreface\nDeep learning uses multilayered neural networks trained with large data sets to\nsolve complex information processing tasks and has emerged as the most successful\nparadigm in the ﬁeld of machine learning. Over the last decade, deep learning has\nrevolutionized many domains including computer vision, speech recognition, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 6, "text": "natural language processing, and it is being used in a growing multitude of applica-\ntions across healthcare, manufacturing, commerce, ﬁnance, scientiﬁc discovery, and\nmany other sectors. Recently, massive neural networks, known as large language\nmodels and comprising of the order of a trillion learnable parameters, have been\nfound to exhibit the ﬁrst indications of general artiﬁcial intelligence and are now\ndriving one of the biggest disruptions in the history of technology.\nGoals of the book"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 7, "text": "Goals of the book\nThis expanding impact has been accompanied by an explosion in the number\nand breadth of research publications in machine learning, and the pace of innova-\ntion continues to accelerate. For newcomers to the ﬁeld, the challenge of getting\nto grips with the key ideas, let alone catching up to the research frontier, can seem\ndaunting. Against this backdrop, Deep Learning: Foundations and Concepts aims"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 8, "text": "to provide newcomers to machine learning, as well as those already experienced in\nthe ﬁeld, with a thorough understanding of both the foundational ideas that underpin\ndeep learning as well as the key concepts of modern deep learning architectures and\ntechniques. This material will equip the reader with a strong basis for future spe-\ncialization. Due to the breadth and pace of change in the ﬁeld, we have deliberately"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 9, "text": "avoided trying to create a comprehensive survey of the latest research. Instead, much\nof the value of the book derives from a distillation of key ideas, and although the ﬁeld\nitself can be expected to continue its rapid advance, these foundations and concepts\nare likely to stand the test of time. For example, large language models have been\nevolving very rapidly at the time of writing, yet the underlying transformer archi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 10, "text": "tecture and attention mechanism have remained largely unchanged for the last ﬁve\nyears, while many core principles of machine learning have been known for decades.\nv\nvi PREFACE\nResponsible use of technology\nDeep learning is a powerful technology with broad applicability that has the po-\ntential to create huge value for the world and address some of society’s most pressing\nchallenges. However, these same attributes mean that deep learning also has poten-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 11, "text": "tial both for deliberate misuse and to cause unintended harms. We have chosen not\nto discuss ethical or societal aspects of the use of deep learning, as these topics are of\nsuch importance and complexity that they warrant a more thorough treatment than is\npossible in a technical textbook such as this. Such considerations should, however,\nbe informed by a solid grounding in the underlying technology and how it works,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 12, "text": "and so we hope that this book will make a valuable contribution towards these im-\nportant discussions. The reader is, nevertheless, strongly encouraged to be mindful\nabout the broader implications of their work and to learn about the responsible use\nof deep learning and artiﬁcial intelligence alongside their studies of the technology\nitself.\nStructure of the book\nThe book is structured into a relatively large number of smaller bite-sized chap-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 13, "text": "ters, each of which explores a speciﬁc topic. The book has a linear structure in\nthe sense that each chapter depends only on material covered in earlier chapters. It\nis well suited to teaching a two-semester undergraduate or postgraduate course on\nmachine learning but is equally relevant to those engaged in active research or in\nself-study.\nA clear understanding of machine learning can be achieved only through the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 14, "text": "use of some level of mathematics. Speciﬁcally, three areas of mathematics lie at the\nheart of machine learning: probability theory, linear algebra, and multivariate cal-\nculus. The book provides a self-contained introduction to the required concepts in\nprobability theory and includes an appendix that summarizes some useful results in\nlinear algebra. It is assumed that the reader already has some familiarity with the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 15, "text": "basic concepts of multivariate calculus although there are appendices that provide\nintroductions to the calculus of variations and to Lagrange multipliers. The focus\nof the book, however, is on conveying a clear understanding of ideas, and the em-\nphasis is on techniques that have real-world practical value rather than on abstract\ntheory. Where possible we try to present more complex concepts from multiple com-\nplementary perspectives including textual description, diagrams, and mathematical"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 16, "text": "formulae. In addition, many of the key algorithms discussed in the text are summa-\nrized in separate boxes. These do not address issues of computational efﬁciency, but\nare provided as a complement to the mathematical explanations given in the text.\nWe therefore hope that the material in this book will be accessible to readers from a\nvariety of backgrounds.\nConceptually, this book is perhaps most naturally viewed as a successor toNeu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 17, "text": "ral Networks for Pattern Recognition(Bishop, 1995b), which provided the ﬁrst com-\nprehensive treatment of neural networks from a statistical perspective. It can also\nbe considered as a companion volume to Pattern Recognition and Machine Learn-\ning (Bishop, 2006), which covered a broader range of topics in machine learning\nalthough it predated the deep learning revolution. However, to ensure that this\nPREFACE vii\nnew book is self-contained, appropriate material has been carried over from Bishop"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 18, "text": "(2006) and refactored to focus on those foundational ideas that are needed for deep\nlearning. This means that there are many interesting topics in machine learning dis-\ncussed in Bishop (2006) that remain of interest today but which have been omitted\nfrom this new book. For example, Bishop (2006) discusses Bayesian methods in\nsome depth, whereas this book is almost entirely non-Bayesian.\nThe book is accompanied by a web site that provides supporting material, in-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 19, "text": "cluding a free-to-use digital version of the book as well as solutions to the exercises\nand downloadable versions of the ﬁgures in PDF and JPEG formats:\nhttps://www.bishopbook.com\nThe book can be cited using the following BibTex entry:\n@book{Bishop:DeepLearning24,\nauthor = {Christopher M. Bishop and Hugh Bishop},\ntitle = {Deep Learning: Foundations and Concepts},\nyear = {2024},\npublisher = {Springer}\n}\nIf you have any feedback on the book or would like to report any errors, please"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 20, "text": "send these to feedback@bishopbook.com\nReferences\nIn the spirit of focusing on core ideas, we make no attempt to provide a com-\nprehensive literature review, which in any case would be impossible given the scale\nand pace of change of the ﬁeld. We do, however, provide references to some of the\nkey research papers as well as review articles and other sources of further reading.\nIn many cases, these also provide important implementation details that we gloss"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 21, "text": "over in the text in order not to distract the reader from the central concepts being\ndiscussed.\nMany books have been written on the subject of machine learning in general and\non deep learning in particular. Those which are closest in level and style to this book\ninclude Bishop (2006), Goodfellow, Bengio, and Courville (2016), Murphy (2022),\nMurphy (2023), and Prince (2023).\nOver the last decade, the nature of machine learning scholarship has changed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 22, "text": "signiﬁcantly, with many papers being posted online on archival sites ahead of, or\neven instead of, submission to peer-reviewed conferences and journals. The most\npopular of these sites is arXiv, pronounced ‘archive’, and is available at\nhttps://arXiv.org\nThe site allows papers to be updated, often leading to multiple versions associated\nwith different calendar years, which can result in some ambiguity as to which version"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 23, "text": "should be cited and for which year. It also provides free access to a PDF of each pa-\nper. We have therefore adopted a simple approach of referencing the paper according\nto the year of ﬁrst upload, although we recommend reading the most recent version.\nviii PREFACE\nPapers on arXiv are indexed using a notation arXiv:YYMM.XXXXX where YY and\nMM denote the year and month of ﬁrst upload, respectively. Subsequent versions are\ndenoted by appending a version number N in the form arXiv:YYMM.XXXXXvN."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 24, "text": "Exercises\nEach chapter concludes with a set of exercises designed to reinforce the key\nideas explained in the text or to develop and generalize them in signiﬁcant ways.\nThese exercises form an important part of the text and each is graded according to\ndifﬁculty ranging from (?), which denotes a simple exercise taking a few moments\nto complete, through to (???), which denotes a signiﬁcantly more complex exercise."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 25, "text": "The reader is strongly encouraged to attempt the exercises since active participation\nwith the material greatly increases the effectiveness of learning. Worked solutions to\nall of the exercises are available as a downloadable PDF ﬁle from the book web site.\nMathematical notation\nWe follow the same notation as Bishop (2006). For an overview of mathematics\nin the context of machine learning, see Deisenroth, Faisal, and Ong (2020)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 26, "text": "Vectors are denoted by lower case bold roman letters such asx, whereas matrices\nare denoted by uppercase bold roman letters, such as M. All vectors are assumed to\nbe column vectors unless otherwise stated. A superscriptT denotes the transpose of a\nmatrix or vector, so thatxT will be a row vector. The notation(w1;:::;w M) denotes\na row vector with M elements, and the corresponding column vector is written as\nw = (w1;:::;w M)T. The M ×M identity matrix (also known as the unit matrix)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 27, "text": "is denoted IM, which will be abbreviated to I if there is no ambiguity about its\ndimensionality. It has elements Iij that equal 1 if i= jand 0 if i̸=j. The elements\nof a unit matrix are sometimes denoted by \u000eij. The notation 1 denotes a column\nvector in which all elements have the value 1. a ⊕b denotes the concatenation of\nvectors a and b, so that if a = (a1;:::;a N) and b = (b1;:::;b M) then a ⊕b =\n(a1;:::;a N;b1;:::;b M). |x|denotes the modulus (the positive part) of a scalar x,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 28, "text": "also known as the absolute value. We usedet A to denote the determinant of a matrix\nA.\nThe notation x ∼ p(x) signiﬁes that x is sampled from the distribution p(x).\nWhere there is ambiguity, we will use subscripts as in px(-)to denote which density\nis referred to. The expectation of a functionf(x;y) with respect to a random variable\nxis denoted by Ex[f(x;y)]. In situations where there is no ambiguity as to which\nvariable is being averaged over, this will be simpliﬁed by omitting the sufﬁx, for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 29, "text": "instance E[x]. If the distribution of x is conditioned on another variable z, then\nthe corresponding conditional expectation will be written Ex[f(x)|z]. Similarly, the\nvariance of f(x) is denoted var[f(x)], and for vector variables, the covariance is\nwritten cov[x;y]. We will also use cov[x] as a shorthand notation for cov[x;x].\nThe symbol ∀means ‘for all’, so that∀m∈M denotes all values of mwithin\nthe set M. We use R to denote the real numbers. On a graph, the set of neighbours of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 30, "text": "node iis denoted N(i), which should not be confused with the Gaussian or normal\ndistribution N(x|\u0016;\u001b2). A functional is denoted f[y] where y(x) is some function.\nThe concept of a functional is discussed in Appendix B. Curly braces {}denote a\nPREFACE ix\nset. The notation g(x) = O(f(x)) denotes that |f(x)=g(x)|is bounded as x→∞.\nFor instance, if g(x) = 3x2 + 2, then g(x) = O(x2). The notation ⌊x⌋denotes the\n‘ﬂoor’ ofx, i.e., the largest integer that is less than or equal to x."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 31, "text": "If we have N independent and identically distributed (i.i.d.) values x1;:::; xN\nof a D-dimensional vector x = (x1;:::;x D)T, we can combine the observations\ninto a data matrix X of dimension N ×D in which the nth row of X corresponds\nto the row vector xT\nn. Thus, the n;i element of X corresponds to the ith element of\nthe nth observation xn and is written xni. For one-dimensional variables, we denote\nsuch a matrix by x, which is a column vector whose nth element is xn. Note that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 32, "text": "x (which has dimensionality N) uses a different typeface to distinguish it from x\n(which has dimensionality D).\nAcknowledgements\nWe would like to express our sincere gratitude to the many people who re-\nviewed draft chapters and provided valuable feedback. In particular, we wish to\nthank Samuel Albanie, Cristian Bodnar, John Bronskill, Wessel Bruinsma, Ignas\nBudvytis, Chi Chen, Yaoyi Chen, Long Chen, Fergal Cotter, Sam Devlin, Alek-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 33, "text": "sander Durumeric, Sebastian Ehlert, Katarina Elez, Andrew Foong, Hong Ge, Paul\nGladkov, Paula Gori Giorgi, John Gossman, Tengda Han, Juyeon Heo, Katja Hof-\nmann, Chin-Wei Huang, Yongchaio Huang, Giulio Isacchini, Matthew Johnson,\nPragya Kale, Atharva Kelkar, Leon Klein, Pushmeet Kohli, Bonnie Kruft, Adrian\nLi, Haiguang Liu, Ziheng Lu, Giulia Luise, Stratis Markou, Sergio Valcarcel Macua,\nKrzysztof Maziarz, Mat ˇej Mezera, Laurence Midgley, Usman Munir, F ´elix Musil,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 34, "text": "Elise van der Pol, Tao Qin, Isaac Reid, David Rosenberger, Lloyd Russell, Maxi-\nmilian Schebek, Megan Stanley, Karin Strauss, Clark Templeton, Marlon Tobaben,\nAldo Sayeg Pasos-Trejo, Richard Turner, Max Welling, Furu Wei, Robert Weston,\nChris Williams, Yingce Xia, Shufang Xie, Iryna Zaporozhets, Claudio Zeni, Xieyuan\nZhang, and many other colleagues who contributed through valuable discussions.\nWe would also like to thank our editor Paul Drougas and many others at Springer, as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 35, "text": "well as the copy editor Jonathan Webley, for their support during the production of\nthe book.\nWe would like to say a special thank-you to Markus Svens ´en, who provided\nimmense help with the ﬁgures and typesetting for Bishop (2006) including the LATEX\nstyle ﬁles, which have also been used for this new book. We are also grateful to the\nmany scientists who allowed us to reproduce diagrams from their published work.\nAcknowledgements for speciﬁc ﬁgures appear in the associated ﬁgure captions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 36, "text": "Chris would like to express sincere gratitude to Microsoft for creating a highly\nstimulating research environment and for providing the opportunity to write this\nbook. The views and opinions expressed in this book, however, are those of the\nauthors and are therefore not necessarily the same as those of Microsoft or its afﬁl-\niates. It has been a huge privilege and pleasure to collaborate with my son Hugh in\npreparing this book, which started as a joint project during the ﬁrst Covid lockdown."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 37, "text": "x PREFACE\nHugh would like to thank Wayve Technologies Ltd for generously allowing him\nto work part time so that he could collaborate in writing this book as well as for\nproviding an inspiring and supportive environment for him to work and learn in.\nThe views expressed in this book are not necessarily the same as those of Wayve or\nits afﬁliates. He would like to express his gratitude to his ﬁanc ´ee Jemima for her\nconstant support as well as her grammatical and stylistic consultations. He would"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 38, "text": "also like to thank Chris, who has been an excellent colleague and an inspiration to\nHugh throughout his life.\nFinally, we would both like to say a huge thank-you to our family members\nJenna and Mark for so many things far too numerous to list here. It seems a very\nlong time ago that we all gathered on the beach in Antalya to watch a total eclipse\nof the sun and to take a family photo for the dedication page of Pattern Recognition\nand Machine Learning!\nChris Bishop and Hugh Bishop\nCambridge, UK"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 39, "text": "Chris Bishop and Hugh Bishop\nCambridge, UK\nOctober, 2023\nContents\nPreface v\nContents xi\n1 The Deep Learning Revolution 1\n1.1 The Impact of Deep Learning . . . . . . . . . . . . . . . . . . . . 2\n1.1.1 Medical diagnosis . . . . . . . . . . . . . . . . . . . . . . 2\n1.1.2 Protein structure . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1.3 Image synthesis . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.4 Large language models . . . . . . . . . . . . . . . . . . . . 5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 40, "text": "1.2 A Tutorial Example . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.2.1 Synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.2.2 Linear models . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.3 Error function . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.4 Model complexity . . . . . . . . . . . . . . . . . . . . . . 9\n1.2.5 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 12"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 41, "text": "1.2.6 Model selection . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.3 A Brief History of Machine Learning . . . . . . . . . . . . . . . . 16\n1.3.1 Single-layer networks . . . . . . . . . . . . . . . . . . . . 17\n1.3.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . 18\n1.3.3 Deep networks . . . . . . . . . . . . . . . . . . . . . . . . 20\n2 Probabilities 23\n2.1 The Rules of Probability . . . . . . . . . . . . . . . . . . . . . . . 25"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 42, "text": "2.1.1 A medical screening example . . . . . . . . . . . . . . . . 25\n2.1.2 The sum and product rules . . . . . . . . . . . . . . . . . . 26\n2.1.3 Bayes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.1.4 Medical screening revisited . . . . . . . . . . . . . . . . . 30\n2.1.5 Prior and posterior probabilities . . . . . . . . . . . . . . . 31\nxi\nxii CONTENTS\n2.1.6 Independent variables . . . . . . . . . . . . . . . . . . . . 31"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 43, "text": "2.2 Probability Densities . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.2.1 Example distributions . . . . . . . . . . . . . . . . . . . . 33\n2.2.2 Expectations and covariances . . . . . . . . . . . . . . . . 34\n2.3 The Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . . 36\n2.3.1 Mean and variance . . . . . . . . . . . . . . . . . . . . . . 37\n2.3.2 Likelihood function . . . . . . . . . . . . . . . . . . . . . . 37"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 44, "text": "2.3.3 Bias of maximum likelihood . . . . . . . . . . . . . . . . . 39\n2.3.4 Linear regression . . . . . . . . . . . . . . . . . . . . . . . 40\n2.4 Transformation of Densities . . . . . . . . . . . . . . . . . . . . . 42\n2.4.1 Multivariate distributions . . . . . . . . . . . . . . . . . . . 44\n2.5 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.5.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 45, "text": "2.5.2 Physics perspective . . . . . . . . . . . . . . . . . . . . . . 47\n2.5.3 Differential entropy . . . . . . . . . . . . . . . . . . . . . . 49\n2.5.4 Maximum entropy . . . . . . . . . . . . . . . . . . . . . . 50\n2.5.5 Kullback–Leibler divergence . . . . . . . . . . . . . . . . . 51\n2.5.6 Conditional entropy . . . . . . . . . . . . . . . . . . . . . 53\n2.5.7 Mutual information . . . . . . . . . . . . . . . . . . . . . . 54"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 46, "text": "2.6 Bayesian Probabilities . . . . . . . . . . . . . . . . . . . . . . . . 54\n2.6.1 Model parameters . . . . . . . . . . . . . . . . . . . . . . . 55\n2.6.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.6.3 Bayesian machine learning . . . . . . . . . . . . . . . . . . 57\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n3 Standard Distributions 65\n3.1 Discrete Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 66"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 47, "text": "3.1.1 Bernoulli distribution . . . . . . . . . . . . . . . . . . . . . 66\n3.1.2 Binomial distribution . . . . . . . . . . . . . . . . . . . . . 67\n3.1.3 Multinomial distribution . . . . . . . . . . . . . . . . . . . 68\n3.2 The Multivariate Gaussian . . . . . . . . . . . . . . . . . . . . . . 70\n3.2.1 Geometry of the Gaussian . . . . . . . . . . . . . . . . . . 71\n3.2.2 Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . 74"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 48, "text": "3.2.3 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n3.2.4 Conditional distribution . . . . . . . . . . . . . . . . . . . 76\n3.2.5 Marginal distribution . . . . . . . . . . . . . . . . . . . . . 79\n3.2.6 Bayes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . 81\n3.2.7 Maximum likelihood . . . . . . . . . . . . . . . . . . . . . 84\n3.2.8 Sequential estimation . . . . . . . . . . . . . . . . . . . . . 85"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 49, "text": "3.2.9 Mixtures of Gaussians . . . . . . . . . . . . . . . . . . . . 86\n3.3 Periodic Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n3.3.1 V on Mises distribution . . . . . . . . . . . . . . . . . . . . 89\n3.4 The Exponential Family . . . . . . . . . . . . . . . . . . . . . . . 94\n3.4.1 Sufﬁcient statistics . . . . . . . . . . . . . . . . . . . . . . 97\n3.5 Nonparametric Methods . . . . . . . . . . . . . . . . . . . . . . . 98\nCONTENTS xiii"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 50, "text": "CONTENTS xiii\n3.5.1 Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n3.5.2 Kernel densities . . . . . . . . . . . . . . . . . . . . . . . . 100\n3.5.3 Nearest-neighbours . . . . . . . . . . . . . . . . . . . . . . 103\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n4 Single-layer Networks: Regression 111\n4.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . 112"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 51, "text": "4.1.1 Basis functions . . . . . . . . . . . . . . . . . . . . . . . . 112\n4.1.2 Likelihood function . . . . . . . . . . . . . . . . . . . . . . 114\n4.1.3 Maximum likelihood . . . . . . . . . . . . . . . . . . . . . 115\n4.1.4 Geometry of least squares . . . . . . . . . . . . . . . . . . 117\n4.1.5 Sequential learning . . . . . . . . . . . . . . . . . . . . . . 117\n4.1.6 Regularized least squares . . . . . . . . . . . . . . . . . . . 118"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 52, "text": "4.1.7 Multiple outputs . . . . . . . . . . . . . . . . . . . . . . . 119\n4.2 Decision theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n4.3 The Bias–Variance Trade-off . . . . . . . . . . . . . . . . . . . . . 123\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n5 Single-layer Networks: Classiﬁcation 131\n5.1 Discriminant Functions . . . . . . . . . . . . . . . . . . . . . . . . 132"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 53, "text": "5.1.1 Two classes . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n5.1.2 Multiple classes . . . . . . . . . . . . . . . . . . . . . . . . 134\n5.1.3 1-of-K coding . . . . . . . . . . . . . . . . . . . . . . . . 135\n5.1.4 Least squares for classiﬁcation . . . . . . . . . . . . . . . . 136\n5.2 Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n5.2.1 Misclassiﬁcation rate . . . . . . . . . . . . . . . . . . . . . 139"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 54, "text": "5.2.2 Expected loss . . . . . . . . . . . . . . . . . . . . . . . . . 140\n5.2.3 The reject option . . . . . . . . . . . . . . . . . . . . . . . 142\n5.2.4 Inference and decision . . . . . . . . . . . . . . . . . . . . 143\n5.2.5 Classiﬁer accuracy . . . . . . . . . . . . . . . . . . . . . . 147\n5.2.6 ROC curve . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n5.3 Generative Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . 150"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 55, "text": "5.3.1 Continuous inputs . . . . . . . . . . . . . . . . . . . . . . 152\n5.3.2 Maximum likelihood solution . . . . . . . . . . . . . . . . 153\n5.3.3 Discrete features . . . . . . . . . . . . . . . . . . . . . . . 156\n5.3.4 Exponential family . . . . . . . . . . . . . . . . . . . . . . 156\n5.4 Discriminative Classiﬁers . . . . . . . . . . . . . . . . . . . . . . 157\n5.4.1 Activation functions . . . . . . . . . . . . . . . . . . . . . 158"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 56, "text": "5.4.2 Fixed basis functions . . . . . . . . . . . . . . . . . . . . . 158\n5.4.3 Logistic regression . . . . . . . . . . . . . . . . . . . . . . 159\n5.4.4 Multi-class logistic regression . . . . . . . . . . . . . . . . 161\n5.4.5 Probit regression . . . . . . . . . . . . . . . . . . . . . . . 163\n5.4.6 Canonical link functions . . . . . . . . . . . . . . . . . . . 164\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\nxiv CONTENTS\n6 Deep Neural Networks 171"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 57, "text": "xiv CONTENTS\n6 Deep Neural Networks 171\n6.1 Limitations of Fixed Basis Functions . . . . . . . . . . . . . . . . 172\n6.1.1 The curse of dimensionality . . . . . . . . . . . . . . . . . 172\n6.1.2 High-dimensional spaces . . . . . . . . . . . . . . . . . . . 175\n6.1.3 Data manifolds . . . . . . . . . . . . . . . . . . . . . . . . 176\n6.1.4 Data-dependent basis functions . . . . . . . . . . . . . . . 178\n6.2 Multilayer Networks . . . . . . . . . . . . . . . . . . . . . . . . . 180"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 58, "text": "6.2.1 Parameter matrices . . . . . . . . . . . . . . . . . . . . . . 181\n6.2.2 Universal approximation . . . . . . . . . . . . . . . . . . . 181\n6.2.3 Hidden unit activation functions . . . . . . . . . . . . . . . 182\n6.2.4 Weight-space symmetries . . . . . . . . . . . . . . . . . . 185\n6.3 Deep Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n6.3.1 Hierarchical representations . . . . . . . . . . . . . . . . . 187"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 59, "text": "6.3.2 Distributed representations . . . . . . . . . . . . . . . . . . 187\n6.3.3 Representation learning . . . . . . . . . . . . . . . . . . . 188\n6.3.4 Transfer learning . . . . . . . . . . . . . . . . . . . . . . . 189\n6.3.5 Contrastive learning . . . . . . . . . . . . . . . . . . . . . 191\n6.3.6 General network architectures . . . . . . . . . . . . . . . . 193\n6.3.7 Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 60, "text": "6.4 Error Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n6.4.1 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n6.4.2 Binary classiﬁcation . . . . . . . . . . . . . . . . . . . . . 196\n6.4.3 multiclass classiﬁcation . . . . . . . . . . . . . . . . . . . 197\n6.5 Mixture Density Networks . . . . . . . . . . . . . . . . . . . . . . 198\n6.5.1 Robot kinematics example . . . . . . . . . . . . . . . . . . 198"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 61, "text": "6.5.2 Conditional mixture distributions . . . . . . . . . . . . . . 199\n6.5.3 Gradient optimization . . . . . . . . . . . . . . . . . . . . 201\n6.5.4 Predictive distribution . . . . . . . . . . . . . . . . . . . . 202\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n7 Gradient Descent 209\n7.1 Error Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n7.1.1 Local quadratic approximation . . . . . . . . . . . . . . . . 211"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 62, "text": "7.2 Gradient Descent Optimization . . . . . . . . . . . . . . . . . . . 213\n7.2.1 Use of gradient information . . . . . . . . . . . . . . . . . 214\n7.2.2 Batch gradient descent . . . . . . . . . . . . . . . . . . . . 214\n7.2.3 Stochastic gradient descent . . . . . . . . . . . . . . . . . . 214\n7.2.4 Mini-batches . . . . . . . . . . . . . . . . . . . . . . . . . 216\n7.2.5 Parameter initialization . . . . . . . . . . . . . . . . . . . . 216"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 63, "text": "7.3 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n7.3.1 Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n7.3.2 Learning rate schedule . . . . . . . . . . . . . . . . . . . . 222\n7.3.3 RMSProp and Adam . . . . . . . . . . . . . . . . . . . . . 223\n7.4 Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\n7.4.1 Data normalization . . . . . . . . . . . . . . . . . . . . . . 226\nCONTENTS xv"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 64, "text": "CONTENTS xv\n7.4.2 Batch normalization . . . . . . . . . . . . . . . . . . . . . 227\n7.4.3 Layer normalization . . . . . . . . . . . . . . . . . . . . . 229\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\n8 Backpropagation 233\n8.1 Evaluation of Gradients . . . . . . . . . . . . . . . . . . . . . . . 234\n8.1.1 Single-layer networks . . . . . . . . . . . . . . . . . . . . 234\n8.1.2 General feed-forward networks . . . . . . . . . . . . . . . 235"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 65, "text": "8.1.3 A simple example . . . . . . . . . . . . . . . . . . . . . . 238\n8.1.4 Numerical differentiation . . . . . . . . . . . . . . . . . . . 239\n8.1.5 The Jacobian matrix . . . . . . . . . . . . . . . . . . . . . 240\n8.1.6 The Hessian matrix . . . . . . . . . . . . . . . . . . . . . . 242\n8.2 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . 244\n8.2.1 Forward-mode automatic differentiation . . . . . . . . . . . 246"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 66, "text": "8.2.2 Reverse-mode automatic differentiation . . . . . . . . . . . 249\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n9 Regularization 253\n9.1 Inductive Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n9.1.1 Inverse problems . . . . . . . . . . . . . . . . . . . . . . . 254\n9.1.2 No free lunch theorem . . . . . . . . . . . . . . . . . . . . 255\n9.1.3 Symmetry and invariance . . . . . . . . . . . . . . . . . . . 256"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 67, "text": "9.1.4 Equivariance . . . . . . . . . . . . . . . . . . . . . . . . . 259\n9.2 Weight Decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n9.2.1 Consistent regularizers . . . . . . . . . . . . . . . . . . . . 262\n9.2.2 Generalized weight decay . . . . . . . . . . . . . . . . . . 264\n9.3 Learning Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n9.3.1 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . 266"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 68, "text": "9.3.2 Double descent . . . . . . . . . . . . . . . . . . . . . . . . 268\n9.4 Parameter Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n9.4.1 Soft weight sharing . . . . . . . . . . . . . . . . . . . . . . 271\n9.5 Residual Connections . . . . . . . . . . . . . . . . . . . . . . . . 274\n9.6 Model Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n9.6.1 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 69, "text": "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n10 Convolutional Networks 287\n10.1 Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n10.1.1 Image data . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n10.2 Convolutional Filters . . . . . . . . . . . . . . . . . . . . . . . . . 290\n10.2.1 Feature detectors . . . . . . . . . . . . . . . . . . . . . . . 290\n10.2.2 Translation equivariance . . . . . . . . . . . . . . . . . . . 291"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 70, "text": "10.2.3 Padding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n10.2.4 Strided convolutions . . . . . . . . . . . . . . . . . . . . . 294\n10.2.5 Multi-dimensional convolutions . . . . . . . . . . . . . . . 295\n10.2.6 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\nxvi CONTENTS\n10.2.7 Multilayer convolutions . . . . . . . . . . . . . . . . . . . 298\n10.2.8 Example network architectures . . . . . . . . . . . . . . . . 299"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 71, "text": "10.3 Visualizing Trained CNNs . . . . . . . . . . . . . . . . . . . . . . 302\n10.3.1 Visual cortex . . . . . . . . . . . . . . . . . . . . . . . . . 302\n10.3.2 Visualizing trained ﬁlters . . . . . . . . . . . . . . . . . . . 303\n10.3.3 Saliency maps . . . . . . . . . . . . . . . . . . . . . . . . 305\n10.3.4 Adversarial attacks . . . . . . . . . . . . . . . . . . . . . . 306\n10.3.5 Synthetic images . . . . . . . . . . . . . . . . . . . . . . . 308"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 72, "text": "10.4 Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\n10.4.1 Bounding boxes . . . . . . . . . . . . . . . . . . . . . . . 309\n10.4.2 Intersection-over-union . . . . . . . . . . . . . . . . . . . . 310\n10.4.3 Sliding windows . . . . . . . . . . . . . . . . . . . . . . . 311\n10.4.4 Detection across scales . . . . . . . . . . . . . . . . . . . . 313\n10.4.5 Non-max suppression . . . . . . . . . . . . . . . . . . . . . 314"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 73, "text": "10.4.6 Fast region CNNs . . . . . . . . . . . . . . . . . . . . . . . 314\n10.5 Image Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 315\n10.5.1 Convolutional segmentation . . . . . . . . . . . . . . . . . 315\n10.5.2 Up-sampling . . . . . . . . . . . . . . . . . . . . . . . . . 316\n10.5.3 Fully convolutional networks . . . . . . . . . . . . . . . . . 318\n10.5.4 The U-net architecture . . . . . . . . . . . . . . . . . . . . 319"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 74, "text": "10.6 Style Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\n11 Structured Distributions 325\n11.1 Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\n11.1.1 Directed graphs . . . . . . . . . . . . . . . . . . . . . . . . 326\n11.1.2 Factorization . . . . . . . . . . . . . . . . . . . . . . . . . 327"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 75, "text": "11.1.3 Discrete variables . . . . . . . . . . . . . . . . . . . . . . . 329\n11.1.4 Gaussian variables . . . . . . . . . . . . . . . . . . . . . . 332\n11.1.5 Binary classiﬁer . . . . . . . . . . . . . . . . . . . . . . . 334\n11.1.6 Parameters and observations . . . . . . . . . . . . . . . . . 334\n11.1.7 Bayes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . 336\n11.2 Conditional Independence . . . . . . . . . . . . . . . . . . . . . . 337"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 76, "text": "11.2.1 Three example graphs . . . . . . . . . . . . . . . . . . . . 338\n11.2.2 Explaining away . . . . . . . . . . . . . . . . . . . . . . . 341\n11.2.3 D-separation . . . . . . . . . . . . . . . . . . . . . . . . . 343\n11.2.4 Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . 344\n11.2.5 Generative models . . . . . . . . . . . . . . . . . . . . . . 346\n11.2.6 Markov blanket . . . . . . . . . . . . . . . . . . . . . . . . 347"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 77, "text": "11.2.7 Graphs as ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . 348\n11.3 Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\n11.3.1 Hidden variables . . . . . . . . . . . . . . . . . . . . . . . 352\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nCONTENTS xvii\n12 Transformers 357\n12.1 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n12.1.1 Transformer processing . . . . . . . . . . . . . . . . . . . . 360"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 78, "text": "12.1.2 Attention coefﬁcients . . . . . . . . . . . . . . . . . . . . . 361\n12.1.3 Self-attention . . . . . . . . . . . . . . . . . . . . . . . . . 362\n12.1.4 Network parameters . . . . . . . . . . . . . . . . . . . . . 363\n12.1.5 Scaled self-attention . . . . . . . . . . . . . . . . . . . . . 366\n12.1.6 Multi-head attention . . . . . . . . . . . . . . . . . . . . . 366\n12.1.7 Transformer layers . . . . . . . . . . . . . . . . . . . . . . 368"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 79, "text": "12.1.8 Computational complexity . . . . . . . . . . . . . . . . . . 370\n12.1.9 Positional encoding . . . . . . . . . . . . . . . . . . . . . . 371\n12.2 Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\n12.2.1 Word embedding . . . . . . . . . . . . . . . . . . . . . . . 375\n12.2.2 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . 377\n12.2.3 Bag of words . . . . . . . . . . . . . . . . . . . . . . . . . 378"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 80, "text": "12.2.4 Autoregressive models . . . . . . . . . . . . . . . . . . . . 379\n12.2.5 Recurrent neural networks . . . . . . . . . . . . . . . . . . 380\n12.2.6 Backpropagation through time . . . . . . . . . . . . . . . . 381\n12.3 Transformer Language Models . . . . . . . . . . . . . . . . . . . . 382\n12.3.1 Decoder transformers . . . . . . . . . . . . . . . . . . . . . 383\n12.3.2 Sampling strategies . . . . . . . . . . . . . . . . . . . . . . 386"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 81, "text": "12.3.3 Encoder transformers . . . . . . . . . . . . . . . . . . . . . 388\n12.3.4 Sequence-to-sequence transformers . . . . . . . . . . . . . 390\n12.3.5 Large language models . . . . . . . . . . . . . . . . . . . . 390\n12.4 Multimodal Transformers . . . . . . . . . . . . . . . . . . . . . . 394\n12.4.1 Vision transformers . . . . . . . . . . . . . . . . . . . . . . 395\n12.4.2 Generative image transformers . . . . . . . . . . . . . . . . 396"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 82, "text": "12.4.3 Audio data . . . . . . . . . . . . . . . . . . . . . . . . . . 399\n12.4.4 Text-to-speech . . . . . . . . . . . . . . . . . . . . . . . . 400\n12.4.5 Vision and language transformers . . . . . . . . . . . . . . 402\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n13 Graph Neural Networks 407\n13.1 Machine Learning on Graphs . . . . . . . . . . . . . . . . . . . . 409\n13.1.1 Graph properties . . . . . . . . . . . . . . . . . . . . . . . 410"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 83, "text": "13.1.2 Adjacency matrix . . . . . . . . . . . . . . . . . . . . . . . 410\n13.1.3 Permutation equivariance . . . . . . . . . . . . . . . . . . . 411\n13.2 Neural Message-Passing . . . . . . . . . . . . . . . . . . . . . . . 412\n13.2.1 Convolutional ﬁlters . . . . . . . . . . . . . . . . . . . . . 413\n13.2.2 Graph convolutional networks . . . . . . . . . . . . . . . . 414\n13.2.3 Aggregation operators . . . . . . . . . . . . . . . . . . . . 416"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 84, "text": "13.2.4 Update operators . . . . . . . . . . . . . . . . . . . . . . . 418\n13.2.5 Node classiﬁcation . . . . . . . . . . . . . . . . . . . . . . 419\n13.2.6 Edge classiﬁcation . . . . . . . . . . . . . . . . . . . . . . 420\n13.2.7 Graph classiﬁcation . . . . . . . . . . . . . . . . . . . . . . 420\nxviii CONTENTS\n13.3 General Graph Networks . . . . . . . . . . . . . . . . . . . . . . . 420\n13.3.1 Graph attention networks . . . . . . . . . . . . . . . . . . . 421"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 85, "text": "13.3.2 Edge embeddings . . . . . . . . . . . . . . . . . . . . . . . 421\n13.3.3 Graph embeddings . . . . . . . . . . . . . . . . . . . . . . 422\n13.3.4 Over-smoothing . . . . . . . . . . . . . . . . . . . . . . . 422\n13.3.5 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 423\n13.3.6 Geometric deep learning . . . . . . . . . . . . . . . . . . . 424\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n14 Sampling 429"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 86, "text": "14 Sampling 429\n14.1 Basic Sampling Algorithms . . . . . . . . . . . . . . . . . . . . . 430\n14.1.1 Expectations . . . . . . . . . . . . . . . . . . . . . . . . . 430\n14.1.2 Standard distributions . . . . . . . . . . . . . . . . . . . . 431\n14.1.3 Rejection sampling . . . . . . . . . . . . . . . . . . . . . . 433\n14.1.4 Adaptive rejection sampling . . . . . . . . . . . . . . . . . 435\n14.1.5 Importance sampling . . . . . . . . . . . . . . . . . . . . . 437"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 87, "text": "14.1.6 Sampling-importance-resampling . . . . . . . . . . . . . . 439\n14.2 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . 440\n14.2.1 The Metropolis algorithm . . . . . . . . . . . . . . . . . . 441\n14.2.2 Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 442\n14.2.3 The Metropolis–Hastings algorithm . . . . . . . . . . . . . 445\n14.2.4 Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . . . 446"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 88, "text": "14.2.5 Ancestral sampling . . . . . . . . . . . . . . . . . . . . . . 450\n14.3 Langevin Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . 451\n14.3.1 Energy-based models . . . . . . . . . . . . . . . . . . . . . 452\n14.3.2 Maximizing the likelihood . . . . . . . . . . . . . . . . . . 453\n14.3.3 Langevin dynamics . . . . . . . . . . . . . . . . . . . . . . 454\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456\n15 Discrete Latent Variables 459"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 89, "text": "15 Discrete Latent Variables 459\n15.1 K-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 460\n15.1.1 Image segmentation . . . . . . . . . . . . . . . . . . . . . 464\n15.2 Mixtures of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . 466\n15.2.1 Likelihood function . . . . . . . . . . . . . . . . . . . . . . 468\n15.2.2 Maximum likelihood . . . . . . . . . . . . . . . . . . . . . 470\n15.3 Expectation–Maximization Algorithm . . . . . . . . . . . . . . . . 474"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 90, "text": "15.3.1 Gaussian mixtures . . . . . . . . . . . . . . . . . . . . . . 478\n15.3.2 Relation to K-means . . . . . . . . . . . . . . . . . . . . . 480\n15.3.3 Mixtures of Bernoulli distributions . . . . . . . . . . . . . . 481\n15.4 Evidence Lower Bound . . . . . . . . . . . . . . . . . . . . . . . 485\n15.4.1 EM revisited . . . . . . . . . . . . . . . . . . . . . . . . . 486\n15.4.2 Independent and identically distributed data . . . . . . . . . 488"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 91, "text": "15.4.3 Parameter priors . . . . . . . . . . . . . . . . . . . . . . . 489\n15.4.4 Generalized EM . . . . . . . . . . . . . . . . . . . . . . . 489\n15.4.5 Sequential EM . . . . . . . . . . . . . . . . . . . . . . . . 490\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490\nCONTENTS xix\n16 Continuous Latent Variables 495\n16.1 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . 497\n16.1.1 Maximum variance formulation . . . . . . . . . . . . . . . 497"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 92, "text": "16.1.2 Minimum-error formulation . . . . . . . . . . . . . . . . . 499\n16.1.3 Data compression . . . . . . . . . . . . . . . . . . . . . . . 501\n16.1.4 Data whitening . . . . . . . . . . . . . . . . . . . . . . . . 502\n16.1.5 High-dimensional data . . . . . . . . . . . . . . . . . . . . 504\n16.2 Probabilistic Latent Variables . . . . . . . . . . . . . . . . . . . . 506\n16.2.1 Generative model . . . . . . . . . . . . . . . . . . . . . . . 506"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 93, "text": "16.2.2 Likelihood function . . . . . . . . . . . . . . . . . . . . . . 507\n16.2.3 Maximum likelihood . . . . . . . . . . . . . . . . . . . . . 509\n16.2.4 Factor analysis . . . . . . . . . . . . . . . . . . . . . . . . 513\n16.2.5 Independent component analysis . . . . . . . . . . . . . . . 514\n16.2.6 Kalman ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . 515\n16.3 Evidence Lower Bound . . . . . . . . . . . . . . . . . . . . . . . 516"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 94, "text": "16.3.1 Expectation maximization . . . . . . . . . . . . . . . . . . 518\n16.3.2 EM for PCA . . . . . . . . . . . . . . . . . . . . . . . . . 519\n16.3.3 EM for factor analysis . . . . . . . . . . . . . . . . . . . . 520\n16.4 Nonlinear Latent Variable Models . . . . . . . . . . . . . . . . . . 522\n16.4.1 Nonlinear manifolds . . . . . . . . . . . . . . . . . . . . . 522\n16.4.2 Likelihood function . . . . . . . . . . . . . . . . . . . . . . 524"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 95, "text": "16.4.3 Discrete data . . . . . . . . . . . . . . . . . . . . . . . . . 526\n16.4.4 Four approaches to generative modelling . . . . . . . . . . 527\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527\n17 Generative Adversarial Networks 533\n17.1 Adversarial Training . . . . . . . . . . . . . . . . . . . . . . . . . 534\n17.1.1 Loss function . . . . . . . . . . . . . . . . . . . . . . . . . 535\n17.1.2 GAN training in practice . . . . . . . . . . . . . . . . . . . 536"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 96, "text": "17.2 Image GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539\n17.2.1 CycleGAN . . . . . . . . . . . . . . . . . . . . . . . . . . 539\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544\n18 Normalizing Flows 547\n18.1 Coupling Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n18.2 Autoregressive Flows . . . . . . . . . . . . . . . . . . . . . . . . . 552\n18.3 Continuous Flows . . . . . . . . . . . . . . . . . . . . . . . . . . 554"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 97, "text": "18.3.1 Neural differential equations . . . . . . . . . . . . . . . . . 554\n18.3.2 Neural ODE backpropagation . . . . . . . . . . . . . . . . 555\n18.3.3 Neural ODE ﬂows . . . . . . . . . . . . . . . . . . . . . . 557\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559\nxx CONTENTS\n19 Autoencoders 563\n19.1 Deterministic Autoencoders . . . . . . . . . . . . . . . . . . . . . 564\n19.1.1 Linear autoencoders . . . . . . . . . . . . . . . . . . . . . 564"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 98, "text": "19.1.2 Deep autoencoders . . . . . . . . . . . . . . . . . . . . . . 565\n19.1.3 Sparse autoencoders . . . . . . . . . . . . . . . . . . . . . 566\n19.1.4 Denoising autoencoders . . . . . . . . . . . . . . . . . . . 567\n19.1.5 Masked autoencoders . . . . . . . . . . . . . . . . . . . . . 567\n19.2 Variational Autoencoders . . . . . . . . . . . . . . . . . . . . . . . 569\n19.2.1 Amortized inference . . . . . . . . . . . . . . . . . . . . . 572"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 99, "text": "19.2.2 The reparameterization trick . . . . . . . . . . . . . . . . . 574\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578\n20 Diffusion Models 581\n20.1 Forward Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . 582\n20.1.1 Diffusion kernel . . . . . . . . . . . . . . . . . . . . . . . 583\n20.1.2 Conditional distribution . . . . . . . . . . . . . . . . . . . 584\n20.2 Reverse Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . 585"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 100, "text": "20.2.1 Training the decoder . . . . . . . . . . . . . . . . . . . . . 587\n20.2.2 Evidence lower bound . . . . . . . . . . . . . . . . . . . . 588\n20.2.3 Rewriting the ELBO . . . . . . . . . . . . . . . . . . . . . 589\n20.2.4 Predicting the noise . . . . . . . . . . . . . . . . . . . . . . 591\n20.2.5 Generating new samples . . . . . . . . . . . . . . . . . . . 592\n20.3 Score Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 101, "text": "20.3.1 Score loss function . . . . . . . . . . . . . . . . . . . . . . 595\n20.3.2 Modiﬁed score loss . . . . . . . . . . . . . . . . . . . . . . 596\n20.3.3 Noise variance . . . . . . . . . . . . . . . . . . . . . . . . 597\n20.3.4 Stochastic differential equations . . . . . . . . . . . . . . . 598\n20.4 Guided Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 599\n20.4.1 Classiﬁer guidance . . . . . . . . . . . . . . . . . . . . . . 600"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 102, "text": "20.4.2 Classiﬁer-free guidance . . . . . . . . . . . . . . . . . . . 600\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\nAppendix A Linear Algebra 609\nA.1 Matrix Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . 609\nA.2 Traces and Determinants . . . . . . . . . . . . . . . . . . . . . . . 610\nA.3 Matrix Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 611"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 103, "text": "A.4 Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\nAppendix B Calculus of Variations 617\nAppendix C Lagrange Multipliers 621\nBibliography 625\nIndex 641\n1\nThe Deep\nLearning\nRevolution\nMachine learning today is one of the most important, and fastest growing, ﬁelds\nof technology. Applications of machine learning are becoming ubiquitous, and so-\nlutions learned from data are increasingly displacing traditional hand-crafted algo-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 104, "text": "rithms. This has not only led to improved performance for existing technologies but\nhas opened the door to a vast range of new capabilities that would be inconceivable\nif new algorithms had to be designed explicitly by hand.\nOne particular branch of machine learning, known asdeep learning, has emerged\nas an exceptionally powerful and general-purpose framework for learning from data.\nDeep learning is based on computational models called neural networks which were"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 105, "text": "originally inspired by mechanisms of learning and information processing in the hu-\nman brain. The ﬁeld of artiﬁcial intelligence, or AI, seeks to recreate the powerful\ncapabilities of the brain in machines, and today the terms machine learning and AI\nare often used interchangeably. Many of the AI systems in current use represent ap-\n1© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 106, "text": "C. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 107, "text": "2 1. THE DEEP LEARNING REVOLUTION\nplications of machine learning which are designed to solve very speciﬁc and focused\nproblems, and while these are extremely useful they fall far short of the tremendous\nbreadth of capabilities of the human brain. This has led to the introduction of the\nterm artiﬁcial general intelligence, or AGI, to describe the aspiration of building\nmachines with this much greater ﬂexibility. After many decades of steady progress,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 108, "text": "machine learning has now entered a phase of very rapid development. Recently,\nmassive deep learning systems called large language models have started to exhibitChapter 12\nremarkable capabilities that have been described as the ﬁrst indications of artiﬁcial\ngeneral intelligence (Bubeck et al., 2023).\n1.1.\nThe Impact of Deep Learning\nWe begin our discussion of machine learning by considering four examples drawn\nfrom diverse ﬁelds to illustrate the huge breadth of applicability of this technology"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 109, "text": "and to introduce some basic concepts and terminology. What is particularly remark-\nable about these and many other examples is that they have all been addressed using\nvariants of the same fundamental framework of deep learning. This is in sharp con-\ntrast to conventional approaches in which different applications are tackled using\nwidely differing and specialist techniques. It should be emphasized that the exam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 110, "text": "ples we have chosen represent only a tiny fraction of the breadth of applicability for\ndeep neural networks and that almost every domain where computation has a role is\namenable to the transformational impact of deep learning.\n1.1.1 Medical diagnosis\nConsider ﬁrst the application of machine learning to the problem of diagnosing\nskin cancer. Melanoma is the most dangerous kind of skin cancer but is curable\nif detected early. Figure 1.1 shows example images of skin lesions, with malig-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 111, "text": "nant melanomas on the top row and benign nevi on the bottom row. Distinguishing\nbetween these two classes of image is clearly very challenging, and it would be vir-\ntually impossible to write an algorithm by hand that could successfully classify such\nimages with any reasonable level of accuracy.\nThis problem has been successfully addressed using deep learning (Esteva et\nal., 2017). The solution was created using a large set of lesion images, known as\nFigure 1.1 Examples of skin lesions cor-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 112, "text": "Figure 1.1 Examples of skin lesions cor-\nresponding to dangerous ma-\nlignant melanomas on the top\nrow and benign nevi on the bot-\ntom row. It is difﬁcult for the\nuntrained eye to distinguish be-\ntween these two classes."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 113, "text": "1.1. The Impact of Deep Learning 3\na training set, each of which is labelled as either malignant or benign, where the\nlabels are obtained from a biopsy test that is considered to provide the true class\nof the lesion. The training set is used to determine the values of some 25 million\nadjustable parameters, known as weights, in a deep neural network. This process of\nsetting the parameter values from data is known as learning or training. The goal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 114, "text": "is for the trained network to predict the correct label for a new lesion just from the\nimage alone without needing the time-consuming step of taking a biopsy. This is an\nexample of a supervised learning problem because, for each training example, the\nnetwork is told the correct label. It is also an example of a classiﬁcation problem\nbecause each input must be assigned to a discrete set of classes (benign or malignant"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 115, "text": "in this case). Applications in which the output consists of one or more continuous\nvariables are called regressionproblems. An example of a regression problem would\nbe the prediction of the yield in a chemical manufacturing process in which the inputs\nconsist of the temperature, the pressure, and the concentrations of reactants.\nAn interesting aspect of this application is that the number of labelled training\nimages available, roughly 129,000, is considered relatively small, and so the deep"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 116, "text": "neural network was ﬁrst trained on a much larger data set of 1.28 million images of\neveryday objects (such as dogs, buildings, and mushrooms) and then ﬁne-tuned on\nthe data set of lesion images. This is an example of transfer learning in which the\nnetwork learns the general properties of natural images from the large data set of\neveryday objects and is then specialized to the speciﬁc problem of lesion classiﬁca-\ntion. Through the use of deep learning, the classiﬁcation of skin lesion images has"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 117, "text": "reached a level of accuracy that exceeds that of professional dermatologists (Brinker\net al., 2019).\n1.1.2 Protein structure\nProteins are sometimes called the building blocks of living organisms. They are\nbiological molecules that consist of one or more long chains of units called amino\nacids, of which there are 22 different types, and the protein is speciﬁed by the se-\nquence of amino acids. Once a protein has been synthesized inside a living cell, it"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 118, "text": "folds into a complex three-dimensional structure whose behaviour and interactions\nare strongly determined by its shape. Calculating this 3D structure, given the amino\nacid sequence, has been a fundamental open problem in biology for half a century\nthat had seen relatively little progress until the advent of deep learning.\nThe 3D structure can be measured experimentally using techniques such as X-\nray crystallography, cryogenic electron microscopy, or nuclear magnetic resonance"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 119, "text": "spectroscopy. However, this can be extremely time-consuming and for some pro-\nteins can prove to be challenging, for example due to the difﬁculty of obtaining a\npure sample or because the structure is dependent on the context. In contrast, the\namino acid sequence of a protein can be determined experimentally at lower cost\nand higher throughput. Consequently, there is considerable interest in being able\nto predict the 3D structures of proteins directly from their amino acid sequences in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 120, "text": "order to better understand biological processes or for practical applications such as\ndrug discovery. A deep learning model can be trained to take an amino acid se-\nquence as input and generate the 3D structure as output, in which the training data\n4 1. THE DEEP LEARNING REVOLUTION\nFigure 1.2 Illustration of the 3D shape of a pro-\ntein called T1044/6VR4. The green\nstructure shows the ground truth\nas determined by X-ray crystallog-\nraphy, whereas the superimposed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 121, "text": "raphy, whereas the superimposed\nblue structure shows the prediction\nobtained by a deep learning model\ncalled AlphaFold. [From Jumper et\nal. (2021) with permission.]\nconsist of a set of proteins for which the amino acid sequence and the 3D structure\nare both known. Protein structure prediction is therefore another example of super-\nvised learning. Once the system is trained it can take a new amino acid sequence as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 122, "text": "input and can predict the associated 3D structure (Jumper et al., 2021). Figure 1.2\ncompares the predicted 3D structure of a protein and the ground truth obtained by\nX-ray crystallography.\n1.1.3 Image synthesis\nIn the two applications discussed so far, a neural network learned to transform\nan input (a skin image or an amino acid sequence) into an output (a lesion classiﬁca-\ntion or a 3D protein structure, respectively). We turn now to an example where the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 123, "text": "training data consist simply of a set of sample images and the goal of the trained net-\nwork is to create new images of the same kind. This is an example of unsupervised\nlearning because the images are unlabelled, in contrast to the lesion classiﬁcation\nand protein structure examples. Figure 1.3 shows examples of synthetic images gen-\nerated by a deep neural network trained on a set of images of human faces taken in a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 124, "text": "studio against a plain background. Such synthetic images are of exceptionally high\nquality and it can be difﬁcult tell them apart from photographs of real people.\nThis is an example of a generative model because it can generate new output\nexamples that differ from those used to train the model but which share the same\nstatistical properties. A variant of this approach allows images to be generated that\ndepend on an input text string known, as a prompt, so that the image content reﬂects"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 125, "text": "the semantics of the text input. The termgenerative AI is used to describe deep learn-Chapter 10\ning models that generate outputs in the form of images, video, audio, text, candidate\ndrug molecules, or other modalities.\n1.1. The Impact of Deep Learning 5\nFigure 1.3 Synthetic face images generated by a deep neural network trained using unsupervised learning.\n[From https://generated.photos.]\n1.1.4 Large language models\nOne of most important advances in machine learning in recent years has been"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 126, "text": "the development of powerful models for processing natural language and other forms\nof sequential data such as source code. A large language model, or LLM, uses deep\nlearning to build rich internal representations that capture the semantic properties\nof language. An important class of large language models, called autoregressive\nlanguage models, can generate language as output, and therefore, they are a form of\ngenerative AI. Such models take a sequence of words as the input and for the output,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 127, "text": "generate a single word that represents the next word in the sequence. The augmented\nsequence, with the new word appended at the end, can then be fed through the model\nagain to generate the subsequent word, and this process can be repeated to generate\na long sequence of words. Such models can also output a special ‘stop’ word that\nsignals the end of text generation, thereby allowing them to output text of ﬁnite\nlength and then halt. At that point, a user could append their own series of words"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 128, "text": "to the sequence before feeding the complete sequence back through the model to\ntrigger further word generation. In this way, it is possible for a human to have a\nconversation with the neural network.\nSuch models can be trained on large data sets of text by extracting training pairs\neach consisting of a randomly selected sequence of words as input with the known\nnext word as the target output. This is an example of self-supervised learning in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 129, "text": "which a function from inputs to outputs is learned but where the labelled outputs are\nobtained automatically from the input training data without needing separate human-\n6 1. THE DEEP LEARNING REVOLUTION\nFigure 1.4 Plot of a training data set of N =\n10 points, shown as blue circles,\neach comprising an observation\nof the input variable x along with\nthe corresponding target variable\nt. The green curve shows the\nfunction sin(2\u0019x) used to gener-\nate the data. Our goal is to pre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 130, "text": "ate the data. Our goal is to pre-\ndict the value of t for some new\nvalue of x, without knowledge of\nthe green curve.\n0 1x\n−1\n1\nt\nderived labels. Since large volumes of text are available from multiple sources, this\napproach allows for scaling to very large training sets and associated very large neu-\nral networks.\nLarge language models can exhibit extraordinary capabilities that have been de-\nscribed as the ﬁrst indications of emerging artiﬁcial general intelligence (Bubeck et"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 131, "text": "al., 2023), and we discuss such models at length later in the book. On the next page,Chapter 12\nwe give an illustration of language generation, based on a model called GPT-4 (Ope-\nnAI, 2023), in response to an input prompt ‘Write a proof of the fact that there are\ninﬁnitely many primes; do it in the style of a Shakespeare play through a dialogue\nbetween two parties arguing over the proof.’.\n1.2.\nA Tutorial Example\nFor the newcomer to the ﬁeld of machine learning, many of the basic concepts and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 132, "text": "much of the terminology can be introduced in the context of a simple example in-\nvolving the ﬁtting of a polynomial to a small synthetic data set (Bishop, 2006). This\nis a form of supervised learning problem in which we would like to make a prediction\nfor a target variable, given the value of an input variable.\n1.2.1 Synthetic data\nWe denote the input variable by xand the target variable by t, and we assume\nthat both variables take continuous values on the real axis. Suppose that we are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 133, "text": "given a training set comprising N observations of x, written x1;:::;x N, together\nwith corresponding observations of the values of t, denoted t1;:::;t N. Our goal\nis to predict the value of tfor some new value of x. The ability to make accurate\npredictions on previously unseen inputs is a key goal in machine learning and is\nknown as generalization.\nWe can illustrate this using a synthetic data set generated by sampling from a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 134, "text": "sinusoidal function. Figure 1.4 shows a plot of a training set comprisingN = 10 data\npoints in which the input values were generated by choosing values of xn, for n =\n1;:::;N , spaced uniformly in the range[0;1]. The associated target data values were\nobtained by ﬁrst computing the values of the function sin(2\u0019x) for each value of x\n1.2. A Tutorial Example 7\nGPT -4: Proof of inﬁnitely many primes\nFIRST PARTY:\nThou dost not understand, my friend,\nThere simply is no upper end"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 135, "text": "There simply is no upper end\nTo primes, those numbers indivisible,\nNo matter if it seems impossible.\nSECOND PARTY:\nBut how dost thou so surely know?\nTo prove this truth, what canst thou show?\nFIRST PARTY:\nConsider this, my doubtful peer,\nA clever proof that will make clear:\nAssume that there’s a final prime,\nThe largest one we’ll see in time.\nLet’s call this number \"P\" for short,\nAnd then, my friend, we will purport\nTo build a number \"Q\" that’s new,\nOne larger than \"P\" and prime too."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 136, "text": "One larger than \"P\" and prime too.\nSECOND PARTY:\nBut how, my friend, will this be done?\nThe proof is not yet fairly won.\nFIRST PARTY:\nIt’s simple, friend, just wait and see,\nWe’ll multiply the primes, that’s key.\nAll primes from two to \"P\" combined,\nWill make this new \"Q\" we will find.\nBut one thing more we must include,\nTo make the proof more watertight,\nWe’ll add a one onto the end,\nAnd now the proof is in our sight.\nSECOND PARTY:\nI see it now, it’s crystal clear,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 137, "text": "SECOND PARTY:\nI see it now, it’s crystal clear,\nFor \"Q\" must be a prime, my dear.\nIt can’t divide by any prime,\nAll smaller than itself in time.\nAnd since we chose \"P\" as the last,\nThis proves that our assumption’s past.\nThere is no final prime, we see,\nThe numbers go on endlessly.\n8 1. THE DEEP LEARNING REVOLUTION\nand then adding a small level of random noise (governed by a Gaussian distribution)Section 2.3\nto each such point to obtain the corresponding target value tn. By generating data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 138, "text": "in this way, we are capturing an important property of many real-world data sets,\nnamely that they possess an underlying regularity, which we wish to learn, but that\nindividual observations are corrupted by random noise. This noise might arise from\nintrinsically stochastic (i.e., random) processes such as radioactive decay but more\ntypically is due to there being sources of variability that are themselves unobserved."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 139, "text": "In this tutorial example we know the true process that generated the data, namely\nthe sinusoidal function. In a practical application of machine learning, our goal is to\ndiscover the underlying trends in the data given the ﬁnite training set. Knowing the\nprocess that generated the data, however, allows us to illustrate important concepts\nin machine learning.\n1.2.2 Linear models\nOur goal is to exploit this training set to predict the valueˆtof the target variable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 140, "text": "for some new value ˆxof the input variable. As we will see later, this involves im-\nplicitly trying to discover the underlying function sin(2\u0019x). This is intrinsically a\ndifﬁcult problem as we have to generalize from a ﬁnite data set to an entire function.\nFurthermore, the observed data is corrupted with noise, and so for a given ˆxthere\nis uncertainty as to the appropriate value for ˆt. Probability theory provides a frame-Chapter 2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 141, "text": "work for expressing such uncertainty in a precise and quantitative manner, whereas\ndecision theory allows us to exploit this probabilistic representation to make predic-Chapter 5\ntions that are optimal according to appropriate criteria. Learning probabilities from\ndata lies at the heart of machine learning and will be explored in great detail in this\nbook.\nTo start with, however, we will proceed rather informally and consider a simple"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 142, "text": "approach based on curve ﬁtting. In particular, we will ﬁt the data using a polynomial\nfunction of the form\ny(x;w) = w0 + w1x+ w2x2 + ::: + wMxM =\nM∑\nj=0\nwjxj (1.1)\nwhere M is the order of the polynomial, and xj denotes xraised to the power of j.\nThe polynomial coefﬁcients w0;:::;w M are collectively denoted by the vector w.\nNote that, although the polynomial function y(x;w) is a nonlinear function of x, it\nis a linear function of the coefﬁcientsw. Functions, such as this polynomial, that are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 143, "text": "linear in the unknown parameters have important properties, as well as signiﬁcant\nlimitations, and are called linear models.Chapter 4\n1.2.3 Error function\nThe values of the coefﬁcients will be determined by ﬁtting the polynomial to the\ntraining data. This can be done by minimizing an error function that measures the\nmisﬁt between the function y(x;w), for any given value of w, and the training set\ndata points. One simple choice of error function, which is widely used, is the sum of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 144, "text": "1.2. A Tutorial Example 9\nFigure 1.5 The error function (1.2) cor-\nresponds to (one half of)\nthe sum of the squares of\nthe displacements (shown\nby the vertical green arrows)\nof each data point from the\nfunction y(x;w).\nxn\ntn\ny(xn, w)\nx\nt\nthe squares of the differences between the predictions y(xn;w) for each data point\nxn and the corresponding target value tn, given by\nE(w) = 1\n2\nN∑\nn=1\n{y(xn;w) −tn}2 (1.2)\nwhere the factor of 1=2 is included for later convenience. We will later derive this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 145, "text": "error function starting from probability theory. Here we simply note that it is a non-Section 2.3.4\nnegative quantity that would be zero if, and only if, the function y(x;w) were to\npass exactly through each training data point. The geometrical interpretation of the\nsum-of-squares error function is illustrated in Figure 1.5.\nWe can solve the curve ﬁtting problem by choosing the value of w for which\nE(w) is as small as possible. Because the error function is a quadratic function of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 146, "text": "the coefﬁcients w, its derivatives with respect to the coefﬁcients will be linear in the\nelements of w, and so the minimization of the error function has a unique solution,\ndenoted by w?, which can be found in closed form. The resulting polynomial isExercise 4.1\ngiven by the function y(x;w?).\n1.2.4 Model complexity\nThere remains the problem of choosing the order M of the polynomial, and as\nwe will see this will turn out to be an example of an important concept called model"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 147, "text": "comparison or model selection. In Figure 1.6, we show four examples of the results\nof ﬁtting polynomials having orders M = 0;1;3, and 9 to the data set shown in\nFigure 1.4.\nNotice that the constant (M= 0) and ﬁrst-order (M= 1) polynomials give poor\nﬁts to the data and consequently poor representations of the function sin(2\u0019x). The\nthird-order (M = 3) polynomial seems to give the best ﬁt to the functionsin(2\u0019x) of\nthe examples shown in Figure 1.6. When we go to a much higher order polynomial"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 148, "text": "(M = 9), we obtain an excellent ﬁt to the training data. In fact, the polynomial\n10 1. THE DEEP LEARNING REVOLUTION\n0 1x\n−1\n1\nt\nM = 0\n0 1x\n−1\n1\nt\nM = 1\n0 1x\n−1\n1\nt\nM = 3\n0 1x\n−1\n1\nt\nM = 9\nFigure 1.6 Plots of polynomials having various orders M, shown as red curves, ﬁtted to the data set shown in\nFigure 1.4 by minimizing the error function (1.2).\npasses exactly through each data point and E(w?) = 0 . However, the ﬁtted curve"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 149, "text": "oscillates wildly and gives a very poor representation of the functionsin(2\u0019x). This\nlatter behaviour is known as over-ﬁtting.\nOur goal is to achieve good generalization by making accurate predictions for\nnew data. We can obtain some quantitative insight into the dependence of the gener-\nalization performance on M by considering a separate set of data known as atest set,\ncomprising 100 data points generated using the same procedure as used to generate"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 150, "text": "the training set points. For each value of M, we can evaluate the residual value of\nE(w?) given by (1.2) for the training data, and we can also evaluate E(w?) for the\ntest data set. Instead of evaluating the error function E(w), it is sometimes more\nconvenient to use the root-mean-square (RMS) error deﬁned by\nERMS =\n√ 1\nN\nN∑\nn=1\n{y(xn;w) −tn}2 (1.3)\nin which the division by N allows us to compare different sizes of data sets on an"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 151, "text": "equal footing, and the square root ensures that ERMS is measured on the same scale\n(and in the same units) as the target variablet. Graphs of the training-set and test-set\n1.2. A Tutorial Example 11\nFigure 1.7 Graphs of the root-mean-\nsquare error, deﬁned by (1.3), evaluated on\nthe training set, and on an independent test\nset, for various values of M.\n0 3 6 9\nM\n0\n1\nERMS\nTraining\nTest\nRMS errors are shown, for various values of M, in Figure 1.7. The test set error"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 152, "text": "is a measure of how well we are doing in predicting the values of t for new data\nobservations of x. Note from Figure 1.7 that small values of M give relatively large\nvalues of the test set error, and this can be attributed to the fact that the corresponding\npolynomials are rather inﬂexible and are incapable of capturing the oscillations in\nthe function sin(2\u0019x). Values of M in the range 3 6 M 6 8 give small values"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 153, "text": "for the test set error, and these also give reasonable representations of the generating\nfunction sin(2\u0019x), as can be seen for M = 3 in Figure 1.6.\nFor M = 9, the training set error goes to zero, as we might expect because\nthis polynomial contains 10 degrees of freedom corresponding to the 10 coefﬁcients\nw0;:::;w 9, and so can be tuned exactly to the 10 data points in the training set.\nHowever, the test set error has become very large and, as we saw in Figure 1.6, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 154, "text": "corresponding function y(x;w?) exhibits wild oscillations.\nThis may seem paradoxical because a polynomial of a given order contains all\nlower-order polynomials as special cases. The M = 9 polynomial is therefore ca-\npable of generating results at least as good as the M = 3 polynomial. Furthermore,\nwe might suppose that the best predictor of new data would be the functionsin(2\u0019x)\nfrom which the data was generated (and we will see later that this is indeed the case)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 155, "text": "We know that a power series expansion of the functionsin(2\u0019x) contains terms of all\norders, so we might expect that results should improve monotonically as we increase\nM.\nWe can gain some insight into the problem by examining the values of the co-\nefﬁcients w? obtained from polynomials of various orders, as shown in Table 1.1.\nWe see that, as M increases, the magnitude of the coefﬁcients typically gets larger.\nIn particular for the M = 9 polynomial, the coefﬁcients have become ﬁnely tuned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 156, "text": "to the data. They have large positive and negative values so that the corresponding\npolynomial function matches each of the data points exactly, but between data points\n(particularly near the ends of the range) the function exhibits the large oscillations\nobserved in Figure 1.6. Intuitively, what is happening is that the more ﬂexible poly-\nnomials with larger values of M are increasingly tuned to the random noise on the\ntarget values."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 157, "text": "target values.\nFurther insight into this phenomenon can be gained by examining the behaviour\nof the learned model as the size of the data set is varied, as shown in Figure 1.8. We\nsee that, for a given model complexity, the over-ﬁtting problem become less severe\n12 1. THE DEEP LEARNING REVOLUTION\n0 1x\n−1\n1\nt\nN = 15\n0 1x\n−1\n1\nt\nN = 100\nFigure 1.8 Plots of the solutions obtained by minimizing the sum-of-squares error function (1.2) using the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 158, "text": "M = 9 polynomial for N = 15 data points (left plot) and N = 100 data points (right plot). We see that increasing\nthe size of the data set reduces the over-ﬁtting problem.\nas the size of the data set increases. Another way to say this is that with a larger\ndata set, we can afford to ﬁt a more complex (in other words more ﬂexible) model\nto the data. One rough heuristic that is sometimes advocated in classical statistics"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 159, "text": "is that the number of data points should be no less than some multiple (say 5 or\n10) of the number of learnable parameters in the model. However, when we discuss\ndeep learning later in this book, we will see that excellent results can be obtained\nusing models that have signiﬁcantly more parameters than the number of training\ndata points.Section 9.3.2\n1.2.5 Regularization\nThere is something rather unsatisfying about having to limit the number of pa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 160, "text": "rameters in a model according to the size of the available training set. It would seem\nmore reasonable to choose the complexity of the model according to the complexity\nof the problem being solved. One technique that is often used to control the over-\nﬁtting phenomenon, as an alternative to limiting the number of parameters, is that\nof regularization, which involves adding a penalty term to the error function (1.2) to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 161, "text": "discourage the coefﬁcients from having large magnitudes. The simplest such penalty\nTable 1.1 Table of the coefﬁcients w?\nfor polynomials of various or-\nder. Observe how the typ-\nical magnitude of the coefﬁ-\ncients increases dramatically\nas the order of the polynomial\nincreases.\nM = 0 M = 1 M = 3 M = 9\nw?\n0 0:11 0:90 0:12 0:26\nw?\n1 −1:58 11:20 −66:13\nw?\n2 −33:67 1; 665:69\nw?\n3 22:43 −15;566:61\nw?\n4 76;321:23\nw?\n5 −217;389:15\nw?\n6 370;626:48\nw?\n7 −372;051:47\nw?\n8 202;540:70\nw?\n9 −46;080:94"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 162, "text": "w?\n7 −372;051:47\nw?\n8 202;540:70\nw?\n9 −46;080:94\n1.2. A Tutorial Example 13\n0 1x\n−1\n1\nt\nln λ = −18\n0 1x\n−1\n1\nt\nln λ = 0\nFigure 1.9 Plots of M = 9 polynomials ﬁtted to the data set shown in Figure 1.4 using the regularized error\nfunction (1.4) for two values of the regularization parameter \u0015corresponding to ln \u0015 = −18 and ln \u0015 = 0. The\ncase of no regularizer, i.e., \u0015= 0, corresponding to ln \u0015= −∞, is shown at the bottom right of Figure 1.6."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 163, "text": "term takes the form of the sum of the squares of all of the coefﬁcients, leading to a\nmodiﬁed error function of the form\n˜E(w) = 1\n2\nN∑\nn=1\n{y(xn;w) −tn}2 + \u0015\n2 ∥w∥2 (1.4)\nwhere ∥w∥2 ≡wTw = w2\n0 + w2\n1 + ::: + w2\nM, and the coefﬁcient \u0015governs the rel-\native importance of the regularization term compared with the sum-of-squares error\nterm. Note that often the coefﬁcient w0 is omitted from the regularizer because its"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 164, "text": "inclusion causes the results to depend on the choice of origin for the target variable\n(Hastie, Tibshirani, and Friedman, 2009), or it may be included but with its own\nregularization coefﬁcient. Again, the error function in (1.4) can be minimized ex-Section 9.2.1\nactly in closed form. Techniques such as this are known in the statistics literature asExercise 4.2\nshrinkage methods because they reduce the value of the coefﬁcients. In the context"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 165, "text": "of neural networks, this approach is known as weight decay because the parameters\nin a neural network are called weights and this regularizer encourages them to decay\ntowards zero.\nFigure 1.9 shows the results of ﬁtting the polynomial of order M = 9 to the\nsame data set as before but now using the regularized error function given by (1.4).\nWe see that, for a value of ln \u0015= −18, the over-ﬁtting has been suppressed and we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 166, "text": "now obtain a much closer representation of the underlying function sin(2\u0019x). If,\nhowever, we use too large a value for \u0015then we again obtain a poor ﬁt, as shown in\nFigure 1.9 for ln \u0015= 0. The corresponding coefﬁcients from the ﬁtted polynomials\nare given in Table 1.2, showing that regularization has the desired effect of reducing\nthe magnitude of the coefﬁcients.\nThe impact of the regularization term on the generalization error can be seen by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 167, "text": "plotting the value of the RMS error (1.3) for both training and test sets against ln \u0015,\nas shown in Figure 1.10. We see that \u0015now controls the effective complexity of the\nmodel and hence determines the degree of over-ﬁtting.\n14 1. THE DEEP LEARNING REVOLUTION\nFigure 1.10 Graph of the root-mean-\nsquare error (1.3) versus ln \u0015for the M = 9\npolynomial.\n−30 -20 -10 -0\nM\n0\n0.5\nERMS\nTraining\nTest\n1.2.6 Model selection\nThe quantity \u0015is an example of ahyperparameter whose values are ﬁxed during"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 168, "text": "the minimization of the error function to determine the model parameters w. We\ncannot simply determine the value of\u0015by minimizing the error function jointly with\nrespect to w and \u0015since this will lead to \u0015→ 0 and an over-ﬁtted model with small\nor zero training error. Similarly, the order M of the polynomial is a hyperparameter\nof the model, and simply optimizing the training set error with respect to M will\nlead to large values of M and associated over-ﬁtting. We therefore need to ﬁnd a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 169, "text": "way to determine suitable values for hyperparameters. The results above suggest a\nsimple way of achieving this, namely by taking the available data and partitioning it\ninto a training set, used to determine the coefﬁcientsw, and a separatevalidation set,\nalso called a hold-out set or a development set. We then select the model having the\nlowest error on the validation set. If the model design is iterated many times using a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 170, "text": "data set of limited size, then some over-ﬁtting to the validation data can occur, and\nso it may be necessary to keep aside a third test set on which the performance of the\nselected model can ﬁnally be evaluated.\nFor some applications, the supply of data for training and testing will be limited.\nTo build a good model, we should use as much of the available data as possible for\ntraining. However, if the validation set is too small, it will give a relatively noisy"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 171, "text": "estimate of predictive performance. One solution to this dilemma is to use cross-\nTable 1.2 Table of the coefﬁcients w? for\nM = 9 polynomials with various\nvalues for the regularization param-\neter \u0015. Note that ln \u0015 = −∞ cor-\nresponds to a model with no regu-\nlarization, i.e., to the graph at the\nbottom right in Figure 1.6. We see\nthat, as the value of \u0015 increases,\nthe magnitude of a typical coefﬁ-\ncient gets smaller.\nln \u0015= −∞ ln \u0015= −18 ln \u0015= 0\nw?\n0 0:26 0:26 0:11\nw?\n1 −66:13 0:64 −0:07\nw?"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 172, "text": "w?\n0 0:26 0:26 0:11\nw?\n1 −66:13 0:64 −0:07\nw?\n2 1;665:69 43:68 −0:09\nw?\n3 −15;566:61 −144:00 −0:07\nw?\n4 76;321:23 57:90 −0:05\nw?\n5 −217;389:15 117:36 −0:04\nw?\n6 370;626:48 9:87 −0:02\nw?\n7 −372;051:47 −90:02 −0:01\nw?\n8 202;540:70 −70:90 −0:01\nw?\n9 −46;080:94 75:26 0:00\n1.2. A Tutorial Example 15\nFigure 1.11 The technique of S-fold cross-validation, illus-\ntrated here for the case of S = 4, involves tak-\ning the available data and partitioning it into S"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 173, "text": "ing the available data and partitioning it into S\ngroups of equal size. Then S−1 of the groups\nare used to train a set of models that are then\nevaluated on the remaining group. This proce-\ndure is then repeated for allSpossible choices\nfor the held-out group, indicated here by the\nred blocks, and the performance scores from\nthe S runs are then averaged.\nrun 1\nrun 2\nrun 3\nrun 4\nvalidation, which is illustrated inFigure 1.11. This allows a proportion (S−1)=Sof"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 174, "text": "the available data to be used for training while making use of all of the data to assess\nperformance. When data is particularly scarce, it may be appropriate to consider the\ncase S = N, where N is the total number of data points, which gives the leave-one-\nout technique.\nThe main drawback of cross-validation is that the number of training runs that\nmust be performed is increased by a factor of S, and this can prove problematic for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 175, "text": "models in which the training is itself computationally expensive. A further problem\nwith techniques such as cross-validation that use separate data to assess performance\nis that we might have multiple complexity hyperparameters for a single model (for\ninstance, there might be several regularization hyperparameters). Exploring combi-\nnations of settings for such hyperparameters could, in the worst case, require a num-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 176, "text": "ber of training runs that is exponential in the number of hyperparameters. The state\nof the art in modern machine learning involves extremely large models, trained on\ncommensurately large data sets. Consequently, there is limited scope for exploration\nof hyperparameter settings, and heavy reliance is placed on experience obtained with\nsmaller models and on heuristics.\nThis simple example of ﬁtting a polynomial to a synthetic data set generated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 177, "text": "from a sinusoidal function has illustrated many key ideas from machine learning,\nand we will make further use of this example in future chapters. However, real-\nworld applications of machine learning differ in several important respects. The size\nof the data sets used for training can be many orders of magnitude larger, and there\nwill generally be many more input variables, perhaps numbering in the millions for\nimage analysis, for example, as well as multiple output variables. The learnable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 178, "text": "function that relates outputs to inputs is governed by a class of models known as\nneural networks, and these may have a large number of parameters perhaps num-\nbering in the hundreds of billions, and the error function will be a highly nonlinear\nfunction of those parameters. The error function can no longer be minimized through\na closed-form solution and instead must be minimized through iterative optimization\ntechniques based on evaluation of the derivatives of the error function with respect"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 179, "text": "to the parameters, all of which may require specialist computational hardware and\nincur substantial computational cost.\n16 1. THE DEEP LEARNING REVOLUTION\nFigure 1.12 Schematic illustration\nshowing two neurons from the human\nbrain. These electrically active cells\ncommunicate through junctions called\nsynapses whose strengths change as\nthe network learns.\n1.3. A Brief History of Machine Learning\nMachine learning has a long and rich history, including the pursuit of multiple al-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 180, "text": "ternative approaches. Here we focus on the evolution of machine learning methods\nbased on neural networks as these represent the foundation of deep learning and\nhave proven to be the most effective approach to machine learning for real-world\napplications.\nNeural network models were originally inspired by studies of information pro-\ncessing in the brains of humans and other mammals. The basic processing units in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 181, "text": "the brain are electrically active cells called neurons, as illustrated in Figure 1.12.\nWhen a neuron ‘ﬁres’, it sends an electrical impulse down the axon where it reaches\njunctions, called synapses, which form connections with other neurons. Chemical\nsignals called neurotransmitters are released at the synapses, and these can stimu-\nlate, or inhibit, the ﬁring of subsequent neurons.\nA human brain contains around 90 billion neurons in total, each of which has on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 182, "text": "average several thousand synapses with other neurons, creating a complex network\nhaving a total of around 100 trillion (1014) synapses. If a particular neuron receives\nsufﬁcient stimulation from the ﬁring of other neurons then it too can be induced to\nﬁre. However, some synapses have a negative, or inhibitory, effect whereby the ﬁring\nof the input neuron makes it less likely that the output neuron will ﬁre. The extent to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 183, "text": "which one neuron can cause another to ﬁre depends on the strength of the synapse,\nand it is changes in these strengths that represents a key mechanism whereby the\nbrain can store information and learn from experience.\nThese properties of neurons have been captured in very simple mathematical\nmodels, known as artiﬁcial neural networks, which then form the basis for compu-\ntational approaches to learning (McCulloch and Pitts, 1943). Many of these models"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 184, "text": "describe the properties of a single neuron by forming a linear combination of the\noutputs of other neurons, which is then transformed using a nonlinear function. This\n1.3. A Brief History of Machine Learning 17\nFigure 1.13 A simple neural network diagram representing the trans-\nformations (1.5) and (1.6) describing a single neuron. The\npolynomial function (1.1) can be seen as a special case of\nthis model.\nxM …\nx2\nx1\ny\nwM\nw2\nw1\ncan be expressed mathematically in the form\na=\nM∑\ni=1\nwixi (1.5)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 185, "text": "a=\nM∑\ni=1\nwixi (1.5)\ny= f(a) (1.6)\nwhere x1;:::;x M represent M inputs corresponding to the activities of other neu-\nrons that send connections to this neuron, and w1;:::;w M are continuous variables,\ncalled weights, which represent the strengths of the associated synapses. The quan-\ntity ais called the pre-activation, the nonlinear function f(-)is called the activation\nfunction, and the output y is called the activation. We can see that the polynomial"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 186, "text": "(1.1) can be viewed as a speciﬁc instance of this representation in which the inputs\nxiare given by powers of a single variablex, and the functionf(-)is just the identity\nf(a) = a. The simple mathematical formulation given by (1.5) and (1.6) has formed\nthe basis of neural network models from the 1960s up to the present day, and can be\nrepresented in diagram form as shown in Figure 1.13.\n1.3.1 Single-layer networks"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 187, "text": "1.3.1 Single-layer networks\nThe history of artiﬁcial neural networks can broadly be divided into three distinct\nphases according to the level of sophistication of the networks as measured by the\nnumber of ‘layers’ of processing. A simple neural model described by (1.5) and (1.6)\ncan be viewed as having a single layer of processing corresponding to the single layer\nof connections in Figure 1.13. One of the most important such models in the history"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 188, "text": "of neural computing is the perceptron (Rosenblatt, 1962) in which the activation\nfunction f(-)is a step function of the form\nf(a) =\n{\n0; if a6 0;\n1; if a> 0 : (1.7)\nThis can be viewed as a simpliﬁed model of neural ﬁring in which a neuron ﬁres if,\nand only if, the total weighted input exceeds a threshold of 0. The perceptron was\npioneered by Rosenblatt (1962), who developed a speciﬁc training algorithm that\nhas the interesting property that if there exists a set of weight values for which the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 189, "text": "perceptron can achieve perfect classiﬁcation of its training data then the algorithm\nis guaranteed to ﬁnd the solution in a ﬁnite number of steps (Bishop, 2006). As\nwell as a learning algorithm, the perceptron also had a dedicated analogue hardware\n18 1. THE DEEP LEARNING REVOLUTION\nFigure 1.14 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs\nwere obtained using a simple camera system in which an input scene, in this case a printed character, was"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 190, "text": "illuminated by powerful lights, and an image focused onto a 20 ×20 array of cadmium sulphide photocells,\ngiving a primitive 400-pixel image. The perceptron also had a patch board, shown in the middle photograph,\nwhich allowed different conﬁgurations of input features to be tried. Often these were wired up at random to\ndemonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 191, "text": "digital computer. The photograph on the right shows one of the racks of learnable weights. Each weight was\nimplemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby\nallowing the value of the weight to be adjusted automatically by the learning algorithm.\nimplementation, as shown in Figure 1.14. A typical perceptron conﬁguration had\nmultiple layers of processing, but only one of those layers was learnable from data,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 192, "text": "and so the perceptron is considered to be a ‘single-layer’ neural network.\nAt ﬁrst, the ability of perceptrons to learn from data in a brain-like way was con-\nsidered remarkable. However, it became apparent that the model also has major lim-\nitations. The properties of perceptrons were analysed by Minsky and Papert (1969),\nwho gave formal proofs of the limited capabilities of single-layer networks. Unfortu-\nnately, they also speculated that similar limitations would extend to networks having"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 193, "text": "multiple layers of learnable parameters. Although this latter conjecture proved to\nbe wildly incorrect, the effect was to dampen enthusiasm for neural network mod-\nels, and this contributed to the lack of interest, and funding, for neural networks\nduring the 1970s and early 1980s. Furthermore, researchers were unable to explore\nthe properties of multilayered networks due to the lack of an effective algorithm\nfor training them, since techniques such as the perceptron algorithm were speciﬁc"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 194, "text": "to single-layer models. Note that although perceptrons have long disappeared from\npractical machine learning, the name lives on because a modern neural network is\nalso sometimes called a multilayer perceptron or MLP.\n1.3.2 Backpropagation\nThe solution to the problem of training neural networks having more than one\nlayer of learnable parameters came from the use of differential calculus and the appli-\ncation of gradient-based optimization methods. An important change was to replace"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 195, "text": "the step function (1.7) with continuous differentiable activation functions having a\nnon-zero gradient. Another key modiﬁcation was to introduce differentiable error\nfunctions that deﬁne how well a given choice of parameter values predicts the target\nvariables in the training set. We saw an example of such an error function when we\n1.3. A Brief History of Machine Learning 19\nFigure 1.15 A neural network having two lay-\ners of parameters in which arrows\ndenote the direction of information"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 196, "text": "denote the direction of information\nﬂow through the network. Each of\nthe hidden units and each of the\noutput units computes a function of\nthe form given by (1.5) and (1.6) in\nwhich the activation functionf(-)is\ndifferentiable.\ninputs\n …\nhidden units …\noutputs\n …\nused the sum-of-squares error function (1.2) to ﬁt polynomials.Section 1.2.3\nWith these changes, we now have an error function whose derivatives with re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 197, "text": "spect to each of the parameters in the network can be evaluated. We can now consider\nnetworks having more than one layer of parameters.Figure 1.15 shows a simple net-\nwork with two processing layers. Nodes in the middle layer called hidden units\nbecause their values do not appear in the training set, which only provides values\nfor inputs and outputs. Each of the hidden units and each of the output units in\nFigure 1.15 computes a function of the form given by (1.5) and (1.6). For a given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 198, "text": "set of input values, the states of all of the hidden and output units can be evaluated\nby repeated application of (1.5) and (1.6) in which information is ﬂowing forward\nthrough the network in the direction of the arrows. For this reason, such models are\nsometimes also called feed-forward neural networks.\nTo train such a network the parameters are ﬁrst initialized using a random num-\nber generator and are then iteratively updated using gradient-based optimization"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 199, "text": "techniques. This involves evaluating the derivatives of the error function, which\ncan be done efﬁciently in a process known as error backpropagation. In backpropa-Chapter 8\ngation, information ﬂows backwards through the network from the outputs towards\nthe inputs (Rumelhart, Hinton, and Williams, 1986). There exist many different op-\ntimization algorithms that make use of gradients of the function to be optimized, but"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 200, "text": "the one that is most prevalent in machine learning is also the simplest and is known\nas stochastic gradient descent.Chapter 7\nThe ability to train neural networks having multiple layers of weights was a\nbreakthrough that led to a resurgence of interest in the ﬁeld starting around the mid-\n1980s. This was also a period in which the ﬁeld moved beyond a focus on neurobio-\nlogical inspiration and developed a more rigorous and principled foundation (Bishop,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 201, "text": "1995b). In particular, it was recognized that probability theory, and ideas from the\nﬁeld of statistics, play a central role in neural networks and machine learning. One\nkey insight is that learning from data involves background assumptions, sometimes\ncalled prior knowledge or inductive biases. These might be incorporated explicitly,\nfor example by designing the structure of a neural network such that the classiﬁca-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 202, "text": "tion of a skin lesion does not depend on the location of the lesion within the image,\nor they might take the form of implicit assumptions that arise from the mathematical\n20 1. THE DEEP LEARNING REVOLUTION\nform of the model or the way it is trained.\nThe development of backpropagation and gradient-based optimization dramati-\ncally increased the capability of neural networks to solve practical problems. How-\never, it was also observed that in networks with many layers, it was only weights in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 203, "text": "the ﬁnal two layers that would learn useful values. With a few exceptions, notably\nmodels used for image analysis known as convolutional neural networks (LeCunChapter 10\net al., 1998), there were very few successful applications of networks having more\nthan two layers. Again, this constrained the complexity of the problems that could\nbe addressed effectively with these kinds of network. To achieve reasonable perfor-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 204, "text": "mance on many applications, it was necessary to use hand-crafted pre-processing to\ntransform the input variables into some new space where, it was hoped, the machine\nlearning problem would be easier to solve. This pre-processing stage is sometimes\nalso called feature extraction. Although this approach was sometimes effective, it\nwould clearly be much better if features could be learned from the data rather than\nbeing hand-crafted."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 205, "text": "being hand-crafted.\nBy the start of the new millennium, the available neural network methods were\nonce again reaching the limits of their capability. Researchers began to explore a\nraft of alternatives to neural networks, such as kernel methods, support vector ma-\nchines, Gaussian processes, and many others. Neural networks fell into disfavour\nonce again, although a core of enthusiastic researchers continued to pursue the goal\nof a truly effective approach to training networks with many layers."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 206, "text": "1.3.3 Deep networks\nThe third, and current, phase in the development of neural networks began dur-\ning the second decade of the 21st century. A series of developments allowed neural\nnetworks with many layers of weights to be trained effectively, thereby removing\nprevious limitations on the capabilities of these techniques. Networks with many lay-\ners of weights are called deep neural networks and the sub-ﬁeld of machine learning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 207, "text": "that focuses on such networks is called deep learning (LeCun, Bengio, and Hinton,\n2015).\nOne important theme in the origins of deep learning was a signiﬁcant increase\nin the scale of neural networks, measured in terms of the number of parameters. Al-\nthough networks with a few hundred or a few thousand parameters were common in\nthe 1980s, this steadily rose to the millions, and then billions, whereas current state-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 208, "text": "of-the-art models can have in the region of one trillion (1012) parameters. Networks\nwith many parameters require commensurately large data sets so that the training\nsignals can produced good values for those parameters. The combination of massive\nmodels and massive data sets in turn requires computation on a massive scale when\ntraining the model. Specialist processors called graphics processing units, or GPUs,\nwhich had been developed for very fast rendering of graphical data for applications"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 209, "text": "such as video games, proved to be well suited to the training of neural networks be-\ncause the functions computed by the units in one layer of a network can be evaluated\nin parallel, and this maps well onto the massive parallelism of GPUs (Krizhevsky,\nSutskever, and Hinton, 2012). Today, training for the largest models is performed on\nlarge arrays of thousands of GPUs linked by specialist high-speed interconnections.\n1.3. A Brief History of Machine Learning 21"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 210, "text": "1.3. A Brief History of Machine Learning 21\nFigure 1.16 Plot of the number of compute cycles, measured in petaﬂop/s-days, needed to train a state-of-the-\nart neural network as a function of date, showing two distinct phases of exponential growth. [From OpenAI with\npermission.]\nFigure 1.16 illustrates how the number of compute cycles needed to train a state-\nof-the-art neural network has grown over the years, showing two distinct phases of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 211, "text": "growth. The vertical axis has an exponential scale and has units of petaﬂop/s-days,\nwhere a petaﬂop represents 1015 (a thousand trillion) ﬂoating point operations, and a\npetaﬂop/s is one petaﬂop per second. One petaﬂop/s-day represents computation at\nthe rate of a petaﬂop/s for a period of 24 hours, which is roughly 1020 ﬂoating point\noperations, and therefore, the top line of the graph represents an impressive 1024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 212, "text": "ﬂoating point operations. A straight line on the graph represents exponential growth,\nand we see that from the era of the perceptron up to around 2012, the doubling time\nwas around 2 years, which is consistent with the general growth of computing power\nas a consequence of Moore’s law. From 2012 onward, which marks the era of deep\nlearning, we again see exponential growth but the doubling time is now 3.4 months\ncorresponding to a factor of 10 increase in compute power every year!"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 213, "text": "It is often found that improvements in performance due to innovations in the\narchitecture or incorporation of more sophisticated forms of inductive bias are soon\n22 1. THE DEEP LEARNING REVOLUTION\nsuperseded simply by scaling up the quantity of training data, along with commen-\nsurate scaling of the model size and associated compute power used for training\n(Sutton, 2019). Not only can large models have superior performance on a speciﬁc"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 214, "text": "task but they may be capable of solving a broader range of different problems with\nthe same trained neural network. Large language models are a notable example as a\nsingle network not only has an extraordinary breadth of capability but is even able to\noutperform specialist networks designed to solve speciﬁc problems.Section 12.3.5\nWe have seen that depth plays an important role in allowing neural networks to\nachieve high performance. One way to view the role of the hidden layers in a deep"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 215, "text": "neural network is that of representation learning (Bengio, Courville, and Vincent,\n2012) in which the network learns to transform input data into a new representation\nthat is semantically meaningful thereby creating a much easier problem for the ﬁnal\nlayer or layers to solve. Such internal representations can be repurposed to allow for\nthe solution of related problems through transfer learning, as we saw for skin lesion"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 216, "text": "classiﬁcation. It is interesting to note that neural networks used to process images\nmay learn internal representations that are remarkably like those observed in the\nmammalian visual cortex. Large neural networks that can be adapted or ﬁne-tunedSection 10.3\nto a range of downstream tasks are called foundation models, and can take advan-\ntage of large, heterogeneous data sets to create models having broad applicability\n(Bommasani et al., 2021)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 217, "text": "(Bommasani et al., 2021).\nIn addition to scaling, there were other developments that helped in the suc-\ncess of deep learning. For example, in simple neural networks, the training signals\nbecome weaker as they are backpropagated through successive layers of a deep net-\nwork. One technique for addressing this is the introduction of residual connectionsSection 9.5\n(He et al., 2015a) that facilitate the training of networks having hundreds of layers."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 218, "text": "Another key development was the introduction ofautomatic differentiation methods\nin which the code that performs backpropagation to evaluate error function gradients\nis generated automatically from the code used to specify the forward propagation.\nThis allows researchers to experiment rapidly with different architectures for a neural\nnetwork and to combine different architectural elements in multiple ways very easily"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 219, "text": "since only the relatively simple forward propagation functions need to be coded ex-\nplicitly. Also, much of the research in machine learning has been conducted through\nopen source, allowing researchers to build on the work of others, thereby further\naccelerating the rate of progress in the ﬁeld.\n2\nProbabilities\nIn almost every application of machine learning we have to deal with uncertainty. For\nexample, a system that classiﬁes images of skin lesions as benign or malignant can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 220, "text": "never in practice achieve perfect accuracy. We can distinguish between two kinds of\nuncertainty. The ﬁrst isepistemic uncertainty (derived from the Greek word episteme\nmeaning knowledge), sometimes called systematic uncertainty. It arises because we\nonly get to see data sets of ﬁnite size. As we observe more data, for instance more\nexamples of benign and malignant skin lesion images, we are better able to predict"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 221, "text": "the class of a new example. However, even with an inﬁnitely large data set, we would\nstill not be able to achieve perfect accuracy due to the second kind of uncertainty\nknown as aleatoric uncertainty, also called intrinsic or stochastic uncertainty, or\nsometimes simply called noise. Generally speaking, the noise arises because we are\nable to observe only partial information about the world, and therefore, one way to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 222, "text": "reduce this source of uncertainty is to gather different kinds of data. This is illustrated\n23© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 223, "text": "24 2. PROBABILITIES\nx1\nx2\ny\n(a)\nx1\ny\n (b)\nx1\n (c)\nFigure 2.1 An extension of the simple sine curve regression problem to two dimensions. (a) A plot of the\nfunction y(x1;x2) = sin(2\u0019x1) sin(2\u0019x2). Data is generated by selecting values for x1 and x2, computing the\ncorresponding value of y(x1;x2), and then adding Gaussian noise. (b) Plot of 100 data points in which x2 is\nunobserved showing high levels of noise. (c) Plot of 100 data points in which x2 is ﬁxed to the value x2 = \u0019\n2 ,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 224, "text": "2 ,\nsimulating the effect of being able to measure x2 as well as x1, showing much lower levels of noise.\nusing an extension of the sine curve example to two dimensions in Figure 2.1.Section 1.2\nAs a practical example of this, a biopsy sample of the skin lesion is much more\ninformative than the image alone and might greatly improve the accuracy with which\nwe can determine if a new lesion is malignant. Given both the image and the biopsy"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 225, "text": "data, the intrinsic uncertainty might be very small, and by collecting a large training\ndata set, we may be able to reduce the systematic uncertainty to a low level and\nthereby make predictions of the class of the lesion with high accuracy.\nBoth kinds of uncertainty can be handled using the framework of probability\ntheory, which provides a consistent paradigm for the quantiﬁcation and manipula-\ntion of uncertainty and therefore forms one of the central foundations for machine"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 226, "text": "learning. We will see that probabilities are governed by two simple formulae knownSection 2.1\nas the sum rule and the product rule. When coupled with decision theory, these rulesSection 5.2\nallow us, at least in principle, to make optimal predictions given all the information\navailable to us, even though that information may be incomplete or ambiguous.\nThe concept of probability is often introduced in terms of frequencies of repeat-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 227, "text": "able events. Consider, for example, the bent coin shown in Figure 2.2, and suppose\nthat the shape of the coin is such that if it is ﬂipped a large number of times, it lands\nconcave side up 60% of the time, and therefore lands convex side up 40% of the\ntime. We say that the probability of landing concave side up is 60% or 0.6. Strictly,\nthe probability is deﬁned in the limit of an inﬁnite number of ‘trials’ or coin ﬂips"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 228, "text": "in this case. Because the coin must land either concave side up or convex side up,\nthese probabilities add to 100% or 1.0. This deﬁnition of probability in terms of the\nfrequency of repeatable events is the basis for the frequentist view of statistics.\nNow suppose that, although we know that the probability that the coin will land\nconcave side up is 0.6, we are not allowed to look at the coin itself and we do not\n2.1. The Rules of Probability 25\nFigure 2.2 Probability can be viewed ei-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 229, "text": "Figure 2.2 Probability can be viewed ei-\nther as a frequency associated\nwith a repeatable event or as\na quantiﬁcation of uncertainty.\nA bent coin can be used to il-\nlustrate the difference, as dis-\ncussed in the text.\n60% 40%\nknow which side is heads and which is tails. If asked to take a bet on whether the coin\nwill land heads or tails when ﬂipped, then symmetry suggests that our bet should be\nbased on the assumption that the probability of seeing heads is 0.5, and indeed a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 230, "text": "more careful analysis shows that, in the absence of any additional information, this\nis indeed the rational choice. Here we are using probabilities in a more general sense\nthan simply the frequency of events. Whether the convex side of the coin is heads or\ntails is not itself a repeatable event, it is simply unknown. The use of probability as a\nquantiﬁcation of uncertainty is the Bayesian perspective and is more general in thatSection 2.6"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 231, "text": "it includes frequentist probability as a special case. We can learn about which side\nof the coin is heads if we are given results from a sequence of coin ﬂips by making\nuse of Bayesian reasoning. The more results we observe, the lower our uncertaintyExercise 2.40\nas to which side of the coin is which.\nHaving introduced the concept of probability informally, we turn now to a more\ndetailed exploration of probabilities and discuss how to use them quantitatively. Con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 232, "text": "cepts developed in the remainder of this chapter will form a core foundation for many\nof the topics discussed throughout the book.\n2.1.\nThe Rules of Probability\nIn this section we will derive two simple rules that govern the behaviour of proba-\nbilities. However, in spite of their apparent simplicity, these rules will prove to be\nvery powerful and widely applicable. We will motivate the rules of probability by\nﬁrst introducing a simple example.\n2.1.1 A medical screening example"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 233, "text": "2.1.1 A medical screening example\nConsider the problem of screening a population in order to provide early detec-\ntion of cancer, and let us suppose that 1% of the population actually have cancer.\nIdeally our test for cancer would give a positive result for anyone who has cancer\nand a negative result for anyone who does not. However, tests are not perfect, so\nwe will suppose that when the test is given to people who are free of cancer, 3% of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 234, "text": "them will test positive. These are known as false positives. Similarly, when the test\nis given to people who do have cancer, 10% of them will test negative. These are\ncalled false negatives. The various error rates are illustrated in Figure 2.3.\nGiven this information, we might ask the following questions: (1) ‘If we screen\nthe population, what is the probability that someone will test positive?’, (2) ‘If some-\n26 2. PROBABILITIES\nFigure 2.3 Illustration of the accuracy of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 235, "text": "Figure 2.3 Illustration of the accuracy of\na cancer test. Out of ev-\nery hundred people taking the\ntest who do not have cancer,\nshown on the left, on average\nthree will test positive. For\nthose who have cancer, shown\non the right, out of every hun-\ndred people taking the test, on\naverage 90 will test positive.\nNo Cancer Cancer\none receives a positive test result, what is the probability that they actually have can-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 236, "text": "cer?’. We could answer such questions by working through the cancer screening case\nin detail. Instead, however, we will pause our discussion of this speciﬁc example and\nﬁrst derive the general rules of probability, known as thesum rule of probability and\nthe product rule. We will then illustrate the use of these rules by answering our two\nquestions.\n2.1.2 The sum and product rules\nTo derive the rules of probability, consider the slightly more general example"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 237, "text": "shown in Figure 2.4 involving two variables X and Y. In our cancer example, X\ncould represent the presence or absence of cancer, and Y could be a variable de-\nnoting the outcome of the test. Because the values of these variables can vary from\none person to another in a way that is generally unknown, they are called random\nvariables or stochastic variables. We will suppose that Xcan take any of the values\nxi where i= 1;:::;L and that Y can take the values yj where j = 1;:::;M . Con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 238, "text": "sider a total of N trials in which we sample both of the variables X and Y, and let\nthe number of such trials in which X = xi and Y = yj be nij. Also, let the number\nof trials in which X takes the value xi (irrespective of the value that Y takes) be\ndenoted by ci, and similarly let the number of trials in whichY takes the value yj be\ndenoted by rj.\nThe probability that X will take the value xi and Y will take the value yj is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 239, "text": "written p(X = xi;Y = yj) and is called the joint probability of X = xi and\nY = yj. It is given by the number of points falling in the cell i,jas a fraction of the\ntotal number of points, and hence\np(X = xi;Y = yj) = nij\nN : (2.1)\nHere we are implicitly considering the limit N →∞. Similarly, the probability that\nX takes the value xi irrespective of the value of Y is written as p(X = xi) and is\n2.1. The Rules of Probability 27\nFigure 2.4 We can derive the sum and product rules"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 240, "text": "of probability by considering a random vari-\nable X, which takes the values {xi}where\ni = 1 ;:::;L , and a second random variable\nY, which takes the values {yj}where j =\n1;:::;M . In this illustration, we have L = 5\nand M = 3. If we consider the total number\nN of instances of these variables, then we de-\nnote the number of instances where X = xi\nand Y = yj by nij, which is the number of in-\nstances in the corresponding cell of the array.\nThe number of instances in column i, corre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 241, "text": "The number of instances in column i, corre-\nsponding to X = xi, is denoted by ci, and the\nnumber of instances in row j, corresponding\nto Y = yj, is denoted by rj.\ng\ng\nci\nrjyj\nxi\nnij\ngiven by the fraction of the total number of points that fall in column i, so that\np(X = xi) = ci\nN: (2.2)\nSince ∑\nici = N, we see that\nL∑\ni=1\np(X = xi) = 1 (2.3)\nand, hence, the probabilities sum to one as required. Because the number of instances"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 242, "text": "in column iin Figure 2.4 is just the sum of the number of instances in each cell of\nthat column, we have ci = ∑\njnij and therefore, from (2.1) and (2.2), we have\np(X = xi) =\nM∑\nj=1\np(X = xi;Y = yj); (2.4)\nwhich is the sum rule of probability. Note that p(X = xi) is sometimes called the\nmarginal probability and is obtained by marginalizing, or summing out, the other\nvariables (in this case Y).\nIf we consider only those instances for which X = xi, then the fraction of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 243, "text": "such instances for which Y = yj is written p(Y = yj|X= xi) and is called the\nconditional probability of Y = yj given X = xi. It is obtained by ﬁnding the\nfraction of those points in column ithat fall in cell i,jand, hence, is given by\np(Y = yj|X= xi) = nij\nci\n: (2.5)\nSumming both sides over jand using ∑\njnij = ci, we obtain\nM∑\nj=1\np(Y = yj|X= xi) = 1 (2.6)\n28 2. PROBABILITIES\nshowing that the conditional probabilities are correctly normalized. From (2.1),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 244, "text": "(2.2), and (2.5), we can then derive the following relationship:\np(X = xi;Y = yj) = nij\nN = nij\nci\n-ci\nN\n= p(Y = yj|X= xi)p(X = xi); (2.7)\nwhich is the product rule of probability.\nSo far, we have been quite careful to make a distinction between a random vari-\nable, such as X, and the values that the random variable can take, for example xi.\nThus, the probability that X takes the value xi is denoted p(X = xi). Although"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 245, "text": "this helps to avoid ambiguity, it leads to a rather cumbersome notation, and in many\ncases there will be no need for such pedantry. Instead, we may simply writep(X) to\ndenote a distribution over the random variable X, or p(xi) to denote the distribution\nevaluated for the particular valuexi, provided that the interpretation is clear from the\ncontext.\nWith this more compact notation, we can write the two fundamental rules of\nprobability theory in the following form:\nsum rule p(X) =\n∑\nY"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 246, "text": "sum rule p(X) =\n∑\nY\np(X;Y ) (2.8)\nproduct rule p(X;Y ) = p(Y|X)p(X): (2.9)\nHere p(X;Y ) is a joint probability and is verbalized as ‘the probability of X and\nY’. Similarly, the quantity p(Y|X) is a conditional probability and is verbalized as\n‘the probability ofY given X’. Finally, the quantity p(X) is a marginal probability\nand is simply ‘the probability ofX’. These two simple rules form the basis for all of\nthe probabilistic machinery that we will use throughout this book."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 247, "text": "2.1.3 Bayes’ theorem\nFrom the product rule, together with the symmetry propertyp(X;Y ) = p(Y;X),\nwe immediately obtain the following relationship between conditional probabilities:\np(Y|X) = p(X|Y)p(Y)\np(X) ; (2.10)\nwhich is called Bayes’ theoremand which plays an important role in machine learn-\ning. Note how Bayes’ theorem relates the conditional distribution p(Y|X) on the\nleft-hand side of the equation, to the ‘reversed’ conditional distribution p(X|Y) on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 248, "text": "the right-hand side. Using the sum rule, the denominator in Bayes’ theorem can be\nexpressed in terms of the quantities appearing in the numerator:\np(X) =\n∑\nY\np(X|Y)p(Y): (2.11)\nThus, we can view the denominator in Bayes’ theorem as being the normalization\nconstant required to ensure that the sum over the conditional probability distribution\non the left-hand side of (2.10) over all values of Y equals one.\n2.1. The Rules of Probability 29\np(X, Y)\nX\nY = 2\nY = 1\np(Y )\np(X)\nX\nX\np(X|Y = 1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 249, "text": "p(X, Y)\nX\nY = 2\nY = 1\np(Y )\np(X)\nX\nX\np(X|Y = 1)\nFigure 2.5 An illustration of a distribution over two variables, X, which takes nine possible values, and Y,\nwhich takes two possible values. The top left ﬁgure shows a sample of 60 points drawn from a joint probability\ndistribution over these variables. The remaining ﬁgures show histogram estimates of the marginal distributions\np(X) and p(Y), as well as the conditional distribution p(X|Y= 1) corresponding to the bottom row in the top left\nﬁgure."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 250, "text": "ﬁgure.\nIn Figure 2.5, we show a simple example involving a joint distribution over two\nvariables to illustrate the concept of marginal and conditional distributions. Here a\nﬁnite sample of N = 60 data points has been drawn from the joint distribution and\nis shown in the top left. In the top right is a histogram of the fractions of data points\nhaving each of the two values of Y. From the deﬁnition of probability, these frac-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 251, "text": "tions would equal the corresponding probabilities p(Y) in the limit when the sample\nsize N →∞. We can view the histogram as a simple way to model a probability\ndistribution given only a ﬁnite number of points drawn from that distribution. TheSection 3.5.1\nremaining two plots in Figure 2.5 show the corresponding histogram estimates of\np(X) and p(X|Y= 1).\n30 2. PROBABILITIES\n2.1.4 Medical screening revisited\nLet us now return to our cancer screening example and apply the sum and prod-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 252, "text": "uct rules of probability to answer our two questions. For clarity, when working\nthrough this example, we will once again be explicit about distinguishing between\nthe random variables and their instantiations. We will denote the presence or absence\nof cancer by the variable C, which can take two values: C = 0 corresponds to ‘no\ncancer’ and C = 1 corresponds to ‘cancer’. We have assumed that one person in a\nhundred in the population has cancer, and so we have\np(C = 1) = 1=100 (2.12)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 253, "text": "p(C = 1) = 1=100 (2.12)\np(C = 0) = 99=100; (2.13)\nrespectively. Note that these satisfy p(C = 0) + p(C = 1) = 1.\nNow let us introduce a second random variable T representing the outcome of a\nscreening test, whereT = 1 denotes a positive result, indicative of cancer, andT = 0\na negative result, indicative of the absence of cancer. As illustrated inFigure 2.3, we\nknow that for those who have cancer the probability of a positive test result is 90%,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 254, "text": "while for those who do not have cancer the probability of a positive test result is 3%.\nWe can therefore write out all four conditional probabilities:\np(T = 1|C= 1) = 90=100 (2.14)\np(T = 0|C= 1) = 10=100 (2.15)\np(T = 1|C= 0) = 3=100 (2.16)\np(T = 0|C= 0) = 97=100: (2.17)\nAgain, note that these probabilities are normalized so that\np(T = 1|C= 1) + p(T = 0|C= 1) = 1 (2.18)\nand similarly\np(T = 1|C= 0) + p(T = 0|C= 0) = 1: (2.19)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 255, "text": "p(T = 1|C= 0) + p(T = 0|C= 0) = 1: (2.19)\nWe can now use the sum and product rules of probability to answer our ﬁrst\nquestion and evaluate the overall probability that someone who is tested at random\nwill have a positive test result:\np(T = 1) = p(T = 1|C= 0)p(C = 0) + p(T = 1|C= 1)p(C = 1)\n= 3\n100 × 99\n100 + 90\n100 × 1\n100 = 387\n10;000 = 0:0387: (2.20)\nWe see that if a person is tested at random there is a roughly 4% chance that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 256, "text": "test will be positive even though there is a 1% chance that they actually have cancer.\nFrom this it follows, using the sum rule, that p(T = 0) = 1 −387=10;000 =\n9613=10;000 = 0:9613 and, hence, there is a roughly 96% chance that the do not\nhave cancer.\nNow consider our second question, which is the one that is of particular interest\nto a person being screened: if a test is positive, what is the probability that the person\n2.1. The Rules of Probability 31"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 257, "text": "2.1. The Rules of Probability 31\nhas cancer? This requires that we evaluate the probability of cancer conditional\non the outcome of the test, whereas the probabilities in (2.14) to (2.17) give the\nprobability distribution over the test outcome conditioned on whether the person has\ncancer. We can solve the problem of reversing the conditional probability by using\nBayes’ theorem (2.10) to give\np(C = 1|T= 1) = p(T = 1|C= 1)p(C = 1)\np(T = 1) (2.21)\n= 90\n100 × 1\n100 ×10;000\n387 = 90\n387 ≃0:23 (2.22)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 258, "text": "100 × 1\n100 ×10;000\n387 = 90\n387 ≃0:23 (2.22)\nso that if a person is tested at random and the test is positive, there is a 23% proba-\nbility that they actually have cancer. From the sum rule, it then follows that p(C =\n0|T= 1) = 1 −90=387 = 297=387 ≃0:77, which is a 77% chance that they do not\nhave cancer.\n2.1.5 Prior and posterior probabilities\nWe can use the cancer screening example to provide an important interpretation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 259, "text": "of Bayes’ theorem as follows. If we had been asked whether someone is likely to\nhave cancer, before they have received a test, then the most complete information we\nhave available is provided by the probabilityp(C). We call this the prior probability\nbecause it is the probability available before we observe the result of the test. Once\nwe are told that this person has received a positive test, we can then use Bayes’ theo-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 260, "text": "rem to compute the probability p(C|T), which we will call the posterior probability\nbecause it is the probability obtained after we have observed the test result T.\nIn this example, the prior probability of having cancer is 1%. However, once we\nhave observed that the test result is positive, we ﬁnd that the posterior probability of\ncancer is now 23%, which is a substantially higher probability of cancer, as we would"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 261, "text": "intuitively expect. We note, however, that a person with a positive test still has only a\n23% change of actually having cancer, even though the test appears, fromFigure 2.3\nto be reasonably ‘accurate’. This conclusion seems counter-intuitive to many people.Exercise 2.1\nThe reason has to do with the low prior probability of having cancer. Although\nthe test provides strong evidence of cancer, this has to be combined with the prior"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 262, "text": "probability using Bayes’ theorem to arrive at the correct posterior probability.\n2.1.6 Independent variables\nFinally, if the joint distribution of two variables factorizes into the product of the\nmarginals, so that p(X;Y ) = p(X)p(Y), then X and Y are said to be independent.\nAn example of independent events would be the successive ﬂips of a coin. From\nthe product rule, we see that p(Y|X) = p(Y), and so the conditional distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 263, "text": "of Y given X is indeed independent of the value of X. In our cancer screening\nexample, if the probability of a positive test is independent of whether the person has\ncancer, then p(T|C) = p(T), which means that from Bayes’ theorem (2.10) we have\np(C|T) = p(C), and therefore probability of cancer is not changed by observing the\ntest outcome. Of course, such a test would be useless because the outcome of the\ntest tells us nothing about whether the person has cancer.\n32 2. PROBABILITIES"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 264, "text": "32 2. PROBABILITIES\nFigure 2.6 The concept of probability for\ndiscrete variables can be ex-\ntended to that of a probabil-\nity density p(x) over a contin-\nuous variable x and is such\nthat the probability of x lying\nin the interval (x;x + \u000ex) is\ngiven by p(x)\u000ex for \u000ex → 0.\nThe probability density can\nbe expressed as the deriva-\ntive of a cumulative distribu-\ntion function P(x).\nx\u000ex\np(x) P(x)\n2.2. Probability Densities"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 265, "text": "x\u000ex\np(x) P(x)\n2.2. Probability Densities\nAs well as considering probabilities deﬁned over discrete sets of values, we also\nwish to consider probabilities with respect to continuous variables. For instance, we\nmight wish to predict what dose of drug to give to a patient. Since there will be\nuncertainty in this prediction, we want to quantify this uncertainty and again we can\nmake use of probabilities. However, we cannot simply apply the concepts of proba-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 266, "text": "bility discussed so far directly, since the probability of observing a speciﬁc value for\na continuous variable, to inﬁnite precision, will effectively be zero. Instead, we need\nto introduce the concept of a probability density. Here we will limit ourselves to a\nrelatively informal discussion.\nWe deﬁne the probability density p(x) over a continuous variable xto be such\nthat the probability of x falling in the interval (x;x + \u000ex) is given by p(x)\u000ex for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 267, "text": "\u000ex→ 0. This is illustrated in Figure 2.6. The probability that xwill lie in an interval\n(a;b) is then given by\np(x∈(a;b)) =\n∫b\na\np(x) dx: (2.23)\nBecause probabilities are non-negative, and because the value of xmust lie some-\nwhere on the real axis, the probability density p(x) must satisfy the two conditions\np(x) > 0 (2.24)∫∞\n−∞\np(x) dx= 1: (2.25)\nThe probability that x lies in the interval (−∞;z ) is given by the cumulative\ndistribution function deﬁned by\nP(z) =\n∫z\n−∞\np(x) dx; (2.26)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 268, "text": "P(z) =\n∫z\n−∞\np(x) dx; (2.26)\n2.2. Probability Densities 33\nwhich satisﬁes P′(x) = p(x), as shown in Figure 2.6.\nIf we have several continuous variables x1;:::;x D, denoted collectively by the\nvector x, then we can deﬁne a joint probability density p(x) = p(x1;:::;x D) such\nthat the probability of x falling in an inﬁnitesimal volume \u000ex containing the point x\nis given by p(x)\u000ex. This multivariate probability density must satisfy\np(x) > 0 (2.27)∫\np(x) dx = 1 (2.28)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 269, "text": "p(x) > 0 (2.27)∫\np(x) dx = 1 (2.28)\nin which the integral is taken over the whole ofx space. More generally, we can also\nconsider joint probability distributions over a combination of discrete and continuous\nvariables.\nThe sum and product rules of probability, as well as Bayes’ theorem, also apply\nto probability densities as well as to combinations of discrete and continuous vari-\nables. If x and y are two real variables, then the sum and product rules take the\nform\nsum rule p(x) =\n∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 270, "text": "form\nsum rule p(x) =\n∫\np(x;y) dy (2.29)\nproduct rule p(x;y) = p(y|x)p(x): (2.30)\nSimilarly, Bayes’ theorem can be written in the form\np(y|x) =p(x|y)p(y)\np(x) (2.31)\nwhere the denominator is given by\np(x) =\n∫\np(x|y)p(y) dy: (2.32)\nA formal justiﬁcation of the sum and product rules for continuous variables re-\nquires a branch of mathematics called measure theory (Feller, 1966) and lies outside\nthe scope of this book. Its validity can be seen informally, however, by dividing each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 271, "text": "real variable into intervals of width ∆ and considering the discrete probability dis-\ntribution over these intervals. Taking the limit ∆ → 0 then turns sums into integrals\nand gives the desired result.\n2.2.1 Example distributions\nThere are many forms of probability density that are in widespread use and\nthat are important both in their own right and as building blocks for more complex\nprobabilistic models. The simplest form would be one in which p(x) is a constant,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 272, "text": "independent of x, but this cannot be normalized because the integral in (2.28) will\nbe divergent. Distributions that cannot be normalized are called improper. We can,\nhowever, have the uniform distribution that is constant over a ﬁnite region, say(c;d),\nand zero elsewhere, in which case (2.28) implies\np(x) = 1=(d−c); x ∈(c;d): (2.33)\n34 2. PROBABILITIES\nFigure 2.7 Plots of a uniform distribution over\nthe range (−1;1), shown in red,\nthe exponential distribution with\n\u0015 = 1, shown in blue, and a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 273, "text": "\u0015 = 1, shown in blue, and a\nLaplace distribution with \u0016 = 1\nand \r = 1, shown in green.\n−2 −1 0 1 2 3 4\nx\n0.0\n0.5\n1.0\np(x)\nAnother simple form of density is the exponential distribution given by\np(x|\u0015) =\u0015exp(−\u0015x); x > 0: (2.34)\nA variant of the exponential distribution, known as the Laplace distribution, allows\nthe peak to be moved to a location \u0016and is given by\np(x|\u0016;\r) = 1\n2\r exp\n(\n−|x−\u0016|\n\r\n)\n: (2.35)\nThe constant, exponential, and Laplace distributions are illustrated in Figure 2.7."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 274, "text": "Another important distribution is the Dirac delta function, which is written\np(x|\u0016) =\u000e(x−\u0016): (2.36)\nThis is deﬁned to be zero everywhere except atx= \u0016and to have the property of in-\ntegrating to unity according to (2.28). Informally, we can think of this as an inﬁnitely\nnarrow and inﬁnitely tall spike located atx= \u0016with the property of having unit area.\nFinally, if we have a ﬁnite set of observations of xgiven by D= {x1;:::;x N}then"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 275, "text": "we can use the delta function to construct the empirical distribution given by\np(x|D) = 1\nN\nN∑\nn=1\n\u000e(x−xn); (2.37)\nwhich consists of a Dirac delta function centred on each of the data points. The\nprobability density deﬁned by (2.37) integrates to one as required.Exercise 2.6\n2.2.2 Expectations and covariances\nOne of the most important operations involving probabilities is that of ﬁnding\nweighted averages of functions. The weighted average of some function f(x) under"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 276, "text": "a probability distribution p(x) is called the expectation of f(x) and will be denoted\nby E[f]. For a discrete distribution, it is given by summing over all possible values\nof xin the form\nE[f] =\n∑\nx\np(x)f(x) (2.38)\n2.2. Probability Densities 35\nwhere the average is weighted by the relative probabilities of the different values of\nx. For continuous variables, expectations are expressed in terms of an integration\nwith respect to the corresponding probability density:\nE[f] =\n∫\np(x)f(x) dx: (2.39)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 277, "text": "E[f] =\n∫\np(x)f(x) dx: (2.39)\nIn either case, if we are given a ﬁnite numberN of points drawn from the probability\ndistribution or probability density, then the expectation can be approximated as a\nﬁnite sum over these points:Exercise 2.7\nE[f] ≃ 1\nN\nN∑\nn=1\nf(xn): (2.40)\nThe approximation in (2.40) becomes exact in the limit N →∞.\nSometimes we will be considering expectations of functions of several variables,\nin which case we can use a subscript to indicate which variable is being averaged"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 278, "text": "over, so that for instance\nEx[f(x;y)] (2.41)\ndenotes the average of the functionf(x;y) with respect to the distribution ofx. Note\nthat Ex[f(x;y)] will be a function of y.\nWe can also consider a conditional expectation with respect to a conditional\ndistribution, so that\nEx[f|y] =\n∑\nx\np(x|y)f(x); (2.42)\nwhich is also a function of y. For continuous variables, the conditional expectation\ntakes the form\nEx[f|y] =\n∫\np(x|y)f(x) dx: (2.43)\nThe variance of f(x) is deﬁned by\nvar[f] = E\n["}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 279, "text": "The variance of f(x) is deﬁned by\nvar[f] = E\n[\n(f(x) −E[f(x)])2\n]\n(2.44)\nand provides a measure of how much f(x) varies around its mean value E[f(x)].\nExpanding out the square, we see that the variance can also be written in terms of\nthe expectations of f(x) and f(x)2:Exercise 2.8\nvar[f] = E[f(x)2] −E[f(x)]2: (2.45)\nIn particular, we can consider the variance of the variable xitself, which is given by\nvar[x] = E[x2] −E[x]2: (2.46)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 280, "text": "var[x] = E[x2] −E[x]2: (2.46)\nFor two random variables xand y, the covariance measures the extent to which\nthe two variables vary together and is deﬁned by\ncov[x;y] = Ex;y[{x−E[x]}{y−E[y]}]\n= Ex;y[xy] −E[x]E[y]: (2.47)\n36 2. PROBABILITIES\nFigure 2.8 Plot of a Gaussian distribu-\ntion for a single continuous\nvariable xshowing the mean\n\u0016and the standard deviation\n\u001b.\nN(xj\u0016;\u001b2)\nx\n2\u001b\n\u0016\nIf xand yare independent, then their covariance equals zero.Exercise 2.9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 281, "text": "For two vectors x and y, their covariance is a matrix given by\ncov[x;y] = Ex;y\n[\n{x−E[x]}{yT −E[yT]}\n]\n= Ex;y[xyT] −E[x]E[yT]: (2.48)\nIf we consider the covariance of the components of a vector x with each other, then\nwe use a slightly simpler notation cov[x] ≡cov[x;x].\n2.3. The Gaussian Distribution\nOne of the most important probability distributions for continuous variables is called\nthe normal or Gaussian distribution, and we will make extensive use of this distribu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 282, "text": "tion throughout the rest of the book. For a single real-valued variablex, the Gaussian\ndistribution is deﬁned by\nN\n(\nx|\u0016;\u001b2)\n= 1\n(2\u0019\u001b2)1=2 exp\n{\n− 1\n2\u001b2 (x−\u0016)2\n}\n; (2.49)\nwhich represents a probability density over xgoverned by two parameters: \u0016, called\nthe mean, and \u001b2, called the variance. The square root of the variance, given by\n\u001b, is called the standard deviation, and the reciprocal of the variance, written as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 283, "text": "= 1=\u001b2, is called the precision. We will see the motivation for this terminology\nshortly. Figure 2.8 shows a plot of the Gaussian distribution. Although the form\nof the Gaussian distribution might seem arbitrary, we will see later that it arises\nnaturally from the concept of maximum entropy and from the perspective of theSection 2.5.4\ncentral limit theorem.Section 3.2\nFrom (2.49) we see that the Gaussian distribution satisﬁes\nN(x|\u0016;\u001b2) >0: (2.50)\n2.3. The Gaussian Distribution 37"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 284, "text": "2.3. The Gaussian Distribution 37\nAlso, it is straightforward to show that the Gaussian is normalized, so thatExercise 2.12\n∫∞\n−∞\nN\n(\nx|\u0016;\u001b2)\ndx= 1: (2.51)\nThus, (2.49) satisﬁes the two requirements for a valid probability density.\n2.3.1 Mean and variance\nWe can readily ﬁnd expectations of functions of xunder the Gaussian distribu-\ntion. In particular, the average value of xis given byExercise 2.13\nE[x] =\n∫∞\n−∞\nN\n(\nx|\u0016;\u001b2)\nxdx= \u0016: (2.52)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 285, "text": "E[x] =\n∫∞\n−∞\nN\n(\nx|\u0016;\u001b2)\nxdx= \u0016: (2.52)\nBecause the parameter \u0016represents the average value of xunder the distribution, it\nis referred to as the mean. The integral in (2.52) is known as the ﬁrst-order moment\nof the distribution because it is the expectation of xraised to the power one. We can\nsimilarly evaluate the second-order moment given by\nE[x2] =\n∫∞\n−∞\nN\n(\nx|\u0016;\u001b2)\nx2 dx= \u00162 + \u001b2: (2.53)\nFrom (2.52) and (2.53), it follows that the variance of xis given by\nvar[x] = E[x2] −E[x]2 = \u001b2 (2.54)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 286, "text": "var[x] = E[x2] −E[x]2 = \u001b2 (2.54)\nand hence \u001b2 is referred to as the variance parameter. The maximum of a distribution\nis known as its mode. For a Gaussian, the mode coincides with the mean.Exercise 2.14\n2.3.2 Likelihood function\nSuppose that we have a data set of observations represented as a row vector\nx = (x1;:::;x N), representing N observations of the scalar variable x. Note that\nwe are using the typeface x to distinguish this from a single observation of a D-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 287, "text": "dimensional vector-valued variable, which we represent by a column vector x =\n(x1;:::;x D)T. We will suppose that the observations are drawn independently from\na Gaussian distribution whose mean \u0016and variance \u001b2 are unknown, and we would\nlike to determine these parameters from the data set. The problem of estimating a\ndistribution, given a ﬁnite set of observations, is known as density estimation. It\nshould be emphasized that the problem of density estimation is fundamentally ill-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 288, "text": "posed, because there are inﬁnitely many probability distributions that could have\ngiven rise to the observed ﬁnite data set. Indeed, any distribution p(x) that is non-\nzero at each of the data pointsx1;:::; xN is a potential candidate. Here we constrain\nthe space of distributions to be Gaussians, which leads to a well-deﬁned solution.\nData points that are drawn independently from the same distribution are said to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 289, "text": "be independent and identically distributed, which is often abbreviated to i.i.d. or\nIID. We have seen that the joint probability of two independent events is given by\nthe product of the marginal probabilities for each event separately. Because our data\n38 2. PROBABILITIES\nFigure 2.9 Illustration of the likelihood func-\ntion for the Gaussian distribution\nshown by the red curve. Here\nthe grey points denote a data set\nof values {xn}, and the likelihood\nfunction (2.55) is given by the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 290, "text": "function (2.55) is given by the\nproduct of the corresponding val-\nues of p(x) denoted by the blue\npoints. Maximizing the likelihood\ninvolves adjusting the mean and\nvariance of the Gaussian so as to\nmaximize this product.\nxn\nN\n(\nxn|µ, σ2)\nx\np(x)\nset x is i.i.d., we can therefore write the probability of the data set, given \u0016and \u001b2,\nin the form\np(x|\u0016;\u001b2) =\nN∏\nn=1\nN\n(\nxn|\u0016;\u001b2)\n: (2.55)\nWhen viewed as a function of \u0016and \u001b2, this is called the likelihood function for the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 291, "text": "Gaussian and is interpreted diagrammatically in Figure 2.9.\nOne common approach for determining the parameters in a probability distribu-\ntion using an observed data set, known as maximum likelihood, is to ﬁnd the param-\neter values that maximize the likelihood function. This might appear to be a strange\ncriterion because, from our foregoing discussion of probability theory, it would seem\nmore natural to maximize the probability of the parameters given the data, not the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 292, "text": "probability of the data given the parameters. In fact, these two criteria are related.Section 2.6.2\nTo start with, however, we will determine values for the unknown parameters \u0016\nand \u001b2 in the Gaussian by maximizing the likelihood function (2.55). In practice,\nit is more convenient to maximize the log of the likelihood function. Because the\nlogarithm is a monotonically increasing function of its argument, maximizing the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 293, "text": "log of a function is equivalent to maximizing the function itself. Taking the log not\nonly simpliﬁes the subsequent mathematical analysis, but it also helps numerically\nbecause the product of a large number of small probabilities can easily underﬂow the\nnumerical precision of the computer, and this is resolved by computing the sum of\nthe log probabilities instead. From (2.49) and (2.55), the log likelihood function can\nbe written in the form\nln p\n(\nx|\u0016;\u001b2)\n= − 1\n2\u001b2\nN∑\nn=1\n(xn −\u0016)2 −N"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 294, "text": "ln p\n(\nx|\u0016;\u001b2)\n= − 1\n2\u001b2\nN∑\nn=1\n(xn −\u0016)2 −N\n2 ln \u001b2 −N\n2 ln(2\u0019): (2.56)\nMaximizing (2.56) with respect to \u0016, we obtain the maximum likelihood solution\ngiven byExercise 2.15\n\u0016ML = 1\nN\nN∑\nn=1\nxn; (2.57)\n2.3. The Gaussian Distribution 39\nwhich is the sample mean, i.e., the mean of the observed values {xn}. Similarly,\nmaximizing (2.56) with respect to \u001b2, we obtain the maximum likelihood solution\nfor the variance in the form\n\u001b2\nML = 1\nN\nN∑\nn=1\n(xn −\u0016ML)2; (2.58)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 295, "text": "\u001b2\nML = 1\nN\nN∑\nn=1\n(xn −\u0016ML)2; (2.58)\nwhich is the sample variance measured with respect to the sample mean \u0016ML. Note\nthat we are performing a joint maximization of (2.56) with respect to \u0016and \u001b2, but\nfor a Gaussian distribution, the solution for \u0016decouples from that for \u001b2 so that we\ncan ﬁrst evaluate (2.57) and then subsequently use this result to evaluate (2.58).\n2.3.3 Bias of maximum likelihood\nThe technique of maximum likelihood is widely used in deep learning and forms"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 296, "text": "the foundation for most machine learning algorithms. However, it has some limita-\ntions, which we can illustrate using a univariate Gaussian.\nWe ﬁrst note that the maximum likelihood solutions\u0016ML and \u001b2\nML are functions\nof the data set values x1;:::;x N. Suppose that each of these values has been gen-\nerated independently from a Gaussian distribution whose true parameters are \u0016and\n\u001b2. Now consider the expectations of \u0016ML and \u001b2\nML with respect to these data set"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 297, "text": "ML with respect to these data set\nvalues. It is straightforward to show thatExercise 2.16\nE[\u0016ML] = \u0016 (2.59)\nE[\u001b2\nML] =\n(N −1\nN\n)\n\u001b2: (2.60)\nWe see that, when averaged over data sets of a given size, the maximum likelihood\nsolution for the mean will equal the true mean. However, the maximum likelihood\nestimate of the variance will underestimate the true variance by a factor(N−1)=N.\nThis is an example of a phenomenon called bias in which the estimator of a random"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 298, "text": "quantity is systematically different from the true value. The intuition behind this\nresult is given by Figure 2.10.\nNote that bias arises because the variance is measured relative to the maximum\nlikelihood estimate of the mean, which itself is tuned to the data. Suppose instead\nwe had access to the true mean \u0016and we used this to determine the variance using\nthe estimator\nˆ\u001b2 = 1\nN\nN∑\nn=1\n(xn −\u0016)2: (2.61)\nThen we ﬁnd thatExercise 2.17\nE\n[\nˆ\u001b2]\n= \u001b2; (2.62)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 299, "text": "E\n[\nˆ\u001b2]\n= \u001b2; (2.62)\nwhich is unbiased. Of course, we do not have access to the true mean but only\nto the observed data values. From the result (2.60) it follows that for a Gaussian\ndistribution, the following estimate for the variance parameter is unbiased:\n˜\u001b2 = N\nN −1\u001b2\nML = 1\nN −1\nN∑\nn=1\n(xn −\u0016ML)2: (2.63)\n40 2. PROBABILITIES\nµ\nµ\nµ\nFigure 2.10 Illustration of how bias arises when using maximum likelihood to determine the mean and variance"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 300, "text": "of a Gaussian. The red curves show the true Gaussian distribution from which data is generated, and the three\nblue curves show the Gaussian distributions obtained by ﬁtting to three data sets, each consisting of two data\npoints shown in green, using the maximum likelihood results (2.57) and (2.58). Averaged across the three data\nsets, the mean is correct, but the variance is systematically underestimated because it is measured relative to\nthe sample mean and not relative to the true mean."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 301, "text": "Correcting for the bias of maximum likelihood in complex models such as neural\nnetworks is not so easy, however.\nNote that the bias of the maximum likelihood solution becomes less signiﬁcant\nas the number N of data points increases. In the limit N → ∞ the maximum\nlikelihood solution for the variance equals the true variance of the distribution that\ngenerated the data. In the case of the Gaussian, for anything other than smallN, this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 302, "text": "bias will not prove to be a serious problem. However, throughout this book we will\nbe interested in complex models with many parameters, for which the bias problems\nassociated with maximum likelihood will be much more severe. In fact, the issue of\nbias in maximum likelihood is closely related to the problem of over-ﬁtting.Section 2.6.3\n2.3.4 Linear regression\nWe have seen how the problem of linear regression can be expressed in terms of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 303, "text": "error minimization. Here we return to this example and view it from a probabilisticSection 1.2\nperspective, thereby gaining some insights into error functions and regularization.\nThe goal in the regression problem is to be able to make predictions for the\ntarget variable t given some new value of the input variable x by using a set of\ntraining data comprising N input values x = (x1;:::;x N) and their corresponding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 304, "text": "target values t = (t1;:::;t N). We can express our uncertainty over the value of the\ntarget variable using a probability distribution. For this purpose, we will assume that,\ngiven the value of x, the corresponding value of thas a Gaussian distribution with\na mean equal to the value y(x;w) of the polynomial curve given by (1.1), where w\nare the polynomial coefﬁcients, and a variance \u001b2. Thus, we have\np(t|x;w;\u001b2) = N\n(\nt|y(x;w);\u001b2)\n: (2.64)\nThis is illustrated schematically in Figure 2.11."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 305, "text": "This is illustrated schematically in Figure 2.11.\nWe now use the training data {x;t}to determine the values of the unknown\nparameters w and \u001b2 by maximum likelihood. If the data is assumed to be drawn\n2.3. The Gaussian Distribution 41\nFigure 2.11 Schematic illustration of a Gaus-\nsian conditional distribution for t\ngiven x, deﬁned by (2.64), in\nwhich the mean is given by the\npolynomial function y(x;w), and\nthe variance is given by the pa-\nrameter \u001b2.\nt\nx\nx0\np(t|x0, w, σ2)\ny(x, w)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 306, "text": "rameter \u001b2.\nt\nx\nx0\np(t|x0, w, σ2)\ny(x, w)\nindependently from the distribution (2.64), then the likelihood function is given by\np(t|x;w;\u001b2) =\nN∏\nn=1\nN\n(\ntn|y(xn;w);\u001b2)\n: (2.65)\nAs we did for the simple Gaussian distribution earlier, it is convenient to maximize\nthe logarithm of the likelihood function. Substituting for the Gaussian distribution,\ngiven by (2.49), we obtain the log likelihood function in the form\nln p(t|x;w;\u001b2) = − 1\n2\u001b2\nN∑\nn=1\n{y(xn;w) −tn}2 −N\n2 ln \u001b2 −N\n2 ln(2\u0019): (2.66)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 307, "text": "n=1\n{y(xn;w) −tn}2 −N\n2 ln \u001b2 −N\n2 ln(2\u0019): (2.66)\nConsider ﬁrst the evaluation of the maximum likelihood solution for the polynomial\ncoefﬁcients, which will be denoted by wML. These are determined by maximizing\n(2.66) with respect to w. For this purpose, we can omit the last two terms on the\nright-hand side of (2.66) because they do not depend on w. Also, note that scaling\nthe log likelihood by a positive constant coefﬁcient does not alter the location of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 308, "text": "maximum with respect to w, and so we can replace the coefﬁcient 1=2\u001b2 with 1=2.\nFinally, instead of maximizing the log likelihood, we can equivalently minimize the\nnegative log likelihood. We therefore see that maximizing the likelihood is equiva-\nlent, so far as determining w is concerned, to minimizing the sum-of-squares error\nfunction deﬁned by\nE(w) = 1\n2\nN∑\nn=1\n{y(xn;w) −tn}2 : (2.67)\nThus, the sum-of-squares error function has arisen as a consequence of maximizing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 309, "text": "the likelihood under the assumption of a Gaussian noise distribution.\n42 2. PROBABILITIES\nWe can also use maximum likelihood to determine the variance parameter \u001b2.\nMaximizing (2.66) with respect to \u001b2 givesExercise 2.18\n\u001b2\nML = 1\nN\nN∑\nn=1\n{y(xn;wML) −tn}2 : (2.68)\nNote that we can ﬁrst determine the parameter vector wML governing the mean,\nand subsequently use this to ﬁnd the variance \u001b2\nML as was the case for the simple\nGaussian distribution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 310, "text": "Gaussian distribution.\nHaving determined the parameters w and \u001b2, we can now make predictions for\nnew values of x. Because we now have a probabilistic model, these are expressed\nin terms of the predictive distribution that gives the probability distribution over t,\nrather than simply a point estimate, and is obtained by substituting the maximum\nlikelihood parameters into (2.64) to give\np(t|x;wML;\u001b2\nML) = N\n(\nt|y(x;wML);\u001b2\nML\n)\n: (2.69)\n2.4. Transformation of Densities"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 311, "text": "ML\n)\n: (2.69)\n2.4. Transformation of Densities\nWe turn now to a discussion of how a probability density transforms under a nonlin-\near change of variable. This property will play a crucial role when we discuss a class\nof generative models called normalizing ﬂows. It also highlights that a probabilityChapter 18\ndensity has a different behaviour than a simple function under such transformations.\nConsider a single variable xand suppose we make a change of variables x ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 312, "text": "g(y), then a function f(x) becomes a new function ˜f(y) deﬁned by\n˜f(y) = f(g(y)): (2.70)\nNow consider a probability density px(x), and again change variables using x =\ng(y), giving rise to a density py(y) with respect to the new variable y, where the\nsufﬁxes denote that px(x) and py(y) are different densities. Observations falling in\nthe range (x;x + \u000ex) will, for small values of \u000ex, be transformed into the range\n(y;y + \u000ey), where x = g(y), and px(x)\u000ex ≃py(y)\u000ey. Hence, if we take the limit"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 313, "text": "\u000ex→ 0, we obtain\npy(y) = px(x)\n⏐⏐⏐⏐\ndx\ndy\n⏐\n⏐⏐⏐\n= px(g(y))\n⏐\n⏐⏐⏐\ndg\ndy\n⏐\n⏐⏐⏐: (2.71)\nHere the modulus|-|arises because the derivative dy=dxcould be negative, whereas\nthe density is scaled by the ratio of lengths, which is always positive.\n2.4. Transformation of Densities 43\nThis procedure for transforming densities can be very powerful. Any density\np(y) can be obtained from a ﬁxed density q(x) that is everywhere non-zero by mak-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 314, "text": "ing a nonlinear change of variable y = f(x) in which f(x) is a monotonic function\nso that 0 6 f′(x) <∞.Exercise 2.19\nOne consequence of the transformation property (2.71) is that the concept of the\nmaximum of a probability density is dependent on the choice of variable. Suppose\nf(x) has a mode (i.e., a maximum) at ˆxso that f′(ˆx) = 0. The corresponding mode\nof ˜f(y) will occur for a value ˆyobtained by differentiating both sides of (2.70) with\nrespect to y:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 315, "text": "respect to y:\n˜f′(ˆy) = f′(g(ˆy))g′(ˆy) = 0: (2.72)\nAssuming g′(ˆy) ̸= 0at the mode, then f′(g(ˆy)) = 0 . However, we know that\nf′(ˆx) = 0, and so we see that the locations of the mode expressed in terms of each\nof the variables xand yare related by ˆx= g(ˆy), as one would expect. Thus, ﬁnding\na mode with respect to the variablexis equivalent to ﬁrst transforming to the variable\ny, then ﬁnding a mode with respect to y, and then transforming back to x."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 316, "text": "Now consider the behaviour of a probability density px(x) under the change of\nvariables x= g(y), where the density with respect to the new variable is py(y) and\nis given by (2.71). To deal with the modulus in (2.71) we can write g′(y) = s|g′(y)|\nwhere s∈{−1;+1}. Then (2.71) can be written as\npy(y) = px(g(y))sg′(y)\nwhere we have used 1=s= s. Differentiating both sides with respect to ythen gives\np′\ny(y) = sp′\nx(g(y)){g′(y)}2 + spx(g(y))g′′(y): (2.73)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 317, "text": "x(g(y)){g′(y)}2 + spx(g(y))g′′(y): (2.73)\nDue to the presence of the second term on the right-hand side of (2.73), the rela-\ntionship ˆx = g(ˆy) no longer holds. Thus, the value of xobtained by maximizing\npx(x) will not be the value obtained by transforming to py(y) then maximizing with\nrespect to y and then transforming back to x. This causes modes of densities to be\ndependent on the choice of variables. However, for a linear transformation, the sec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 318, "text": "ond term on the right-hand side of (2.73) vanishes, and so in this case the location of\nthe maximum transforms according to ˆx= g(ˆy).\nThis effect can be illustrated with a simple example, as shown inFigure 2.12. We\nbegin by considering a Gaussian distribution px(x) over xshown by the red curve\nin Figure 2.12. Next we draw a sample of N = 50;000 points from this distribution\nand plot a histogram of their values, which as expected agrees with the distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 319, "text": "px(x). Now consider a nonlinear change of variables from xto ygiven by\nx= g(y) = ln(y) −ln(1 −y) + 5: (2.74)\nThe inverse of this function is given by\ny= g−1 (x) = 1\n1 + exp(−x+ 5); (2.75)\nwhich is a logistic sigmoid function and is shown in Figure 2.12 by the blue curve.\n44 2. PROBABILITIES\nFigure 2.12 Example of the transformation of\nthe mode of a density under a\nnonlinear change of variables, il-\nlustrating the different behaviour\ncompared to a simple function.\n0 5 10\n0\n0.5\n1\ng−1(x)\npx(x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 320, "text": "0 5 10\n0\n0.5\n1\ng−1(x)\npx(x)\npy(y)\ny\nx\nIf we simply transform px(x) as a function of x we obtain the green curve\npx(g(y)) shown in Figure 2.12, and we see that the mode of the density px(x) is\ntransformed via the sigmoid function to the mode of this curve. However, the den-\nsity over ytransforms instead according to (2.71) and is shown by the magenta curve\non the left side of the diagram. Note that this has its mode shifted relative to the mode\nof the green curve."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 321, "text": "of the green curve.\nTo conﬁrm this result, we take our sample of 50;000 values of x, evaluate the\ncorresponding values of yusing (2.75), and then plot a histogram of their values. We\nsee that this histogram matches the magenta curve in Figure 2.12 and not the green\ncurve.\n2.4.1 Multivariate distributions\nWe can extend the result (2.71) to densities deﬁned over multiple variables. Con-\nsider a density p(x) over a D-dimensional variable x = (x1;:::;x D)T, and suppose"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 322, "text": "we transform to a new variable y = (y1;:::;y D)T where x = g(y). Here we will\nlimit ourselves to the case where x and y have the same dimensionality. The trans-\nformed density is then given by the generalization of (2.71) in the form\npy(y) = px(x) |detJ| (2.76)\nwhere J is the Jacobian matrix whose elements are given by the partial derivatives\nJij = @gi=@yj, so that\nJ =\n\n\n@g1\n@y1\n::: @g1\n@yD\n..\n. … .\n.\n.\n@gD\n@y1\n::: @gD\n@yD\n\n\n: (2.77)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 323, "text": "..\n. … .\n.\n.\n@gD\n@y1\n::: @gD\n@yD\n\n\n: (2.77)\nIntuitively, we can view the change of variables as expanding some regions of space\nand contracting others, with an inﬁnitesimal region∆x around a point x being trans-\nformed to a region ∆y around the point y = g(x). The absolute value of the deter-\nminant of the Jacobian represents the ratio of these volumes and is the same factor\n2.4. Transformation of Densities 45\nx1\nx2\ny1\ny2\nx1\nx2\ny1\ny2\nx1\nx2\ny1\ny2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 324, "text": "x1\nx2\ny1\ny2\nx1\nx2\ny1\ny2\nx1\nx2\ny1\ny2\nFigure 2.13 Illustration of the effect of a change of variables on a probability distribution in two dimensions.\nThe left column shows the transforming of the variables whereas the middle and right columns show the corre-\nsponding effects on a Gaussian distribution and on samples from that distribution, respectively.\nthat arises when changing variables within an integral. The formula (2.77) follows"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 325, "text": "from the fact that the probability mass in region ∆x is the same as the probability\nmass in ∆y . Once again, we take the modulus to ensure that the density is non-\nnegative.\nWe can illustrate this by applying a change of variables to a Gaussian distribution\nin two dimensions, as shown in the top row in Figure 2.13. Here the transformation\nfrom x to y is given byExercise 2.20\ny1 = x1 + tanh(5x1) (2.78)\ny2 = x2 + tanh(5x2) + x3\n1\n3 : (2.79)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 326, "text": "y2 = x2 + tanh(5x2) + x3\n1\n3 : (2.79)\nAlso shown on the bottom row are samples from a Gaussian distribution in x-space\nalong with the corresponding transformed samples in y-space.\n46 2. PROBABILITIES\n2.5. Information Theory\nProbability theory forms the basis for another important framework called informa-\ntion theory, which quantiﬁes the information present in a data set and which plays\nan important role in machine learning. Here we give a brief introduction to some of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 327, "text": "the key elements of information theory that we will need later in the book, including\nthe important concept of entropy in its various forms. For a more comprehensive in-\ntroduction to information theory, with connections to machine learning, see MacKay\n(2003).\n2.5.1 Entropy\nWe begin by considering a discrete random variable x and we ask how much\ninformation is received when we observe a speciﬁc value for this variable. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 328, "text": "amount of information can be viewed as the ‘degree of surprise’ on learning the\nvalue of x. If we are told that a highly improbable event has just occurred, we will\nhave received more information than if we were told that some very likely event has\njust occurred, and if we knew that the event was certain to happen, we would receive\nno information. Our measure of information content will therefore depend on the\nprobability distribution p(x), and so we look for a quantity h(x) that is a monotonic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 329, "text": "function of the probabilityp(x) and that expresses the information content. The form\nof h(-)can be found by noting that if we have two events xand ythat are unrelated,\nthen the information gained from observing both of them should be the sum of the\ninformation gained from each of them separately, so that h(x;y) = h(x) + h(y).\nTwo unrelated events are statistically independent and so p(x;y) = p(x)p(y). From\nthese two relationships, it is easily shown that h(x) must be given by the logarithm"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 330, "text": "of p(x) and so we haveExercise 2.21\nh(x) = −log2 p(x) (2.80)\nwhere the negative sign ensures that information is positive or zero. Note that low\nprobability events xcorrespond to high information content. The choice of base for\nthe logarithm is arbitrary, and for the moment we will adopt the convention prevalent\nin information theory of using logarithms to the base of 2. In this case, as we will\nsee shortly, the units of h(x) are bits (‘binary digits’)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 331, "text": "Now suppose that a sender wishes to transmit the value of a random variable to\na receiver. The average amount of information that they transmit in the process is\nobtained by taking the expectation of (2.80) with respect to the distributionp(x) and\nis given by\nH[x] = −\n∑\nx\np(x) log2 p(x): (2.81)\nThis important quantity is called the entropy of the random variable x. Note that\nlim\u000f→0 (\u000fln \u000f) = 0 and so we will take p(x) lnp(x) = 0 whenever we encounter a\nvalue for xsuch that p(x) = 0."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 332, "text": "value for xsuch that p(x) = 0.\nSo far, we have given a rather heuristic motivation for the deﬁnition of informa-\ntion (2.80) and the corresponding entropy (2.81). We now show that these deﬁnitions\n2.5. Information Theory 47\nindeed possess useful properties. Consider a random variablexhaving eight possible\nstates, each of which is equally likely. To communicate the value of xto a receiver,\nwe would need to transmit a message of length 3 bits. Notice that the entropy of this\nvariable is given by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 333, "text": "variable is given by\nH[x] = −8 ×1\n8 log2\n1\n8 = 3 bits:\nNow consider an example (Cover and Thomas, 1991) of a variable having eight\npossible states {a;b;c;d;e;f;g;h }for which the respective probabilities are given\nby (1\n2 ;1\n4 ;1\n8 ; 1\n16 ; 1\n64 ; 1\n64 ; 1\n64 ; 1\n64 ). The entropy in this case is given by\nH[x] = −1\n2 log2\n1\n2 −1\n4 log2\n1\n4 −1\n8 log2\n1\n8 − 1\n16 log2\n1\n16 − 4\n64 log2\n1\n64 = 2 bits:\nWe see that the nonuniform distribution has a smaller entropy than the uniform one,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 334, "text": "and we will gain some insight into this shortly when we discuss the interpretation of\nentropy in terms of disorder. For the moment, let us consider how we would transmit\nthe identity of the variable’s state to a receiver. We could do this, as before, using\na 3-bit number. However, we can take advantage of the nonuniform distribution by\nusing shorter codes for the more probable events, at the expense of longer codes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 335, "text": "for the less probable events, in the hope of getting a shorter average code length.\nThis can be done by representing the states {a;b;c;d;e;f;g;h} using, for instance,\nthe following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, and\n111111. The average length of the code that has to be transmitted is then\naverage code length = 1\n2 ×1 + 1\n4 ×2 + 1\n8 ×3 + 1\n16 ×4 + 4× 1\n64 ×6 = 2 bits;\nwhich again is the same as the entropy of the random variable. Note that shorter code"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 336, "text": "strings cannot be used because it must be possible to disambiguate a concatenation\nof such strings into its component parts. For instance, 11001110 decodes uniquely\ninto the state sequence c, a, d. This relation between entropy and shortest coding\nlength is a general one. The noiseless coding theorem (Shannon, 1948) states that\nthe entropy is a lower bound on the number of bits needed to transmit the state of a\nrandom variable."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 337, "text": "random variable.\nFrom now on, we will switch to the use of natural logarithms in deﬁning entropy,\nas this will provide a more convenient link with ideas elsewhere in this book. In this\ncase, the entropy is measured in units of nats (from ‘natural logarithm’) instead of\nbits, which differ simply by a factor of ln 2.\n2.5.2 Physics perspective\nWe have introduced the concept of entropy in terms of the average amount of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 338, "text": "information needed to specify the state of a random variable. In fact, the concept of\nentropy has much earlier origins in physics where it was introduced in the context\nof equilibrium thermodynamics and later given a deeper interpretation as a measure\nof disorder through developments in statistical mechanics. We can understand this\nalternative view of entropy by considering a set of N identical objects that are to be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 339, "text": "divided amongst a set of bins, such that there are ni objects in the ith bin. Consider\n48 2. PROBABILITIES\nthe number of different ways of allocating the objects to the bins. There are N\nways to choose the ﬁrst object, (N −1) ways to choose the second object, and\nso on, leading to a total of N! ways to allocate all N objects to the bins, where N!\n(pronounced ‘Nfactorial’) denotes the productN×(N−1)×---×2×1. However,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 340, "text": "we do not wish to distinguish between rearrangements of objects within each bin. In\nthe ith bin there are ni! ways of reordering the objects, and so the total number of\nways of allocating the N objects to the bins is given by\nW = N!∏\nini!; (2.82)\nwhich is called the multiplicity. The entropy is then deﬁned as the logarithm of the\nmultiplicity scaled by a constant factor 1=Nso that\nH = 1\nN ln W = 1\nN ln N! − 1\nN\n∑\ni\nln ni!: (2.83)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 341, "text": "H = 1\nN ln W = 1\nN ln N! − 1\nN\n∑\ni\nln ni!: (2.83)\nWe now consider the limit N →∞, in which the fractions ni=Nare held ﬁxed, and\napply Stirling’s approximation:\nln N! ≃Nln N −N; (2.84)\nwhich gives\nH = − lim\nN→∞\n∑\ni\n(ni\nN\n)\nln\n(ni\nN\n)\n= −\n∑\ni\npiln pi (2.85)\nwhere we have used ∑\nini = N. Here pi = limN→∞ (ni=N) is the probability of\nan object being assigned to theith bin. In physics terminology, the speciﬁc allocation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 342, "text": "of objects into bins is called a microstate, and the overall distribution of occupation\nnumbers, expressed through the ratiosni=N, is called amacrostate. The multiplicity\nW, which expresses the number of microstates in a given macrostate, is also known\nas the weight of the macrostate.\nWe can interpret the bins as the statesxi of a discrete random variableX, where\np(X = xi) = pi. The entropy of the random variable X is then\nH[p] = −\n∑\ni\np(xi) lnp(xi): (2.86)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 343, "text": "H[p] = −\n∑\ni\np(xi) lnp(xi): (2.86)\nDistributions p(xi) that are sharply peaked around a few values will have a relatively\nlow entropy, whereas those that are spread more evenly across many values will have\nhigher entropy, as illustrated in Figure 2.14.\nBecause 0 6 pi 6 1, the entropy is non-negative, and it will equal its minimum\nvalue of 0 when one of the pi = 1 and all other pj̸=i= 0. The maximum entropy"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 344, "text": "conﬁguration can be found by maximizing H using a Lagrange multiplier to enforceAppendix C\nthe normalization constraint on the probabilities. Thus, we maximize\n˜H = −\n∑\ni\np(xi) lnp(xi) + \u0015\n(∑\ni\np(xi) −1\n)\n(2.87)\n2.5. Information Theory 49\nprobabilities\nH = 1.77\n0\n0.25\n0.5\nprobabilities\nH = 3.09\n0\n0.25\n0.5\nFigure 2.14 Histograms of two probability distributions over 30 bins illustrating the higher value of the entropy"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 345, "text": "H for the broader distribution. The largest entropy would arise from a uniform distribution which would give\nH = −ln(1=30) = 3:40.\nfrom which we ﬁnd that all of the p(xi) are equal and are given by p(xi) = 1 =M\nwhere M is the total number of states xi. The corresponding value of the entropy\nis then H = ln M. This result can also be derived from Jensen’s inequality (to beExercise 2.22\ndiscussed shortly). To verify that the stationary point is indeed a maximum, we canExercise 2.23"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 346, "text": "evaluate the second derivative of the entropy, which gives\n@˜H\n@p(xi)@p(xj) = −Iij\n1\npi\n(2.88)\nwhere Iij are the elements of the identity matrix. We see that these values are all\nnegative and, hence, the stationary point is indeed a maximum.\n2.5.3 Differential entropy\nWe can extend the deﬁnition of entropy to include distributions p(x) over con-\ntinuous variables xas follows. First divide xinto bins of width ∆. Then, assuming"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 347, "text": "that p(x) is continuous, the mean value theorem (Weisstein, 1999) tells us that, for\neach such bin, there must exist a valuexi in the range i∆ 6 xi 6 (i+ 1)∆ such that\n∫(i+1)\u0001\ni\u0001\np(x) dx= p(xi)∆: (2.89)\nWe can now quantize the continuous variablexby assigning any valuexto the value\nxi whenever xfalls in the ith bin. The probability of observing the value xi is then\n50 2. PROBABILITIES\np(xi)∆. This gives a discrete distribution for which the entropy takes the form\nH\u0001 = −\n∑\ni"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 348, "text": "H\u0001 = −\n∑\ni\np(xi)∆ ln (p(xi)∆) = −\n∑\ni\np(xi)∆ ln p(xi) −ln ∆ (2.90)\nwhere we have used∑\nip(xi)∆ = 1, which follows from (2.89) and (2.25). We now\nomit the second term −ln ∆ on the right-hand side of (2.90), since it is independent\nof p(x), and then consider the limit ∆ → 0. The ﬁrst term on the right-hand side of\n(2.90) will approach the integral of p(x) lnp(x) in this limit so that\nlim\n\u0001→0\n{\n−\n∑\ni\np(xi)∆ ln p(xi)\n}\n= −\n∫\np(x) lnp(x) dx (2.91)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 349, "text": "∑\ni\np(xi)∆ ln p(xi)\n}\n= −\n∫\np(x) lnp(x) dx (2.91)\nwhere the quantity on the right-hand side is called the differential entropy. We see\nthat the discrete and continuous forms of the entropy differ by a quantityln ∆, which\ndiverges in the limit ∆ → 0. This reﬂects that specifying a continuous variable\nvery precisely requires a large number of bits. For a density deﬁned over multiple\ncontinuous variables, denoted collectively by the vectorx, the differential entropy is\ngiven by\nH[x] = −\n∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 350, "text": "given by\nH[x] = −\n∫\np(x) lnp(x) dx: (2.92)\n2.5.4 Maximum entropy\nWe saw for discrete distributions that the maximum entropy conﬁguration cor-\nresponds to a uniform distribution of probabilities across the possible states of the\nvariable. Let us now consider the corresponding result for a continuous variable. If\nthis maximum is to be well deﬁned, it will be necessary to constrain the ﬁrst and\nsecond moments of p(x) and to preserve the normalization constraint. We therefore"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 351, "text": "maximize the differential entropy with the three constraints:\n∫∞\n−∞\np(x) dx = 1 (2.93)\n∫∞\n−∞\nxp(x) dx = \u0016 (2.94)\n∫∞\n−∞\n(x−\u0016)2p(x) dx = \u001b2: (2.95)\nThe constrained maximization can be performed using Lagrange multipliers so thatAppendix C\nwe maximize the following functional with respect to p(x):\n−\n∫∞\n−∞\np(x) lnp(x) dx+ \u00151\n(∫∞\n−∞\np(x) dx−1\n)\n+ \u00152\n(∫∞\n−∞\nxp(x) dx−\u0016\n)\n+ \u00153\n(∫∞\n−∞\n(x−\u0016)2p(x) dx−\u001b2\n)\n: (2.96)\n2.5. Information Theory 51"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 352, "text": ")\n: (2.96)\n2.5. Information Theory 51\nUsing the calculus of variations, we set the derivative of this functional to zero givingAppendix B\np(x) = exp\n{\n−1 + \u00151 + \u00152x+ \u00153(x−\u0016)2}\n: (2.97)\nThe Lagrange multipliers can be found by back-substitution of this result into the\nthree constraint equations, leading ﬁnally to the result:Exercise 2.24\np(x) = 1\n(2\u0019\u001b2)1=2 exp\n{\n−(x−\u0016)2\n2\u001b2\n}\n; (2.98)\nand so the distribution that maximizes the differential entropy is the Gaussian. Note"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 353, "text": "that we did not constrain the distribution to be non-negative when we maximized the\nentropy. However, because the resulting distribution is indeed non-negative, we see\nwith hindsight that such a constraint is not necessary.\nIf we evaluate the differential entropy of the Gaussian, we obtainExercise 2.25\nH[x] = 1\n2\n{\n1 + ln(2\u0019\u001b2)\n}\n: (2.99)\nThus, we see again that the entropy increases as the distribution becomes broader,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 354, "text": "i.e., as \u001b2 increases. This result also shows that the differential entropy, unlike the\ndiscrete entropy, can be negative, becauseH(x) <0 in (2.99) for \u001b2 <1=(2\u0019e).\n2.5.5 Kullback–Leibler divergence\nSo far in this section, we have introduced a number of concepts from informa-\ntion theory, including the key notion of entropy. We now start to relate these ideas\nto machine learning. Consider some unknown distribution p(x), and suppose that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 355, "text": "we have modelled this using an approximating distribution q(x). If we use q(x) to\nconstruct a coding scheme for transmitting values ofx to a receiver, then the average\nadditional amount of information (in nats) required to specify the value ofx (assum-\ning we choose an efﬁcient coding scheme) as a result of using q(x) instead of the\ntrue distribution p(x) is given by\nKL(p∥q) = −\n∫\np(x) lnq(x) dx −\n(\n−\n∫\np(x) lnp(x) dx\n)\n= −\n∫\np(x) ln\n{ q(x)\np(x)\n}\ndx: (2.100)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 356, "text": ")\n= −\n∫\np(x) ln\n{ q(x)\np(x)\n}\ndx: (2.100)\nThis is known as the relative entropy or Kullback–Leibler divergence, or KL diver-\ngence (Kullback and Leibler, 1951), between the distributions p(x) and q(x). Note\nthat it is not a symmetrical quantity, that is to say KL(p∥q) ̸≡KL(q∥p).\nWe now show that the Kullback–Leibler divergence satisﬁesKL(p∥q) > 0 with\nequality if, and only if, p(x) = q(x). To do this we ﬁrst introduce the concept of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 357, "text": "convex functions. A function f(x) is said to be convex if it has the property that\nevery chord lies on or above the function, as shown in Figure 2.15.\nAny value of x in the interval from x = a to x = b can be written in the\nform \u0015a+ (1 −\u0015)b where 0 6 \u0015 6 1. The corresponding point on the chord\n52 2. PROBABILITIES\nFigure 2.15 A convex function f(x) is one for which ev-\nery chord (shown in blue) lies on or above\nthe function (shown in red).\nxa bx\u0015\nchord\nx\u0015\nf(x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 358, "text": "the function (shown in red).\nxa bx\u0015\nchord\nx\u0015\nf(x)\nis given by \u0015f(a) + (1 −\u0015)f(b), and the corresponding value of the function is\nf(\u0015a+ (1 −\u0015)b). Convexity then implies\nf(\u0015a+ (1 −\u0015)b) 6 \u0015f(a) + (1−\u0015)f(b): (2.101)\nThis is equivalent to the requirement that the second derivative of the function be\neverywhere positive. Examples of convex functions arexln x(for x> 0) and x2. AExercise 2.32\nfunction is called strictly convex if the equality is satisﬁed only for \u0015= 0 and \u0015= 1."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 359, "text": "If a function has the opposite property, namely that every chord lies on or below the\nfunction, it is called concave, with a corresponding deﬁnition for strictly concave. If\na function f(x) is convex, then −f(x) will be concave.\nUsing the technique of proof by induction, we can show from (2.101) that aExercise 2.33\nconvex function f(x) satisﬁes\nf\n(M∑\ni=1\n\u0015ixi\n)\n6\nM∑\ni=1\n\u0015if(xi) (2.102)\nwhere \u0015i > 0 and ∑\ni\u0015i = 1, for any set of points{xi}. The result (2.102) is known"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 360, "text": "as Jensen’s inequality. If we interpret the \u0015i as the probability distribution over a\ndiscrete variable xtaking the values {xi}, then (2.102) can be written\nf(E[x]) 6 E[f(x)] (2.103)\nwhere E[-]denotes the expectation. For continuous variables, Jensen’s inequality\ntakes the form\nf\n(∫\nxp(x) dx\n)\n6\n∫\nf(x)p(x) dx: (2.104)\nWe can apply Jensen’s inequality in the form (2.104) to the Kullback–Leibler\ndivergence (2.100) to give\nKL(p∥q) = −\n∫\np(x) ln\n{ q(x)\np(x)\n}\ndx > −ln\n∫\nq(x) dx = 0 (2.105)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 361, "text": "{ q(x)\np(x)\n}\ndx > −ln\n∫\nq(x) dx = 0 (2.105)\n2.5. Information Theory 53\nwhere we have used −ln x is a convex function, together with the normalization\ncondition\n∫\nq(x) dx = 1. In fact, −ln xis a strictly convex function, so the equality\nwill hold if, and only if, q(x) = p(x) for all x. Thus, we can interpret the Kullback–\nLeibler divergence as a measure of the dissimilarity of the two distributionsp(x) and\nq(x).\nWe see that there is an intimate relationship between data compression and den-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 362, "text": "sity estimation (i.e., the problem of modelling an unknown probability distribution)\nbecause the most efﬁcient compression is achieved when we know the true distri-\nbution. If we use a distribution that is different from the true one, then we must\nnecessarily have a less efﬁcient coding, and on average the additional information\nthat must be transmitted is (at least) equal to the Kullback–Leibler divergence be-\ntween the two distributions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 363, "text": "tween the two distributions.\nSuppose that data is being generated from an unknown distributionp(x) that we\nwish to model. We can try to approximate this distribution using some parametric\ndistribution q(x|\u0012), governed by a set of adjustable parameters \u0012. One way to de-\ntermine \u0012is to minimize the Kullback–Leibler divergence betweenp(x) and q(x|\u0012)\nwith respect to\u0012. We cannot do this directly because we do not knowp(x). Suppose,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 364, "text": "however, that we have observed a ﬁnite set of training points xn, for n= 1;:::;N ,\ndrawn from p(x). Then the expectation with respect to p(x) can be approximated by\na ﬁnite sum over these points, using (2.40), so that\nKL(p∥q) ≃ 1\nN\nN∑\nn=1\n{\n−ln q(xn|\u0012) + lnp(xn)\n}\n: (2.106)\nThe second term on the right-hand side of (2.106) is independent of \u0012, and the ﬁrst\nterm is the negative log likelihood function for \u0012under the distribution q(x|\u0012) eval-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 365, "text": "uated using the training set. Thus, we see that minimizing this Kullback–Leibler\ndivergence is equivalent to maximizing the log likelihood function.Exercise 2.34\n2.5.6 Conditional entropy\nNow consider the joint distribution between two sets of variables x and y given\nby p(x;y) from which we draw pairs of values of x and y. If a value of x is already\nknown, then the additional information needed to specify the corresponding value of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 366, "text": "y is given by−ln p(y|x). Thus the average additional information needed to specify\ny can be written as\nH[y|x] =−\n∫∫\np(y;x) lnp(y|x) dy dx; (2.107)\nwhich is called the conditional entropy of y given x. It is easily seen, using the\nproduct rule, that the conditional entropy satisﬁes the relation:Exercise 2.35\nH[x;y] = H[y|x] + H[x] (2.108)\nwhere H[x;y] is the differential entropy of p(x;y) and H[x] is the differential en-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 367, "text": "tropy of the marginal distribution p(x). Thus, the information needed to describe x\nand y is given by the sum of the information needed to describe x alone plus the\nadditional information required to specify y given x.\n54 2. PROBABILITIES\n2.5.7 Mutual information\nWhen two variables x and y are independent, their joint distribution will factor-\nize into the product of their marginals p(x;y) = p(x)p(y). If the variables are not"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 368, "text": "independent, we can gain some idea of whether they are ‘close’ to being independent\nby considering the Kullback–Leibler divergence between the joint distribution and\nthe product of the marginals, given by\nI[x;y] ≡ KL(p(x;y)∥p(x)p(y))\n= −\n∫∫\np(x;y) ln\n(p(x)p(y)\np(x;y)\n)\ndx dy; (2.109)\nwhich is called the mutual information between the variables x and y. From the\nproperties of the Kullback–Leibler divergence, we see that I[x;y] > 0 with equal-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 369, "text": "ity if, and only if, x and y are independent. Using the sum and product rules of\nprobability, we see that the mutual information is related to the conditional entropy\nthroughExercise 2.38\nI[x;y] = H[x] −H[x|y] = H[y] −H[y|x]: (2.110)\nThus, the mutual information represents the reduction in the uncertainty about x by\nvirtue of being told the value of y (or vice versa). From a Bayesian perspective, we\ncan view p(x) as the prior distribution for x and p(x|y) as the posterior distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 370, "text": "after we have observed new data y. The mutual information therefore represents the\nreduction in uncertainty about x as a consequence of the new observation y.\n2.6.\nBayesian Probabilities\nWhen we considered the bent coin in Figure 2.2, we introduced the concept of prob-\nability in terms of the frequencies of random, repeatable events, such as the prob-\nability of the coin landing concave side up. We will refer to this as the classical"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 371, "text": "or frequentist interpretation of probability. We also introduced the more general\nBayesian view, in which probabilities provide a quantiﬁcation of uncertainty. In this\ncase, our uncertainty is whether the concave side of the coin is heads or tails.\nThe use of probability to represent uncertainty is not an ad hoc choice but is\ninevitable if we are to respect common sense while making rational and coherent\ninferences. For example, Cox (1946) showed that if numerical values are used to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 372, "text": "represent degrees of belief, then a simple set of axioms encoding common sense\nproperties of such beliefs leads uniquely to a set of rules for manipulating degrees of\nbelief that are equivalent to the sum and product rules of probability. It is therefore\nnatural to refer to these quantities as (Bayesian) probabilities.\nFor the bent coin we assumed, in the absence of further information, that the\nprobability of the concave side of the coin being heads is 0.5. Now suppose we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 373, "text": "are told the results of ﬂipping the coin a few times. Intuitively, it seems that this\nshould provide us with some information as to whether the concave side is heads.\nFor instance, suppose we see many more ﬂips that land tails than land heads. Given\n2.6. Bayesian Probabilities 55\nthat the coin is more likely to land concave side up, this provides evidence to suggest\nthat the concave side is more likely to be tails. In fact, this intuition is correct, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 374, "text": "furthermore, we can quantify this using the rules of probability. Bayes’ theorem nowExercise 2.40\nacquires a new signiﬁcance, because it allows us to convert the prior probability for\nthe concave side being heads into a posterior probability by incorporating the data\nprovided by the coin ﬂips. Moreover, this process is iterative, meaning the posterior\nprobability becomes the prior for incorporating data from further coin ﬂips."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 375, "text": "One aspect of the Bayesian viewpoint is that the inclusion of prior knowledge\narises naturally. Suppose, for instance, that a fair-looking coin is tossed three times\nand lands heads each time. The maximum likelihood estimate of the probability\nof landing heads would give 1, implying that all future tosses will land heads! BySection 3.1.2\ncontrast, a Bayesian approach with any reasonable prior will lead to a less extreme\nconclusion.\n2.6.1 Model parameters"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 376, "text": "conclusion.\n2.6.1 Model parameters\nThe Bayesian perspective provides valuable insights into several aspects of ma-\nchine learning, and we can illustrate these using the sine curve regression example.Section 1.2\nHere we denote the training data set by D. We have already seen in the context of\nlinear regression that the parameters can be chosen using maximum likelihood, in\nwhich w is set to the value that maximizes the likelihood function p(D|w). This"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 377, "text": "corresponds to choosing the value of w for which the probability of the observed\ndata set is maximized. In the machine learning literature, the negative log of the\nlikelihood function is called an error function. Because the negative logarithm is a\nmonotonically decreasing function, maximizing the likelihood is equivalent to min-\nimizing the error. This leads to a speciﬁc choice of parameter values, denoted wML,\nwhich are then used to make predictions for new data."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 378, "text": "We have seen that different choices of training data set, for example containing\ndifferent numbers of data points, give rise to different solutions for wML. From a\nBayesian perspective, we can also use the machinery of probability theory to describe\nthis uncertainty in the model parameters. We can capture our assumptions about w,\nbefore observing the data, in the form of a prior probability distribution p(w). The\neffect of the observed data Dis expressed through the likelihood function p(D|w),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 379, "text": "and Bayes’ theorem now takes the form\np(w|D) = p(D|w)p(w)\np(D) ; (2.111)\nwhich allows us to evaluate the uncertainty in w after we have observed Din the\nform of the posterior probability p(w|D).\nIt is important to emphasize that the quantity p(D|w) is called the likelihood\nfunction when it is viewed as a function of the parameter vector w, and it expresses\nhow probable the observed data set is for different values of w. Note that the likeli-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 380, "text": "hood p(D|w) is not a probability distribution overw, and its integral with respect to\nw does not (necessarily) equal one.\nGiven this deﬁnition of likelihood, we can state Bayes’ theorem in words:\nposterior ∝likelihood ×prior (2.112)\n56 2. PROBABILITIES\nwhere all of these quantities are viewed as functions of w. The denominator in\n(2.111) is the normalization constant, which ensures that the posterior distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 381, "text": "on the left-hand side is a valid probability density and integrates to one. Indeed, by\nintegrating both sides of (2.111) with respect to w, we can express the denominator\nin Bayes’ theorem in terms of the prior distribution and the likelihood function:\np(D) =\n∫\np(D|w)p(w) dw: (2.113)\nIn both the Bayesian and frequentist paradigms, the likelihood function p(D|w)\nplays a central role. However, the manner in which it is used is fundamentally dif-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 382, "text": "ferent in the two approaches. In a frequentist setting, w is considered to be a ﬁxed\nparameter, whose value is determined by some form of ‘estimator’, and error bars on\nthis estimate are determined (conceptually, at least) by considering the distribution\nof possible data sets D. By contrast, from the Bayesian viewpoint there is only a\nsingle data set D(namely the one that is actually observed), and the uncertainty in\nthe parameters is expressed through a probability distribution over w."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 383, "text": "2.6.2 Regularization\nWe can use this Bayesian perspective to gain insight into the technique of regu-\nlarization that was used in the sine curve regression example to reduce over-ﬁtting.Section 1.2.5\nInstead of choosing the model parameters by maximizing the likelihood function\nwith respect tow, we can maximize the posterior probability (2.111). This technique\nis called the maximum a posteriori estimate, or simply MAP estimate. Equivalently,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 384, "text": "we can minimize the negative log of the posterior probability. Taking negative logs\nof both sides of (2.111), we have\n−ln p(w|D) = −ln p(D|w) −ln p(w) + lnp(D): (2.114)\nThe ﬁrst term on the right-hand side of (2.114) is the usual log likelihood. The third\nterm can be omitted since it does not depend on w. The second term takes the form\nof a function of w, which is added to the log likelihood, and we can recognize this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 385, "text": "as a form of regularization. To make this more explicit, suppose we choose the prior\ndistribution p(w) to be the product of independent zero-mean Gaussian distributions\nfor each of the elements of w such that each has the same variance s2 so that\np(w|s) =\nM∏\ni=0\nN(wi|0;s2) =\nM∏\ni=0\n( 1\n2\u0019s2\n)1=2\nexp\n{\n−w2\ni\n2s2\n}\n: (2.115)\nSubstituting into (2.114), we obtain\n−ln p(w|D) = −ln p(D|w) + 1\n2s2\nM∑\ni=0\nw2\ni + const: (2.116)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 386, "text": "2s2\nM∑\ni=0\nw2\ni + const: (2.116)\nIf we consider the particular case of the linear regression model whose log likeli-\nhood is given by (2.66), then we ﬁnd that maximizing the posterior distribution is\nequivalent to minimizing the functionExercise 2.41\n2.6. Bayesian Probabilities 57\nE(w) = 1\n2\u001b2\nN∑\nn=1\n{y(xn;w) −tn}2 + 1\n2s2 wTw: (2.117)\nWe see that this takes the form of the regularized sum-of-squares error function en-\ncountered earlier in the form (1.4).\n2.6.3 Bayesian machine learning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 387, "text": "2.6.3 Bayesian machine learning\nThe Bayesian perspective has allowed us to motivate the use of regularization\nand to derive a speciﬁc form for the regularization term. However, the use of Bayes’\ntheorem alone does not constitute a truly Bayesian treatment of machine learning\nsince it is still ﬁnding a single solution for w and does not therefore take account\nof uncertainty in the value of w. Suppose we have a training data set Dand our"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 388, "text": "goal is to predict some target variable tgiven a new input value x. We are therefore\ninterested in the distribution of t given both x and D. From the sum and product\nrules of probability, we have\np(t|x;D) =\n∫\np(t|x;w)p(w|D) dw: (2.118)\nWe see that the prediction is obtained by taking a weighted averagep(t|x;w) over all\npossible values of w in which the weighting function is given by the posterior prob-\nability distribution p(w|D). The key difference that distinguishes Bayesian methods"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 389, "text": "is this integration over the space of parameters. By contrast, conventional frequentist\nmethods use point estimates for parameters obtained by optimizing a loss function\nsuch as a regularized sum-of-squares.\nThis fully Bayesian treatment of machine learning offers some powerful in-\nsights. For example, the problem of over-ﬁtting, encountered earlier in the context\nof polynomial regression, is an example of a pathology arising from the use of max-Section 1.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 390, "text": "imum likelihood, and does not arise when we marginalize over parameters using the\nBayesian approach. Similarly, we may have multiple potential models that we could\nuse to solve a given problem, such as polynomials of different orders in the regres-\nsion example. A maximum likelihood approach simply picks the model that gives\nthe highest probability of the data, but this favours ever more complex models, lead-\ning to over-ﬁtting. A fully Bayesian treatment involves averaging over all possible"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 391, "text": "models, with the contribution of each model weighted by its posterior probability.Section 9.6\nMoreover, this probability is typically highest for models of intermediate complexity.\nVery simple models (such as polynomials of low order) have low probability as they\nare unable to ﬁt the data well, whereas very complex models (such as polynomials\nof very high order) also have low probability because the Bayesian integration over"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 392, "text": "parameters automatically and elegantly penalizes complexity. For a comprehensive\noverview of Bayesian methods applied to machine learning, including neural net-\nworks, see Bishop (2006).\nUnfortunately, there is a major drawback with the Bayesian framework, and\nthis is apparent in (2.118), which involves integrating over the space of parameters.\nModern deep learning models can have millions or billions of parameters and even"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 393, "text": "simple approximations to such integrals are typically infeasible. In fact, given a\n58 2. PROBABILITIES\nlimited compute budget and an ample source of training data, it will often be better to\napply maximum likelihood techniques, generally augmented with one or more forms\nof regularization, to a large neural network rather than apply a Bayesian treatment to\na much smaller model.\nExercises\n2.1 (?) In the cancer screening example, we used a prior probability of cancer of p(C ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 394, "text": "1) = 0 :01. In reality, the prevalence of cancer is generally very much lower. Con-\nsider a situation in which p(C = 1) = 0 :001, and recompute the probability of\nhaving cancer given a positive test p(C = 1|T = 1). Intuitively, the result can ap-\npear surprising to many people since the test seems to have high accuracy and yet a\npositive test still leads to a low probability of having cancer.\n2.2 (??) Deterministic numbers satisfy the property of transitivity, so that if x > yand"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 395, "text": "y > zthen it follows that x > z. When we go to random numbers, however, this\nproperty need no longer apply. Figure 2.16 shows a set of four cubical dice that have\nbeen arranged in a cyclic order. Show that each of the four dice has a 2/3 probability\nof rolling a higher number than the previous die in the cycle. Such dice are known\nas non-transitive dice, and the speciﬁc examples shown here are called Efron dice.\nFigure 2.16 An example of non-transitive cu-\nbical dice, in which each die"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 396, "text": "bical dice, in which each die\nhas been ‘ﬂattened’ to reveal the\nnumbers on each of the faces.\nThe dice have been arranged in\na cycle, such that each die has a\n2/3 probability of rolling a higher\nnumber than the previous die in\nthe cycle.\n2.3 (?) Consider a variable y given by the sum of two independent random variables\ny = u + v where u ∼pu(u) and v ∼pv(v). Show that the distribution py(y) is\ngiven by\np(y) =\n∫\npu(u)pv(y −u) du: (2.119)\nThis is known as the convolution of pu(u) and pv(v)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 397, "text": "2.4 (??) Verify that the uniform distribution (2.33) is correctly normalized, and ﬁnd\nexpressions for its mean and variance.\n2.5 (??) Verify that the exponential distribution (2.34) and the Laplace distribution (2.35)\nare correctly normalized.\nExercises 59\n2.6 (?) Using the properties of the Dirac delta function, show that the empirical density\n(2.37) is correctly normalized.\n2.7 (?) By making use of the empirical density (2.37), show that the expectation given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 398, "text": "by (2.39) can be approximated by a sum over a ﬁnite set of samples drawn from the\ndensity in the form (2.40).\n2.8 (?) Using the deﬁnition (2.44), show that var[f(x)] satisﬁes (2.45).\n2.9 (?) Show that if two variables xand yare independent, then their covariance is zero.\n2.10 (?) Suppose that the two variables xand z are statistically independent. Show that\nthe mean and variance of their sum satisﬁes\nE[x+ z] = E[x] + E[z] (2.120)\nvar[x+ z] = var[ x] + var[z]: (2.121)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 399, "text": "var[x+ z] = var[ x] + var[z]: (2.121)\n2.11 (?) Consider two variables xand ywith joint distribution p(x;y). Prove the follow-\ning two results:\nE[x] = Ey[Ex[x|y]] (2.122)\nvar[x] = Ey[varx[x|y]] + vary[Ex[x|y]] : (2.123)\nHere Ex[x|y] denotes the expectation of xunder the conditional distribution p(x|y),\nwith a similar notation for the conditional variance.\n2.12 (???) In this exercise, we prove the normalization condition (2.51) for the univariate\nGaussian. To do this consider, the integral\nI =\n∫∞"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 400, "text": "I =\n∫∞\n−∞\nexp\n(\n− 1\n2\u001b2 x2\n)\ndx (2.124)\nwhich we can evaluate by ﬁrst writing its square in the form\nI2 =\n∫∞\n−∞\n∫∞\n−∞\nexp\n(\n− 1\n2\u001b2 x2 − 1\n2\u001b2 y2\n)\ndxdy: (2.125)\nNow make the transformation from Cartesian coordinates(x;y) to polar coordinates\n(r;\u0012) and then substitute u= r2. Show that, by performing the integrals over \u0012and\nuand then taking the square root of both sides, we obtain\nI =\n(\n2\u0019\u001b2)1=2\n: (2.126)\nFinally, use this result to show that the Gaussian distribution N(x|\u0016;\u001b2) is normal-\nized."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 401, "text": "ized.\n60 2. PROBABILITIES\n2.13 (??) By using a change of variables, verify that the univariate Gaussian distribution\ngiven by (2.49) satisﬁes (2.52). Next, by differentiating both sides of the normaliza-\ntion condition ∫∞\n−∞\nN\n(\nx|\u0016;\u001b2)\ndx= 1 (2.127)\nwith respect to \u001b2, verify that the Gaussian satisﬁes (2.53). Finally, show that (2.54)\nholds.\n2.14 (?) Show that the mode (i.e., the maximum) of the Gaussian distribution (2.49) is\ngiven by \u0016."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 402, "text": "given by \u0016.\n2.15 (?) By setting the derivatives of the log likelihood function (2.56) with respect to \u0016\nand \u001b2 equal to zero, verify the results (2.57) and (2.58).\n2.16 (??) Using the results (2.52) and (2.53), show that\nE[xnxm] = \u00162 + Inm\u001b2 (2.128)\nwhere xnand xmdenote data points sampled from a Gaussian distribution with mean\n\u0016 and variance \u001b2 and Inm satisﬁes Inm = 1 if n = m and Inm = 0 otherwise.\nHence prove the results (2.59) and (2.60)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 403, "text": "Hence prove the results (2.59) and (2.60).\n2.17 (??) Using the deﬁnition (2.61), prove the result (2.62) which shows that the expec-\ntation of the variance estimator for a Gaussian distribution based on the true mean is\ngiven by the true variance \u001b2.\n2.18 (?) Show that maximizing (2.66) with respect to \u001b2 gives the result (2.68).\n2.19 (??) Use the transformation property (2.71) of a probability density under a change"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 404, "text": "of variable to show that any density p(y) can be obtained from a ﬁxed density q(x)\nthat is everywhere non-zero by making a nonlinear change of variable y = f(x) in\nwhich f(x) is a monotonic function so that 0 6 f′(x) <∞. Write down the differ-\nential equation satisﬁed by f(x) and draw a diagram illustrating the transformation\nof the density.\n2.20 (?) Evaluate the elements of the Jacobian matrix for the transformation deﬁned by\n(2.78) and (2.79)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 405, "text": "(2.78) and (2.79).\n2.21 (?) In Section 2.5, we introduced the idea of entropy h(x) as the information gained\non observing the value of a random variable x having distribution p(x). We saw\nthat, for independent variables xand y for which p(x;y) = p(x)p(y), the entropy\nfunctions are additive, so that h(x;y) = h(x) +h(y). In this exercise, we derive the\nrelation between hand pin the form of a function h(p). First show that h(p2) ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 406, "text": "2h(p) and, hence, by induction that h(pn) = nh(p) where nis a positive integer.\nHence, show that h(pn=m) = ( n=m)h(p) where mis also a positive integer. This\nimplies that h(px) = xh(p) where x is a positive rational number and, hence, by\ncontinuity when it is a positive real number. Finally, show that this implies h(p)\nmust take the form h(p) ∝ln p.\nExercises 61\n2.22 (?) Use a Lagrange multiplier to show that maximization of the entropy (2.86) for a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 407, "text": "discrete variable gives a distribution in which all of the probabilities p(xi) are equal\nand that the corresponding value of the entropy is then ln M.\n2.23 (?) Consider an M-state discrete random variable x, and use Jensen’s inequality in\nthe form (2.102) to show that the entropy of its distribution p(x) satisﬁes H[x] 6\nln M.\n2.24 (??) Use the calculus of variations to show that the stationary point of the functional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 408, "text": "(2.96) is given by (2.97). Then use the constraints (2.93), (2.94), and (2.95) to elim-\ninate the Lagrange multipliers and, hence, show that the maximum entropy solution\nis given by the Gaussian (2.98).\n2.25 (?) Use the results (2.94) and (2.95) to show that the entropy of the univariate Gaus-\nsian (2.98) is given by (2.99).\n2.26 (??) Suppose that p(x) is some ﬁxed distribution and that we wish to approximate it\nusing a Gaussian distribution q(x) = N(x|\u0016;\u0006). By writing down the form of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 409, "text": "Kullback–Leibler divergence KL(p∥q) for a Gaussian q(x) and then differentiating,\nshow that minimization of KL(p∥q) with respect to \u0016and \u0006 leads to the result that\n\u0016is given by the expectation of x under p(x) and that \u0006 is given by the covariance.\n2.27 (??) Evaluate the Kullback–Leibler divergence (2.100) between the two Gaussians\np(x) = N(x|\u0016;\u001b2) and q(x) = N(x|m;s2).\n2.28 (??) The alpha family of divergences is deﬁned by\nD\u000b(p∥q) = 4\n1 −\u000b2\n(\n1 −\n∫\np(x)(1+\u000b)=2q(x)(1−\u000b)=2 dx\n)\n(2.129)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 410, "text": "1 −\u000b2\n(\n1 −\n∫\np(x)(1+\u000b)=2q(x)(1−\u000b)=2 dx\n)\n(2.129)\nwhere −∞ < \u000b <∞ is a continuous parameter. Show that the Kullback–Leibler\ndivergence KL(p∥q) corresponds to \u000b → 1. This can be done by writing p\u000f =\nexp(\u000fln p) = 1 + \u000fln p + O(\u000f2) and then taking \u000f → 0. Similarly, show that\nKL(q∥p)corresponds to \u000b→−1.\n2.29 (??) Consider two variables x and y having joint distribution p(x;y). Show that the\ndifferential entropy of this pair of variables satisﬁes\nH[x;y] 6 H[x] + H[y] (2.130)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 411, "text": "H[x;y] 6 H[x] + H[y] (2.130)\nwith equality if, and only if, x and y are statistically independent.\n2.30 (?) Consider a vector x of continuous variables with distribution p(x) and corre-\nsponding entropy H[x]. Suppose that we make a non-singular linear transformation\nof x to obtain a new variabley = Ax. Show that the corresponding entropy is given\nby H[y] = H[x] + ln detA where det A denotes the determinant of A."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 412, "text": "2.31 (??) Suppose that the conditional entropy H[y|x] between two discrete random vari-\nables xand yis zero. Show that, for all values of xsuch that p(x) >0, the variable\ny must be a function of x. In other words, for each xthere is only one value of y\nsuch that p(y|x)̸= 0.\n62 2. PROBABILITIES\n2.32 (?) A strictly convex function is deﬁned as one for which every chord lies above the\nfunction. Show that this is equivalent to the condition that the second derivative of\nthe function is positive."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 413, "text": "the function is positive.\n2.33 (??) Using proof by induction, show that the inequality (2.101) for convex functions\nimplies the result (2.102).\n2.34 (?) Show that, up to an additive constant, the Kullback–Leibler divergence (2.100)\nbetween the empirical distribution (2.37) and a model distributionq(x|\u0012) is equal to\nthe negative log likelihood function.\n2.35 (?) Using the deﬁnition (2.107) together with the product rule of probability, prove\nthe result (2.108)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 414, "text": "the result (2.108).\n2.36 (???) Consider two binary variables xand yhaving the joint distribution given by\ny\n0 1\nx 0 1/3 1/3\n1 0 1/3\nEvaluate the following quantities:\n(a) H[x] (c) H[y|x] (e) H[x;y]\n(b) H[y] (d) H[x|y] (f) I[x;y].\nDraw a Venn diagram to show the relationship between these various quantities.\n2.37 (?) By applying Jensen’s inequality (2.102) with f(x) = ln x, show that the arith-\nmetic mean of a set of real numbers is never less than their geometrical mean."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 415, "text": "2.38 (?) Using the sum and product rules of probability, show that the mutual information\nI(x;y) satisﬁes the relation (2.110).\n2.39 (??) Suppose that two variables z1 and z2 are independent so that p(z1;z2) =\np(z1)p(z2). Show that the covariance matrix between these variables is diagonal.\nThis shows that independence is a sufﬁcient condition for two variables to be uncor-\nrelated. Now consider two variables y1 and y2 where y1 is symmetrically distributed\naround 0 and y2 = y2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 416, "text": "around 0 and y2 = y2\n1. Write down the conditional distribution p(y2|y1) and observe\nthat this is dependent on y1, thus showing that the two variables are not independent.\nNow show that the covariance matrix between these two variables is again diagonal.\nTo do this, use the relation p(y1;y2) = p(y1)p(y2|y1) to show that the off-diagonal\nterms are zero. This counterexample shows that zero correlation is not a sufﬁcient\ncondition for independence."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 417, "text": "condition for independence.\n2.40 (?) Consider the bent coin in Figure 2.2. Assume that the prior probability that the\nconvex side is heads is 0:1. Now suppose the coin is ﬂipped 10 times and we are\ntold that eight of the ﬂips landed heads up and two of the ﬂips landed tails up. Use\nBayes’ theorem to evaluate the posterior probability that the concave side is heads.\nCalculate the probability that the next ﬂip will land heads up.\nExercises 63"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 418, "text": "Exercises 63\n2.41 (?) By substituting (2.115) into (2.114) and making use of the result (2.66) for the log\nlikelihood of the linear regression model, derive the result (2.117) for the regularized\nerror function.\n3\nStandard\nDistributions\nIn this chapter we discuss some speciﬁc examples of probability distributions and\ntheir properties. As well as being of interest in their own right, these distributions\ncan form building blocks for more complex models and will be used extensively"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 419, "text": "throughout the book.\nOne role for the distributions discussed in this chapter is to model the prob-\nability distribution p(x) of a random variable x, given a ﬁnite set x1;:::; xN of\nobservations. This problem is known as density estimation. It should be emphasized\nthat the problem of density estimation is fundamentally ill-posed, because there are\ninﬁnitely many probability distributions that could have given rise to the observed ﬁ-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 420, "text": "nite data set. Indeed, any distribution p(x) that is non-zero at each of the data points\nx1;:::; xN is a potential candidate. The issue of choosing an appropriate distribu-\ntion relates to the problem of model selection, which has already been encountered\nin the context of polynomial curve ﬁtting and which is a central issue in machineSection 1.2\n65© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 421, "text": "C. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 422, "text": "66 3. STANDARD DISTRIBUTIONS\nlearning.\nWe begin by considering distributions for discrete variables before exploring\nthe Gaussian distribution for continuous variables. These are speciﬁc examples of\nparametric distributions, so called because they are governed by a relatively small\nnumber of adjustable parameters, such as the mean and variance of a Gaussian. To\napply such models to the problem of density estimation, we need a procedure for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 423, "text": "determining suitable values for the parameters, given an observed data set, and our\nmain focus will be on maximizing the likelihood function. In this chapter, we will\nassume that the data observations are independent and identically distributed (i.i.d.),\nwhereas in future chapters we will explore more complex scenarios involving struc-\ntured data where this assumption no longer holds.\nOne limitation of the parametric approach is that it assumes a speciﬁc functional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 424, "text": "form for the distribution, which may turn out to be inappropriate for a particular\napplication. An alternative approach is given by nonparametric density estimation\nmethods in which the form of the distribution typically depends on the size of the data\nset. Such models still contain parameters, but these control the model complexity\nrather than the form of the distribution. We end this chapter by brieﬂy considering"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 425, "text": "three nonparametric methods based respectively on histograms, nearest neighbours,\nand kernels. A major limitation of nonparametric techniques such as these is that\nthey involve storing all the training data. In other words, the number of parameters\ngrows with the size of the data set, so that the method become very inefﬁcient for\nlarge data sets. Deep learning combines the efﬁciency of parametric models with the\ngenerality of nonparametric methods by considering ﬂexible distributions based on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 426, "text": "neural networks having a large, but ﬁxed, number of parameters.\n3.1.\nDiscrete Variables\nWe begin by considering simple distributions for discrete variables, starting with\nbinary variables and then moving on to multi-state variables.\n3.1.1 Bernoulli distribution\nConsider a single binary random variable x ∈{0; 1}. For example, x might\ndescribe the outcome of ﬂipping a coin, with x= 1 representing ‘heads’ andx= 0"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 427, "text": "representing ‘tails’. If this were a damaged coin, such as the one shown inFigure 2.2,\nthe probability of landing heads is not necessarily the same as that of landing tails.\nThe probability of x= 1 will be denoted by the parameter \u0016so that\np(x= 1|\u0016) =\u0016 (3.1)\nwhere 0 6 \u00166 1, from which it follows that p(x= 0|\u0016) = 1−\u0016. The probability\ndistribution over xcan therefore be written in the form\nBern(x|\u0016) =\u0016x(1 −\u0016)1−x; (3.2)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 428, "text": "Bern(x|\u0016) =\u0016x(1 −\u0016)1−x; (3.2)\nwhich is known as theBernoulli distribution. It is easily veriﬁed that this distributionExercise 3.1\n3.1. Discrete Variables 67\nis normalized and that it has mean and variance given by\nE[x] = \u0016 (3.3)\nvar[x] = \u0016(1 −\u0016): (3.4)\nNow suppose we have a data set D= {x1;:::;x N}of observed values of x.\nWe can construct the likelihood function, which is a function of\u0016, on the assumption\nthat the observations are drawn independently from p(x|\u0016), so that\np(D|\u0016) =\nN∏\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 429, "text": "p(D|\u0016) =\nN∏\nn=1\np(xn|\u0016) =\nN∏\nn=1\n\u0016xn(1 −\u0016)1−xn: (3.5)\nWe can estimate a value for\u0016by maximizing the likelihood function or equivalently\nby maximizing the logarithm of the likelihood, since the log is a monotonic function.\nThe log likelihood function of the Bernoulli distribution is given by\nln p(D|\u0016) =\nN∑\nn=1\nln p(xn|\u0016) =\nN∑\nn=1\n{xnln \u0016+ (1 −xn) ln(1−\u0016)}: (3.6)\nAt this point, note that the log likelihood function depends on theN observations xn\nonly through their sum∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 430, "text": "only through their sum∑\nnxn. This sum provides an example of asufﬁcient statistic\nfor the data under this distribution. If we set the derivative ofln p(D|\u0016)with respectSection 3.4\nto \u0016equal to zero, we obtain the maximum likelihood estimator:\n\u0016ML = 1\nN\nN∑\nn=1\nxn; (3.7)\nwhich is also known as the sample mean. Denoting the number of observations of\nx= 1 (heads) within this data set by m, we can write (3.7) in the form\n\u0016ML = m\nN (3.8)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 431, "text": "\u0016ML = m\nN (3.8)\nso that the probability of landing heads is given, in this maximum likelihood frame-\nwork, by the fraction of observations of heads in the data set.\n3.1.2 Binomial distribution\nWe can also work out the distribution for the binary variable x of the number\nmof observations of x = 1, given that the data set has size N. This is called the\nbinomial distribution, and from (3.5) we see that it is proportional to\u0016m(1−\u0016)N−m."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 432, "text": "To obtain the normalization coefﬁcient, note that out ofN coin ﬂips, we have to add\nup all of the possible ways of obtaining mheads, so that the binomial distribution\ncan be written as\nBin(m|N;\u0016) =\n(N\nm\n)\n\u0016m(1 −\u0016)N−m (3.9)\n68 3. STANDARD DISTRIBUTIONS\nFigure 3.1 Histogram plot of the binomial\ndistribution (3.9) as a function of\nmfor N = 10 and \u0016= 0:25.\nm\n0 1 2 3 4 5 6 7 8 9 10\n0\n0.1\n0.2\n0.3\nwhere (N\nm\n)\n≡ N!\n(N −m)!m! (3.10)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 433, "text": "0\n0.1\n0.2\n0.3\nwhere (N\nm\n)\n≡ N!\n(N −m)!m! (3.10)\nis the number of ways of choosing m objects out of a total of N identical objects\nwithout replacement. Figure 3.1 shows a plot of the binomial distribution forN = 10Exercise 3.3\nand \u0016= 0:25.\nThe mean and variance of the binomial distribution can be found by using the\nresults that, for independent events, the mean of the sum is the sum of the means and\nthe variance of the sum is the sum of the variances. Because m = x1 + ::: + xNExercise 2.10"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 434, "text": "and because for each observation the mean and variance are given by (3.3) and (3.4),\nrespectively, we have\nE[m] ≡\nN∑\nm=0\nmBin(m|N;\u0016) = N\u0016 (3.11)\nvar[m] ≡\nN∑\nm=0\n(m−E[m])2 Bin(m|N;\u0016) = N\u0016(1 −\u0016): (3.12)\nThese results can also be proved directly by using calculus.Exercise 3.4\n3.1.3 Multinomial distribution\nBinary variables can be used to describe quantities that can take one of two\npossible values. Often, however, we encounter discrete variables that can take on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 435, "text": "one of K possible mutually exclusive states. Although there are various alternative\nways to express such variables, we will see shortly that a particularly convenient\nrepresentation is the 1-of-Kscheme, sometimes called ‘one-hot encoding’, in which\nthe variable is represented by aK-dimensional vector x in which one of the elements\nxkequals 1 and all remaining elements equal 0. So, for instance, if we have a variable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 436, "text": "that can take K= 6 states and a particular observation of the variable happens to\n3.1. Discrete Variables 69\ncorrespond to the state where x3 = 1, then x will be represented by\nx = (0;0;1;0;0;0)T: (3.13)\nNote that such vectors satisfy ∑K\nk=1 xk = 1. If we denote the probability of xk = 1\nby the parameter \u0016k, then the distribution of x is given by\np(x|\u0016) =\nK∏\nk=1\n\u0016xk\nk (3.14)\nwhere \u0016= (\u00161;:::;\u0016 K)T, and the parameters \u0016k are constrained to satisfy \u0016k > 0\nand ∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 437, "text": "and ∑\nk\u0016k = 1, because they represent probabilities. The distribution (3.14) can be\nregarded as a generalization of the Bernoulli distribution to more than two outcomes.\nIt is easily seen that the distribution is normalized:\n∑\nx\np(x|\u0016) =\nK∑\nk=1\n\u0016k = 1 (3.15)\nand that\nE[x|\u0016] =\n∑\nx\np(x|\u0016)x= \u0016: (3.16)\nNow consider a data set Dof N independent observations x1;:::; xN. The\ncorresponding likelihood function takes the form\np(D|\u0016) =\nN∏\nn=1\nK∏\nk=1\n\u0016xnk\nk =\nK∏\nk=1\n\u0016(\nP\nn xnk)\nk =\nK∏\nk=1\n\u0016mk\nk (3.17)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 438, "text": "k =\nK∏\nk=1\n\u0016(\nP\nn xnk)\nk =\nK∏\nk=1\n\u0016mk\nk (3.17)\nwhere we see that the likelihood function depends on theN data points only through\nthe Kquantities:\nmk =\nN∑\nn=1\nxnk; (3.18)\nwhich represent the number of observations ofxk = 1. These are called thesufﬁcient\nstatistics for this distribution. Note that the variablesmk are subject to the constraintSection 3.4\nK∑\nk=1\nmk = N: (3.19)\nTo ﬁnd the maximum likelihood solution for\u0016, we need to maximize ln p(D|\u0016)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 439, "text": "with respect to \u0016k taking account of the constraint (3.15) that the \u0016k must sum to\none. This can be achieved using a Lagrange multiplier \u0015and maximizingAppendix C\nK∑\nk=1\nmkln \u0016k + \u0015\n(K∑\nk=1\n\u0016k −1\n)\n: (3.20)\n70 3. STANDARD DISTRIBUTIONS\nSetting the derivative of (3.20) with respect to \u0016k to zero, we obtain\n\u0016k = −mk=\u0015: (3.21)\nWe can solve for the Lagrange multiplier \u0015by substituting (3.21) into the constraint∑\nk\u0016k = 1 to give \u0015= −N. Thus, we obtain the maximum likelihood solution for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 440, "text": "\u0016k in the form\n\u0016ML\nk = mk\nN ; (3.22)\nwhich is the fraction of the N observations for which xk = 1.\nWe can also consider the joint distribution of the quantitiesm1;:::;m K, condi-\ntioned on the parameter vector \u0016and on the total number N of observations. From\n(3.17), this takes the form\nMult(m1;m2;:::;m K|\u0016;N) =\n( N\nm1m2 :::m K\n) K∏\nk=1\n\u0016mk\nk ; (3.23)\nwhich is known as the multinomial distribution. The normalization coefﬁcient is the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 441, "text": "number of ways of partitioning N objects into Kgroups of size m1;:::;m K and is\ngiven by ( N\nm1m2 :::m K\n)\n= N!\nm1!m2! :::m K!: (3.24)\nNote that two-state quantities can be represented either as binary variables and\nmodelled using the binomial distribution (3.9) or as 1-of-2 variables and modelled\nusing the distribution (3.14) with K = 2.\n3.2. The Multivariate Gaussian\nThe Gaussian, also known as the normal distribution, is a widely used model for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 442, "text": "the distribution of continuous variables. We have already seen that for of a single\nvariable x, the Gaussian distribution can be written in the formSection 2.3\nN(x|\u0016;\u001b2) = 1\n(2\u0019\u001b2)1=2 exp\n{\n− 1\n2\u001b2 (x−\u0016)2\n}\n(3.25)\nwhere \u0016 is the mean and \u001b2 is the variance. For a D-dimensional vector x, the\nmultivariate Gaussian distribution takes the form\nN(x|\u0016;\u0006) = 1\n(2\u0019)D=2\n1\n|\u0006|1=2 exp\n{\n−1\n2(x −\u0016)T\u0006−1 (x −\u0016)\n}\n(3.26)\nwhere \u0016is the D-dimensional mean vector, \u0006 is the D×Dcovariance matrix, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 443, "text": "det \u0006 denotes the determinant of \u0006.\nThe Gaussian distribution arises in many different contexts and can be motivated\nfrom a variety of different perspectives. For example, we have already seen that forSection 2.5\n3.2. The Multivariate Gaussian 71\nN = 1\n0 0.5 1\n0\n1\n2\n3\nN = 2\n0 0.5 1\n0\n1\n2\n3\nN = 10\n0 0.5 1\n0\n1\n2\n3\nFigure 3.2 Histogram plots of the mean of N uniformly distributed numbers for various values of N. We\nobserve that as N increases, the distribution tends towards a Gaussian."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 444, "text": "a single real variable, the distribution that maximizes the entropy is the Gaussian.\nThis property applies also to the multivariate Gaussian.Exercise 3.8\nAnother situation in which the Gaussian distribution arises is when we consider\nthe sum of multiple random variables. The central limit theorem tells us that, subject\nto certain mild conditions, the sum of a set of random variables, which is of course\nitself a random variable, has a distribution that becomes increasingly Gaussian as the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 445, "text": "number of terms in the sum increases (Walker, 1969). We can illustrate this by con-\nsidering N variables x1;:::;x N each of which has a uniform distribution over the\ninterval [0;1] and then considering the distribution of the mean (x1 + ---+ xN)=N.\nFor large N, this distribution tends to a Gaussian, as illustrated in Figure 3.2. In\npractice, the convergence to a Gaussian as N increases can be very rapid. One con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 446, "text": "sequence of this result is that the binomial distribution (3.9), which is a distribution\nover mdeﬁned by the sum of N observations of the random binary variable x, will\ntend to a Gaussian as N →∞ (see Figure 3.1 for N = 10).\nThe Gaussian distribution has many important analytical properties, and we will\nconsider several of these in detail. As a result, this section will be rather more tech-\nnically involved than some of the earlier sections and will require familiarity with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 447, "text": "various matrix identities.Appendix A\n3.2.1 Geometry of the Gaussian\nWe begin by considering the geometrical form of the Gaussian distribution. The\nfunctional dependence of the Gaussian on x is through the quadratic form\n∆ 2 = (x −\u0016)T\u0006−1 (x −\u0016); (3.27)\nwhich appears in the exponent. The quantity ∆ is called the Mahalanobis distance\nfrom \u0016to x. It reduces to the Euclidean distance when \u0006 is the identity matrix.\nThe Gaussian distribution is constant on surfaces inx-space for which this quadratic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 448, "text": "form is constant.\nFirst, note that the matrix \u0006 can be taken to be symmetric, without loss of gen-\nerality, because any antisymmetric component would disappear from the exponent.Exercise 3.11\nNow consider the eigenvector equation for the covariance matrix\n\u0006ui = \u0015iui (3.28)\n72 3. STANDARD DISTRIBUTIONS\nwhere i= 1;:::;D . Because \u0006 is a real, symmetric matrix, its eigenvalues will be\nreal, and its eigenvectors can be chosen to form an orthonormal set, so thatExercise 3.12\nuT\ni uj = Iij (3.29)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 449, "text": "uT\ni uj = Iij (3.29)\nwhere Iij is the i;j element of the identity matrix and satisﬁes\nIij =\n{\n1; if i= j\n0; otherwise. (3.30)\nThe covariance matrix \u0006 can be expressed as an expansion in terms of its eigenvec-\ntors in the formExercise 3.13\n\u0006 =\nD∑\ni=1\n\u0015iuiuT\ni (3.31)\nand similarly the inverse covariance matrix \u0006−1 can be expressed as\n\u0006−1 =\nD∑\ni=1\n1\n\u0015i\nuiuT\ni : (3.32)\nSubstituting (3.32) into (3.27), the quadratic form becomes\n∆ 2 =\nD∑\ni=1\ny2\ni\n\u0015i\n(3.33)\nwhere we have deﬁned\nyi = uT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 450, "text": "i=1\ny2\ni\n\u0015i\n(3.33)\nwhere we have deﬁned\nyi = uT\ni (x −\u0016): (3.34)\nWe can interpret{yi}as a new coordinate system deﬁned by the orthonormal vectors\nui that are shifted and rotated with respect to the original xi coordinates. Forming\nthe vector y = (y1;:::;y D)T, we have\ny = U(x −\u0016) (3.35)\nwhere U is a matrix whose rows are given by uT\ni . From (3.29) it follows that U is\nan orthogonal matrix, i.e., it satisﬁes UUT = UTU = I, where I is the identityAppendix A\nmatrix."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 451, "text": "matrix.\nThe quadratic form, and hence the Gaussian density, is constant on surfaces for\nwhich (3.33) is constant. If all the eigenvalues \u0015i are positive, then these surfaces\nrepresent ellipsoids, with their centres at\u0016and their axes oriented alongui, and with\nscaling factors in the directions of the axes given by\u00151=2\ni , as illustrated in Figure 3.3.\nFor the Gaussian distribution to be well deﬁned, it is necessary for all the eigen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 452, "text": "values \u0015i of the covariance matrix to be strictly positive, otherwise the distribution\ncannot be properly normalized. A matrix whose eigenvalues are strictly positive is\nsaid to be positive deﬁnite. When we discuss latent variable models, we will en-Chapter 16\ncounter Gaussian distributions for which one or more of the eigenvalues are zero, in\n3.2. The Multivariate Gaussian 73\nFigure 3.3 The red curve shows the ellip-\ntical surface of constant proba-\nbility density for a Gaussian in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 453, "text": "bility density for a Gaussian in\na two-dimensional space x =\n(x1;x2) on which the density is\nexp(−1=2) of its value at x =\n\u0016. The axes of the ellipse are\ndeﬁned by the eigenvectors ui\nof the covariance matrix, with\ncorresponding eigenvalues \u0015i.\nx1\nx2\n\u00151=2\n1\n\u00151=2\n2\ny1\ny2\nu1\nu2\n\u0016\nwhich case the distribution is singular and is conﬁned to a subspace of lower dimen-\nsionality. If all the eigenvalues are non-negative, then the covariance matrix is said\nto be positive semideﬁnite."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 454, "text": "to be positive semideﬁnite.\nNow consider the form of the Gaussian distribution in the new coordinate system\ndeﬁned by the yi. In going from thex to the y coordinate system, we have a Jacobian\nmatrix J with elements given by\nJij = @xi\n@yj\n= Uji (3.36)\nwhere Uji are the elements of the matrix UT. Using the orthonormality property of\nthe matrix U, we see that the square of the determinant of the Jacobian matrix is\n|J|2 =\n⏐⏐UT⏐\n⏐2\n=\n⏐\n⏐UT⏐\n⏐|U|=\n⏐\n⏐UTU\n⏐\n⏐= |I|= 1 (3.37)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 455, "text": "⏐⏐UT⏐\n⏐2\n=\n⏐\n⏐UT⏐\n⏐|U|=\n⏐\n⏐UTU\n⏐\n⏐= |I|= 1 (3.37)\nand, hence, |J|= 1 . Also, the determinant |\u0006|of the covariance matrix can be\nwritten as the product of its eigenvalues, and hence\n|\u0006|1=2 =\nD∏\nj=1\n\u00151=2\nj : (3.38)\nThus, in the yj coordinate system, the Gaussian distribution takes the form\np(y) = p(x)|J|=\nD∏\nj=1\n1\n(2\u0019\u0015j)1=2 exp\n{\n−y2\nj\n2\u0015j\n}\n; (3.39)\nwhich is the product of Dindependent univariate Gaussian distributions. The eigen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 456, "text": "vectors therefore deﬁne a new set of shifted and rotated coordinates with respect\nto which the joint probability distribution factorizes into a product of independent\ndistributions. The integral of the distribution in the y coordinate system is then\n∫\np(y) dy =\nD∏\nj=1\n∫∞\n−∞\n1\n(2\u0019\u0015j)1=2 exp\n{\n−y2\nj\n2\u0015j\n}\ndyj = 1 (3.40)\n74 3. STANDARD DISTRIBUTIONS\nwhere we have used the result (2.51) for the normalization of the univariate Gaussian."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 457, "text": "This conﬁrms that the multivariate Gaussian (3.26) is indeed normalized.\n3.2.2 Moments\nWe now look at the moments of the Gaussian distribution and thereby provide an\ninterpretation of the parameters \u0016and \u0006. The expectation of x under the Gaussian\ndistribution is given by\nE[x] = 1\n(2\u0019)D=2\n1\n|\u0006|1=2\n∫\nexp\n{\n−1\n2(x −\u0016)T\u0006−1 (x −\u0016)\n}\nx dx\n= 1\n(2\u0019)D=2\n1\n|\u0006|1=2\n∫\nexp\n{\n−1\n2zT\u0006−1 z\n}\n(z + \u0016) dz (3.41)\nwhere we have changed variables usingz = x−\u0016. Note that the exponent is an even"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 458, "text": "function of the components of z, and because the integrals over these are taken over\nthe range (−∞; ∞), the term in z in the factor (z + \u0016) will vanish by symmetry.\nThus,\nE[x] = \u0016; (3.42)\nand so we refer to \u0016as the mean of the Gaussian distribution.\nWe now consider second-order moments of the Gaussian. In the univariate case,\nwe considered the second-order moment given by E[x2]. For the multivariate Gaus-\nsian, there are D2 second-order moments given by E[xixj], which we can group"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 459, "text": "together to form the matrix E[xxT]. This matrix can be written as\nE[xxT] = 1\n(2\u0019)D=2\n1\n|\u0006|1=2\n∫\nexp\n{\n−1\n2(x −\u0016)T\u0006−1 (x −\u0016)\n}\nxxT dx\n= 1\n(2\u0019)D=2\n1\n|\u0006|1=2\n∫\nexp\n{\n−1\n2zT\u0006−1 z\n}\n(z + \u0016)(z + \u0016)T dz (3.43)\nwhere again we have changed variables using z = x −\u0016. Note that the cross-terms\ninvolving \u0016zT and \u0016Tz will again vanish by symmetry. The term \u0016\u0016T is constant\nand can be taken outside the integral, which itself is unity because the Gaussian"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 460, "text": "distribution is normalized. Consider the term involving zzT. Again, we can make\nuse of the eigenvector expansion of the covariance matrix given by (3.28), together\nwith the completeness of the set of eigenvectors, to write\nz =\nD∑\nj=1\nyjuj (3.44)\n3.2. The Multivariate Gaussian 75\nwhere yj = uT\njz, which gives\n1\n(2\u0019)D=2\n1\n|\u0006|1=2\n∫\nexp\n{\n−1\n2zT\u0006−1z\n}\nzzT dz\n= 1\n(2\u0019)D=2\n1\n|\u0006|1=2\nD∑\ni=1\nD∑\nj=1\nuiuT\nj\n∫\nexp\n{\n−\nD∑\nk=1\ny2\nk\n2\u0015k\n}\nyiyjdy\n=\nD∑\ni=1\nuiuT\ni \u0015i = \u0006 (3.45)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 461, "text": "y2\nk\n2\u0015k\n}\nyiyjdy\n=\nD∑\ni=1\nuiuT\ni \u0015i = \u0006 (3.45)\nwhere we have made use of the eigenvector equation (3.28), together with the fact\nthat the integral on the middle line vanishes by symmetry unless i = j. In the ﬁnal\nline we have made use of the results (2.53) and (3.38), together with (3.31). Thus,\nwe have\nE[xxT] = \u0016\u0016T + \u0006: (3.46)\nWhen deﬁning the variance for a single random variable, we subtracted the mean\nbefore taking the second moment. Similarly, in the multivariate case it is again"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 462, "text": "convenient to subtract off the mean, giving rise to thecovariance of a random vector\nx deﬁned by\ncov[x] = E\n[\n(x −E[x])(x −E[x])T]\n: (3.47)\nFor the speciﬁc case of a Gaussian distribution, we can make use of E[x] = \u0016,\ntogether with the result (3.46), to give\ncov[x] = \u0006: (3.48)\nBecause the parameter matrix \u0006 governs the covariance of x under the Gaussian\ndistribution, it is called the covariance matrix.\n3.2.3 Limitations\nAlthough the Gaussian distribution (3.26) is often used as a simple density"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 463, "text": "model, it suffers from some signiﬁcant limitations. Consider the number of free\nparameters in the distribution. A general symmetric covariance matrix \u0006 will have\nD(D+ 1)=2 independent parameters, and there are another Dindependent parame-Exercise 3.15\nters in \u0016, giving D(D+ 3)=2 parameters in total. For large D, the total number of\nparameters therefore grows quadratically with D, and the computational task of ma-\nnipulating and inverting the large matrices can become prohibitive. One way to ad-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 464, "text": "dress this problem is to use restricted forms of the covariance matrix. If we consider\ncovariance matrices that are diagonal, so that \u0006 = diag(\u001b2\ni), we then have a total\nof 2Dindependent parameters in the density model. The corresponding contours of\nconstant density are given by axis-aligned ellipsoids. We could further restrict the\ncovariance matrix to be proportional to the identity matrix, \u0006 = \u001b2I, known as an\nisotropic covariance, giving D + 1 independent parameters in the model together"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 465, "text": "with spherical surfaces of constant density. The three possibilities of general, diag-\nonal, and isotropic covariance matrices are illustrated in Figure 3.4. Unfortunately,\n76 3. STANDARD DISTRIBUTIONS\nFigure 3.4 Contours of constant\nprobability density for a Gaussian\ndistribution in two dimensions in\nwhich the covariance matrix is (a)\nof general form, (b) diagonal, in\nwhich case the elliptical contours\nare aligned with the coordinate axes,\nand (c) proportional to the identity"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 466, "text": "and (c) proportional to the identity\nmatrix, in which case the contours\nare concentric circles.\nx1\nx2\n(a)\nx1\nx2 (b)\nx1\nx2 (c)\nwhereas such approaches limit the number of degrees of freedom in the distribu-\ntion and make inversion of the covariance matrix a much faster operation, they also\ngreatly restrict the form of the probability density and limit its ability to capture\ninteresting correlations in the data.\nA further limitation of the Gaussian distribution is that it is intrinsically uni-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 467, "text": "modal (i.e., has a single maximum) and so is unable to provide a good approximation\nto multimodal distributions. Thus, the Gaussian distribution can be both too ﬂexible,\nin the sense of having too many parameters, and too limited in the range of distribu-\ntions that it can adequately represent. We will see later that the introduction oflatent\nvariables, also called hidden variables or unobserved variables, allows both of these"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 468, "text": "problems to be addressed. In particular, a rich family of multimodal distributions is\nobtained by introducing discrete latent variables leading to mixtures of Gaussians.Section 3.2.9\nSimilarly, the introduction of continuous latent variables leads to models in which the\nnumber of free parameters can be controlled independently of the dimensionality D\nof the data space while still allowing the model to capture the dominant correlations\nin the data set.Chapter 16\n3.2.4 Conditional distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 469, "text": "3.2.4 Conditional distribution\nAn important property of a multivariate Gaussian distribution is that if two sets\nof variables are jointly Gaussian, then the conditional distribution of one set condi-\ntioned on the other is again Gaussian. Similarly, the marginal distribution of either\nset is also Gaussian.\nFirst, consider the case of conditional distributions. Suppose that x is a D-\ndimensional vector with Gaussian distribution N(x|\u0016;\u0006) and that we partition x"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 470, "text": "into two disjoint subsets xa and xb. Without loss of generality, we can take xa\nto form the ﬁrst M components of x, with xb comprising the remaining D −M\ncomponents, so that\nx =\n(\nxa\nxb\n)\n: (3.49)\nWe also deﬁne corresponding partitions of the mean vector \u0016given by\n\u0016=\n(\n\u0016a\n\u0016b\n)\n(3.50)\n3.2. The Multivariate Gaussian 77\nand of the covariance matrix \u0006 given by\n\u0006 =\n(\n\u0006aa \u0006ab\n\u0006ba \u0006bb\n)\n: (3.51)\nNote that the symmetry \u0006T = \u0006 of the covariance matrix implies that \u0006aa and \u0006bb"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 471, "text": "are symmetric and that \u0006ba = \u0006T\nab.\nIn many situations, it will be convenient to work with the inverse of the covari-\nance matrix:\n\u0003 ≡\u0006−1 ; (3.52)\nwhich is known as the precision matrix. In fact, we will see that some properties\nof Gaussian distributions are most naturally expressed in terms of the covariance,\nwhereas others take a simpler form when viewed in terms of the precision. We\ntherefore also introduce the partitioned form of the precision matrix:\n\u0003 =\n(\n\u0003aa \u0003ab\n\u0003ba \u0003bb\n)\n(3.53)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 472, "text": "\u0003 =\n(\n\u0003aa \u0003ab\n\u0003ba \u0003bb\n)\n(3.53)\ncorresponding to the partitioning (3.49) of the vector x. Because the inverse of a\nsymmetric matrix is also symmetric, we see that \u0003aa and \u0003bb are symmetric andExercise 3.16\nthat \u0003ba = \u0003T\nab. It should be stressed at this point that, for instance, \u0003aa is not\nsimply given by the inverse of \u0006aa. In fact, we will shortly examine the relation\nbetween the inverse of a partitioned matrix and the inverses of its partitions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 473, "text": "We begin by ﬁnding an expression for the conditional distribution p(xa|xb).\nFrom the product rule of probability, we see that this conditional distribution can be\nevaluated from the joint distribution p(x) = p(xa;xb) simply by ﬁxing xb to the\nobserved value and normalizing the resulting expression to obtain a valid probability\ndistribution over xa. Instead of performing this normalization explicitly, we can\nobtain the solution more efﬁciently by considering the quadratic form in the exponent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 474, "text": "of the Gaussian distribution given by (3.27) and then reinstating the normalization\ncoefﬁcient at the end of the calculation. If we make use of the partitioning (3.49),\n(3.50), and (3.53), we obtain\n−1\n2(x −\u0016)T\u0006−1 (x −\u0016) =\n−1\n2(xa −\u0016a)T\u0003aa(xa −\u0016a) −1\n2(xa −\u0016a)T\u0003ab(xb −\u0016b)\n−1\n2(xb −\u0016b)T\u0003ba(xa −\u0016a) −1\n2(xb −\u0016b)T\u0003bb(xb −\u0016b): (3.54)\nWe see that as a function of xa, this is again a quadratic form, and hence, the cor-\nresponding conditional distribution p(xa|xb) will be Gaussian. Because this distri-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 475, "text": "bution is completely characterized by its mean and its covariance, our goal will be\nto identify expressions for the mean and covariance of p(xa|xb) by inspection of\n(3.54).\nThis is an example of a rather common operation associated with Gaussian\ndistributions, sometimes called ‘completing the square’, in which we are given a\n78 3. STANDARD DISTRIBUTIONS\nquadratic form deﬁning the exponent terms in a Gaussian distribution and we need"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 476, "text": "to determine the corresponding mean and covariance. Such problems can be solved\nstraightforwardly by noting that the exponent in a general Gaussian distribution\nN(x|\u0016;\u0006) can be written as\n−1\n2(x −\u0016)T\u0006−1 (x −\u0016) = −1\n2xT\u0006−1 x + xT\u0006−1 \u0016+ const (3.55)\nwhere ‘const’ denotes terms that are independent of x, We have also made use of\nthe symmetry of \u0006. Thus, if we take our general quadratic form and express it in\nthe form given by the right-hand side of (3.55), then we can immediately equate the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 477, "text": "matrix of coefﬁcients entering the second-order term in x to the inverse covariance\nmatrix \u0006−1 and the coefﬁcient of the linear term in x to \u0006−1 \u0016, from which we can\nobtain \u0016.\nNow let us apply this procedure to the conditional Gaussian distributionp(xa|xb)\nfor which the quadratic form in the exponent is given by (3.54). We will denote the\nmean and covariance of this distribution by \u0016a|band \u0006a|b, respectively. Consider"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 478, "text": "the functional dependence of (3.54) on xa in which xb is regarded as a constant. If\nwe pick out all terms that are second order in xa, we have\n−1\n2xT\na\u0003aaxa (3.56)\nfrom which we can immediately conclude that the covariance (inverse precision) of\np(xa|xb) is given by\n\u0006a|b= \u0003−1\naa: (3.57)\nNow consider all the terms in (3.54) that are linear in xa:\nxT\na {\u0003aa\u0016a −\u0003ab(xb −\u0016b)} (3.58)\nwhere we have used \u0003T\nba = \u0003ab. From our discussion of the general form (3.55),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 479, "text": "the coefﬁcient of xa in this expression must equal \u0006−1\na|b\u0016a|band, hence,\n\u0016a|b = \u0006a|b{\u0003aa\u0016a −\u0003ab(xb −\u0016b)}\n= \u0016a −\u0003−1\naa\u0003ab(xb −\u0016b) (3.59)\nwhere we have made use of (3.57).\nThe results (3.57) and (3.59) are expressed in terms of the partitioned precision\nmatrix of the original joint distribution p(xa;xb). We can also express these results\nin terms of the corresponding partitioned covariance matrix. To do this, we make use"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 480, "text": "of the following identity for the inverse of a partitioned matrix:Exercise 3.18\n(\nA B\nC D\n)−1\n=\n(\nM −MBD−1\n−D−1 CM D −1 + D−1 CMBD−1\n)\n(3.60)\nwhere we have deﬁned\nM = (A −BD−1 C)−1 : (3.61)\n3.2. The Multivariate Gaussian 79\nThe quantity M−1 is known as the Schur complement of the matrix on the left-hand\nside of (3.60) with respect to the submatrix D. Using the deﬁnition\n(\n\u0006aa \u0006ab\n\u0006ba \u0006bb\n)−1\n=\n(\n\u0003aa \u0003ab\n\u0003ba \u0003bb\n)\n(3.62)\nand making use of (3.60), we have\n\u0003aa = (\u0006 aa −\u0006ab\u0006−1\nbb \u0006ba)−1 (3.63)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 481, "text": "\u0003aa = (\u0006 aa −\u0006ab\u0006−1\nbb \u0006ba)−1 (3.63)\n\u0003ab = −(\u0006aa −\u0006ab\u0006−1\nbb \u0006ba)−1 \u0006ab\u0006−1\nbb : (3.64)\nFrom these we obtain the following expressions for the mean and covariance of the\nconditional distribution p(xa|xb):\n\u0016a|b = \u0016a + \u0006ab\u0006−1\nbb (xb −\u0016b) (3.65)\n\u0006a|b = \u0006aa −\u0006ab\u0006−1\nbb \u0006ba: (3.66)\nComparing (3.57) and (3.66), we see that the conditional distributionp(xa|xb) takes\na simpler form when expressed in terms of the partitioned precision matrix than"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 482, "text": "when it is expressed in terms of the partitioned covariance matrix. Note that the\nmean of the conditional distributionp(xa|xb), given by (3.65), is a linear function of\nxb and that the covariance, given by (3.66), is independent of xb. This represents an\nexample of a linear-Gaussian model.Section 11.1.4\n3.2.5 Marginal distribution\nWe have seen that if a joint distribution p(xa;xb) is Gaussian, then the condi-\ntional distribution p(xa|xb) will again be Gaussian. Now we turn to a discussion of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 483, "text": "the marginal distribution given by\np(xa) =\n∫\np(xa;xb) dxb; (3.67)\nwhich, as we will see, is also Gaussian. Once again, our strategy for calculating this\ndistribution will be to focus on the quadratic form in the exponent of the joint distri-\nbution and thereby to identify the mean and covariance of the marginal distribution\np(xa).\nThe quadratic form for the joint distribution can be expressed, using the parti-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 484, "text": "tioned precision matrix, in the form (3.54). Our goal is to integrate out xb, which is\nmost easily achieved by ﬁrst considering the terms involvingxb and then completing\nthe square to facilitate the integration. Picking out just those terms that involve xb,\nwe have\n−1\n2xT\nb\u0003bbxb+xT\nb m = −1\n2(xb−\u0003−1\nbb m)T\u0003bb(xb−\u0003−1\nbb m)+ 1\n2mT\u0003−1\nbb m (3.68)\nwhere we have deﬁned\nm = \u0003bb\u0016b −\u0003ba(xa −\u0016a): (3.69)\n80 3. STANDARD DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 485, "text": "80 3. STANDARD DISTRIBUTIONS\nWe see that the dependence onxb has been cast into the standard quadratic form of a\nGaussian distribution corresponding to the ﬁrst term on the right-hand side of (3.68)\nplus a term that does not depend on xb (but that does depend on xa). Thus, when\nwe take the exponential of this quadratic form, we see that the integration over xb\nrequired by (3.67) will take the form\n∫\nexp\n{\n−1\n2(xb −\u0003−1\nbb m)T\u0003bb(xb −\u0003−1\nbb m)\n}\ndxb: (3.70)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 486, "text": "2(xb −\u0003−1\nbb m)T\u0003bb(xb −\u0003−1\nbb m)\n}\ndxb: (3.70)\nThis integration is easily performed by noting that it is the integral over an unnor-\nmalized Gaussian, and so the result will be the reciprocal of the normalization coef-\nﬁcient. We know from the form of the normalized Gaussian given by (3.26) that this\ncoefﬁcient is independent of the mean and depends only on the determinant of the\ncovariance matrix. Thus, by completing the square with respect to xb, we can inte-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 487, "text": "grate out xb so that the only term remaining from the contributions on the left-hand\nside of (3.68) that depends on xa is the last term on the right-hand side of (3.68) in\nwhich m is given by (3.69). Combining this term with the remaining terms from\n(3.54) that depend on xa, we obtain\n1\n2 [\u0003bb\u0016b −\u0003ba(xa −\u0016a)]T \u0003−1\nbb [\u0003bb\u0016b −\u0003ba(xa −\u0016a)]\n−1\n2xT\na\u0003aaxa + xT\na(\u0003aa\u0016a + \u0003ab\u0016b) + const\n= −1\n2xT\na(\u0003aa −\u0003ab\u0003−1\nbb \u0003ba)xa\n+xT\na(\u0003aa −\u0003ab\u0003−1\nbb \u0003ba)\u0016a + const (3.71)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 488, "text": "+xT\na(\u0003aa −\u0003ab\u0003−1\nbb \u0003ba)\u0016a + const (3.71)\nwhere ‘const’ denotes quantities independent of xa. Again, by comparison with\n(3.55), we see that the covariance of the marginal distribution p(xa) is given by\n\u0006a = (\u0003aa −\u0003ab\u0003−1\nbb \u0003ba)−1 : (3.72)\nSimilarly, the mean is given by\n\u0006a(\u0003aa −\u0003ab\u0003−1\nbb \u0003ba)\u0016a = \u0016a (3.73)\nwhere we have used (3.72). The covariance (3.72) is expressed in terms of the par-\ntitioned precision matrix given by (3.53). We can rewrite this in terms of the cor-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 489, "text": "responding partitioning of the covariance matrix given by (3.51), as we did for the\nconditional distribution. These partitioned matrices are related by\n(\n\u0003aa \u0003ab\n\u0003ba \u0003bb\n)−1\n=\n(\n\u0006aa \u0006ab\n\u0006ba \u0006bb\n)\n: (3.74)\nMaking use of (3.60), we then have\n(\n\u0003aa −\u0003ab\u0003−1\nbb \u0003ba\n)−1\n= \u0006aa: (3.75)\n3.2. The Multivariate Gaussian 81\nThus, we obtain the intuitively satisfying result that the marginal distribution p(xa)\nhas mean and covariance given by\nE[xa] = \u0016a (3.76)\ncov[xa] = \u0006aa: (3.77)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 490, "text": "E[xa] = \u0016a (3.76)\ncov[xa] = \u0006aa: (3.77)\nWe see that for a marginal distribution, the mean and covariance are most simply ex-\npressed in terms of the partitioned covariance matrix, in contrast to the conditional\ndistribution for which the partitioned precision matrix gives rise to simpler expres-\nsions.\nOur results for the marginal and conditional distributions of a partitioned Gaus-\nsian can be summarized as follows. Given a joint Gaussian distribution N(x|\u0016;\u0006)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 491, "text": "with \u0003 ≡\u0006−1 and the following partitions\nx =\n(\nxa\nxb\n)\n; \u0016=\n(\n\u0016a\n\u0016b\n)\n(3.78)\n\u0006 =\n(\n\u0006aa \u0006ab\n\u0006ba \u0006bb\n)\n; \u0003 =\n(\n\u0003aa \u0003ab\n\u0003ba \u0003bb\n)\n(3.79)\nthen the conditional distribution is given by\np(xa|xb) = N(x|\u0016a|b;\u0003−1\naa) (3.80)\n\u0016a|b = \u0016a −\u0003−1\naa\u0003ab(xb −\u0016b) (3.81)\nand the marginal distribution is given by\np(xa) = N(xa|\u0016a;\u0006aa): (3.82)\nWe illustrate the idea of conditional and marginal distributions associated with\na multivariate Gaussian using an example involving two variables inFigure 3.5."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 492, "text": "3.2.6 Bayes’ theorem\nIn Sections 3.2.4 and 3.2.5 we considered a Gaussian p(x) in which we parti-\ntioned the vector x into two subvectors x = (x a;xb) and then found expressions\nfor the conditional distribution p(xa|xb) and the marginal distribution p(xa). We\nnoted that the mean of the conditional distribution p(xa|xb) was a linear function of\nxb. Here we will suppose that we are given a Gaussian marginal distribution p(x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 493, "text": "and a Gaussian conditional distribution p(y|x)in which p(y|x)has a mean that is a\nlinear function of x and a covariance that is independent of x. This is an example\nof a linear-Gaussian model (Roweis and Ghahramani, 1999). We wish to ﬁnd theSection 11.1.4\nmarginal distribution p(y) and the conditional distribution p(x|y). This is a struc-\nture that arises in several types of generative model and it will prove convenient toChapter 16\nderive the general results here."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 494, "text": "derive the general results here.\nWe will take the marginal and conditional distributions to be\np(x) = N\n(\nx|\u0016;\u0003−1 )\n(3.83)\np(y|x) = N\n(\ny|Ax+ b;L−1 )\n(3.84)\n82 3. STANDARD DISTRIBUTIONS\n0 0.5 1\n0\n0.5\n1\nxa\nxb\nxb = 0.7\np(xa, xb)\n(a)\n0 0.5 1\n0\n5\n10\nxa\np(xa)\np(xa|xb = 0.7) (b)\nFigure 3.5 (a) Contours of a Gaussian distribution p(xa;xb) over two variables. (b) The marginal distribution\np(xa) (blue curve) and the conditional distribution p(xa|xb) for xb = 0:7 (red curve)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 495, "text": "where \u0016, A, and b are parameters governing the means, and \u0003 and L are precision\nmatrices. If x has dimensionality M and y has dimensionality D, then the matrix A\nhas size D×M.\nFirst we ﬁnd an expression for the joint distribution overx and y. To do this, we\ndeﬁne\nz =\n(\nx\ny\n)\n(3.85)\nand then consider the log of the joint distribution:\nln p(z) = ln p(x) + lnp(y|x)\n= −1\n2(x −\u0016)T\u0003(x −\u0016)\n−1\n2(y −Ax −b)TL(y −Ax −b) + const (3.86)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 496, "text": "−1\n2(y −Ax −b)TL(y −Ax −b) + const (3.86)\nwhere ‘const’ denotes terms independent ofx and y. As before, we see that this is a\nquadratic function of the components of z, and hence, p(z) is Gaussian distribution.\nTo ﬁnd the precision of this Gaussian, we consider the second-order terms in (3.86),\nwhich can be written as\n−1\n2xT(\u0003 + ATLA)x −1\n2yTLy + 1\n2yTLAx + 1\n2xTATLy\n= −1\n2\n(\nx\ny\n)T (\n\u0003 + ATLA −ATL\n−LA L\n)(\nx\ny\n)\n= −1\n2zTRz (3.87)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 497, "text": "\u0003 + ATLA −ATL\n−LA L\n)(\nx\ny\n)\n= −1\n2zTRz (3.87)\nand so the Gaussian distribution over z has precision (inverse covariance) matrix\n3.2. The Multivariate Gaussian 83\ngiven by\nR =\n(\n\u0003 + ATLA −ATL\n−LA L\n)\n: (3.88)\nThe covariance matrix is found by taking the inverse of the precision, which can be\ndone using the matrix inversion formula (3.60) to giveExercise 3.23\ncov[z] = R−1 =\n(\n\u0003−1 \u0003−1 AT\nA\u0003−1 L−1 + A\u0003−1 AT\n)\n: (3.89)\nSimilarly, we can ﬁnd the mean of the Gaussian distribution over z by identify-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 498, "text": "ing the linear terms in (3.86), which are given by\nxT\u0003\u0016−xTATLb + yTLb =\n(\nx\ny\n)T (\n\u0003\u0016−ATLb\nLb\n)\n: (3.90)\nUsing our earlier result (3.55) obtained by completing the square over the quadratic\nform of a multivariate Gaussian, we ﬁnd that the mean of z is given by\nE[z] = R−1\n(\n\u0003\u0016−ATLb\nLb\n)\n: (3.91)\nMaking use of (3.89), we then obtainExercise 3.24\nE[z] =\n(\n\u0016\nA\u0016+ b\n)\n: (3.92)\nNext we ﬁnd an expression for the marginal distribution p(y) in which we have"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 499, "text": "marginalized over x. Recall that the marginal distribution over a subset of the com-\nponents of a Gaussian random vector takes a particularly simple form when ex-\npressed in terms of the partitioned covariance matrix. Speciﬁcally, its mean andSection 3.2\ncovariance are given by (3.76) and (3.77), respectively. Making use of (3.89) and\n(3.92), we see that the mean and covariance of the marginal distribution p(y) are\ngiven by\nE[y] = A\u0016+ b (3.93)\ncov[y] = L−1 + A\u0003−1 AT: (3.94)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 500, "text": "cov[y] = L−1 + A\u0003−1 AT: (3.94)\nA special case of this result is when A = I, in which case the marginal distribution\nreduces to the convolution of two Gaussians, for which we see that the mean of the\nconvolution is the sum of the means of the two Gaussians and the covariance of the\nconvolution is the sum of their covariances.\nFinally, we seek an expression for the conditionalp(x|y). Recall that the results\nfor the conditional distribution are most easily expressed in terms of the partitioned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 501, "text": "precision matrix, using (3.57) and (3.59). Applying these results to (3.89) and (3.92),Section 3.2\nwe see that the conditional distribution p(x|y) has mean and covariance given by\nE[x|y] = ( \u0003 + ATLA)−1 {\nATL(y −b) + \u0003\u0016\n}\n(3.95)\ncov[x|y] = ( \u0003 + ATLA)−1 : (3.96)\n84 3. STANDARD DISTRIBUTIONS\nThe evaluation of this conditional distribution can be seen as an example of\nBayes’ theorem, in which we interpret p(x) as a prior distribution over x. If the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 502, "text": "variable y is observed, then the conditional distribution p(x|y) represents the corre-\nsponding posterior distribution over x. Having found the marginal and conditional\ndistributions, we have effectively expressed the joint distributionp(z) = p(x)p(y|x)\nin the form p(x|y)p(y).\nThese results can be summarized as follows. Given a marginal Gaussian distri-\nbution for x and a conditional Gaussian distribution for y given x in the form\np(x) = N(x|\u0016;\u0003−1 ) (3.97)\np(y|x) = N(y|Ax+ b;L−1 ); (3.98)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 503, "text": "p(y|x) = N(y|Ax+ b;L−1 ); (3.98)\nthen the marginal distribution of y and the conditional distribution of x given y are\ngiven by\np(y) = N(y|A\u0016+ b;L−1 + A\u0003−1 AT) (3.99)\np(x|y) = N(x|\u0006{ATL(y −b) + \u0003\u0016};\u0006) (3.100)\nwhere\n\u0006 = (\u0003 + ATLA)−1 : (3.101)\n3.2.7 Maximum likelihood\nGiven a data set X = (x 1;:::; xN)T in which the observations {xn}are as-\nsumed to be drawn independently from a multivariate Gaussian distribution, we can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 504, "text": "estimate the parameters of the distribution by maximum likelihood. The log likeli-\nhood function is given by\nln p(X|\u0016;\u0006) = −ND\n2 ln(2\u0019)−N\n2 ln |\u0006|−1\n2\nN∑\nn=1\n(xn−\u0016)T\u0006−1 (xn−\u0016): (3.102)\nBy simple rearrangement, we see that the likelihood function depends on the data set\nonly through the two quantities\nN∑\nn=1\nxn;\nN∑\nn=1\nxnxT\nn: (3.103)\nThese are known as the sufﬁcient statistics for the Gaussian distribution. Using"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 505, "text": "(A.19), the derivative of the log likelihood with respect to \u0016is given byAppendix A\n@\n@\u0016ln p(X|\u0016;\u0006) =\nN∑\nn=1\n\u0006−1 (xn −\u0016); (3.104)\nand setting this derivative to zero, we obtain the solution for the maximum likelihood\nestimate of the mean:\n\u0016ML = 1\nN\nN∑\nn=1\nxn; (3.105)\n3.2. The Multivariate Gaussian 85\nwhich is the mean of the observed set of data points. The maximization of (3.102)\nwith respect to \u0006 is rather more involved. The simplest approach is to ignore the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 506, "text": "symmetry constraint and show that the resulting solution is symmetric as required.Exercise 3.28\nAlternative derivations of this result, which impose the symmetry and positive deﬁ-\nniteness constraints explicitly, can be found in Magnus and Neudecker (1999). The\nresult is as expected and takes the form\n\u0006ML = 1\nN\nN∑\nn=1\n(xn −\u0016ML)(xn −\u0016ML)T; (3.106)\nwhich involves \u0016ML because this is the result of a joint maximization with respect"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 507, "text": "to \u0016and \u0006. Note that the solution (3.105) for \u0016ML does not depend on \u0006ML, and so\nwe can ﬁrst evaluate \u0016ML and then use this to evaluate \u0006ML.\nIf we evaluate the expectations of the maximum likelihood solutions under the\ntrue distribution, we obtain the following resultsExercise 3.29\nE[\u0016ML] = \u0016 (3.107)\nE[\u0006ML] = N −1\nN \u0006: (3.108)\nWe see that the expectation of the maximum likelihood estimate for the mean is equal\nto the true mean. However, the maximum likelihood estimate for the covariance has"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 508, "text": "an expectation that is less than the true value, and hence, it is biased. We can correct\nthis bias by deﬁning a different estimator ˜\u0006 given by\n˜\u0006 = 1\nN −1\nN∑\nn=1\n(xn −\u0016ML)(xn −\u0016ML)T: (3.109)\nClearly from (3.106) and (3.108), the expectation of ˜\u0006 is equal to \u0006.\n3.2.8 Sequential estimation\nOur discussion of the maximum likelihood solution represents a batch method\nin which the entire training data set is considered at once. An alternative is to use"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 509, "text": "sequential methods, which allow data points to be processed one at a time and then\ndiscarded. These are important for online applications and for large data when the\nbatch processing of all data points at once is infeasible.\nConsider the result (3.105) for the maximum likelihood estimator of the mean\n\u0016ML, which we will denote by \u0016(N)\nML when it is based on N observations. If we\n86 3. STANDARD DISTRIBUTIONS\nFigure 3.6 Plots of the Old Faith-\nful data in which the red curves are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 510, "text": "ful data in which the red curves are\ncontours of constant probability den-\nsity. (a) A single Gaussian distribu-\ntion which has been ﬁtted to the data\nusing maximum likelihood. Note that\nthis distribution fails to capture the\ntwo clumps in the data and indeed\nplaces much of its probability mass\nin the central region between the\nclumps where the data are relatively\nsparse. (b) The distribution given by\na linear combination of two Gaus-\nsians, also ﬁtted by maximum likeli-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 511, "text": "sians, also ﬁtted by maximum likeli-\nhood, which gives a better represen-\ntation of the data.\n1 2 3 4 5 6\n40\n60\n80\n100\n(a)\n1 2 3 4 5 6\n40\n60\n80\n100 (b)\ndissect out the contribution from the ﬁnal data point xN, we obtain\n\u0016(N)\nML = 1\nN\nN∑\nn=1\nxn\n= 1\nNxN + 1\nN\nN−1∑\nn=1\nxn\n= 1\nNxN + N −1\nN \u0016(N−1)\nML\n= \u0016(N−1)\nML + 1\nN(xN −\u0016(N−1)\nML ): (3.110)\nThis result has a nice interpretation, as follows. After observing N −1 data points,\nwe estimate \u0016by \u0016(N−1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 512, "text": "we estimate \u0016by \u0016(N−1)\nML . We now observe data pointxN, and we obtain our revised\nestimate \u0016(N)\nML by moving the old estimate a small amount, proportional to 1=N, in\nthe direction of the ‘error signal’(xN −\u0016(N−1)\nML ). Note that, as N increases, so the\ncontributions from successive data points get smaller.\n3.2.9 Mixtures of Gaussians\nAlthough the Gaussian distribution has some important analytical properties, it\nsuffers from signiﬁcant limitations when used to model modelling real data sets."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 513, "text": "Consider the example shown in Figure 3.6(a). This is known as the ‘Old Faithful’\ndata set, and comprises 272 measurements of the eruption of the Old Faithful geyser\nin Yellowstone National Park in the USA. Each measurement gives the duration of\nthe eruption in minutes (horizontal axis) and the time in minutes to the next eruption\n(vertical axis). We see that the data set forms two dominant clumps, and that a simple\nGaussian distribution is unable to capture this structure."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 514, "text": "We might expect that a superposition of two Gaussian distributions would be\nable to do a much better job of representing the structure in this data set, and indeed\n3.2. The Multivariate Gaussian 87\nFigure 3.7 Example of a Gaussian mixture distri-\nbution in one dimension showing three\nGaussians (each scaled by a coefﬁcient)\nin blue and their sum in red.\nt\np(t|x)\nthis proves to be the case, as can be seen from Figure 3.6(b). Such superpositions,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 515, "text": "formed by taking linear combinations of more basic distributions such as Gaussians,\ncan be formulated as probabilistic models known asmixture distributions. In this sec-Chapter 15\ntion we will consider Gaussians to illustrate the framework of mixture models. More\ngenerally, mixture models can comprise linear combinations of other distributions,\nfor example mixtures of Bernoulli distributions for binary variables. InFigure 3.7 we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 516, "text": "see that a linear combination of Gaussians can give rise to very complex densities.\nBy using a sufﬁcient number of Gaussians and by adjusting their means and covari-\nances as well as the coefﬁcients in the linear combination, almost any continuous\ndistribution can be approximated to arbitrary accuracy.\nWe therefore consider a superposition of KGaussian densities of the form\np(x) =\nK∑\nk=1\n\u0019kN(x|\u0016k;\u0006k); (3.111)\nwhich is called a mixture of Gaussians. Each Gaussian density N(x|\u0016k;\u0006k) is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 517, "text": "called a component of the mixture and has its own mean \u0016k and covariance \u0006k.\nContour and surface plots for a Gaussian mixture in two dimensions having three\ncomponents are shown in Figure 3.8.\nThe parameters \u0019k in (3.111) are called mixing coefﬁcients. If we integrate both\nsides of (3.111) with respect tox, and note that bothp(x) and the individual Gaussian\ncomponents are normalized, we obtain\nK∑\nk=1\n\u0019k = 1: (3.112)\nAlso, given thatN(x|\u0016k;\u0006k) > 0, a sufﬁcient condition for the requirementp(x) >"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 518, "text": "0 is that \u0019k > 0 for all k. Combining this with the condition (3.112), we obtain\n0 6 \u0019k 6 1: (3.113)\nWe can therefore see that the mixing coefﬁcients satisfy the requirements to be prob-\nabilities, and we will show that this probabilistic interpretation of mixture distribu-\ntions is very powerful.Chapter 15\n88 3. STANDARD DISTRIBUTIONS\nx1\nx2\nπ1 = 0.5\nπ2 = 0.3\nπ3 = 0.2\n(a)\nx1\nx2\np(x) (b)\nx1\nx2 (c)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 519, "text": "π2 = 0.3\nπ3 = 0.2\n(a)\nx1\nx2\np(x) (b)\nx1\nx2 (c)\nFigure 3.8 Illustration of a mixture of three Gaussians in a two-dimensional space. (a) Contours of constant\ndensity for each of the mixture components, in which the three components are denoted red, blue, and green, and\nthe values of the mixing coefﬁcients are shown below each component. (b) Contours of the marginal probability\ndensity p(x) of the mixture distribution. (c) A surface plot of the distribution p(x)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 520, "text": "From the sum and product rules of probability, the marginal density can be writ-\nten as\np(x) =\nK∑\nk=1\np(k)p(x|k); (3.114)\nwhich is equivalent to (3.111) in which we can view \u0019k = p(k) as the prior proba-\nbility of picking the kth component, and the density N(x|\u0016k;\u0006k) = p(x|k) as the\nprobability of x conditioned on k. As we will see in later chapters, an important role\nis played by the corresponding posterior probabilities p(k|x), which are also known"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 521, "text": "as responsibilities. From Bayes’ theorem, these are given by\n\rk(x) ≡ p(k|x)\n= p(k)p(x|k)\n∑\nlp(l)p(x|l)\n= \u0019kN(x|\u0016k;\u0006k)∑\nl\u0019lN(x|\u0016l;\u0006l): (3.115)\nThe form of the Gaussian mixture distribution is governed by the parameters \u0019,\n\u0016, and \u0006, where we have used the notation\u0019≡{\u00191;:::;\u0019 K}, \u0016≡{\u00161;:::; \u0016K},\nand \u0006 ≡{\u00061;::: \u0006K}. One way to set the values of these parameters is to use\nmaximum likelihood. From (3.111), the log of the likelihood function is given by\nln p(X|\u0019;\u0016;\u0006) =\nN∑\nn=1\nln\n{ K∑\nk=1\n\u0019kN(xn|\u0016k;\u0006k)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 522, "text": "ln p(X|\u0019;\u0016;\u0006) =\nN∑\nn=1\nln\n{ K∑\nk=1\n\u0019kN(xn|\u0016k;\u0006k)\n}\n(3.116)\nwhere X = {x1;:::; xN}. We immediately see that the situation is now much more\ncomplex than with a single Gaussian, due to the summation over k inside the log-\n3.3. Periodic Variables 89\narithm. As a result, the maximum likelihood solution for the parameters no longer\nhas a closed-form analytical solution. One approach for maximizing the likelihood\nfunction is to use iterative numerical optimization techniques. Alternatively, we can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 523, "text": "employ a powerful framework called expectation maximization, which has wide ap-Chapter 15\nplicability to a variety of different deep generative models.\n3.3. Periodic Variables\nAlthough Gaussian distributions are of great practical signiﬁcance, both in their own\nright and as building blocks for more complex probabilistic models, there are situa-\ntions in which they are inappropriate as density models for continuous variables. One"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 524, "text": "important case, which arises in practical applications, is that of periodic variables.\nAn example of a periodic variable is the wind direction at a particular geographi-\ncal location. We might, for instance, measure the wind direction at multiple locations\nand wish to summarize this data using a parametric distribution. Another example\nis calendar time, where we may be interested in modelling quantities that are be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 525, "text": "lieved to be periodic over 24 hours or over an annual cycle. Such quantities can\nconveniently be represented using an angular (polar) coordinate 0 6 \u0012< 2\u0019.\nWe might be tempted to treat periodic variables by choosing some direction\nas the origin and then applying a conventional distribution such as the Gaussian.\nSuch an approach, however, would give results that were strongly dependent on the\narbitrary choice of origin. Suppose, for instance, that we have two observations at"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 526, "text": "\u00121 = 1-and \u00122 = 359-, and we model them using a standard univariate Gaussian\ndistribution. If we place the origin at0-, then the sample mean of this data set will be\n180-with standard deviation 179-, whereas if we place the origin at 180-, then the\nmean will be 0-and the standard deviation will be 1-. We clearly need to develop a\nspecial approach for periodic variables.\n3.3.1 Von Mises distribution\nLet us consider the problem of evaluating the mean of a set of observations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 527, "text": "D= {\u00121;:::;\u0012 N}of a periodic variable \u0012where \u0012is measured in radians. We have\nalready seen that the simple average (\u00121 + ---+ \u0012N)=Nwill be strongly coordinate\ndependent. To ﬁnd an invariant measure of the mean, note that the observations\ncan be viewed as points on the unit circle and can therefore be described instead by\ntwo-dimensional unit vectors x1;:::; xN where ∥xn∥= 1 for n = 1;:::;N , as\nillustrated in Figure 3.9. We can average the vectors {xn}instead to give\nx = 1\nN\nN∑\nn=1\nxn (3.117)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 528, "text": "x = 1\nN\nN∑\nn=1\nxn (3.117)\nand then ﬁnd the corresponding angle \u0012of this average. Clearly, this deﬁnition will\nensure that the location of the mean is independent of the origin of the angular coor-\ndinate. Note that x will typically lie inside the unit circle. The Cartesian coordinates\n90 3. STANDARD DISTRIBUTIONS\nFigure 3.9 Illustration of the representation of val-\nues \u0012n of a periodic variable as two-\ndimensional vectors xn living on the unit\ncircle. Also shown is the average x of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 529, "text": "circle. Also shown is the average x of\nthose vectors.\nx1\nx2\nx1\nx2\nx3x4\nx\nr\n\u0012\nof the observations are given by xn = (cos \u0012n;sin \u0012n), and we can write the Carte-\nsian coordinates of the sample mean in the form x = (rcos \u0012;rsin \u0012). Substituting\ninto (3.117) and equating the x1 and x2 components then gives\nx1 = rcos \u0012= 1\nN\nN∑\nn=1\ncos \u0012n; x2 = rsin \u0012= 1\nN\nN∑\nn=1\nsin \u0012n: (3.118)\nTaking the ratio, and using the identity tan \u0012 = sin \u0012=cos \u0012, we can solve for \u0012 to\ngive\n\u0012= tan−1\n{ ∑\nnsin \u0012n∑\nncos \u0012n\n}"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 530, "text": "give\n\u0012= tan−1\n{ ∑\nnsin \u0012n∑\nncos \u0012n\n}\n: (3.119)\nShortly, we will see how this result arises naturally as a maximum likelihood estima-\ntor.\nFirst, we need to deﬁne a periodic generalization of the Gaussian called the\nvon Mises distribution. Here we will limit our attention to univariate distributions,\nalthough analogous periodic distributions can also be found over hyperspheres of\narbitrary dimension (Mardia and Jupp, 2000)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 531, "text": "arbitrary dimension (Mardia and Jupp, 2000).\nBy convention, we will consider distributions p(\u0012) that have period 2\u0019. Any\nprobability density p(\u0012) deﬁned over \u0012must not only be non-negative and integrate\nto one, but it must also be periodic. Thus, p(\u0012) must satisfy the three conditions:\np(\u0012) > 0 (3.120)\n∫2\u0019\n0\np(\u0012) d\u0012 = 1 (3.121)\np(\u0012+ 2\u0019) = p(\u0012): (3.122)\nFrom (3.122), it follows that p(\u0012+ M2\u0019) = p(\u0012) for any integer M."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 532, "text": "We can easily obtain a Gaussian-like distribution that satisﬁes these three prop-\nerties as follows. Consider a Gaussian distribution over two variables x = (x1;x2)\n3.3. Periodic Variables 91\nFigure 3.10 The von Mises distribution can be derived by considering\na two-dimensional Gaussian of the form (3.123), whose\ndensity contours are shown in blue, and conditioning on\nthe unit circle shown in red.\nx1\nx2\np(x)\nr = 1\nhaving mean \u0016= (\u00161;\u00162) and a covariance matrix \u0006 = \u001b2I where I is the 2 ×2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 533, "text": "identity matrix, so that\np(x1;x2) = 1\n2\u0019\u001b2 exp\n{\n−(x1 −\u00161)2 + (x2 −\u00162)2\n2\u001b2\n}\n: (3.123)\nThe contours of constant p(x) are circles, as illustrated in Figure 3.10.\nNow suppose we consider the value of this distribution along a circle of ﬁxed\nradius. Then by construction, this distribution will be periodic, although it will not\nbe normalized. We can determine the form of this distribution by transforming from\nCartesian coordinates (x1;x2) to polar coordinates (r;\u0012) so that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 534, "text": "x1 = rcos \u0012; x 2 = rsin \u0012: (3.124)\nWe also map the mean \u0016into polar coordinates by writing\n\u00161 = r0 cos \u00120; \u0016 2 = r0 sin \u00120: (3.125)\nNext we substitute these transformations into the two-dimensional Gaussian distribu-\ntion (3.123), and then condition on the unit circler= 1, noting that we are interested\nonly in the dependence on \u0012. Focusing on the exponent in the Gaussian distribution\nwe have\n− 1\n2\u001b2\n{\n(rcos \u0012−r0 cos \u00120)2 + (rsin \u0012−r0 sin \u00120)2}\n= − 1\n2\u001b2\n{\n1 + r2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 535, "text": "= − 1\n2\u001b2\n{\n1 + r2\n0 −2r0 cos \u0012cos \u00120 −2r0 sin \u0012sin \u00120\n}\n= r0\n\u001b2 cos(\u0012−\u00120) + const (3.126)\nwhere ‘const’ denotes terms independent of \u0012. We have made use of the following\ntrigonometrical identities:\ncos2 A+ sin2 A = 1 (3.127)\ncos Acos B+ sinAsin B = cos(A −B): (3.128)\nIf we now deﬁne m = r0=\u001b2, we obtain our ﬁnal expression for the distribution of\np(\u0012) along the unit circle r= 1 in the form\np(\u0012|\u00120;m) = 1\n2\u0019I0(m) exp {mcos(\u0012−\u00120)}; (3.129)\n92 3. STANDARD DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 536, "text": "m = 5,θ0 = π/4\nm = 1,θ0 = 3π/4\n2π\n0\nπ/4\n3π/4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 537, "text": "m = 5, θ0 = π/4\nm = 1,θ0 = 3π/4\nFigure 3.11 The von Mises distribution plotted for two different parameter values, shown as a Cartesian plot\non the left and as the corresponding polar plot on the right.\nwhich is called the von Mises distribution or the circular normal. Here the param-\neter \u00120 corresponds to the mean of the distribution, whereas m, which is known\nas the concentration parameter, is analogous to the inverse variance (i.e. the pre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 538, "text": "cision) for the Gaussian. The normalization coefﬁcient in (3.129) is expressed in\nterms of I0(m), which is the zeroth-order modiﬁed Bessel function of the ﬁrst kind\n(Abramowitz and Stegun, 1965) and is deﬁned by\nI0(m) = 1\n2\u0019\n∫2\u0019\n0\nexp {mcos \u0012}d\u0012: (3.130)\nFor large m, the distribution becomes approximately Gaussian. The von Mises dis-Exercise 3.31\ntribution is plotted in Figure 3.11, and the function I0(m) is plotted in Figure 3.12."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 539, "text": "Now consider the maximum likelihood estimators for the parameters \u00120 and m\nfor the von Mises distribution. The log likelihood function is given by\nln p(D|\u00120;m) = −Nln(2\u0019) −Nln I0(m) + m\nN∑\nn=1\ncos(\u0012n −\u00120): (3.131)\nSetting the derivative with respect to \u00120 equal to zero gives\nN∑\nn=1\nsin(\u0012n −\u00120) = 0: (3.132)\nTo solve for \u00120, we make use of the trigonometric identity\nsin(A−B) = cos Bsin A−cos Asin B (3.133)\nfrom which we obtainExercise 3.32\n3.3. Periodic Variables 93\nI0(m)\nm\n0 5 10\n0\n1000\n2000"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 540, "text": "I0(m)\nm\n0 5 10\n0\n1000\n2000\n3000\nA(m)\nm\n0 5 10\n0\n0.5\n1\nFigure 3.12 Plot of the Bessel function I0(m) deﬁned by (3.130), together with the function A(m) deﬁned by\n(3.136).\n\u0012ML\n0 = tan−1\n{ ∑\nnsin \u0012n∑\nncos \u0012n\n}\n; (3.134)\nwhich we recognize as the result (3.119) obtained earlier for the mean of the obser-\nvations viewed in a two-dimensional Cartesian space.\nSimilarly, maximizing (3.131) with respect to m and making use of I′\n0(m) =\nI1(m) (Abramowitz and Stegun, 1965), we have\nA(mML) = 1\nN\nN∑\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 541, "text": "A(mML) = 1\nN\nN∑\nn=1\ncos(\u0012n −\u0012ML\n0 ) (3.135)\nwhere we have substituted for the maximum likelihood solution for \u0012ML\n0 (recalling\nthat we are performing a joint optimization over \u0012and m), and we have deﬁned\nA(m) = I1(m)\nI0(m): (3.136)\nThe function A(m) is plotted in Figure 3.12. Making use of the trigonometric iden-\ntity (3.128), we can write (3.135) in the form\nA(mML) =\n(\n1\nN\nN∑\nn=1\ncos \u0012n\n)\ncos \u0012ML\n0 +\n(\n1\nN\nN∑\nn=1\nsin \u0012n\n)\nsin \u0012ML\n0 : (3.137)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 542, "text": "0 +\n(\n1\nN\nN∑\nn=1\nsin \u0012n\n)\nsin \u0012ML\n0 : (3.137)\nThe right-hand side of (3.137) is easily evaluated, and the function A(m) can be in-\nverted numerically. One limitation of the von Mises distribution is that it is unimodal.\nBy forming mixtures of von Mises distributions, we obtain a ﬂexible framework for\nmodelling periodic variables that can handle multimodality.\nFor completeness, we mention brieﬂy some alternative techniques for construct-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 543, "text": "ing periodic distributions. The simplest approach is to use a histogram of observa-\ntions in which the angular coordinate is divided into ﬁxed bins. This has the virtue of\n94 3. STANDARD DISTRIBUTIONS\nsimplicity and ﬂexibility but also suffers from signiﬁcant limitations, as we will see\nwhen we discuss histogram methods in more detail later. Another approach starts,Section 3.5\nlike the von Mises distribution, from a Gaussian distribution over a Euclidean space"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 544, "text": "but now marginalizes onto the unit circle rather than conditioning (Mardia and Jupp,\n2000). However, this leads to more complex forms of distribution and will not be\ndiscussed further. Finally, any valid distribution over the real axis (such as a Gaus-\nsian) can be turned into a periodic distribution by mapping successive intervals of\nwidth 2\u0019 onto the periodic variable (0;2\u0019), which corresponds to ‘wrapping’ the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 545, "text": "real axis around the unit circle. Again, the resulting distribution is more complex to\nhandle than the von Mises distribution.\n3.4.\nThe Exponential Family\nThe probability distributions that we have studied so far in this chapter (with the\nexception of mixture models) are speciﬁc examples of a broad class of distributions\ncalled the exponential family (Duda and Hart, 1973; Bernardo and Smith, 1994).\nMembers of the exponential family have many important properties in common, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 546, "text": "it is illuminating to discuss these properties in some generality.\nThe exponential family of distributions overx, given parameters \u0011, is deﬁned to\nbe the set of distributions of the form\np(x|\u0011) = h(x)g(\u0011) exp\n{\n\u0011Tu(x)\n}\n(3.138)\nwhere x may be scalar or vector and may be discrete or continuous. Here\u0011are called\nthe natural parameters of the distribution, and u(x) is some function of x. The\nfunction g(\u0011) can be interpreted as the coefﬁcient that ensures that the distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 547, "text": "is normalized, and therefore, it satisﬁes\ng(\u0011)\n∫\nh(x) exp\n{\n\u0011Tu(x)\n}\ndx = 1 (3.139)\nwhere the integration is replaced by summation if x is a discrete variable.\nWe begin by taking some examples of the distributions introduced earlier in\nthe chapter and showing that they are indeed members of the exponential family.\nConsider ﬁrst the Bernoulli distribution:\np(x|\u0016) = Bern(x|\u0016) =\u0016x(1 −\u0016)1−x: (3.140)\nExpressing the right-hand side as the exponential of the logarithm, we have"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 548, "text": "p(x|\u0016) = exp {xln \u0016+ (1 −x) ln(1−\u0016)}\n= (1 −\u0016) exp\n{\nln\n( \u0016\n1 −\u0016\n)\nx\n}\n: (3.141)\nComparison with (3.138) allows us to identify\n\u0011= ln\n( \u0016\n1 −\u0016\n)\n(3.142)\n3.4. The Exponential Family 95\nwhich we can solve for \u0016to give \u0016= \u001b(\u0011), where\n\u001b(\u0011) = 1\n1 + exp(−\u0011) (3.143)\nis called the logistic sigmoid function. Thus, we can write the Bernoulli distribution\nusing the standard representation (3.138) in the form\np(x|\u0011) = \u001b(−\u0011) exp(\u0011x) (3.144)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 549, "text": "p(x|\u0011) = \u001b(−\u0011) exp(\u0011x) (3.144)\nwhere we have used 1 −\u001b(\u0011) = \u001b(−\u0011), which is easily proved from (3.143). Com-\nparison with (3.138) shows that\nu(x) = x (3.145)\nh(x) = 1 (3.146)\ng(\u0011) = \u001b(−\u0011): (3.147)\nNext consider the multinomial distribution which, for a single observation x,\ntakes the form\np(x|\u0016) =\nM∏\nk=1\n\u0016xk\nk = exp\n{ M∑\nk=1\nxkln \u0016k\n}\n(3.148)\nwhere x = (x1;:::;x M)T. Again, we can write this in the standard representation\n(3.138) so that\np(x|\u0011) = exp(\u0011Tx) (3.149)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 550, "text": "(3.138) so that\np(x|\u0011) = exp(\u0011Tx) (3.149)\nwhere \u0011k = ln \u0016k, and we have deﬁned \u0011= (\u00111;:::;\u0011 M)T. Again, comparing with\n(3.138) we have\nu(x) = x (3.150)\nh(x) = 1 (3.151)\ng(\u0011) = 1 : (3.152)\nNote that the parameters \u0011k are not independent because the parameters \u0016k are sub-\nject to the constraint\nM∑\nk=1\n\u0016k = 1 (3.153)\nso that, given any M−1 of the parameters \u0016k, the value of the remaining parameter\nis ﬁxed. In some circumstances, it will be convenient to remove this constraint by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 551, "text": "expressing the distribution in terms of onlyM−1 parameters. This can be achieved\nby using the relationship (3.153) to eliminate \u0016M by expressing it in terms of the\nremaining {\u0016k}where k= 1;:::;M −1, thereby leaving M−1 parameters. Note\nthat these remaining parameters are still subject to the constraints\n0 6 \u0016k 6 1;\nM−1∑\nk=1\n\u0016k 6 1: (3.154)\n96 3. STANDARD DISTRIBUTIONS\nMaking use of the constraint (3.153), the multinomial distribution in this representa-\ntion then becomes\nexp\n{ M∑\nk=1\nxkln \u0016k\n}"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 552, "text": "tion then becomes\nexp\n{ M∑\nk=1\nxkln \u0016k\n}\n= exp\n{M−1∑\nk=1\nxkln \u0016k +\n(\n1 −\nM−1∑\nk=1\nxk\n)\nln\n(\n1 −\nM−1∑\nk=1\n\u0016k\n)}\n= exp\n{M−1∑\nk=1\nxkln\n(\n\u0016k\n1 −∑M−1\nj=1 \u0016j\n)\n+ ln\n(\n1 −\nM−1∑\nk=1\n\u0016k\n)}\n: (3.155)\nWe now identify\nln\n(\n\u0016k\n1 −∑\nj\u0016j\n)\n= \u0011k; (3.156)\nwhich we can solve for \u0016k by ﬁrst summing both sides over kand then rearranging\nand back-substituting to give\n\u0016k = exp(\u0011k)\n1 + ∑\njexp(\u0011j): (3.157)\nThis is called the softmax function or the normalized exponential. In this representa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 553, "text": "tion, the multinomial distribution therefore takes the form\np(x|\u0011) =\n(\n1 +\nM−1∑\nk=1\nexp(\u0011k)\n)−1\nexp(\u0011Tx): (3.158)\nThis is the standard form of the exponential family, with parameter vector \u0011 =\n(\u00111;:::;\u0011 M−1 )T in which\nu(x) = x (3.159)\nh(x) = 1 (3.160)\ng(\u0011) =\n(\n1 +\nM−1∑\nk=1\nexp(\u0011k)\n)−1\n: (3.161)\nFinally, let us consider the Gaussian distribution. For the univariate Gaussian,\nwe have\np(x|\u0016;\u001b2) = 1\n(2\u0019\u001b2)1=2 exp\n{\n− 1\n2\u001b2 (x−\u0016)2\n}\n(3.162)\n= 1\n(2\u0019\u001b2)1=2 exp\n{\n− 1\n2\u001b2 x2 + \u0016\n\u001b2 x− 1\n2\u001b2 \u00162\n}"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 554, "text": "(2\u0019\u001b2)1=2 exp\n{\n− 1\n2\u001b2 x2 + \u0016\n\u001b2 x− 1\n2\u001b2 \u00162\n}\n; (3.163)\nwhich, after some simple rearranging, can be cast in the standard exponential family\nform (3.138) withExercise 3.35\n3.4. The Exponential Family 97\n\u0011 =\n(\n\u0016=\u001b2\n−1=2\u001b2\n)\n(3.164)\nu(x) =\n(\nx\nx2\n)\n(3.165)\nh(x) = (2 \u0019)−1=2 (3.166)\ng(\u0011) = ( −2\u00112)1=2 exp\n(\u00112\n1\n4\u00112\n)\n: (3.167)\nFinally, we shall sometimes make use of a restricted form of (3.138) in which\nwe choose u(x) = x. However, this can be somewhat generalized by noting that if"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 555, "text": "f(x) is a normalized density then\n1\nsf\n(1\nsx\n)\n(3.168)\nis also a normalized density, where s> 0 is a scale parameter. Combining these, we\narrive at a restricted set of exponential family class-conditional densities of the form\np(x|\u0015k;s) = 1\nsh\n(1\nsx\n)\ng(\u0015k) exp\n{ 1\ns\u0015T\nkx\n}\n: (3.169)\nNote that we are allowing each class to have its own parameter vector\u0015k but we are\nassuming that the classes share the same scale parameter s.\n3.4.1 Sufﬁcient statistics"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 556, "text": "3.4.1 Sufﬁcient statistics\nLet us now consider the problem of estimating the parameter vector\u0011in the gen-\neral exponential family distribution (3.138) using the technique of maximum likeli-\nhood. Taking the gradient of both sides of (3.139) with respect to \u0011, we have\n∇g(\u0011)\n∫\nh(x) exp\n{\n\u0011Tu(x)\n}\ndx\n+ g(\u0011)\n∫\nh(x) exp\n{\n\u0011Tu(x)\n}\nu(x) dx = 0: (3.170)\nRearranging and making use again of (3.139) then gives\n− 1\ng(\u0011)∇g(\u0011) = g(\u0011)\n∫\nh(x) exp\n{\n\u0011Tu(x)\n}\nu(x) dx = E[u(x)]: (3.171)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 557, "text": "∫\nh(x) exp\n{\n\u0011Tu(x)\n}\nu(x) dx = E[u(x)]: (3.171)\nWe therefore obtain the result\n−∇ln g(\u0011) = E[u(x)]: (3.172)\nNote that the covariance of u(x) can be expressed in terms of the second derivatives\nof g(\u0011), and similarly for higher-order moments. Thus, provided we can normalize aExercise 3.36\ndistribution from the exponential family, we can always ﬁnd its moments by simple\ndifferentiation.\n98 3. STANDARD DISTRIBUTIONS\nNow consider a set of independent identically distributed data denoted by X ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 558, "text": "{x1;:::; xn}, for which the likelihood function is given by\np(X|\u0011) =\n(N∏\nn=1\nh(xn)\n)\ng(\u0011)N exp\n{\n\u0011T\nN∑\nn=1\nu(xn)\n}\n: (3.173)\nSetting the gradient of ln p(X|\u0011) with respect to \u0011 to zero, we get the following\ncondition to be satisﬁed by the maximum likelihood estimator \u0011ML:\n−∇ln g(\u0011ML) = 1\nN\nN∑\nn=1\nu(xn); (3.174)\nwhich can in principle be solved to obtain \u0011ML. We see that the solution for the\nmaximum likelihood estimator depends on the data only through ∑\nnu(xn), which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 559, "text": "nu(xn), which\nis therefore called the sufﬁcient statistic of the distribution (3.138). We do not need\nto store the entire data set itself but only the value of the sufﬁcient statistic. For\nthe Bernoulli distribution, for example, the function u(x) is given just by x and\nso we need only keep the sum of the data points {xn}, whereas for the Gaussian\nu(x) = (x;x2)T, and so we should keep both the sum of{xn}and the sum of {x2\nn}."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 560, "text": "n}.\nIf we consider the limit N →∞, then the right-hand side of (3.174) becomes\nE[u(x)], and so by comparing with (3.172) we see that in this limit, \u0011ML will equal\nthe true value \u0011.\n3.5. Nonparametric Methods\nThroughout this chapter, we have focused on the use of probability distributions\nhaving speciﬁc functional forms governed by a small number of parameters whose\nvalues are to be determined from a data set. This is called the parametric approach"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 561, "text": "to density modelling. An important limitation of this approach is that the chosen\ndensity might be a poor model of the distribution that generates the data, which can\nresult in poor predictive performance. For instance, if the process that generates the\ndata is multimodal, then this aspect of the distribution can never be captured by a\nGaussian, which is necessarily unimodal. In this ﬁnal section, we consider some\nnonparametric approaches to density estimation that make few assumptions about"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 562, "text": "the form of the distribution.\n3.5.1 Histograms\nLet us start with a discussion of histogram methods for density estimation, which\nwe have already encountered in the context of marginal and conditional distributions\nin Figure 2.5 and in the context of the central limit theorem in Figure 3.2. Here we\nexplore the properties of histogram density models in more detail, focusing on cases\nwith a single continuous variable x. Standard histograms simply partition x into"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 563, "text": "distinct bins of width ∆ i and then count the number ni of observations of xfalling\n3.5. Nonparametric Methods 99\nFigure 3.13 An illustration of the histogram\napproach to density estimation,\nin which a data set of 50 data\npoints is generated from the dis-\ntribution shown by the green\ncurve. Histogram density esti-\nmates, based on (3.175) with a\ncommon bin width \u0001, are shown\nfor various values of \u0001.\n∆ = 0.04\n0 0.5 1\n0\n5\n∆ = 0.08\n0 0.5 1\n0\n5\n∆ = 0.25\n0 0.5 1\n0\n5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 564, "text": "0\n5\n∆ = 0.08\n0 0.5 1\n0\n5\n∆ = 0.25\n0 0.5 1\n0\n5\nin bin i. To turn this count into a normalized probability density, we simply divide\nby the total number N of observations and by the width ∆ i of the bins to obtain\nprobability values for each bin:\npi = ni\nN∆ i\n(3.175)\nfor which it is easily seen that\n∫\np(x) dx = 1. This gives a model for the density\np(x) that is constant over the width of each bin. Often the bins are chosen to have\nthe same width ∆ i = ∆."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 565, "text": "the same width ∆ i = ∆.\nIn Figure 3.13, we show an example of histogram density estimation. Here the\ndata is drawn from the distribution corresponding to the green curve, which is formed\nfrom a mixture of two Gaussians. Also shown are three examples of histogram\ndensity estimates corresponding to three different choices for the bin width ∆. We\nsee that when ∆ is very small (top ﬁgure), the resulting density model is very spiky,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 566, "text": "with a lot of structure that is not present in the underlying distribution that generated\nthe data set. Conversely, if ∆ is too large (bottom ﬁgure) then the result is a model\nthat is too smooth and consequently fails to capture the bimodal property of the\ngreen curve. The best results are obtained for some intermediate value of ∆ (middle\nﬁgure). In principle, a histogram density model is also dependent on the choice of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 567, "text": "edge location for the bins, though this is typically much less signiﬁcant than the bin\nwidth ∆.\nNote that the histogram method has the property (unlike the methods to be dis-\ncussed shortly) that, once the histogram has been computed, the data set itself can\nbe discarded, which can be advantageous if the data set is large. Also, the histogram\napproach is easily applied if the data points arrive sequentially.\nIn practice, the histogram technique can be useful for obtaining a quick visual-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 568, "text": "ization of data in one or two dimensions but is unsuited to most density estimation\napplications. One obvious problem is that the estimated density has discontinuities\nthat are due to the bin edges rather than any property of the underlying distribution\nthat generated the data. A major limitation of the histogram approach is its scal-\ning with dimensionality. If we divide each variable in a D-dimensional space into\n100 3. STANDARD DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 569, "text": "100 3. STANDARD DISTRIBUTIONS\nM bins, then the total number of bins will be MD. This exponential scaling with\nD is an example of the curse of dimensionality. In a space of high dimensionality,Section 6.1.1\nthe quantity of data needed to provide meaningful estimates of the local probability\ndensity would be prohibitive.\nThe histogram approach to density estimation does, however, teach us two im-\nportant lessons. First, to estimate the probability density at a particular location,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 570, "text": "we should consider the data points that lie within some local neighbourhood of that\npoint. Note that the concept of locality requires that we assume some form of dis-\ntance measure, and here we have been assuming Euclidean distance. For histograms,\nthis neighbourhood property was deﬁned by the bins, and there is a natural ‘smooth-\ning’ parameter describing the spatial extent of the local region, in this case the bin"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 571, "text": "width. Second, to obtain good results, the value of the smoothing parameter should\nbe neither too large nor too small. This is reminiscent of the choice of model com-\nplexity in polynomial regression where the degree M of the polynomial, or alterna-Chapter 1\ntively the value\u0015of the regularization parameter, was optimal for some intermediate\nvalue, neither too large nor too small. Armed with these insights, we turn now to a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 572, "text": "discussion of two widely used nonparametric techniques for density estimation, ker-\nnel estimators and nearest neighbours, which have better scaling with dimensionality\nthan the simple histogram model.\n3.5.2 Kernel densities\nLet us suppose that observations are being drawn from some unknown probabil-\nity density p(x) in some D-dimensional space, which we will take to be Euclidean,\nand we wish to estimate the value of p(x). From our earlier discussion of locality,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 573, "text": "let us consider some small region Rcontaining x. The probability mass associated\nwith this region is given by\nP =\n∫\nR\np(x) dx: (3.176)\nNow suppose that we have collected a data set comprising N observations drawn\nfrom p(x). Because each data point has a probability P of falling within R, the total\nnumber K of points that lie inside Rwill be distributed according to the binomial\ndistribution:Section 3.1.2\nBin(K|N;P) = N!\nK!(N −K)!PK(1 −P)N−K: (3.177)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 574, "text": "Bin(K|N;P) = N!\nK!(N −K)!PK(1 −P)N−K: (3.177)\nUsing (3.11), we see that the mean fraction of points falling inside the region is\nE[K=N] = P, and similarly using (3.12), we see that the variance around this mean\nis var[K=N] = P(1 −P)=N. For large N, this distribution will be sharply peaked\naround the mean and so\nK ≃NP: (3.178)\nIf, however, we also assume that the regionRis sufﬁciently small so that the proba-\nbility density p(x) is roughly constant over the region, then we have\nP ≃p(x)V (3.179)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 575, "text": "P ≃p(x)V (3.179)\n3.5. Nonparametric Methods 101\nwhere V is the volume of R. Combining (3.178) and (3.179), we obtain our density\nestimate in the form\np(x) = K\nNV : (3.180)\nNote that the validity of (3.180) depends on two contradictory assumptions, namely\nthat the region Ris sufﬁciently small that the density is approximately constant over\nthe region and yet sufﬁciently large (in relation to the value of that density) that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 576, "text": "number Kof points falling inside the region is sufﬁcient for the binomial distribution\nto be sharply peaked.\nWe can exploit the result (3.180) in two different ways. Either we can ﬁxKand\ndetermine the value of V from the data, which gives rise to theK-nearest-neighbour\ntechnique discussed shortly, or we can ﬁx V and determine K from the data, giv-\ning rise to the kernel approach. It can be shown that both the K-nearest-neighbour"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 577, "text": "density estimator and the kernel density estimator converge to the true probability\ndensity in the limit N →∞ provided that V shrinks with N and that K grows with\nN, at an appropriate rate (Duda and Hart, 1973).\nWe begin by discussing the kernel method in detail. To start with we take the\nregion Rto be a small hypercube centred on the point x at which we wish to de-\ntermine the probability density. To count the number K of points falling within this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 578, "text": "region, it is convenient to deﬁne the following function:\nk(u) =\n{\n1; |ui|6 1=2, i= 1;:::;D ,\n0; otherwise; (3.181)\nwhich represents a unit cube centred on the origin. The function k(u) is an example\nof a kernel function, and in this context, it is also called a Parzen window. From\n(3.181), the quantity k((x −xn)=h) will be 1 if the data point xn lies inside a cube\nof side hcentred on x, and zero otherwise. The total number of data points lying\ninside this cube will therefore be\nK =\nN∑\nn=1\nk"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 579, "text": "inside this cube will therefore be\nK =\nN∑\nn=1\nk\n(x −xn\nh\n)\n: (3.182)\nSubstituting this expression into (3.180) then gives the following result for the esti-\nmated density at x:\np(x) = 1\nN\nN∑\nn=1\n1\nhDk\n(x −xn\nh\n)\n(3.183)\nwhere we have used V = hD for the volume of a hypercube of side h in D di-\nmensions. Using the symmetry of the function k(u), we can now reinterpret this\nequation, not as a single cube centred on x but as the sum over N cubes centred on\nthe N data points xn."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 580, "text": "the N data points xn.\nAs it stands, the kernel density estimator (3.183) will suffer from one of the same\nproblems that the histogram method suffered from, namely the presence of artiﬁcial\ndiscontinuities, in this case at the boundaries of the cubes. We can obtain a smoother\n102 3. STANDARD DISTRIBUTIONS\nFigure 3.14 Illustration of the kernel den-\nsity model (3.184) applied to the\nsame data set used to demon-\nstrate the histogram approach in\nFigure 3.13. We see that h acts"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 581, "text": "Figure 3.13. We see that h acts\nas a smoothing parameter and\nthat if it is set too small (top\npanel), the result is a very noisy\ndensity model, whereas if it is\nset too large (bottom panel), then\nthe bimodal nature of the under-\nlying distribution from which the\ndata is generated (shown by the\ngreen curve) is washed out. The\nbest density model is obtained\nfor some intermediate value of h\n(middle panel).\nh = 0.005\n0 0.5 1\n0\n5\nh = 0.07\n0 0.5 1\n0\n5\nh = 0.2\n0 0.5 1\n0\n5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 582, "text": "0\n5\nh = 0.07\n0 0.5 1\n0\n5\nh = 0.2\n0 0.5 1\n0\n5\ndensity model if we choose a smoother kernel function, and a common choice is the\nGaussian, which gives rise to the following kernel density model:\np(x) = 1\nN\nN∑\nn=1\n1\n(2\u0019h2)D=2 exp\n{\n−∥x−xn∥2\n2h2\n}\n(3.184)\nwhere hrepresents the standard deviation of the Gaussian components. Thus, our\ndensity model is obtained by placing a Gaussian over each data point, adding up the\ncontributions over the whole data set, and then dividing by N so that the density"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 583, "text": "is correctly normalized. In Figure 3.14, we apply the model (3.184) to the data\nset used earlier to demonstrate the histogram technique. We see that, as expected,\nthe parameter h plays the role of a smoothing parameter, and there is a trade-off\nbetween sensitivity to noise at small h and over-smoothing at large h. Again, the\noptimization of his a problem in model complexity, analogous to the choice of bin\nwidth in histogram density estimation or the degree of the polynomial used in curve"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 584, "text": "ﬁtting.\nWe can choose any other kernel function k(u) in (3.183) subject to the condi-\ntions\nk(u) > 0; (3.185)∫\nk(u) du = 1; (3.186)\nwhich ensure that the resulting probability distribution is non-negative everywhere\nand integrates to one. The class of density model given by (3.183) is called a kernel\ndensity estimator orParzenestimator. It has a great merit that there is no computation\ninvolved in the ‘training’ phase because this simply requires the training set to be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 585, "text": "stored. However, this is also one of its great weaknesses because the computational\ncost of evaluating the density grows linearly with the size of the data set.\n3.5. Nonparametric Methods 103\nFigure 3.15 Illustration of K-nearest-\nneighbour density estimation\nusing the same data set as in\nFigures 3.14 and 3.13. We see\nthat the parameter K governs\nthe degree of smoothing, so\nthat a small value of K leads\nto a very noisy density model\n(top panel), whereas a large\nvalue (bottom panel) smooths"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 586, "text": "value (bottom panel) smooths\nout the bimodal nature of the true\ndistribution (shown by the green\ncurve) from which the data set\nwas generated.\nK = 1\n0 0.5 1\n0\n5\nK = 5\n0 0.5 1\n0\n5\nK = 30\n0 0.5 1\n0\n5\n3.5.3 Nearest-neighbours\nOne of the difﬁculties with the kernel approach to density estimation is that the\nparameter hgoverning the kernel width is ﬁxed for all kernels. In regions of high\ndata density, a large value of h may lead to over-smoothing and a washing out of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 587, "text": "structure that might otherwise be extracted from the data. However, reducing hmay\nlead to noisy estimates elsewhere in the data space where the density is smaller.\nThus, the optimal choice for h may be dependent on the location within the data\nspace. This issue is addressed by nearest-neighbour methods for density estimation.\nWe therefore return to our general result (3.180) for local density estimation,\nand instead of ﬁxing V and determining the value of K from the data, we consider"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 588, "text": "a ﬁxed value of K and use the data to ﬁnd an appropriate value for V. To do this,\nwe consider a small sphere centred on the point x at which we wish to estimate the\ndensity p(x), and we allow the radius of the sphere to grow until it contains precisely\nK data points. The estimate of the density p(x) is then given by (3.180) with V\nset to the volume of the resulting sphere. This technique is known as K nearest\nneighbours and is illustrated in Figure 3.15 for various choices of the parameter K"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 589, "text": "using the same data set as used in Figures 3.13 and 3.14. We see that the value of K\nnow governs the degree of smoothing and that again there is an optimum choice for\nKthat is neither too large nor too small. Note that the model produced byKnearest\nneighbours is not a true density model because the integral over all space diverges.Exercise 3.38\nWe close this chapter by showing how the K-nearest-neighbour technique for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 590, "text": "density estimation can be extended to the problem of classiﬁcation. To do this, we\napply the K-nearest-neighbour density estimation technique to each class separately\nand then make use of Bayes’ theorem. Let us suppose that we have a data set com-\nprising Nk points in class Ck with N points in total, so that ∑\nkNk = N. If we\nwish to classify a new point x, we draw a sphere centred on x containing precisely\nKpoints irrespective of their class. Suppose this sphere has volume V and contains"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 591, "text": "Kk points from class Ck. Then (3.180) provides an estimate of the density associated\n104 3. STANDARD DISTRIBUTIONS\nFigure 3.16 (a) In the K-nearest-\nneighbour classiﬁer, a new point,\nshown by the black diamond, is clas-\nsiﬁed according to the majority class\nmembership of the K closest train-\ning data points, in this case K =\n3. (b) In the nearest-neighbour\n(K = 1 ) approach to classiﬁcation,\nthe resulting decision boundary is\ncomposed of hyperplanes that form\nperpendicular bisectors of pairs of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 592, "text": "perpendicular bisectors of pairs of\npoints from different classes.\nx1\nx2\n(a)\nx1\nx2\n(b)\nwith each class:\np(x|Ck) = Kk\nNkV : (3.187)\nSimilarly, the unconditional density is given by\np(x) = K\nNV (3.188)\nand the class priors are given by\np(Ck) = Nk\nN : (3.189)\nWe can now combine (3.187), (3.188), and (3.189) using Bayes’ theorem to obtain\nthe posterior probability of class membership:\np(Ck|x) =p(x|Ck)p(Ck)\np(x) = Kk\nK : (3.190)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 593, "text": "p(Ck|x) =p(x|Ck)p(Ck)\np(x) = Kk\nK : (3.190)\nWe can minimize the probability of misclassiﬁcation by assigning the test point x to\nthe class having the largest posterior probability, corresponding to the largest value\nof Kk=K. Thus, to classify a new point, we identify the K nearest points from the\ntraining data set and then assign the new point to the class having the largest number\nof representatives amongst this set. Ties can be broken at random. The particular"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 594, "text": "case of K = 1 is called the nearest-neighbour rule, because a test point is simply\nassigned to the same class as the nearest point from the training set. These concepts\nare illustrated in Figure 3.16.\nAn interesting property of the nearest-neighbour (K= 1) classiﬁer is that, in the\nlimit N →∞, the error rate is never more than twice the minimum achievable error\nrate of an optimal classiﬁer, i.e., one that uses the true class distributions (Cover and\nHart, 1967) ."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 595, "text": "Hart, 1967) .\nAs discussed so far, both the K-nearest-neighbour method and the kernel den-\nsity estimator require the entire training data set to be stored, leading to expensive\nExercises 105\ncomputation if the data set is large. This effect can be offset, at the expense of some\nadditional one-off computation, by constructing tree-based search structures to allow\n(approximate) near neighbours to be found efﬁciently without doing an exhaustive"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 596, "text": "search of the data set. Nevertheless, these nonparametric methods are still severely\nlimited. On the other hand, we have seen that simple parametric models are very\nrestricted in terms of the forms of distribution that they can represent. We therefore\nneed to ﬁnd density models that are very ﬂexible and yet for which the complexity\nof the models can be controlled independently of the size of the training set, and this\ncan be achieved using deep neural networks.\nExercises"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 597, "text": "Exercises\n3.1 (?) Verify that the Bernoulli distribution (3.2) satisﬁes the following properties:\n1∑\nx=0\np(x|\u0016) = 1 (3.191)\nE[x] = \u0016 (3.192)\nvar[x] = \u0016(1 −\u0016): (3.193)\nShow that the entropy H[x] of a Bernoulli-distributed random binary variable x is\ngiven by\nH[x] = −\u0016ln \u0016−(1 −\u0016) ln(1−\u0016): (3.194)\n3.2 (??) The form of the Bernoulli distribution given by (3.2) is not symmetric between\nthe two values of x. In some situations, it will be more convenient to use an equiva-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 598, "text": "lent formulation for whichx∈{−1;1}, in which case the distribution can be written\np(x|\u0016) =\n(1 −\u0016\n2\n)(1−x)=2 (1 + \u0016\n2\n)(1+x)=2\n(3.195)\nwhere \u0016∈[−1;1]. Show that the distribution (3.195) is normalized, and evaluate its\nmean, variance, and entropy.\n3.3 (??) In this exercise, we prove that the binomial distribution (3.9) is normalized.\nFirst, use the deﬁnition (3.10) of the number of combinations of midentical objects\nchosen from a total of N to show that\n(N\nm\n)\n+\n( N\nm−1\n)\n=\n(N + 1\nm\n)\n: (3.196)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 599, "text": "(N\nm\n)\n+\n( N\nm−1\n)\n=\n(N + 1\nm\n)\n: (3.196)\nUse this result to prove by induction the following result:\n(1 + x)N =\nN∑\nm=0\n(N\nm\n)\nxm; (3.197)\n106 3. STANDARD DISTRIBUTIONS\nwhich is known as the binomial theorem and which is valid for all real values of x.\nFinally, show that the binomial distribution is normalized, so that\nN∑\nm=0\n(N\nm\n)\n\u0016m(1 −\u0016)N−m = 1; (3.198)\nwhich can be done by ﬁrst pulling a factor (1 −\u0016)N out of the summation and then\nmaking use of the binomial theorem."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 600, "text": "making use of the binomial theorem.\n3.4 (??) Show that the mean of the binomial distribution is given by (3.11). To do this,\ndifferentiate both sides of the normalization condition (3.198) with respect to \u0016and\nthen rearrange to obtain an expression for the mean ofn. Similarly, by differentiating\n(3.198) twice with respect to \u0016and making use of the result (3.11) for the mean of\nthe binomial distribution, prove the result (3.12) for the variance of the binomial."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 601, "text": "3.5 (?) Show that the mode of the multivariate Gaussian (3.26) is given by \u0016.\n3.6 (??) Suppose that x has a Gaussian distribution with mean \u0016and covariance \u0006.\nShow that the linearly transformed variable Ax + b is also Gaussian, and ﬁnd its\nmean and covariance.\n3.7 (???) Show that the Kullback–Leibler divergence between two Gaussian distribu-\ntions q(x) = N(x|\u0016q;\u0006q) and p(x) = N(x|\u0016p;\u0006p) is given by\nKL (q(x)∥p(x))\n= 1\n2\n{\nln |\u0006p|\n|\u0006q|−D+ Tr\n(\n\u0006−1\np \u0006q\n)\n+ (\u0016p −\u0016q)T\u0006−1\np (\u0016p −\u0016q)\n}\n(3.199)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 602, "text": "(\n\u0006−1\np \u0006q\n)\n+ (\u0016p −\u0016q)T\u0006−1\np (\u0016p −\u0016q)\n}\n(3.199)\nwhere Tr(-)denotes the trace of a matrix, and Dis the dimensionality of x.\n3.8 (??) This exercise demonstrates that the multivariate distribution with maximum\nentropy, for a given covariance, is a Gaussian. The entropy of a distribution p(x) is\ngiven by\nH[x] = −\n∫\np(x) lnp(x) dx: (3.200)\nWe wish to maximize H[x] over all distributions p(x) subject to the constraints that\np(x) is normalized and that it has a speciﬁc mean and covariance, so that\n∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 603, "text": "∫\np(x) dx = 1 (3.201)\n∫\np(x)x dx = \u0016 (3.202)\n∫\np(x)(x −\u0016)(x −\u0016)T dx = \u0006: (3.203)\nBy performing a variational maximization of (3.200) and using Lagrange multipliers\nto enforce the constraints (3.201), (3.202), and (3.203), show that the maximum\nlikelihood distribution is given by the Gaussian (3.26).\nExercises 107\n3.9 (???) Show that the entropy of the multivariate Gaussian N(x|\u0016;\u0006) is given by\nH[x] = 1\n2 ln |\u0006|+ D\n2 (1 + ln(2\u0019)) (3.204)\nwhere Dis the dimensionality of x."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 604, "text": "where Dis the dimensionality of x.\n3.10 (???) Consider two random variables x1 and x2 having Gaussian distributions with\nmeans \u00161 and \u00162 and precisions \u001c1 and \u001c2, respectively. Derive an expression for the\ndifferential entropy of the variablex= x1 + x2. To do this, ﬁrst ﬁnd the distribution\nof xby using the relation\np(x) =\n∫∞\n−∞\np(x|x2)p(x2) dx2 (3.205)\nand completing the square in the exponent. Then observe that this represents the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 605, "text": "convolution of two Gaussian distributions, which itself will be Gaussian, and ﬁnally\nmake use of the result (2.99) for the entropy of the univariate Gaussian.\n3.11 (?) Consider the multivariate Gaussian distribution given by (3.26). By writing the\nprecision matrix (inverse covariance matrix) as the sum of a symmetric and an anti-\nsymmetric matrix, show that the antisymmetric term does not appear in the exponent\nof the Gaussian, and hence, that the precision matrix may be taken to be symmetric"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 606, "text": "without loss of generality. Because the inverse of a symmetric matrix is also sym-\nmetric (see Exercise 3.16), it follows that the covariance matrix may also be chosen\nto be symmetric without loss of generality.\n3.12 (???) Consider a real, symmetric matrix \u0006 whose eigenvalue equation is given by\n(3.28). By taking the complex conjugate of this equation, subtracting the original\nequation, and then forming the inner product with eigenvector ui, show that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 607, "text": "eigenvalues \u0015i are real. Similarly, use the symmetry property of \u0006 to show that two\neigenvectors ui and uj will be orthogonal provided \u0015j ̸=\u0015i. Finally, show that,\nwithout loss of generality, the set of eigenvectors can be chosen to be orthonormal,\nso that they satisfy (3.29), even if some of the eigenvalues are zero.\n3.13 (??) Show that a real, symmetric matrix \u0006 having the eigenvector equation (3.28)\ncan be expressed as an expansion in the eigenvectors, with coefﬁcients given by the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 608, "text": "eigenvalues, of the form (3.31). Similarly, show that the inverse matrix \u0006−1 has a\nrepresentation of the form (3.32).\n3.14 (??) A positive deﬁnite matrix \u0006 can be deﬁned as one for which the quadratic form\naT\u0006a (3.206)\nis positive for any real value of the vector a. Show that a necessary and sufﬁcient\ncondition for \u0006 to be positive deﬁnite is that all the eigenvalues \u0015i of \u0006, deﬁned by\n(3.28), are positive.\n3.15 (?) Show that a real, symmetric matrix of size D×Dhas D(D+ 1)=2 independent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 609, "text": "parameters.\n108 3. STANDARD DISTRIBUTIONS\n3.16 (?) Show that the inverse of a symmetric matrix is itself symmetric.\n3.17 (??) By diagonalizing the coordinate system using the eigenvector expansion (3.31),\nshow that the volume contained within the hyperellipsoid corresponding to a constant\nMahalanobis distance ∆ is given by\nVD|\u0006|1=2∆ D (3.207)\nwhere VD is the volume of the unit sphere in D dimensions, and the Mahalanobis\ndistance is deﬁned by (3.27)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 610, "text": "distance is deﬁned by (3.27).\n3.18 (??) Prove the identity (3.60) by multiplying both sides by the matrix\n(\nA B\nC D\n)\n(3.208)\nand making use of the deﬁnition (3.61).\n3.19 (???) In Sections 3.2.4 and 3.2.5, we considered the conditional and marginal distri-\nbutions for a multivariate Gaussian. More generally, we can consider a partitioning\nof the components of x into three groups xa, xb, and xc, with a corresponding par-\ntitioning of the mean vector \u0016and of the covariance matrix \u0006 in the form\n\u0016="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 611, "text": "\u0016=\n\n\n\u0016a\n\u0016b\n\u0016c\n\n; \u0006 =\n\n\n\u0006aa \u0006ab \u0006ac\n\u0006ba \u0006bb \u0006bc\n\u0006ca \u0006cb \u0006cc\n\n: (3.209)\nBy making use of the results of Section 3.2, ﬁnd an expression for the conditional\ndistribution p(xa|xb) in which xc has been marginalized out.\n3.20 (??) A very useful result from linear algebra is the Woodbury matrix inversion for-\nmula given by\n(A + BCD)−1 = A−1 −A−1 B(C−1 + DA−1 B)−1 DA−1 : (3.210)\nBy multiplying both sides by (A + BCD), prove the correctness of this result."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 612, "text": "3.21 (?) Let x and z be two independent random vectors, so that p(x;z) = p(x)p(z).\nShow that the mean of their sumy = x+ z is given by the sum of the means of each\nof the variables separately. Similarly, show that the covariance matrix of y is given\nby the sum of the covariance matrices of x and z.\n3.22 (???) Consider a joint distribution over the variable\nz =\n(\nx\ny\n)\n(3.211)\nwhose mean and covariance are given by (3.92) and (3.89), respectively. By making"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 613, "text": "use of the results (3.76) and (3.77), show that the marginal distributionp(x) is given\nby (3.83). Similarly, by making use of the results (3.65) and (3.66), show that the\nconditional distribution p(y|x)is given by (3.84).\nExercises 109\n3.23 (??) Using the partitioned matrix inversion formula (3.60), show that the inverse of\nthe precision matrix (3.88) is given by the covariance matrix (3.89).\n3.24 (??) By starting from (3.91) and making use of the result (3.89), verify the result\n(3.92)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 614, "text": "(3.92).\n3.25 (??) Consider two multi-dimensional random vectors x and z having Gaussian dis-\ntributions p(x) = N(x|\u0016x;\u0006x) and p(z) = N(z|\u0016z;\u0006z), respectively, together\nwith their sum y = x + z. By considering the linear-Gaussian model comprising\nthe product of the marginal distribution p(x) and the conditional distribution p(y|x)\nand making use of the results (3.93) and (3.94), show that the marginal distribution\nof p(y) is given by\np(y) = N(y|\u0016x + \u0016z;\u0006x + \u0006z): (3.212)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 615, "text": "p(y) = N(y|\u0016x + \u0016z;\u0006x + \u0006z): (3.212)\n3.26 (???) This exercise and the next provide practice at manipulating the quadratic\nforms that arise in linear-Gaussian models, and they also serve as an independent\ncheck of results derived in the main text. Consider a joint distribution p(x;y) de-\nﬁned by the marginal and conditional distributions given by (3.83) and (3.84). By\nexamining the quadratic form in the exponent of the joint distribution and using the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 616, "text": "technique of ‘completing the square’ discussed in Section 3.2, ﬁnd expressions for\nthe mean and covariance of the marginal distribution p(y) in which the variable x\nhas been integrated out. To do this, make use of the Woodbury matrix inversion\nformula (3.210). Verify that these results agree with (3.93) and (3.94).\n3.27 (???) Consider the same joint distribution as in Exercise 3.26, but now use the tech-\nnique of completing the square to ﬁnd expressions for the mean and covariance of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 617, "text": "the conditional distribution p(x|y). Again, verify that these agree with the corre-\nsponding expressions (3.95) and (3.96).\n3.28 (??) To ﬁnd the maximum likelihood solution for the covariance matrix of a mul-\ntivariate Gaussian, we need to maximize the log likelihood function (3.102) with\nrespect to \u0006, noting that the covariance matrix must be symmetric and positive def-\ninite. Here we proceed by ignoring these constraints and doing a straightforward"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 618, "text": "maximization. Using the results (A.21), (A.26), and (A.28) from Appendix A, show\nthat the covariance matrix \u0006 that maximizes the log likelihood function (3.102) is\ngiven by the sample covariance (3.106). We note that the ﬁnal result is necessarily\nsymmetric and positive deﬁnite (provided the sample covariance is non-singular).\n3.29 (??) Use the result (3.42) to prove (3.46). Now, using the results (3.42) and (3.46),\nshow that\nE[xnxT\nm] = \u0016\u0016T + Inm\u0006 (3.213)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 619, "text": "show that\nE[xnxT\nm] = \u0016\u0016T + Inm\u0006 (3.213)\nwhere xn denotes a data point sampled from a Gaussian distribution with mean \u0016\nand covariance\u0006, and Inmdenotes the (n;m) element of the identity matrix. Hence,\nprove the result (3.108).\n3.30 (?) The various trigonometric identities used in the discussion of periodic variables\nin this chapter can be proven easily from the relation\nexp(iA) = cos A+ isin A (3.214)\n110 3. STANDARD DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 620, "text": "110 3. STANDARD DISTRIBUTIONS\nin which iis the square root of minus one. By considering the identity\nexp(iA) exp(−iA) = 1 (3.215)\nprove the result (3.127). Similarly, using the identity\ncos(A−B) = ℜexp{i(A−B)} (3.216)\nwhere ℜdenotes the real part, prove (3.128). Finally, by using sin(A −B) =\nℑexp{i(A−B)}, whereℑdenotes the imaginary part, prove the result (3.133).\n3.31 (??) For large m, the von Mises distribution (3.129) becomes sharply peaked around"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 621, "text": "the mode \u00120. By deﬁning \u0018 = m1=2(\u0012−\u00120) and taking the Taylor expansion of the\ncosine function given by\ncos \u000b= 1 −\u000b2\n2 + O(\u000b4) (3.217)\nshow that as m→∞, the von Mises distribution tends to a Gaussian.\n3.32 (?) Using the trigonometric identity (3.133), show that solution of (3.132) for \u00120 is\ngiven by (3.134).\n3.33 (?) By computing the ﬁrst and second derivatives of the von Mises distribution\n(3.129), and using I0(m) >0 for m> 0, show that the maximum of the distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 622, "text": "occurs when \u0012= \u00120 and that the minimum occurs when \u0012= \u00120 + \u0019(mod 2\u0019).\n3.34 (?) By making use of the result (3.118) together with (3.134) and the trigonometric\nidentity (3.128), show that the maximum likelihood solutionmML for the concentra-\ntion of the von Mises distribution satisﬁes A(mML) = rwhere ris the radius of the\nmean of the observations viewed as unit vectors in the two-dimensional Euclidean\nplane, as illustrated in Figure 3.9."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 623, "text": "plane, as illustrated in Figure 3.9.\n3.35 (?) Verify that the multivariate Gaussian distribution can be cast in exponential fam-\nily form (3.138), and derive expressions for \u0011, u(x), h(x), and g(\u0011) analogous to\n(3.164) to (3.167).\n3.36 (?) The result (3.172) showed that the negative gradient ofln g(\u0011) for the exponential\nfamily is given by the expectation of u(x). By taking the second derivatives of\n(3.139), show that\n−∇∇ln g(\u0011) = E[u(x)u(x)T] −E[u(x)]E[u(x)T] = cov[u(x)]: (3.218)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 624, "text": "3.37 (??) Consider a histogram-like density model in which the space x is divided into\nﬁxed regions for which the density p(x) takes the constant value hi over the ith re-\ngion. The volume of regioniis denoted ∆ i. Suppose we have a set ofNobservations\nof x such that ni of these observations fall in region i. Using a Lagrange multiplier\nto enforce the normalization constraint on the density, derive an expression for the\nmaximum likelihood estimator for the {hi}."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 625, "text": "maximum likelihood estimator for the {hi}.\n3.38 (?) Show that the K-nearest-neighbour density model deﬁnes an improper distribu-\ntion whose integral over all space is divergent.\n4\nSingle-layer\nNetworks:\nRegression\nIn this chapter we discuss some of the basic ideas behind neural networks using the\nframework of linear regression, which we encountered brieﬂy in the context of poly-Section 1.2\nnomial curve ﬁtting. We will see that a linear regression model corresponds to a sim-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 626, "text": "ple form of neural network having a single layer of learnable parameters. Although\nsingle-layer networks have very limited practical applicability, they have simple an-\nalytical properties and provide an excellent framework for introducing many of the\ncore concepts that will lay a foundation for our discussion of deep neural networks\nin later chapters.\n111© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 627, "text": "C. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 628, "text": "112 4. SINGLE-LAYER NETWORKS: REGRESSION\n4.1. Linear Regression\nThe goal of regression is to predict the value of one or more continuous target vari-\nables tgiven the value of a D-dimensional vector x of input variables. Typically we\nare given a training data set comprising N observations {xn}, wheren= 1;:::;N ,\ntogether with corresponding target values{tn}, and the goal is to predict the value of\ntfor a new value of x. To do this, we formulate a function y(x;w) whose values for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 629, "text": "new inputs x constitute the predictions for the corresponding values of t, and where\nw represents a vector of parameters that can be learned from the training data.\nThe simplest model for regression is one that involves a linear combination of\nthe input variables:\ny(x;w) = w0 + w1x1 + ::: + wDxD (4.1)\nwhere x = (x1;:::;x D)T. The term linear regression sometimes refers speciﬁcally\nto this form of model. The key property of this model is that it is a linear function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 630, "text": "of the parameters w0;:::;w D. It is also, however, a linear function of the input\nvariables xi, and this imposes signiﬁcant limitations on the model.\n4.1.1 Basis functions\nWe can extend the class of models deﬁned by (4.1) by considering linear com-\nbinations of ﬁxed nonlinear functions of the input variables, of the form\ny(x;w) = w0 +\nM−1∑\nj=1\nwj\u001ej(x) (4.2)\nwhere \u001ej(x) are known as basis functions. By denoting the maximum value of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 631, "text": "index jby M −1, the total number of parameters in this model will be M.\nThe parameter w0 allows for any ﬁxed offset in the data and is sometimes called\na bias parameter (not to be confused with bias in a statistical sense). It is oftenSection 4.3\nconvenient to deﬁne an additional dummy basis function \u001e0(x) whose value is ﬁxed\nat \u001e0(x) = 1 so that (4.2) becomes\ny(x;w) =\nM−1∑\nj=0\nwj\u001ej(x) = wT\u001e(x) (4.3)\nwhere w = (w0;:::;w M−1 )T and \u001e = (\u001e0;:::;\u001e M−1 )T. We can represent the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 632, "text": "model (4.3) using a neural network diagram, as shown in Figure 4.1.\nBy using nonlinear basis functions, we allow the function y(x;w) to be a non-\nlinear function of the input vector x. Functions of the form (4.2) are called linear\nmodels, however, because they are linear in w. It is this linearity in the parameters\nthat will greatly simplify the analysis of this class of models. However, it also leads\nto some signiﬁcant limitations.Section 6.1\n4.1. Linear Regression 113"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 633, "text": "4.1. Linear Regression 113\nFigure 4.1 The linear regression model (4.3) can be ex-\npressed as a simple neural network diagram\ninvolving a single layer of parameters. Here\neach basis function \u001ej(x) is represented by\nan input node, with the solid node repre-\nsenting the ‘bias’ basis function \u001e0, and the\nfunction y(x;w) is represented by an output\nnode. Each of the parameters wj is shown\nby a line connecting the corresponding basis\nfunction to the output.\n\u001eM−1(x)\n …\n\u001e1(x)\n\u001e0(x)\ny(x;w)\nwM−1\nw1\nw0"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 634, "text": "M−1(x)\n …\n\u001e1(x)\n\u001e0(x)\ny(x;w)\nwM−1\nw1\nw0\nBefore the advent of deep learning it was common practice in machine learning\nto use some form of ﬁxed pre-processing of the input variablesx, also known as fea-\nture extraction, expressed in terms of a set of basis functions{\u001ej(x)}. The goal was\nto choose a sufﬁciently powerful set of basis functions that the resulting learning task\ncould be solved using a simple network model. Unfortunately, it is very difﬁcult to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 635, "text": "hand-craft suitable basis functions for anything but the simplest applications. Deep\nlearning avoids this problem by learning the required nonlinear transformations of\nthe data from the data set itself.\nWe have already encountered an example of a regression problem when we dis-\ncussed curve ﬁtting using polynomials. The polynomial function (1.1) can be ex-Chapter 1\npressed in the form (4.3) if we consider a single input variable xand if we choose"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 636, "text": "basis functions deﬁned by \u001ej(x) = xj. There are many other possible choices for\nthe basis functions, for example\n\u001ej(x) = exp\n{\n−(x−\u0016j)2\n2s2\n}\n(4.4)\nwhere the \u0016j govern the locations of the basis functions in input space, and the\nparameter sgoverns their spatial scale. These are usually referred to as ‘Gaussian’\nbasis functions, although it should be noted that they are not required to have a\nprobabilistic interpretation. In particular the normalization coefﬁcient is unimportant"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 637, "text": "because these basis functions will be multiplied by learnable parameters wj.\nAnother possibility is the sigmoidal basis function of the form\n\u001ej(x) = \u001b\n(x−\u0016j\ns\n)\n(4.5)\nwhere \u001b(a) is the logistic sigmoid function deﬁned by\n\u001b(a) = 1\n1 + exp(−a): (4.6)\nEquivalently, we can use the tanh function because this is related to the logistic\nsigmoid by tanh(a) = 2\u001b(2a) −1, and so a general linear combination of logistic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 638, "text": "sigmoid functions is equivalent to a general linear combination of tanh functions inExercise 4.3\nthe sense that they can represent the same class of input–output functions. These\nvarious choices of basis function are illustrated in Figure 4.2.\n114 4. SINGLE-LAYER NETWORKS: REGRESSION\n−1 0 1\n−1\n−0.5\n0\n0.5\n1\n−1 0 1\n0\n0.25\n0.5\n0.75\n1\n−1 0 1\n0\n0.25\n0.5\n0.75\n1\nFigure 4.2 Examples of basis functions, showing polynomials on the left, Gaussians of the form (4.4) in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 639, "text": "centre, and sigmoidal basis functions of the form (4.5) on the right.\nYet another possible choice of basis function is the Fourier basis, which leads to\nan expansion in sinusoidal functions. Each basis function represents a speciﬁc fre-\nquency and has inﬁnite spatial extent. By contrast, basis functions that are localized\nto ﬁnite regions of input space necessarily comprise a spectrum of different spatial\nfrequencies. In signal processing applications, it is often of interest to consider basis"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 640, "text": "functions that are localized in both space and frequency, leading to a class of func-\ntions known as wavelets (Ogden, 1997; Mallat, 1999; Vidakovic, 1999). These are\nalso deﬁned to be mutually orthogonal, to simplify their application. Wavelets are\nmost applicable when the input values live on a regular lattice, such as the successive\ntime points in a temporal sequence or the pixels in an image.\nMost of the discussion in this chapter, however, is independent of the choice of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 641, "text": "basis function set, and so we will not specify the particular form of the basis func-\ntions, except for numerical illustration. Furthermore, to keep the notation simple, we\nwill focus on the case of a single target variable t, although we will brieﬂy outline\nthe modiﬁcations needed to deal with multiple target variables.Section 4.1.7\n4.1.2 Likelihood function\nWe solved the problem of ﬁtting a polynomial function to data by minimizing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 642, "text": "a sum-of-squares error function, and we also showed that this error function couldSection 1.2\nbe motivated as the maximum likelihood solution under an assumed Gaussian noise\nmodel. We now return to this discussion and consider the least-squares approach,\nand its relation to maximum likelihood, in more detail.\nAs before, we assume that the target variable tis given by a deterministic func-\ntion y(x;w) with additive Gaussian noise so that\nt= y(x;w) + \u000f (4.7)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 643, "text": "t= y(x;w) + \u000f (4.7)\nwhere \u000fis a zero-mean Gaussian random variable with variance \u001b2. Thus, we can\nwrite\np(t|x;w;\u001b2) = N(t|y(x;w);\u001b2): (4.8)\n4.1. Linear Regression 115\nNow consider a data set of inputs X = {x1;:::; xN}with corresponding target\nvalues t1;:::;t N. We group the target variables {tn}into a column vector that we\ndenote by t where the typeface is chosen to distinguish it from a single observation\nof a multivariate target, which would be denotedt. Making the assumption that these"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 644, "text": "data points are drawn independently from the distribution (4.8), we obtain an expres-\nsion for the likelihood function, which is a function of the adjustable parameters w\nand \u001b2:\np(t|X;w;\u001b2) =\nN∏\nn=1\nN(tn|wT\u001e(xn);\u001b2) (4.9)\nwhere we have used (4.3). Taking the logarithm of the likelihood function and mak-\ning use of the standard form (2.49) for the univariate Gaussian, we have\nln p(t|X;w;\u001b2) =\nN∑\nn=1\nln N(tn|wT\u001e(xn);\u001b2)\n= −N\n2 ln \u001b2 −N\n2 ln(2\u0019) − 1\n\u001b2 ED(w) (4.10)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 645, "text": "= −N\n2 ln \u001b2 −N\n2 ln(2\u0019) − 1\n\u001b2 ED(w) (4.10)\nwhere the sum-of-squares error function is deﬁned by\nED(w) = 1\n2\nN∑\nn=1\n{tn −wT\u001e(xn)}2: (4.11)\nThe ﬁrst two terms in (4.10) can be treated as constants when determining w be-\ncause they are independent of w. Therefore, as we saw previously, maximizing theSection 2.3.4\nlikelihood function under a Gaussian noise distribution is equivalent to minimizing\nthe sum-of-squares error function (4.11).\n4.1.3 Maximum likelihood"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 646, "text": "4.1.3 Maximum likelihood\nHaving written down the likelihood function, we can use maximum likelihood\nto determine w and \u001b2. Consider ﬁrst the maximization with respect to w. The\ngradient of the log likelihood function (4.10) with respect to w takes the form\n∇w ln p(t|X;w;\u001b2) = 1\n\u001b2\nN∑\nn=1\n{\ntn −wT\u001e(xn)\n}\n\u001e(xn)T: (4.12)\nSetting this gradient to zero gives\n0 =\nN∑\nn=1\ntn\u001e(xn)T −wT\n(N∑\nn=1\n\u001e(xn)\u001e(xn)T\n)\n: (4.13)\nSolving for w we obtain\nwML =\n(\n\bT\b\n)−1\n\bTt; (4.14)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 647, "text": "wML =\n(\n\bT\b\n)−1\n\bTt; (4.14)\n116 4. SINGLE-LAYER NETWORKS: REGRESSION\nwhich are known as thenormal equations for the least-squares problem. Here\b is an\nN×Mmatrix, called thedesign matrix, whose elements are given byΦ nj = \u001ej(xn),\nso that\n\b =\n\n\n\u001e0(x1) \u001e1(x1) ---\u001eM−1 (x1)\n\u001e0(x2) \u001e1(x2) ---\u001eM−1 (x2)\n..\n. .\n.\n. … .\n.\n.\n\u001e0(xN) \u001e1(xN) ---\u001eM−1 (xN)\n\n: (4.15)\nThe quantity\n\b†≡\n(\n\bT\b\n)−1\n\bT (4.16)\nis known as the Moore–Penrose pseudo-inverse of the matrix \b (Rao and Mitra,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 648, "text": "1971; Golub and Van Loan, 1996). It can be regarded as a generalization of the no-\ntion of a matrix inverse to non-square matrices. Indeed, if\b is square and invertible,\nthen using the property (AB)−1 = B−1 A−1 we see that \b†≡\b−1 .\nAt this point, we can gain some insight into the role of the bias parameterw0. If\nwe make the bias parameter explicit, then the error function (4.11) becomes\nED(w) = 1\n2\nN∑\nn=1\n{tn −w0 −\nM−1∑\nj=1\nwj\u001ej(xn)}2: (4.17)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 649, "text": "2\nN∑\nn=1\n{tn −w0 −\nM−1∑\nj=1\nwj\u001ej(xn)}2: (4.17)\nSetting the derivative with respect to w0 equal to zero and solving for w0, we obtain\nw0 = t−\nM−1∑\nj=1\nwj\u001ej (4.18)\nwhere we have deﬁned\nt= 1\nN\nN∑\nn=1\ntn; \u001ej = 1\nN\nN∑\nn=1\n\u001ej(xn): (4.19)\nThus, the bias w0 compensates for the difference between the averages (over the\ntraining set) of the target values and the weighted sum of the averages of the basis\nfunction values.\nWe can also maximize the log likelihood function (4.10) with respect to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 650, "text": "variance \u001b2, giving\n\u001b2\nML = 1\nN\nN∑\nn=1\n{tn −wT\nML\u001e(xn)}2; (4.20)\nand so we see that the maximum likelihood value of the variance parameter is given\nby the residual variance of the target values around the regression function.\n4.1. Linear Regression 117\nFigure 4.3 Geometrical interpretation of the least-\nsquares solution in an N-dimensional space\nwhose axes are the values of t1;:::;t N. The\nleast-squares regression function is obtained\nby ﬁnding the orthogonal projection of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 651, "text": "by ﬁnding the orthogonal projection of the\ndata vector t onto the subspace spanned by\nthe basis functions \u001ej(x) in which each basis\nfunction is viewed as a vector 'j of length N\nwith elements \u001ej(xn).\nS\nt\ny'1\n'2\n4.1.4 Geometry of least squares\nAt this point, it is instructive to consider the geometrical interpretation of the\nleast-squares solution. To do this, we consider an N-dimensional space whose axes\nare given by the tn, so that t = (t1;:::;t N)T is a vector in this space. Each basis"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 652, "text": "function \u001ej(xn), evaluated at theNdata points, can also be represented as a vector in\nthe same space, denoted by'j, as illustrated inFigure 4.3. Note that 'j corresponds\nto the jth column of\b, whereas \u001e(xn) corresponds to the transpose of thenth row of\n\b. If the number M of basis functions is smaller than the number N of data points,\nthen the M vectors \u001ej(xn) will span a linear subspace Sof dimensionality M. We\ndeﬁne y to be an N-dimensional vector whose nth element is given by y(xn;w),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 653, "text": "where n = 1;:::;N . Because y is an arbitrary linear combination of the vectors\n'j, it can live anywhere in the M-dimensional subspace. The sum-of-squares error\n(4.11) is then equal (up to a factor of1=2) to the squared Euclidean distance between\ny and t. Thus, the least-squares solution for w corresponds to that choice of y that\nlies in subspace Sand is closest to t. Intuitively, from Figure 4.3, we anticipate that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 654, "text": "this solution corresponds to the orthogonal projection of t onto the subspace S. This\nis indeed the case, as can easily be veriﬁed by noting that the solution for y is given\nby \bwML and then conﬁrming that this takes the form of an orthogonal projection.Exercise 4.4\nIn practice, a direct solution of the normal equations can lead to numerical difﬁ-\nculties when \bT\b is close to singular. In particular, when two or more of the basis"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 655, "text": "vectors 'j are co-linear, or nearly so, the resulting parameter values can have large\nmagnitudes. Such near degeneracies will not be uncommon when dealing with real\ndata sets. The resulting numerical difﬁculties can be addressed using the technique\nof singular value decomposition, or SVD (Deisenroth, Faisal, and Ong, 2020). Note\nthat the addition of a regularization term ensures that the matrix is non-singular, even\nin the presence of degeneracies.\n4.1.5 Sequential learning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 656, "text": "4.1.5 Sequential learning\nThe maximum likelihood solution (4.14) involves processing the entire training\nset in one go and is known as a batch method. This can become computationally\ncostly for large data sets. If the data set is sufﬁciently large, it may be worthwhile\nto use sequential algorithms, also known as online algorithms, in which the data\npoints are considered one at a time and the model parameters updated after each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 657, "text": "such presentation. Sequential learning is also appropriate for real-time applications\nin which the data observations arrive in a continuous stream and predictions must be\n118 4. SINGLE-LAYER NETWORKS: REGRESSION\nmade before all the data points are seen.\nWe can obtain a sequential learning algorithm by applying the technique of\nstochastic gradient descent, also known assequential gradient descent, as follows. IfChapter 7\nthe error function comprises a sum over data pointsE = ∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 658, "text": "nEn, then after presenta-\ntion of data point n, the stochastic gradient descent algorithm updates the parameter\nvector w using\nw(\u001c+1) = w(\u001c) −\u0011∇En (4.21)\nwhere \u001c denotes the iteration number, and \u0011 is a suitably chosen learning rate pa-\nrameter. The value of w is initialized to some starting vector w(0). For the sum-of-\nsquares error function (4.11), this gives\nw(\u001c+1) = w(\u001c) + \u0011(tn −w(\u001c)T\u001en)\u001en (4.22)\nwhere \u001en = \u001e(xn). This is known as the least-mean-squares or the LMS algorithm."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 659, "text": "4.1.6 Regularized least squares\nWe have previously introduced the idea of adding a regularization term to anSection 1.2\nerror function to control over-ﬁtting, so that the total error function to be minimized\ntakes the form\nED(w) + \u0015EW(w) (4.23)\nwhere \u0015is the regularization coefﬁcient that controls the relative importance of the\ndata-dependent error ED(w) and the regularization term EW(w). One of the sim-\nplest forms of regularizer is given by the sum of the squares of the weight vector"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 660, "text": "elements:\nEW(w) = 1\n2\n∑\nj\nw2\nj = 1\n2wTw: (4.24)\nIf we also consider the sum-of-squares error function given by\nED(w) = 1\n2\nN∑\nn=1\n{tn −wT\u001e(xn)}2; (4.25)\nthen the total error function becomes\n1\n2\nN∑\nn=1\n{tn −wT\u001e(xn)}2 + \u0015\n2 wTw: (4.26)\nIn statistics, this regularizer provides an example of a parameter shrinkage method\nbecause it shrinks parameter values towards zero. It has the advantage that the error\nfunction remains a quadratic function of w, and so its exact minimizer can be found"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 661, "text": "in closed form. Speciﬁcally, setting the gradient of (4.26) with respect to w to zero\nand solving for w as before, we obtainExercise 4.6\nw =\n(\n\u0015I + \bT\b\n)−1\n\bTt: (4.27)\nThis represents a simple extension of the least-squares solution (4.14).\n4.1. Linear Regression 119\nFigure 4.4 Representation of a linear regres-\nsion model as a neural network hav-\ning a single layer of connections.\nEach basis function is represented\nby a node, with the solid node rep-\nresenting the ‘bias’ basis function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 662, "text": "resenting the ‘bias’ basis function\n\u001e0. Likewise each output y1;:::;y K\nis represented by a node. The\nlinks between the nodes represent\nthe corresponding weight and bias\nparameters.\n\u001eM−1(x)\n …\n\u001e1(x)\n\u001e0(x)\nyK(x;w)\n …\ny1(x;w)\n4.1.7 Multiple outputs\nSo far, we have considered situations with a single target variable t. In some\napplications, we may wish to predict K >1 target variables, which we denote col-\nlectively by the target vector t = (t1;:::;t K)T. This could be done by introducing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 663, "text": "a different set of basis functions for each component of t, leading to multiple, inde-\npendent regression problems. However, a more common approach is to use the same\nset of basis functions to model all of the components the target vector so that\ny(x;w) = WT\u001e(x) (4.28)\nwhere y is a K-dimensional column vector, W is an M ×K matrix of parameters,\nand \u001e(x) is an M-dimensional column vector with elements \u001ej(x) with \u001e0(x) = 1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 664, "text": "as before. Again, this can be represented as a neural network having a single layer\nof parameters, as shown in Figure 4.4.\nSuppose we take the conditional distribution of the target vector to be an isotropic\nGaussian of the form\np(t|x;W;\u001b2) = N(t|WT\u001e(x);\u001b2I): (4.29)\nIf we have a set of observations t1;:::; tN, we can combine these into a matrix T\nof size N ×K such that the nth row is given by tT\nn. Similarly, we can combine the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 665, "text": "n. Similarly, we can combine the\ninput vectors x1;:::; xN into a matrix X. The log likelihood function is then given\nby\nln p(T|X;W;\u001b2) =\nN∑\nn=1\nln N(tn|WT\u001e(xn);\u001b2I)\n= −NK\n2 ln\n(\n2\u0019\u001b2)\n− 1\n2\u001b2\nN∑\nn=1\ntn −WT\u001e(xn)\n\n2\n: (4.30)\nAs before, we can maximize this function with respect to W, giving\nWML =\n(\n\bT\b\n)−1\n\bTT (4.31)\nwhere we have combined the input feature vectors \u001e(x1);:::; \u001e(xN) into a matrix\n\b. If we examine this result for each target variable tk, we have\nwk =\n(\n\bT\b\n)−1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 666, "text": "wk =\n(\n\bT\b\n)−1\n\bTtk = \b†tk (4.32)\n120 4. SINGLE-LAYER NETWORKS: REGRESSION\nwhere tk is an N-dimensional column vector with components tnk for n= 1;:::N .\nThus, the solution to the regression problem decouples between the different target\nvariables, and we need compute only a single pseudo-inverse matrix \b†, which is\nshared by all the vectors wk.\nThe extension to general Gaussian noise distributions having arbitrary covari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 667, "text": "ance matrices is straightforward. Again, this leads to a decoupling into K inde-Exercise 4.7\npendent regression problems. This result is unsurprising because the parameters W\ndeﬁne only the mean of the Gaussian noise distribution, and we know that the max-\nimum likelihood solution for the mean of a multivariate Gaussian is independent of\nthe covariance. From now on, we will therefore consider a single target variable tSection 3.2.7\nfor simplicity.\n4.2. Decision theory"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 668, "text": "for simplicity.\n4.2. Decision theory\nWe have formulated the regression task as one of modelling a conditional proba-\nbility distribution p(t|x), and we have chosen a speciﬁc form for the conditional\nprobability, namely a Gaussian (4.8) with an x-dependent mean y(x;w) governed\nby parameters w and with variance given by the parameter\u001b2. Both w and \u001b2 can be\nlearned from data using maximum likelihood. The result is a predictive distribution\ngiven by\np(t|x;wML;\u001b2\nML) = N(t|y(x;wML);\u001b2\nML): (4.33)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 669, "text": "p(t|x;wML;\u001b2\nML) = N(t|y(x;wML);\u001b2\nML): (4.33)\nThe predictive distribution expresses our uncertainty over the value of t for some\nnew input x. However, for many practical applications we need to predict a speciﬁc\nvalue for t rather than returning an entire distribution, particularly where we must\ntake a speciﬁc action. For example, if our goal is to determine the optimal level of\nradiation to use for treating a tumour and our model predicts a probability distri-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 670, "text": "bution over radiation dose, then we must use that distribution to decide the speciﬁc\ndose to be administered. Our task therefore breaks down into two stages. In the ﬁrst\nstage, called the inference stage, we use the training data to determine a predictive\ndistribution p(t|x). In the second stage, known as the decision stage, we use this\npredictive distribution to determine a speciﬁc value f(x), which will be dependent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 671, "text": "on the input vector x, that is optimal according to some criterion. We can do this\nby minimizing a loss function that depends on both the predictive distributionp(t|x)\nand on f.\nIntuitively we might choose the mean of the conditional distribution, so that\nwe would use f(x) = y(x;wML). In some cases this intuition will be correct, but\nin other situations it can give very poor results. It is therefore useful to formalize"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 672, "text": "this so that we can understand when it applies and under what assumptions, and the\nframework for doing this is called decision theory.\nSuppose that we choose a value f(x) for our prediction when the true value is\nt. In doing so, we incur some form of penalty or cost. This is determined by a\nloss, which we denote L(t;f(x)). Of course, we do not know the true value of t, so\ninstead of minimizing Litself, we minimize the average, or expected, loss which is\n4.2. Decision theory 121"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 673, "text": "4.2. Decision theory 121\nFigure 4.5 The regression function f?(x),\nwhich minimizes the expected\nsquared loss, is given by the\nmean of the conditional distribu-\ntion p(t|x).\nt\nx\np(t|x0, w, σ2)\nf⋆(x)\ngiven by\nE[L] =\n∫∫\nL(t;f(x))p(x;t) dx dt (4.34)\nwhere we are averaging over the distribution of both input and target variables,\nweighted by their joint distribution p(x;t). A common choice of loss function in\nregression problems is the squared loss given by L(t;f(x)) = {f(x) −t}2. In this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 674, "text": "case, the expected loss can be written\nE[L] =\n∫∫\n{f(x) −t}2p(x;t) dx dt: (4.35)\nIt is important not to confuse the squared-loss function with the sum-of-squares\nerror function introduced earlier. The error function is used to set the parameters\nduring training in order to determine the conditional probability distribution p(t|x),\nwhereas the loss function governs how the conditional distribution is used to arrive\nat a predictive function f(x) specifying a prediction for each value of x."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 675, "text": "Our goal is to choose f(x) so as to minimize E[L]. If we assume a completely\nﬂexible function f(x), we can do this formally using the calculus of variations toAppendix B\ngive\n\u000eE[L]\n\u000ef(x) = 2\n∫\n{f(x) −t}p(x;t) dt= 0: (4.36)\nSolving for f(x) and using the sum and product rules of probability, we obtain\nf?(x) = 1\np(x)\n∫\ntp(x;t) dt=\n∫\ntp(t|x) dt= Et[t|x]; (4.37)\nwhich is the conditional average oftconditioned on x and is known as theregression"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 676, "text": "function. This result is illustrated inFigure 4.5. It can readily be extended to multiple\ntarget variables represented by the vector t, in which case the optimal solution is the\nconditional average f?(x) = Et[t|x]. For a Gaussian conditional distribution of theExercise 4.8\n122 4. SINGLE-LAYER NETWORKS: REGRESSION\nform (4.8), the conditional mean will be simply\nE[t|x] =\n∫\ntp(t|x) dt= y(x;w): (4.38)\nThe use of calculus of variations to derive (4.37) implies that we are optimiz-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 677, "text": "ing over all possible functions f(x). Although any parametric model that we can\nimplement in practice is limited in the range of functions that it can represent, the\nframework of deep neural networks, discussed extensively in later chapters, provides\na highly ﬂexible class of functions that, for many practical purposes, can approxi-\nmate any desired function to high accuracy.\nWe can derive this result in a slightly different way, which will also shed light"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 678, "text": "on the nature of the regression problem. Armed with the knowledge that the optimal\nsolution is the conditional expectation, we can expand the square term as follows\n{f(x) −t}2 = {f(x) −E[t|x] +E[t|x]−t}2\n= {f(x) −E[t|x]}2 + 2{f(x) −E[t|x]}{E[t|x]−t}+ {E[t|x]−t}2\nwhere, to keep the notation uncluttered, we useE[t|x]to denote Et[t|x]. Substituting\ninto the loss function (4.35) and performing the integral overt, we see that the cross-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 679, "text": "term vanishes and we obtain an expression for the loss function in the form\nE[L] =\n∫\n{f(x) −E[t|x]}2 p(x) dx +\n∫\nvar [t|x]p(x) dx: (4.39)\nThe function f(x) we seek to determine appears only in the ﬁrst term, which will be\nminimized when f(x) is equal to E[t|x], in which case this term will vanish. This is\nsimply the result that we derived previously, and shows that the optimal least-squares\npredictor is given by the conditional mean. The second term is the variance of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 680, "text": "distribution of t, averaged overx, and represents the intrinsic variability of the target\ndata and can be regarded as noise. Because it is independent of f(x), it represents\nthe irreducible minimum value of the loss function.\nThe squared loss is not the only possible choice of loss function for regression.\nHere we consider brieﬂy one simple generalization of the squared loss, called the\nMinkowski loss, whose expectation is given by\nE[Lq] =\n∫∫\n|f(x) −t|qp(x;t) dx dt; (4.40)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 681, "text": "E[Lq] =\n∫∫\n|f(x) −t|qp(x;t) dx dt; (4.40)\nwhich reduces to the expected squared loss for q = 2. The function |f−t|q is\nplotted against f−tfor various values of qin Figure 4.6. The minimum of E[Lq] is\ngiven by the conditional mean for q = 2, the conditional median for q = 1, and the\nconditional mode for q→ 0.Exercise 4.12\nNote that the Gaussian noise assumption implies that the conditional distribution\nof t given x is unimodal, which may be inappropriate for some applications. In"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 682, "text": "this case a squared loss can lead to very poor results and we need to develop more\nsophisticated approaches. For example, we can extend this model by using mixtures\n4.3. The Bias–Variance Trade-off 123\n−2 −1 0 1 2\nf − t\n0\n1\n2\n|f− t|0.3\nq = 0.3\n−2 −1 0 1 2\nf − t\n0\n1\n2\n|f− t|1\nq = 1\n−2 −1 0 1 2\nf − t\n0\n1\n2\n|f− t|2\nq = 2\n−2 −1 0 1 2\nf − t\n0\n1\n2\n|f− t|10\nq = 10\nFigure 4.6 Plots of the quantity Lq = |f−t|q for various values of q."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 683, "text": "of Gaussians to give multimodal conditional distributions, which often arise in theSection 6.5\nsolution of inverse problems. Our focus in this section has been on decision theory\nfor regression problems, and in the next chapter we shall develop analogous concepts\nfor classiﬁcation tasks.Section 5.2\n4.3. The Bias–Variance Trade-off\nSo far in our discussion of linear models for regression, we have assumed that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 684, "text": "form and number of basis functions are both given. We have also seen that the useSection 1.2\nof maximum likelihood can lead to severe over-ﬁtting if complex models are trained\nusing data sets of limited size. However, limiting the number of basis functions\nto avoid over-ﬁtting has the side effect of limiting the ﬂexibility of the model to\ncapture interesting and important trends in the data. Although a regularization term"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 685, "text": "can control over-ﬁtting for models with many parameters, this raises the question of\nhow to determine a suitable value for the regularization coefﬁcient \u0015. Seeking the\n124 4. SINGLE-LAYER NETWORKS: REGRESSION\nsolution that minimizes the regularized error function with respect to both the weight\nvector w and the regularization coefﬁcient \u0015is clearly not the right approach, since\nthis leads to the unregularized solution with \u0015= 0."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 686, "text": "It is instructive to consider a frequentist viewpoint of the model complexity is-\nsue, known as the bias–variance trade-off. Although we will introduce this concept\nin the context of linear basis function models, where it is easy to illustrate the ideas\nusing simple examples, the discussion has very general applicability. Note, however,\nthat over-ﬁtting is really an unfortunate property of maximum likelihood and does"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 687, "text": "not arise when we marginalize over parameters in a Bayesian setting (Bishop, 2006).\nWhen we discussed decision theory for regression problems, we considered var-Section 4.2\nious loss functions, each of which leads to a corresponding optimal prediction once\nwe are given the conditional distributionp(t|x). A popular choice is the squared-loss\nfunction, for which the optimal prediction is given by the conditional expectation,\nwhich we denote by h(x) and is given by\nh(x) = E[t|x] =\n∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 688, "text": "h(x) = E[t|x] =\n∫\ntp(t|x) dt: (4.41)\nWe have also seen that the expected squared loss can be written in the form\nE[L] =\n∫\n{f(x) −h(x)}2 p(x) dx +\n∫∫\n{h(x)−t}2p(x;t) dx dt: (4.42)\nRecall that the second term, which is independent of f(x), arises from the intrin-\nsic noise on the data and represents the minimum achievable value of the expected\nloss. The ﬁrst term depends on our choice for the function f(x), and we will seek a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 689, "text": "solution for f(x) that makes this term a minimum. Because it is non-negative, the\nsmallest value that we can hope to achieve for this term is zero. If we had an unlim-\nited supply of data (and unlimited computational resources), we could in principle\nﬁnd the regression function h(x) to any desired degree of accuracy, and this would\nrepresent the optimal choice for f(x). However, in practice we have a data set D\ncontaining only a ﬁnite number N of data points, and consequently, we cannot know"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 690, "text": "the regression function h(x) exactly.\nIf we were to model h(x) using a function governed by a parameter vector w,\nthen from a Bayesian perspective, the uncertainty in our model would be expressed\nthrough a posterior distribution over w. A frequentist treatment, however, involves\nmaking a point estimate ofw based on the data setDand tries instead to interpret the\nuncertainty of this estimate through the following thought experiment. Suppose we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 691, "text": "had a large number of data sets each of size N and each drawn independently from\nthe distribution p(t;x). For any given data set D, we can run our learning algorithm\nand obtain a prediction function f(x; D). Different data sets from the ensemble will\ngive different functions and consequently different values of the squared loss. The\nperformance of a particular learning algorithm is then assessed by taking the average\nover this ensemble of data sets."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 692, "text": "over this ensemble of data sets.\nConsider the integrand of the ﬁrst term in (4.42), which for a particular data set\nDtakes the form\n{f(x; D) −h(x)}2: (4.43)\n4.3. The Bias–Variance Trade-off 125\nBecause this quantity will be dependent on the particular data setD, we take its aver-\nage over the ensemble of data sets. If we add and subtract the quantity ED[f(x; D)]\ninside the braces, and then expand, we obtain\n{f(x; D) −ED[f(x; D)] + ED[f(x; D)] −h(x)}2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 693, "text": "{f(x; D) −ED[f(x; D)] + ED[f(x; D)] −h(x)}2\n= {f(x; D) −ED[f(x; D)]}2 + {ED[f(x; D)] −h(x)}2\n+ 2{f(x; D) −ED[f(x; D)]}{ED[f(x; D)] −h(x)}: (4.44)\nWe now take the expectation of this expression with respect to Dand note that the\nﬁnal term will vanish, giving\nED\n[\n{f(x; D) −h(x)}2]\n= {ED[f(x; D)] −h(x)}2\n  \n(bias)2\n+ ED\n[\n{f(x; D) −ED[f(x; D)]}2]\n  \nvariance\n: (4.45)\nWe see that the expected squared difference between f(x; D) and the regression"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 694, "text": "function h(x) can be expressed as the sum of two terms. The ﬁrst term, called the\nsquared bias, represents the extent to which the average prediction over all data sets\ndiffers from the desired regression function. The second term, called the variance,\nmeasures the extent to which the solutions for individual data sets vary around their\naverage, and hence, this measures the extent to which the function f(x; D) is sen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 695, "text": "sitive to the particular choice of data set. We will provide some intuition to support\nthese deﬁnitions shortly when we consider a simple example.\nSo far, we have considered a single input valuex. If we substitute this expansion\nback into (4.42), we obtain the following decomposition of the expected squared\nloss:\nexpected loss = (bias)2 + variance + noise (4.46)\nwhere\n(bias)2 =\n∫\n{ED[f(x; D)] −h(x)}2p(x) dx (4.47)\nvariance =\n∫\nED\n[\n{f(x; D) −ED[f(x; D)]}2]\np(x) dx (4.48)\nnoise =\n∫∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 696, "text": "p(x) dx (4.48)\nnoise =\n∫∫\n{h(x)−t}2p(x;t) dx dt (4.49)\nand the bias and variance terms now refer to integrated quantities.\nOur goal is to minimize the expected loss, which we have decomposed into the\nsum of a (squared) bias, a variance, and a constant noise term. As we will see, there is\na trade-off between bias and variance, with very ﬂexible models having low bias and\nhigh variance, and relatively rigid models having high bias and low variance. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 697, "text": "model with the optimal predictive capability is the one that leads to the best balance\nbetween bias and variance. This is illustrated by considering the sinusoidal data set\nintroduced earlier. Here we independently generate 100 data sets, each containingSection 1.2\n126 4. SINGLE-LAYER NETWORKS: REGRESSION\nN = 25 data points, from the sinusoidal curve h(x) = sin(2\u0019x). The data sets are\nindexed by l = 1;:::;L , where L = 100. For each data set D(l), we ﬁt a model"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 698, "text": "with M = 24 Gaussian basis functions along with a constant ‘bias’ basis function to\ngive a total of 25 parameters. By minimizing the regularized error function (4.26),\nwe obtain a prediction function f(l)(x), as shown in Figure 4.7.\nThe top row corresponds to a large value of the regularization coefﬁcient \u0015that\ngives low variance (because the red curves in the left plot look similar) but high\nbias (because the two curves in the right plot are very different). Conversely on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 699, "text": "the bottom row, for which \u0015 is small, there is large variance (shown by the high\nvariability between the red curves in the left plot) but low bias (shown by the good\nﬁt between the average model ﬁt and the original sinusoidal function). Note that\nthe result of averaging many solutions for the complex model with M = 25 is a\nvery good ﬁt to the regression function, which suggests that averaging may be a\nbeneﬁcial procedure. Indeed, a weighted averaging of multiple solutions lies at the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 700, "text": "heart of a Bayesian approach, although the averaging is with respect to the posterior\ndistribution of parameters, not with respect to multiple data sets.\nWe can also examine the bias–variance trade-off quantitatively for this example.\nThe average prediction is estimated from\nf(x) = 1\nL\nL∑\nl=1\nf(l)(x); (4.50)\nand the integrated squared bias and integrated variance are then given by\n(bias)2 = 1\nN\nN∑\nn=1\n{\nf(xn) −h(xn)\n}2\n(4.51)\nvariance = 1\nN\nN∑\nn=1\n1\nL\nL∑\nl=1\n{\nf(l)(xn) −f(xn)\n}2\n(4.52)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 701, "text": "N\nN∑\nn=1\n1\nL\nL∑\nl=1\n{\nf(l)(xn) −f(xn)\n}2\n(4.52)\nwhere the integral over x, weighted by the distribution p(x), is approximated by a\nﬁnite sum over data points drawn from that distribution. These quantities, along with\ntheir sum, are plotted as a function of ln \u0015in Figure 4.8. We see that small values\nof \u0015allow the model to become ﬁnely tuned to the noise on each individual data set\nleading to large variance. Conversely, a large value of\u0015pulls the weight parameters"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 702, "text": "towards zero leading to large bias.\nNote that the bias–variance decomposition is of limited practical value because\nit is based on averages with respect to ensembles of data sets, whereas in practice\nwe have only the single observed data set. If we had a large number of independent\ntraining sets of a given size, we would be better off combining them into a single\nlarger training set, which of course would reduce the level of over-ﬁtting for a given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 703, "text": "model complexity. Nevertheless, the bias–variance decomposition often provides\nuseful insights into the model complexity issue, and although we have introduced it\nin this chapter from the perspective of regression problems, the underlying intuition\nhas broad applicability.\n4.3. The Bias–Variance Trade-off 127\n0 1x\n−1\n1\nt\nln λ = 3\n0 1x\n−1\n1\nt\n0 1x\n−1\n1\nt\nln λ = 1\n0 1x\n−1\n1\nt\n0 1x\n−1\n1\nt\nln λ = −3\n0 1x\n−1\n1\nt"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 704, "text": "0 1x\n−1\n1\nt\n0 1x\n−1\n1\nt\nln λ = −3\n0 1x\n−1\n1\nt\nFigure 4.7 Illustration of the dependence of bias and variance on model complexity governed by a regulariza-\ntion parameter \u0015, using the sinusoidal data from Chapter 1. There are L= 100 data sets, each having N = 25\ndata points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is\nM = 25 including the bias parameter. The left column shows the result of ﬁtting the model to the data sets for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 705, "text": "various values of ln \u0015(for clarity, only 20 of the 100 ﬁts are shown). The right column shows the corresponding\naverage of the 100 ﬁts (red) along with the sinusoidal function from which the data sets were generated (green).\n128 4. SINGLE-LAYER NETWORKS: REGRESSION\nFigure 4.8 Plot of squared bias and vari-\nance, together with their sum, correspond-\ning to the results shown inFigure 4.7. Also\nshown is the average test set error for a\ntest data set size of 1,000 points. The min-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 706, "text": "test data set size of 1,000 points. The min-\nimum value of (bias)2 + variance occurs\naround ln \u0015 = 0:43, which is close to the\nvalue that gives the minimum error on the\ntest data.\n−3 0 3\nln λ\n0\n0.25\n(bias)2\nvariance\n(bias)2 + variance\ntest error\nExercises\n4.1 (?) Consider the sum-of-squares error function given by (1.2) in which the function\ny(x;w) is given by the polynomial (1.1). Show that the coefﬁcients w = {wi}that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 707, "text": "minimize this error function are given by the solution to the following set of linear\nequations:\nM∑\nj=0\nAijwj = Ti (4.53)\nwhere\nAij =\nN∑\nn=1\n(xn)i+j; Ti =\nN∑\nn=1\n(xn)itn: (4.54)\nHere a sufﬁx ior jdenotes the index of a component, whereas (x)i denotes xraised\nto the power of i.\n4.2 (?) Write down the set of coupled linear equations, analogous to (4.53), satisﬁed by\nthe coefﬁcients withat minimize the regularized sum-of-squares error function given\nby (1.4)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 708, "text": "by (1.4).\n4.3 (?) Show that the tanh function deﬁned by\ntanh(a) = ea −e−a\nea + e−a (4.55)\nand the logistic sigmoid function deﬁned by (4.6) are related by\ntanh(a) = 2\u001b(2a) −1: (4.56)\nHence, show that a general linear combination of logistic sigmoid functions of the\nform\ny(x;w) = w0 +\nM∑\nj=1\nwj\u001b\n(x−\u0016j\ns\n)\n(4.57)\nExercises 129\nis equivalent to a linear combination of tanh functions of the form\ny(x;u) = u0 +\nM∑\nj=1\nujtanh\n(x−\u0016j\n2s\n)\n(4.58)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 709, "text": "y(x;u) = u0 +\nM∑\nj=1\nujtanh\n(x−\u0016j\n2s\n)\n(4.58)\nand ﬁnd expressions to relate the new parameters {u1;:::;u M}to the original pa-\nrameters {w1;:::;w M}.\n4.4 (???) Show that the matrix\n\b(\bT\b)−1 \bT (4.59)\ntakes any vector v and projects it onto the space spanned by the columns of \b. Use\nthis result to show that the least-squares solution (4.14) corresponds to an orthogonal\nprojection of the vector t onto the manifold S, as shown in Figure 4.3."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 710, "text": "4.5 (?) Consider a data set in which each data point tn is associated with a weighting\nfactor rn >0, so that the sum-of-squares error function becomes\nED(w) = 1\n2\nN∑\nn=1\nrn\n{\ntn −wT\u001e(xn)\n}2\n: (4.60)\nFind an expression for the solution w? that minimizes this error function. Give two\nalternative interpretations of the weighted sum-of-squares error function in terms of\n(i) data-dependent noise variance and (ii) replicated data points."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 711, "text": "4.6 (?) By setting the gradient of (4.26) with respect to w to zero, show that the exact\nminimum of the regularized sum-of-squares error function for linear regression is\ngiven by (4.27).\n4.7 (??) Consider a linear basis function regression model for a multivariate target vari-\nable t having a Gaussian distribution of the form\np(t|W;\u0006) = N(t|y(x;W);\u0006) (4.61)\nwhere\ny(x;W) = WT\u001e(x) (4.62)\ntogether with a training data set comprising input basis vectors \u001e(xn) and corre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 712, "text": "sponding target vectors tn, with n= 1;:::;N . Show that the maximum likelihood\nsolution WML for the parameter matrix W has the property that each column is\ngiven by an expression of the form (4.14), which was the solution for an isotropic\nnoise distribution. Note that this is independent of the covariance matrix \u0006. Show\nthat the maximum likelihood solution for \u0006 is given by\n\u0006 = 1\nN\nN∑\nn=1\n(\ntn −WT\nML\u001e(xn)\n)(\ntn −WT\nML\u001e(xn)\n)T\n: (4.63)\n130 4. SINGLE-LAYER NETWORKS: REGRESSION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 713, "text": ": (4.63)\n130 4. SINGLE-LAYER NETWORKS: REGRESSION\n4.8 (?) Consider the generalization of the squared-loss function (4.35) for a single target\nvariable tto multiple target variables described by the vector t given by\nE[L(t;f(x))] =\n∫∫\n∥f(x) −t∥2p(x;t) dx dt: (4.64)\nUsing the calculus of variations, show that the functionf(x) for which this expected\nloss is minimized is given by\nf(x) = Et[t|x]: (4.65)\n4.9 (?) By expansion of the square in (4.64), derive a result analogous to (4.39) and,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 714, "text": "hence, show that the function f(x) that minimizes the expected squared loss for a\nvector t of target variables is again given by the conditional expectation of t in the\nform (4.65).\n4.10 (??) Rederive the result (4.65) by ﬁrst expanding (4.64) analogous to (4.39).\n4.11 (??) The following distribution\np(x|\u001b2;q) = q\n2(2\u001b2)1=qΓ(1=q) exp\n(\n−|x|q\n2\u001b2\n)\n(4.66)\nis a generalization of the univariate Gaussian distribution. Here Γ(x) is the gamma\nfunction deﬁned by\nΓ(x) =\n∫∞\n−∞\nux−1 e−u du: (4.67)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 715, "text": "Γ(x) =\n∫∞\n−∞\nux−1 e−u du: (4.67)\nShow that this distribution is normalized so that∫∞\n−∞\np(x|\u001b2;q) dx= 1 (4.68)\nand that it reduces to the Gaussian when q = 2. Consider a regression model in\nwhich the target variable is given byt= y(x;w)+ \u000fand \u000fis a random noise variable\ndrawn from the distribution (4.66). Show that the log likelihood function overw and\n\u001b2, for an observed data set of input vectors X = {x1;:::; xN}and corresponding\ntarget variables t = (t1;:::;t N)T, is given by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 716, "text": "target variables t = (t1;:::;t N)T, is given by\nln p(t|X;w;\u001b2) = − 1\n2\u001b2\nN∑\nn=1\n|y(xn;w) −tn|q −N\nq ln(2\u001b2) + const (4.69)\nwhere ‘const’ denotes terms independent of bothw and \u001b2. Note that, as a function\nof w, this is the Lq error function considered in Section 4.2.\n4.12 (??) Consider the expected loss for regression problems under the Lq loss function\ngiven by (4.40). Write down the condition that y(x) must satisfy to minimize E[Lq]."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 717, "text": "Show that, for q = 1, this solution represents the conditional median, i.e., the func-\ntion y(x) such that the probability mass for t < y(x) is the same as for t > y(x).\nAlso show that the minimum expected Lq loss for q→ 0 is given by the conditional\nmode, i.e., by the function y(x) being equal to the value of tthat maximizes p(t|x)\nfor each x.\n5\nSingle-layer\nNetworks:\nClassiﬁcation\nIn the previous chapter, we explored a class of regression models in which the out-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 718, "text": "put variables were linear functions of the model parameters and which can therefore\nbe expressed as simple neural networks having a single layer of weight and bias\nparameters. We turn now to a discussion of classiﬁcation problems, and in this chap-\nter, we will focus on an analogous class of models that again can be expressed as\nsingle-layer neural networks. These will allow us to introduce many of the key con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 719, "text": "cepts of classiﬁcation before dealing with more general deep neural networks in later\nchapters.\nThe goal in classiﬁcation is to take an input vector x ∈RD and assign it to one\nof K discrete classes Ck where k = 1;:::;K . In the most common scenario, the\nclasses are taken to be disjoint, so that each input is assigned to one and only one\nclass. The input space is thereby divided into decision regions whose boundaries are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 720, "text": "called decision boundaries or decision surfaces. In this chapter, we consider linear\n131© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 721, "text": "132 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nmodels for classiﬁcation, by which we mean that the decision surfaces are linear\nfunctions of the input vector x and, hence, are deﬁned by (D −1)-dimensional\nhyperplanes within the D-dimensional input space. Data sets whose classes can\nbe separated exactly by linear decision surfaces are said to be linearly separable.\nLinear classiﬁcation models can be applied to data sets that are not linearly separable,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 722, "text": "although not all inputs will be correctly classiﬁed.\nWe can broadly identify three distinct approaches to solving classiﬁcation prob-\nlems. The simplest involves constructing adiscriminant function that directly assigns\neach vector x to a speciﬁc class. A more powerful approach, however, models the\nconditional probability distributions p(Ck|x)in an inference stage and subsequently\nuses these distributions to make optimal decisions. Separating inference and deci-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 723, "text": "sion brings numerous beneﬁts. There are two different approaches to determiningSection 5.2.4\nthe conditional probabilities p(Ck|x). One technique is to model them directly, for\nexample by representing them as parametric models and then optimizing the param-\neters using a training set. This will be called a discriminative probabilistic model.\nAlternatively, we can model the class-conditional densities p(x|Ck), together with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 724, "text": "the prior probabilities p(Ck) for the classes, and then compute the required posterior\nprobabilities using Bayes’ theorem:\np(Ck|x) =p(x|Ck)p(Ck)\np(x) : (5.1)\nThis will be called a generative probabilistic model because it offers the opportunity\nto generate samples from each of the class-conditional densities p(x|Ck). In this\nchapter, we will discuss examples of all three approaches: discriminant functions,\ngenerative probabilistic models, and discriminative probabilistic models."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 725, "text": "5.1. Discriminant Functions\nA discriminant is a function that takes an input vector x and assigns it to one of K\nclasses, denoted Ck. In this chapter, we will restrict attention tolinear discriminants,\nnamely those for which the decision surfaces are hyperplanes. To simplify the dis-\ncussion, we consider ﬁrst two classes and then investigate the extension to K >2\nclasses.\n5.1.1 Two classes\nThe simplest representation of a linear discriminant function is obtained by tak-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 726, "text": "ing a linear function of the input vector so that\ny(x) = wTx + w0 (5.2)\nwhere w is called a weight vector, and w0 is a bias (not to be confused with bias in\nthe statistical sense). An input vector x is assigned to class C1 if y(x) > 0 and to\nclass C2 otherwise. The corresponding decision boundary is therefore deﬁned by the\nrelation y(x) = 0, which corresponds to a (D−1)-dimensional hyperplane within\n5.1. Discriminant Functions 133\nFigure 5.1 Illustration of the geometry of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 727, "text": "Figure 5.1 Illustration of the geometry of\na linear discriminant function in two dimen-\nsions. The decision surface, shown in red,\nis perpendicular to w, and its displacement\nfrom the origin is controlled by the bias pa-\nrameter w0. Also, the signed orthogonal\ndistance of a general point x from the deci-\nsion surface is given by y(x)=∥w∥.\nx2\nx1\nw\nx\ny(x)\n∥w∥\nx⊥\n−w0\n∥w∥\ny = 0\ny < 0\ny >0\nR2\nR1\nthe D-dimensional input space. Consider two points xA and xB both of which lie on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 728, "text": "the decision surface. Because y(xA) = y(xB) = 0, we have wT(xA −xB) = 0 and\nhence the vector w is orthogonal to every vector lying within the decision surface,\nand so w determines the orientation of the decision surface. Similarly, if x is a point\non the decision surface, then y(x) = 0, and so the normal distance from the origin\nto the decision surface is given by\nwTx\n∥w∥= − w0\n∥w∥: (5.3)\nWe therefore see that the bias parameter w0 determines the location of the decision"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 729, "text": "surface. These properties are illustrated for the case of D= 2 in Figure 5.1.\nFurthermore, note that the value of y(x) gives a signed measure of the perpen-\ndicular distance rof the point x from the decision surface. To see this, consider an\narbitrary point x and let x⊥ be its orthogonal projection onto the decision surface,\nso that\nx = x⊥+ r w\n∥w∥: (5.4)\nMultiplying both sides of this result bywT and adding w0, and making use ofy(x) =\nwTx + w0 and y(x⊥) = wTx⊥+ w0 = 0, we have\nr= y(x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 730, "text": "r= y(x)\n∥w∥: (5.5)\nThis result is illustrated in Figure 5.1.\nAs with linear regression models, it is sometimes convenient to use a more com-Section 4.1.1\npact notation in which we introduce an additional dummy ‘input’ valuex0 = 1 and\nthen deﬁne ˜w = (w0;w) and ˜x = (x0;x) so that\ny(x) = ˜wT˜x: (5.6)\n134 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nR1\nR2\nR3\n?\nC1\nnot C1\nC2\nnot C2\nR1\nR2\nR3\n?C1\nC2\nC1\nC3\nC2\nC3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 731, "text": "?\nC1\nnot C1\nC2\nnot C2\nR1\nR2\nR3\n?C1\nC2\nC1\nC3\nC2\nC3\nFigure 5.2 Attempting to construct a K-class discriminant from a set of two-class discriminant functions leads\nto ambiguous regions, as shown in green. On the left is an example with two discriminant functions designed to\ndistinguish points in class Ck from points not in class Ck. On the right is an example involving three discriminant\nfunctions each of which is used to separate a pair of classes Ck and Cj."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 732, "text": "In this case, the decision surfaces are D-dimensional hyperplanes passing through\nthe origin of the (D+ 1)-dimensional expanded input space.\n5.1.2 Multiple classes\nNow consider the extension of linear discriminant functions to K >2 classes.\nWe might be tempted to build a K-class discriminant by combining a number of\ntwo-class discriminant functions. However, this leads to some serious difﬁculties\n(Duda and Hart, 1973), as we now show."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 733, "text": "(Duda and Hart, 1973), as we now show.\nConsider a model with K−1 classiﬁers, each of which solves a two-class prob-\nlem of separating points in a particular class Ck from points not in that class. This\nis known as a one-versus-the-rest classiﬁer. The left-hand example in Figure 5.2\nshows an example involving three classes where this approach leads to regions of\ninput space that are ambiguously classiﬁed.\nAn alternative is to introduce K(K −1)=2 binary discriminant functions, one"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 734, "text": "for every possible pair of classes. This is known as a one-versus-one classiﬁer. Each\npoint is then classiﬁed according to a majority vote amongst the discriminant func-\ntions. However, this too runs into the problem of ambiguous regions, as illustrated\nin the right-hand diagram of Figure 5.2.\nWe can avoid these difﬁculties by considering a single K-class discriminant\ncomprising Klinear functions of the form\nyk(x) = wT\nkx + wk0 (5.7)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 735, "text": "yk(x) = wT\nkx + wk0 (5.7)\nand then assigning a point x to class Ck if yk(x) >yj(x) for all j ̸=k. The decision\nboundary between class Ck and class Cj is therefore given by yk(x) = yj(x) and\n5.1. Discriminant Functions 135\nFigure 5.3 Illustration of the decision regions for a\nmulti-class linear discriminant, with the\ndecision boundaries shown in red. If\ntwo points xA and xB both lie inside the\nsame decision region Rk, then any point\nbx that lies on the line connecting these"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 736, "text": "bx that lies on the line connecting these\ntwo points must also lie inRk, and hence,\nthe decision region must be singly con-\nnected and convex.\nRi\nRj\nRk\nxA\nxB\nbx\nhence corresponds to a (D−1)-dimensional hyperplane deﬁned by\n(wk −wj)Tx + (wk0 −wj0) = 0: (5.8)\nThis has the same form as the decision boundary for the two-class case discussed in\nSection 5.1.1, and so analogous geometrical properties apply.\nThe decision regions of such a discriminant are always singly connected and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 737, "text": "convex. To see this, consider two pointsxA and xB both of which lie inside decision\nregion Rk, as illustrated in Figure 5.3. Any point ˆx that lies on the line connecting\nxA and xB can be expressed in the form\nˆx = \u0015xA + (1 −\u0015)xB (5.9)\nwhere 0 6 \u00156 1. From the linearity of the discriminant functions, it follows that\nyk(ˆx) = \u0015yk(xA) + (1−\u0015)yk(xB): (5.10)\nBecause both xA and xB lie inside Rk, it follows that yk(xA) > yj(xA) and that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 738, "text": "yk(xB) >yj(xB), for all j ̸=k, and hence yk(ˆx) >yj(ˆx), and so ˆx also lies inside\nRk. Thus, Rk is singly connected and convex.\nNote that for two classes, we can either employ the formalism discussed here,\nbased on two discriminant functions y1(x) and y2(x), or else use the simpler but\nessentially equivalent formulation based on a single discriminant function y(x).Section 5.1.1\n5.1.3 1-of-K coding\nFor regression problems, the target variablet was simply the vector of real num-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 739, "text": "bers whose values we wish to predict. In classiﬁcation, there are various ways of\nusing target values to represent class labels. For two-class problems, the most conve-\nnient is the binary representation in which there is a single target variable t∈{0;1}\nsuch that t = 1 represents class C1 and t = 0 represents class C2. We can interpret\nthe value of tas the probability that the class isC1, with the probability values taking"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 740, "text": "only the extreme values of 0 and 1. For K >2 classes, it is convenient to use a\n1-of-K coding scheme, also known as the one-hot encoding scheme, in which t is\na vector of length K such that if the class is Cj, then all elements tk of t are zero\n136 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nexcept element tj, which takes the value 1. For instance, if we have K = 5 classes,\nthen a data point from class 2 would be given the target vector\nt = (0;1;0;0;0)T: (5.11)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 741, "text": "t = (0;1;0;0;0)T: (5.11)\nAgain, we can interpret the value oftk as the probability that the class isCk in which\nthe probabilities take only the values 0 and 1.\n5.1.4 Least squares for classiﬁcation\nWith linear regression models, the minimization of a sum-of-squares error func-\ntion leads to a simple closed-form solution for the parameter values. It is thereforeSection 4.1.3\ntempting to see if we can apply the same least-squares formalism to classiﬁcation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 742, "text": "problems. Consider a general classiﬁcation problem with K classes and a 1-of-K\nbinary coding scheme for the target vectort. One justiﬁcation for using least squares\nin such a context is that it approximates the conditional expectation E[t|x]of the\ntarget values given the input vector. For a binary coding scheme, this conditional ex-\npectation is given by the vector of posterior class probabilities. Unfortunately, theseExercise 5.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 743, "text": "probabilities are typically approximated rather poorly, and indeed the approxima-\ntions can have values outside the range (0;1). However, it is instructional to explore\nthese simple models and to understand how these limitations arise.\nEach class Ck is described by its own linear model so that\nyk(x) = wT\nkx + wk0 (5.12)\nwhere k = 1;:::;K . We can conveniently group these together using vector nota-\ntion so that\ny(x) = ˜WT˜x (5.13)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 744, "text": "tion so that\ny(x) = ˜WT˜x (5.13)\nwhere ˜W is a matrix whose kth column comprises the (D+ 1)-dimensional vector\n˜wk = (wk0;wT\nk)T and ˜x is the corresponding augmented input vector(1;xT)T with\na dummy input x0 = 1. A new input x is then assigned to the class for which the\noutput yk = ˜wT\nk˜x is largest.\nWe now determine the parameter matrix ˜W by minimizing a sum-of-squares\nerror function. Consider a training data set {xn;tn}where n = 1;:::;N , and\ndeﬁne a matrix T whose nth row is the vector tT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 745, "text": "deﬁne a matrix T whose nth row is the vector tT\nn, together with a matrix ˜X whose\nnth row is ˜xT\nn. The sum-of-squares error function can then be written as\nED(˜W) = 1\n2Tr\n{\n( ˜X˜W −T)T( ˜X˜W −T)\n}\n: (5.14)\nSetting the derivative with respect to ˜W to zero and rearranging, we obtain the solu-\ntion for ˜W in the form\n˜W = ( ˜XT ˜X)−1 ˜XTT = ˜X†T (5.15)\nwhere ˜X†is the pseudo-inverse of the matrix ˜X. We then obtain the discriminantSection 4.1.3\n5.1. Discriminant Functions 137"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 746, "text": "5.1. Discriminant Functions 137\nfunction in the form\ny(x) = ˜WT˜x = TT\n(\n˜X†\n)T\n˜x: (5.16)\nAn interesting property of least-squares solutions with multiple target variables\nis that if every target vector in the training set satisﬁes some linear constraint\naTtn + b= 0 (5.17)\nfor some constants a and b, then the model prediction for any value of x will satisfy\nthe same constraint so thatExercise 5.3\naTy(x) + b= 0: (5.18)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 747, "text": "aTy(x) + b= 0: (5.18)\nThus, if we use a 1-of-K coding scheme for K classes, then the predictions made\nby the model will have the property that the elements of y(x) will sum to 1 for any\nvalue of x. However, this summation constraint alone is not sufﬁcient to allow the\nmodel outputs to be interpreted as probabilities because they are not constrained to\nlie within the interval (0;1).\nThe least-squares approach gives an exact closed-form solution for the discrim-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 748, "text": "inant function parameters. However, even as a discriminant function (where we use\nit to make decisions directly and dispense with any probabilistic interpretation), it\nsuffers from some severe problems. We have seen that the sum-of-squares error\nfunction can be viewed as the negative log likelihood under the assumption of a\nGaussian noise distribution. If the true distribution of the data is markedly differentSection 2.3.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 749, "text": "from being Gaussian, then least squares can give poor results. In particular, least\nsquares is very sensitive to the presence of outliers, which are data points located a\nlong way from the bulk of the data. This is illustrated inFigure 5.4. Here we see that\nthe additional data points in the right-hand ﬁgure produce a signiﬁcant change in the\nlocation of the decision boundary, even though these points would be correctly clas-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 750, "text": "siﬁed by the original decision boundary in the left-hand ﬁgure. The sum-of-squares\nerror function gives too much weight to data points that are a long way from the\ndecision boundary, even though they are correctly classiﬁed. Outliers can arise due\nto rare events or may simply be due to mistakes in the data set. Techniques that are\nsensitive to a very few data points are said to lack robustness. For comparison, Fig-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 751, "text": "ure 5.4 also shows results from a technique called logistic regression, which is moreSection 5.4.3\nrobust to outliers.\nThe failure of least squares should not surprise us when we recall that it cor-\nresponds to maximum likelihood under the assumption of a Gaussian conditional\ndistribution, whereas binary target vectors clearly have a distribution that is far from\nGaussian. By adopting more appropriate probabilistic models, we can obtain clas-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 752, "text": "siﬁcation techniques with much better properties than least squares, and which can\nalso be generalized to give ﬂexible nonlinear neural network models, as we will see\nin later chapters.\n138 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\n−4 −2 0 2 4 6 8\n−8\n−6\n−4\n−2\n0\n2\n4\n−4 −2 0 2 4 6 8\n−8\n−6\n−4\n−2\n0\n2\n4\nFigure 5.4 The left-hand plot shows data from two classes, denoted by red crosses and blue circles, together"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 753, "text": "with the decision boundaries found by least squares (magenta curve) and by a logistic regression model (green\ncurve). The right-hand plot shows the corresponding results obtained when extra data points are added at the\nbottom right of the diagram, showing that least squares is highly sensitive to outliers, unlike logistic regression.\n5.2.\nDecision Theory\nWhen we discussed linear regression we saw how the process of making predictions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 754, "text": "in machine learning can be broken down into the two stages of inference and de-\ncision. We now explore this perspective in much greater depth speciﬁcally in theSection 4.2\ncontext of classiﬁers.\nSuppose we have an input vector x together with a corresponding vector t of\ntarget variables, and our goal is to predict t given a new value for x. For regression\nproblems, t will comprise continuous variables and in general will be a vector as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 755, "text": "we may wish to predict several related quantities. For classiﬁcation problems, t will\nrepresent class labels. Again, t will in general be a vector if we have more than\ntwo classes. The joint probability distribution p(x;t) provides a complete summary\nof the uncertainty associated with these variables. Determining p(x;t) from a set\nof training data is an example of inference and is typically a very difﬁcult problem"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 756, "text": "whose solution forms the subject of much of this book. In a practical application,\nhowever, we must often make a speciﬁc prediction for the value of t or more gen-\nerally take a speciﬁc action based on our understanding of the values t is likely to\ntake, and this aspect is the subject of decision theory.\nConsider, for example, our earlier medical diagnosis problem in which we have\ntaken an image of a skin lesion on a patient, and we wish to determine whether the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 757, "text": "patient has cancer. In this case, the input vector x is the set of pixel intensities in\n5.2. Decision Theory 139\nthe image, and the output variable twill represent the absence of cancer, which we\ndenote by the class C1, or the presence of cancer, which we denote by the class C2.\nWe might, for instance, choose tto be a binary variable such that t= 0 corresponds\nto class C1 and t = 1 corresponds to class C2. We will see later that this choice of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 758, "text": "label values is particularly convenient when working with probabilities. The gen-\neral inference problem then involves determining the joint distribution p(x;Ck), or\nequivalently p(x;t), which gives us the most complete probabilistic description of\nthe variables. Although this can be a very useful and informative quantity, ultimately,\nwe must decide either to give treatment to the patient or not, and we would like this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 759, "text": "choice to be optimal according to some appropriate criterion (Duda and Hart, 1973).\nThis is the decision step, and the aim of decision theory is that it should tell us how\nto make optimal decisions given the appropriate probabilities. We will see that the\ndecision stage is generally very simple, even trivial, once we have solved the in-\nference problem. Here we give an introduction to the key ideas of decision theory"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 760, "text": "as required for the rest of the book. Further background, as well as more detailed\naccounts, can be found in Berger (1985) and Bather (2000).\nBefore giving a more detailed analysis, let us ﬁrst consider informally how we\nmight expect probabilities to play a role in making decisions. When we obtain the\nskin image x for a new patient, our goal is to decide which of the two classes to assign\nthe image to. We are therefore interested in the probabilities of the two classes, given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 761, "text": "the image, which are given by p(Ck|x). Using Bayes’ theorem, these probabilities\ncan be expressed in the form\np(Ck|x) =p(x|Ck)p(Ck)\np(x) : (5.19)\nNote that any of the quantities appearing in Bayes’ theorem can be obtained from\nthe joint distribution p(x;Ck) by either marginalizing or conditioning with respect to\nthe appropriate variables. We can now interpretp(Ck) as the prior probability for the\nclass Ck and p(Ck|x)as the corresponding posterior probability. Thus, p(C1) repre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 762, "text": "sents the probability that a person has cancer, before the image is taken. Similarly,\np(C1|x)is the posterior probability, revised using Bayes’ theorem in light of the in-\nformation contained in the image. If our aim is to minimize the chance of assigning\nx to the wrong class, then intuitively we would choose the class having the higher\nposterior probability. We now show that this intuition is correct, and we also discuss\nmore general criteria for making decisions.\n5.2.1 Misclassiﬁcation rate"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 763, "text": "5.2.1 Misclassiﬁcation rate\nSuppose that our goal is simply to make as few misclassiﬁcations as possible.\nWe need a rule that assigns each value of x to one of the available classes. Such a\nrule will divide the input space into regionsRk called decision regions, one for each\nclass, such that all points in Rk are assigned to class Ck. The boundaries between\ndecision regions are called decision boundaries or decision surfaces. Note that each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 764, "text": "decision region need not be contiguous but could comprise some number of disjoint\nregions. To ﬁnd the optimal decision rule, consider ﬁrst the case of two classes, as in\nthe cancer problem, for instance. A mistake occurs when an input vector belonging\n140 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nto class C1 is assigned to class C2 or vice versa. The probability of this occurring is\ngiven by\np(mistake) = p(x ∈R1;C2) + p(x ∈R2;C1)\n=\n∫\nR1\np(x;C2) dx +\n∫\nR2\np(x;C1) dx: (5.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 765, "text": "=\n∫\nR1\np(x;C2) dx +\n∫\nR2\np(x;C1) dx: (5.20)\nWe are free to choose the decision rule that assigns each point x to one of the\ntwo classes. Clearly, to minimize p(mistake) we should arrange that each x is as-\nsigned to whichever class has the smaller value of the integrand in (5.20). Thus, if\np(x;C1) > p(x;C2) for a given value of x, then we should assign that x to class\nC1. From the product rule of probability, we have p(x;Ck) = p(Ck|x)p(x). Because"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 766, "text": "the factor p(x) is common to both terms, we can restate this result as saying that\nthe minimum probability of making a mistake is obtained if each value of x is as-\nsigned to the class for which the posterior probability p(Ck|x)is largest. This result\nis illustrated for two classes and a single input variable xin Figure 5.5.\nFor the more general case of K classes, it is slightly easier to maximize the\nprobability of being correct, which is given by\np(correct) =\nK∑\nk=1\np(x ∈Rk;Ck)\n=\nK∑\nk=1\n∫\nRk"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 767, "text": "p(correct) =\nK∑\nk=1\np(x ∈Rk;Ck)\n=\nK∑\nk=1\n∫\nRk\np(x;Ck) dx; (5.21)\nwhich is maximized when the regions Rk are chosen such that each x is assigned\nto the class for which p(x;Ck) is largest. Again, using the product rule p(x;Ck) =\np(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see\nthat each x should be assigned to the class having the largest posterior probability\np(Ck|x).\n5.2.2 Expected loss\nFor many applications, our objective will be more complex than simply mini-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 768, "text": "mizing the number of misclassiﬁcations. Let us consider again the medical diagnosis\nproblem. We note that, if a patient who does not have cancer is incorrectly diagnosed\nas having cancer, the consequences may be that they experience some distress plus\nthere is the need for further investigations. Conversely, if a patient with cancer is\ndiagnosed as healthy, the result may be premature death due to lack of treatment."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 769, "text": "Thus, the consequences of these two types of mistake can be dramatically different.\nIt would clearly be better to make fewer mistakes of the second kind, even if this was\nat the expense of making more mistakes of the ﬁrst kind.\nWe can formalize such issues through the introduction of a loss function, also\ncalled a cost function, which is a single, overall measure of loss incurred in taking\nany of the available decisions or actions. Our goal is then to minimize the total loss"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 770, "text": "incurred. Note that some authors consider instead a utility function, whose value\n5.2. Decision Theory 141\nˆxx0\np(x, C1)\np(x, C2)\nR1 R2\n(a)\nˆx\np(x, C1)\np(x, C2)\nR1 R2\n(b)\nFigure 5.5 Schematic illustration of the joint probabilities p(x;Ck) for each of two classes plotted against x,\ntogether with the decision boundary x = bx. Values of x > bx are classiﬁed as class C2 and hence belong to\ndecision region R2, whereas points x <bx are classiﬁed as C1 and belong to R1. Errors arise from the blue,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 771, "text": "green, and red regions, so that for x <bx, the errors are due to points from class C2 being misclassiﬁed as C1\n(represented by the sum of the red and green regions). Conversely for points in the region x> bx, the errors are\ndue to points from class C1 being misclassiﬁed as C2 (represented by the blue region). By varying the location\nbxof the decision boundary, as indicated by the red double-headed arrow in (a), the combined areas of the blue"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 772, "text": "and green regions remains constant, whereas the size of the red region varies. The optimal choice forbxis where\nthe curves for p(x;C1) and p(x;C2) cross, as shown in (b) and corresponding to bx = x0, because in this case\nthe red region disappears. This is equivalent to the minimum misclassiﬁcation rate decision rule, which assigns\neach value of xto the class having the higher posterior probability p(Ck|x).\n142 5. SINGLE-LAYER NETWORKS: CLASSIFICATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 773, "text": "142 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nFigure 5.6 An example of a loss matrix with elements\nLkj for the cancer treatment problem. The rows cor-\nrespond to the true class, whereas the columns corre-\nspond to the assignment of class made by our decision\ncriterion.\n(normal cancer\nnormal 0 1\ncancer 100 0\n)\nthey aim to maximize. These are equivalent concepts if we take the utility to be\nsimply the negative of the loss. Throughout this text we will use the loss function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 774, "text": "convention. Suppose that, for a new value ofx, the true class isCk and that we assign\nx to class Cj (where j may or may not be equal to k). In so doing, we incur some\nlevel of loss that we denote by Lkj, which we can view as the k;j element of a loss\nmatrix. For instance, in our cancer example, we might have a loss matrix of the form\nshown in Figure 5.6. This particular loss matrix says that there is no loss incurred if"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 775, "text": "the correct decision is made, there is a loss of 1 if a healthy patient is diagnosed as\nhaving cancer, whereas there is a loss of 100 if a patient having cancer is diagnosed\nas healthy.\nThe optimal solution is the one that minimizes the loss function. However, the\nloss function depends on the true class, which is unknown. For a given input vectorx,\nour uncertainty in the true class is expressed through the joint probability distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 776, "text": "p(x;Ck), and so we seek instead to minimize the average loss, where the average is\ncomputed with respect to this distribution and is given by\nE[L] =\n∑\nk\n∑\nj\n∫\nRj\nLkjp(x;Ck) dx: (5.22)\nEach x can be assigned independently to one of the decision regions Rj. Our goal\nis to choose the regions Rj to minimize the expected loss (5.22), which implies that\nfor each x, we should minimize ∑\nkLkjp(x;Ck). As before, we can use the product"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 777, "text": "kLkjp(x;Ck). As before, we can use the product\nrule p(x;Ck) = p(Ck|x)p(x)to eliminate the common factor of p(x). Thus, the\ndecision rule that minimizes the expected loss assigns each new x to the class j for\nwhich the quantity ∑\nk\nLkjp(Ck|x) (5.23)\nis a minimum. Once we have chosen values for the loss matrix elements Lkj, this is\nclearly trivial to do.\n5.2.3 The reject option\nWe have seen that classiﬁcation errors arise from the regions of input space"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 778, "text": "where the largest of the posterior probabilitiesp(Ck|x)is signiﬁcantly less than unity\nor equivalently where the joint distributionsp(x;Ck) have comparable values. These\nare the regions where we are relatively uncertain about class membership. In some\napplications, it will be appropriate to avoid making decisions on the difﬁcult cases\nin anticipation of obtaining a lower error rate on those examples for which a classi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 779, "text": "ﬁcation decision is made. This is known as the reject option. For example, in our\nhypothetical cancer screening example, it may be appropriate to use an automatic\n5.2. Decision Theory 143\nFigure 5.7 Illustration of the reject option. Inputs\nxsuch that the larger of the two poste-\nrior probabilities is less than or equal to\nsome threshold \u0012will be rejected.\nx\np(C1jx) p(C2jx)\n0:0\n1:0\n\u0012\nreject region\nsystem to classify those images for which there is little doubt as to the correct class,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 780, "text": "while requesting a biopsy to classify the more ambiguous cases. We can achieve this\nby introducing a threshold \u0012 and rejecting those inputs x for which the largest of\nthe posterior probabilities p(Ck|x)is less than or equal to \u0012. This is illustrated for\ntwo classes and a single continuous input variable xin Figure 5.7. Note that setting\n\u0012= 1 will ensure that all examples are rejected, whereas if there are Kclasses, then"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 781, "text": "setting \u0012 <1=K will ensure that no examples are rejected. Thus, the fraction of\nexamples that are rejected is controlled by the value of \u0012.\nWe can easily extend the reject criterion to minimize the expected loss, when a\nloss matrix is given, by taking account of the loss incurred when a reject decision is\nmade.Exercise 5.10\n5.2.4 Inference and decision\nWe have broken the classiﬁcation problem down into two separate stages, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 782, "text": "inference stage in which we use training data to learn a model for p(Ck|x)and the\nsubsequent decision stage in which we use these posterior probabilities to make op-\ntimal class assignments. An alternative possibility would be to solve both problems\ntogether and simply learn a function that maps inputsx directly into decisions. Such\na function is called a discriminant function.\nIn fact, we can identify three distinct approaches to solving decision problems,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 783, "text": "all of which have been used in practical applications. These are, in decreasing order\nof complexity, as follows:\n(a) First, solve the inference problem of determining the class-conditional den-\nsities p(x|Ck) for each class Ck individually. Separately infer the prior class\nprobabilities p(Ck). Then use Bayes’ theorem in the form\np(Ck|x) =p(x|Ck)p(Ck)\np(x) (5.24)\nto ﬁnd the posterior class probabilities p(Ck|x). As usual, the denominator in\n144 5. SINGLE-LAYER NETWORKS: CLASSIFICATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 784, "text": "144 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nBayes’ theorem can be found in terms of the quantities in the numerator, using\np(x) =\n∑\nk\np(x|Ck)p(Ck): (5.25)\nEquivalently, we can model the joint distribution p(x;Ck) directly and then\nnormalize to obtain the posterior probabilities. Having found the posterior\nprobabilities, we use decision theory to determine the class membership for\neach new input x. Approaches that explicitly or implicitly model the distribu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 785, "text": "tion of inputs as well as outputs are known as generative models, because by\nsampling from them, it is possible to generate synthetic data points in the input\nspace.\n(b) First, solve the inference problem of determining the posterior class probabili-\nties p(Ck|x), and then subsequently use decision theory to assign each newx to\none of the classes. Approaches that model the posterior probabilities directly\nare called discriminative models."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 786, "text": "are called discriminative models.\n(c) Find a function f(x), called a discriminant function, that maps each input x\ndirectly onto a class label. For instance, for two-class problems, f(-)might be\nbinary valued and such that f = 0 represents class C1 and f = 1 represents\nclass C2. In this case, probabilities play no role.\nLet us consider the relative merits of these three alternatives. Approach (a) is the\nmost demanding because it involves ﬁnding the joint distribution over both x and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 787, "text": "Ck. For many applications, x will have high dimensionality, and consequently, we\nmay need a large training set to be able to determine the class-conditional densities to\nreasonable accuracy. Note that the class priors p(Ck) can often be estimated simply\nfrom the fractions of the training set data points in each of the classes. One advantage\nof approach (a), however, is that it also allows the marginal density of data p(x) to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 788, "text": "be determined from (5.25). This can be useful for detecting new data points that\nhave low probability under the model and for which the predictions may be of low\naccuracy, which is known as outlier detection or novelty detection (Bishop, 1994;\nTarassenko, 1995).\nHowever, if we wish only to make classiﬁcation decisions, then it can be waste-\nful of computational resources and excessively demanding of data to ﬁnd the joint"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 789, "text": "distribution p(x;Ck) when in fact we really need only the posterior probabilities\np(Ck|x), which can be obtained directly through approach (b). Indeed, the class-\nconditional densities may contain a signiﬁcant amount of structure that has little ef-\nfect on the posterior probabilities, as illustrated in Figure 5.8. There has been much\ninterest in exploring the relative merits of generative and discriminative approaches"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 790, "text": "to machine learning and in ﬁnding ways to combine them (Jebara, 2004; Lasserre,\nBishop, and Minka, 2006).\nAn even simpler approach is (c) in which we use the training data to ﬁnd a\ndiscriminant function f(x) that maps each x directly onto a class label, thereby\ncombining the inference and decision stages into a single learning problem. In the\nexample of Figure 5.8, this would correspond to ﬁnding the value of x shown by\n5.2. Decision Theory 145\np(x| C 1)\np(x|C2)\nx\nclass densities"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 791, "text": "p(x| C 1)\np(x|C2)\nx\nclass densities\n0 0.2 0.4 0.6 0.8 1\n0\n1\n2\n3\n4\n5\nx\np(C1|x) p(C2|x)\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nFigure 5.8 Example of the class-conditional densities for two classes having a single input variable x (left\nplot) together with the corresponding posterior probabilities (right plot). Note that the left-hand mode of the\nclass-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 792, "text": "vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiﬁcation\nrate, assuming the prior class probabilities, p(C1) and p(C2), are equal.\nthe vertical green line, because this is the decision boundary giving the minimum\nprobability of misclassiﬁcation.\nWith option (c), however, we no longer have access to the posterior probabilities\np(Ck|x). There are many powerful reasons for wanting to compute the posterior"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 793, "text": "probabilities, even if we subsequently use them to make decisions. These include:\nMinimizing risk. Consider a problem in which the elements of the loss matrix are\nsubjected to revision from time to time (such as might occur in a ﬁnancial\napplication). If we know the posterior probabilities, we can trivially revise the\nminimum risk decision criterion by modifying (5.23) appropriately. If we have\nonly a discriminant function, then any change to the loss matrix would require"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 794, "text": "that we return to the training data and solve the inference problem afresh.\nReject option. Posterior probabilities allow us to determine a rejection criterion that\nwill minimize the misclassiﬁcation rate, or more generally the expected loss,\nfor a given fraction of rejected data points.\nCompensating for class priors. Consider our cancer screening example again, andSection 2.1.1\nsuppose that we have collected a large number of images from the general pop-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 795, "text": "ulation for use as training data, which we use to build an automated screening\nsystem. Because cancer is rare amongst the general population, we might ﬁnd\nthat, say, only 1 in every 1,000 examples corresponds to the presence of cancer.\n146 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nIf we used such a data set to train an adaptive model, we could run into severe\ndifﬁculties due to the small proportion of those in the cancer class. For in-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 796, "text": "stance, a classiﬁer that assigned every point to the normal class would achieve\n99.9% accuracy, and it may be difﬁcult to avoid this trivial solution. Also, even\na large data set will contain very few examples of skin images corresponding\nto cancer, and so the learning algorithm will not be exposed to a broad range\nof examples of such images and hence is not likely to generalize well. A bal-\nanced data set with equal numbers of examples from each of the classes would"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 797, "text": "allow us to ﬁnd a more accurate model. However, we then have to compensate\nfor the effects of our modiﬁcations to the training data. Suppose we have used\nsuch a modiﬁed data set and found models for the posterior probabilities. From\nBayes’ theorem (5.24), we see that the posterior probabilities are proportional\nto the prior probabilities, which we can interpret as the fractions of points in\neach class. We can therefore simply take the posterior probabilities obtained"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 798, "text": "from our artiﬁcially balanced data set, divide by the class fractions in that data\nset, and then multiply by the class fractions in the population to which we wish\nto apply the model. Finally, we need to normalize to ensure that the new poste-\nrior probabilities sum to one. Note that this procedure cannot be applied if we\nhave learned a discriminant function directly instead of determining posterior\nprobabilities.\nCombining models. For complex applications, we may wish to break the problem"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 799, "text": "into a number of smaller sub-problems each of which can be tackled by a sep-\narate module. For example, in our hypothetical medical diagnosis problem,\nwe may have information available from, say, blood tests as well as skin im-\nages. Rather than combine all of this heterogeneous information into one huge\ninput space, it may be more effective to build one system to interpret the im-\nages and a different one to interpret the blood data. If each of the two models"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 800, "text": "gives posterior probabilities for the classes, then we can combine the outputs\nsystematically using the rules of probability. One simple way to do this is to\nassume that, for each class separately, the distributions of inputs for the im-\nages, denoted by xI, and the blood data, denoted by xB, are independent, so\nthat\np(xI;xB|Ck) = p(xI|Ck)p(xB|Ck): (5.26)\nThis is an example of a conditional independence property, because the in-Section 11.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 801, "text": "dependence holds when the distribution is conditioned on the class Ck. The\nposterior probability, given both the image and blood data, is then given by\np(Ck|xI;xB) ∝ p(xI;xB|Ck)p(Ck)\n∝ p(xI|Ck)p(xB|Ck)p(Ck)\n∝ p(Ck|xI)p(Ck|xB)\np(Ck) : (5.27)\nThus, we need the class prior probabilitiesp(Ck), which we can easily estimate\nfrom the fractions of data points in each class, and then we need to normalize\n5.2. Decision Theory 147\nFigure 5.9 The confusion matrix for the cancer treat-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 802, "text": "ment problem, in which the rows correspond to the true\nclass and the columns correspond to the assignment\nof class made by our decision criterion. The elements\nof the matrix show the numbers of true negatives, false\npositives, false negatives, and true positives.\n(normal cancer\nnormal NTN NFP\ncancer NFN NTP\n)\nthe resulting posterior probabilities so they sum to one. The particular condi-\ntional independence assumption (5.26) is an example of a naive Bayes model.Section 11.2.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 803, "text": "Note that the joint marginal distribution p(xI;xB) will typically not factorize\nunder this model. We will see in later chapters how to construct models for\ncombining data that do not require the conditional independence assumption\n(5.26). A further advantage of using models that output probabilities rather\nthan decisions is that they can easily be made differentiable with respect to\nany adjustable parameters (such as the weight coefﬁcients in the polynomial"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 804, "text": "regression example), which allows them to be composed and trained jointly\nusing gradient-based optimization methods.Chapter 7\n5.2.5 Classiﬁer accuracy\nThe simplest measure of performance for a classiﬁer is the fraction of test set\npoints that are correctly classiﬁed. However, we have seen that different types of\nerror can have different consequences, as expressed through the loss matrix, and\noften we therefore do not simply wish to minimize the number of misclassiﬁcations."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 805, "text": "By changing the location of the decision boundary, we can make trade-offs between\ndifferent kinds of error, for example with the goal of minimizing an expected loss.\nBecause this is such an important concept, we will introduce some deﬁnitions and\nterminology so that the performance of a classiﬁer can be better characterized.\nWe will consider again our cancer screening example. For each person tested,Section 2.1.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 806, "text": "there is a ‘true label’ of whether they have cancer or not, and there is also the predic-\ntion made by the classiﬁer. If, for a particular person, the classiﬁer predicts cancer\nand this is in fact the true label, then the prediction is called a true positive. How-\never, if the person does not have cancer it is afalse positive. Likewise, if the classiﬁer\npredicts that a person does not have cancer and this is correct, then the prediction is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 807, "text": "called a true negative, otherwise it is a false negative. The false positives are also\nknown as type 1 errors whereas the false negatives are called type 2 errors. If N is\nthe total number of people taking the test, then NTP is the number of true positives,\nNFP is the number of false positives, NTN is the number of true negatives, and NFN\nis the number of false negatives, where\nN = NTP + NFP + NTN + NFN: (5.28)\nThis can be represented as a confusion matrix as shown in Figure 5.9. Accuracy,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 808, "text": "measured by the fraction of correct classiﬁcations, is then given by\nAccuracy = NTP + NTN\nNTP + NFP + NTN + NFN\n: (5.29)\n148 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nWe can see that accuracy can be misleading if there are strongly imbalanced classes.\nIn our cancer screening example, for instance, where only 1 person in 1;000 has\ncancer, a naive classiﬁer that simply decides that nobody has cancer will achieve\n99:9% accuracy and yet is completely useless."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 809, "text": "99:9% accuracy and yet is completely useless.\nSeveral other quantities can be deﬁned in terms of these numbers, of which the\nmost commonly encountered are\nPrecision = NTP\nNTP + NFP\n(5.30)\nRecall = NTP\nNTP + NFN\n(5.31)\nFalse positive rate = NFP\nNFP + NTN\n(5.32)\nFalse discovery rate = NFP\nNFP + NTP\n(5.33)\nIn our cancer screening example, precision represents an estimate of the probability\nthat a person who has a positive test does indeed have cancer, whereas recall is an"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 810, "text": "estimate of the probability that a person who has cancer is correctly detected by\nthe test. The false positive rate is an estimate of the probability that a person who is\nnormal will be classiﬁed as having cancer, whereas the false discovery rate represents\nthe fraction of those testing positive who do not in fact have cancer.\nBy altering the location of the decision boundary, we can change the trade-offs\nbetween the two kinds of errors. To understand this trade-off, we revisit Figure 5.5,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 811, "text": "but now we label the various regions as shown in Figure 5.10. We can relate the\nlabelled regions to the various true and false rates as follows:\nNFP=N = E (5.34)\nNTP=N = D+ E (5.35)\nNFN=N = B+ C (5.36)\nNTN=N = A+ C (5.37)\nwhere we are implicitly considering the limit N →∞ so that we can relate number\nof observations to probabilities.\n5.2.6 ROC curve\nA probabilistic classiﬁer will output a posterior probability, which can be con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 812, "text": "verted to a decision by setting a threshold. As the value of the threshold is varied, we\ncan reduce type 1 errors at the expense of increasing type 2 errors, or vice versa. To\nbetter understand this trade-off, it is useful to plot thereceiver operating characteris-\ntic or ROC curve (Fawcett, 2006), a name that originates from procedures to measure\nthe performance of radar receivers. This is a graph of true positive rate versus false\npositive rate, as shown in Figure 5.11."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 813, "text": "positive rate, as shown in Figure 5.11.\nAs the decision boundary in Figure 5.10 is moved from −∞ to ∞, the ROC\ncurve is traced out and can then be generated by plotting the cumulative fraction of\n5.2. Decision Theory 149\nˆxx0\np(x, C1)\np(x, C2)\nR1 R2\nA\nB\nC\nD\nE\nFigure 5.10 As in Figure 5.5, with the various regions labelled. In the cancer classiﬁcation problem, region R1\nis assigned to the normal class whereas region R2 is assigned to the cancer class."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 814, "text": "correct detection of cancer on the y-axis versus the cumulative fraction of incorrect\ndetection on the x-axis. Note that a speciﬁc confusion matrix represents one point\nalong the ROC curve. The best possible classiﬁer would be represented by a point at\nthe top left corner of the ROC diagram. The bottom left corner represents a simple\nclassiﬁer that assigns every point to the normal class and therefore has no true posi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 815, "text": "tives but also no false positives. Similarly, the top right corner represents a classiﬁer\nthat assigns everything to the cancer class and therefore has no false negatives but\nalso no true negatives. In Figure 5.11, the classiﬁers represented by the blue curve\nare better than those of the red curve for any choice of, say, false positive rate. It\nis also possible, however, for such curves to cross over, in which case the choice of\nwhich is better will depend on the choice of operating point."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 816, "text": "As a baseline, we can consider a random classiﬁer that simply assigns each data\npoint to cancer with probability \u001aand to normal with probability 1 −\u001a. As we vary\nthe value of \u001a it will trace out an ROC curve given by a diagonal straight line, as\nshown in Figure 5.11. Any classiﬁer below the diagonal line performs worse than\nrandom guessing.\nSometimes it is useful to have a single number that characterises the whole ROC"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 817, "text": "curve. One approach is to measure the area under the curve (AUC). A value of 0:5\nfor the AUC represents random guessing whereas a value of 1:0 represents a perfect\nclassiﬁer.\nAnother measure is the F-score, which is the geometric mean of precision and\n150 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nFigure 5.11 The receiver operator characteristic\n(ROC) curve is a plot of true positive\nrate against false positive rate, and\nit characterizes the trade-off between"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 818, "text": "it characterizes the trade-off between\ntype 1 and type 2 errors in a classiﬁ-\ncation problem. The upper blue curve\nrepresents a better classiﬁer than the\nlower red curve. Here the dashed\ncurve represents the performance of\na simple random classiﬁer.\n0 1False positive rate\n0\n1True positive rate\nrecall, and is therefore deﬁned by\nF = 2 ×precision ×recall\nprecision + recall (5.38)\n= 2NTP\n2NTP + NFP + NFN\n: (5.39)\nOf course, we can also combine the confusion matrix inFigure 5.9 with the loss ma-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 819, "text": "trix in Figure 5.6 to compute the expected loss by multiplying the elements pointwise\nand summing the resulting products.\nAlthough the ROC curve can be extended to more than two classes, it rapidly\nbecomes cumbersome as the number of classes increases.\n5.3. Generative Classiﬁers\nWe turn next to a probabilistic view of classiﬁcation and show how models with\nlinear decision boundaries arise from simple assumptions about the distribution of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 820, "text": "the data. We have already discussed the distinction between the discriminative and\nthe generative approaches to classiﬁcation. Here we will adopt a generative approachSection 5.2.4\nin which we model the class-conditional densities p(x|Ck) as well as the class priors\np(Ck) and then use these to compute posterior probabilities p(Ck|x)through Bayes’\ntheorem.\nFirst, consider problems having two classes. The posterior probability for class\n5.3. Generative Classiﬁers 151"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 821, "text": "5.3. Generative Classiﬁers 151\nFigure 5.12 Plot of the logistic sigmoid\nfunction \u001b(a) deﬁned by\n(5.42), shown in red, together\nwith the scaled probit function\n\b(\u0015a), for \u00152 = \u0019=8, shown\nin dashed blue, where \b(a)\nis deﬁned by (5.86). The\nscaling factor \u0019=8 is chosen\nso that the derivatives of\nthe two curves are equal for\na= 0.\n−5 0 5\n0\n0.5\n1C1 can be written as\np(C1|x) = p(x|C1)p(C1)\np(x|C1)p(C1) + p(x|C2)p(C2)\n= 1\n1 + exp(−a) = \u001b(a) (5.40)\nwhere we have deﬁned\na= ln p(x|C1)p(C1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 822, "text": "where we have deﬁned\na= ln p(x|C1)p(C1)\np(x|C2)p(C2) (5.41)\nand \u001b(a) is the logistic sigmoid function deﬁned by\n\u001b(a) = 1\n1 + exp(−a); (5.42)\nwhich is plotted in Figure 5.12. The term ‘sigmoid’ means S-shaped. This type of\nfunction is sometimes also called a ‘squashing function’ because it maps the whole\nreal axis into a ﬁnite interval. The logistic sigmoid has been encountered already\nin earlier chapters and plays an important role in many classiﬁcation algorithms. It"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 823, "text": "satisﬁes the following symmetry property:\n\u001b(−a) = 1 −\u001b(a) (5.43)\nas is easily veriﬁed. The inverse of the logistic sigmoid is given by\na= ln\n( \u001b\n1 −\u001b\n)\n(5.44)\nand is known as the logit function. It represents the log of the ratio of probabilities\nln [p(C1|x)=p(C2|x)]for the two classes, also known as the log odds.\nNote that in (5.40), we have simply rewritten the posterior probabilities in an\nequivalent form, and so the appearance of the logistic sigmoid may seem artiﬁcial."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 824, "text": "152 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nHowever, it will have signiﬁcance provided a(x) has a constrained functional form.\nWe will shortly consider situations in which a(x) is a linear function of x, in which\ncase the posterior probability is governed by a generalized linear model.\nIf there are K >2 classes, we have\np(Ck|x) = p(x|Ck)p(Ck)∑\njp(x|Cj)p(Cj)\n= exp(ak)∑\njexp(aj); (5.45)\nwhich is known as the normalized exponential and can be regarded as a multi-class"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 825, "text": "generalization of the logistic sigmoid. Here the quantities ak are deﬁned by\nak = ln (p(x|Ck)p(Ck)) : (5.46)\nThe normalized exponential is also known as the softmax function, as it represents\na smoothed version of the ‘max’ function because, if ak ≫ aj for all j ̸=k, then\np(Ck|x)≃1, and p(Cj|x)≃0.\nWe now investigate the consequences of choosing speciﬁc forms for the class-\nconditional densities, looking ﬁrst at continuous input variables x and then dis-\ncussing brieﬂy discrete inputs."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 826, "text": "cussing brieﬂy discrete inputs.\n5.3.1 Continuous inputs\nLet us assume that the class-conditional densities are Gaussian. We will then ex-\nplore the resulting form for the posterior probabilities. To start with, we will assume\nthat all classes share the same covariance matrix \u0006. Thus, the density for class Ck is\ngiven by\np(x|Ck) = 1\n(2\u0019)D=2\n1\n|\u0006|1=2 exp\n{\n−1\n2(x −\u0016k)T\u0006−1 (x −\u0016k)\n}\n: (5.47)\nFirst, suppose that we have two classes. From (5.40) and (5.41), we have\np(C1|x) =\u001b(wTx + w0) (5.48)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 827, "text": "p(C1|x) =\u001b(wTx + w0) (5.48)\nwhere we have deﬁned\nw = \u0006−1 (\u00161 −\u00162) (5.49)\nw0 = −1\n2\u0016T\n1 \u0006−1\u00161 + 1\n2\u0016T\n2 \u0006−1\u00162 + ln p(C1)\np(C2): (5.50)\nWe see that the quadratic terms in x from the exponents of the Gaussian densities\nhave cancelled (due to the assumption of common covariance matrices), leading to\na linear function of x in the argument of the logistic sigmoid. This result is illus-\ntrated for a two-dimensional input space x in Figure 5.13. The resulting decision"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 828, "text": "boundaries correspond to surfaces along which the posterior probabilities p(Ck|x)\nx1 x2\n0\n1\nx1 x2\nFigure 5.13 The left-hand plot shows the class-conditional densities for two classes, denoted red and blue.\nOn the right is the corresponding posterior probability p(C1|x), which is given by a logistic sigmoid of a linear\nfunction of x. The surface in the right-hand plot is coloured using a proportion of red ink given by p(C1|x) and a\nproportion of blue ink given by p(C2|x) = 1 −p(C1|x)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 829, "text": "are constant and so will be given by linear functions of x, and therefore the decision\nboundaries are linear in input space. The prior probabilitiesp(Ck) enter only through\nthe bias parameter w0, so that changes in the priors have the effect of making par-\nallel shifts of the decision boundary and more generally of the parallel contours of\nconstant posterior probability.\nFor the general case of Kclasses, the posterior probabilities are given by (5.45)\nwhere, from (5.46) and (5.47), we have"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 830, "text": "where, from (5.46) and (5.47), we have\nak(x) = wT\nkx + wk0 (5.51)\nin which we have deﬁned\nwk = \u0006−1\u0016k (5.52)\nwk0 = −1\n2\u0016T\nk\u0006−1\u0016k + lnp(Ck): (5.53)\nWe see that theak(x) are again linear functions of x as a consequence of the cancel-\nlation of the quadratic terms due to the shared covariances. The resulting decision\nboundaries, corresponding to the minimum misclassiﬁcation rate, will occur when\ntwo of the posterior probabilities (the two largest) are equal, and so will be deﬁned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 831, "text": "by linear functions of x. Thus, we again have a generalized linear model.\nIf we relax the assumption of a shared covariance matrix and allow each class-\nconditional density p(x|Ck) to have its own covariance matrix \u0006k, then the earlier\ncancellations will no longer occur, and we will obtain quadratic functions of x, giv-\ning rise to a quadratic discriminant. The linear and quadratic decision boundaries\nare illustrated in Figure 5.14.\n5.3.2 Maximum likelihood solution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 832, "text": "5.3.2 Maximum likelihood solution\nOnce we have speciﬁed a parametric functional form for the class-conditional\ndensities p(x|Ck), we can then determine the values of the parameters, together with\n5.3. Generative Classiﬁers 153\n154 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nx1\nx2\nx1\nx2\nFigure 5.14 The left-hand plot shows the class-conditional densities for three classes each having a Gaussian"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 833, "text": "distribution, coloured red, green, and blue, in which the red and blue classes have the same covariance matrix.\nThe right-hand plot shows the corresponding posterior probabilities, in which each point on the image is coloured\nusing proportions of red, blue, and green ink corresponding to the posterior probabilities for the respective\nthree classes. The decision boundaries are also shown. Notice that the boundary between the red and blue"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 834, "text": "classes, which have the same covariance matrix, is linear, whereas those between the other pairs of classes are\nquadratic.\nthe prior class probabilities p(Ck), using maximum likelihood. This requires a data\nset comprising observations of x along with their corresponding class labels.\nFirst, suppose we have two classes, each having a Gaussian class-conditional\ndensity with a shared covariance matrix, and suppose we have a data set {xn;tn}"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 835, "text": "where n= 1;:::;N . Here tn = 1 denotes class C1 and tn = 0 denotes class C2. We\ndenote the prior class probability p(C1) = \u0019, so that p(C2) = 1 −\u0019. For a data point\nxn from class C1, we have tn = 1 and hence\np(xn;C1) = p(C1)p(xn|C1) = \u0019N(xn|\u00161;\u0006):\nSimilarly for class C2, we have tn = 0 and hence\np(xn;C2) = p(C2)p(xn|C2) = (1 −\u0019)N(xn|\u00162;\u0006):\nThus, the likelihood function is given by\np(t;X|\u0019;\u00161;\u00162;\u0006) =\nN∏\nn=1\n[\u0019N(xn|\u00161;\u0006)]tn\n[(1 −\u0019)N(xn|\u00162;\u0006)]1−tn\n(5.54)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 836, "text": "n=1\n[\u0019N(xn|\u00161;\u0006)]tn\n[(1 −\u0019)N(xn|\u00162;\u0006)]1−tn\n(5.54)\nwhere t = ( t1;:::;t N)T. As usual, it is convenient to maximize the log of the\nlikelihood function. Consider ﬁrst the maximization with respect to \u0019. The terms in\n5.3. Generative Classiﬁers 155\nthe log likelihood function that depend on \u0019are\nN∑\nn=1\n{tnln \u0019+ (1 −tn) ln(1−\u0019)}: (5.55)\nSetting the derivative with respect to \u0019equal to zero and rearranging, we obtain\n\u0019= 1\nN\nN∑\nn=1\ntn = N1\nN = N1\nN1 + N2\n(5.56)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 837, "text": "\u0019= 1\nN\nN∑\nn=1\ntn = N1\nN = N1\nN1 + N2\n(5.56)\nwhere N1 denotes the total number of data points in class C1, and N2 denotes the\ntotal number of data points in class C2. Thus, the maximum likelihood estimate\nfor \u0019 is simply the fraction of points in class C1 as expected. This result is easily\ngeneralized to the multi-class case where again the maximum likelihood estimate of\nthe prior probability associated with class Ck is given by the fraction of the training"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 838, "text": "set points assigned to that class.Exercise 5.13\nNow consider the maximization with respect to \u00161. Again, we can pick out of\nthe log likelihood function those terms that depend on \u00161:\nN∑\nn=1\ntnln N(xn|\u00161;\u0006) = −1\n2\nN∑\nn=1\ntn(xn −\u00161)T\u0006−1 (xn −\u00161) + const: (5.57)\nSetting the derivative with respect to \u00161 to zero and rearranging, we obtain\n\u00161 = 1\nN1\nN∑\nn=1\ntnxn; (5.58)\nwhich is simply the mean of all the input vectors xn assigned to class C1. By a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 839, "text": "similar argument, the corresponding result for \u00162 is given by\n\u00162 = 1\nN2\nN∑\nn=1\n(1 −tn)xn; (5.59)\nwhich again is the mean of all the input vectors xn assigned to class C2.\nFinally, consider the maximum likelihood solution for the shared covariance\nmatrix \u0006. Picking out the terms in the log likelihood function that depend on \u0006, we\nhave\n−1\n2\nN∑\nn=1\ntnln |\u0006|−1\n2\nN∑\nn=1\ntn(xn −\u00161)T\u0006−1 (xn −\u00161)\n−1\n2\nN∑\nn=1\n(1 −tn) ln|\u0006|−1\n2\nN∑\nn=1\n(1 −tn)(xn −\u00162)T\u0006−1 (xn −\u00162)\n= −N\n2 ln |\u0006|−N\n2 Tr\n{\n\u0006−1 S\n}\n(5.60)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 840, "text": "= −N\n2 ln |\u0006|−N\n2 Tr\n{\n\u0006−1 S\n}\n(5.60)\n156 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nwhere we have deﬁned\nS = N1\nN S1 + N2\nN S2 (5.61)\nS1 = 1\nN1\n∑\nn∈C1\n(xn −\u00161)(xn −\u00161)T (5.62)\nS2 = 1\nN2\n∑\nn∈C2\n(xn −\u00162)(xn −\u00162)T: (5.63)\nUsing the standard result for the maximum likelihood solution for a Gaussian distri-\nbution, we see that \u0006 = S, which represents a weighted average of the covariance\nmatrices associated with each of the two classes separately."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 841, "text": "This result is easily extended to theK-class problem to obtain the corresponding\nmaximum likelihood solutions for the parameters in which each class-conditional\ndensity is Gaussian with a shared covariance matrix. Note that the approach of ﬁttingExercise 5.14\nGaussian distributions to the classes is not robust to outliers, because the maximum\nlikelihood estimation of a Gaussian is not robust.Section 5.1.4\n5.3.3 Discrete features"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 842, "text": "5.3.3 Discrete features\nLet us now consider discrete feature valuesxi. For simplicity, we begin by look-\ning at binary feature values xi ∈{0;1}and discuss the extension to more general\ndiscrete features shortly. If there are D inputs, then a general distribution would\ncorrespond to a table of 2D numbers for each class and have 2D −1 independent\nvariables (due to the summation constraint). Because this grows exponentially with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 843, "text": "the number of features, we can seek a more restricted representation. Here we will\nmake the naive Bayes assumption in which the feature values are treated as indepen-Section 11.2.3\ndent and conditioned on the class Ck. Thus, we have class-conditional distributions\nof the form\np(x|Ck) =\nD∏\ni=1\n\u0016xi\nki(1 −\u0016ki)1−xi; (5.64)\nwhich contain Dindependent parameters for each class. Substituting into (5.46) then\ngives\nak(x) =\nD∑\ni=1\n{xiln \u0016ki + (1 −xi) ln(1−\u0016ki)}+ lnp(Ck); (5.65)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 844, "text": "{xiln \u0016ki + (1 −xi) ln(1−\u0016ki)}+ lnp(Ck); (5.65)\nwhich again are linear functions of the input values xi. For K = 2 classes, we can\nalternatively consider the logistic sigmoid formulation given by (5.40). Analogous\nresults are obtained for discrete variables that take L> 2 states.Exercise 5.16\n5.3.4 Exponential family\nAs we have seen, for both Gaussian distributed and discrete inputs, the posterior\nclass probabilities are given by generalized linear models with logistic sigmoid (K ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 845, "text": "5.4. Discriminative Classiﬁers 157\n2 classes) or softmax (K> 2 classes) activation functions. These are particular cases\nof a more general result obtained by assuming that the class-conditional densities\np(x|Ck) are members of the subset of the exponential family of distributions givenSection 3.4\nby\np(x|\u0015k;s) = 1\nsh\n(1\nsx\n)\ng(\u0015k) exp\n{ 1\ns\u0015T\nkx\n}\n: (5.66)\nHere the scaling parameter sis shared across all the classes."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 846, "text": "For the two-class problem, we substitute this expression for the class-conditional\ndensities into (5.41) and we see that the posterior class probability is again given by\na logistic sigmoid acting on a linear function a(x), which is given by\na(x) = (\u00151 −\u00152)Tx + lng(\u00151) −ln g(\u00152) + lnp(C1) −ln p(C2): (5.67)\nSimilarly, for the K-class problem, we substitute the class-conditional density ex-\npression into (5.46) to give\nak(x) = \u0015T\nkx + lng(\u0015k) + lnp(Ck) (5.68)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 847, "text": "ak(x) = \u0015T\nkx + lng(\u0015k) + lnp(Ck) (5.68)\nand so again is a linear function of x.\n5.4. Discriminative Classiﬁers\nFor the two-class classiﬁcation problem, we have seen that the posterior probabil-\nity of class C1 can be written as a logistic sigmoid acting on a linear function of\nx, for a wide choice of class-conditional distributions p(x|Ck) from the exponential\nfamily. Similarly, for the multi-class case, the posterior probability of class Ck is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 848, "text": "given by a softmax transformation of linear functions of x. For speciﬁc choices of\nthe class-conditional densities p(x|Ck), we have used maximum likelihood to deter-\nmine the parameters of the densities as well as the class priors p(Ck) and then used\nBayes’ theorem to ﬁnd the posterior class probabilities. This represents an example\nof generative modelling, because we could take such a model and generate synthetic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 849, "text": "data by drawing values of x from the marginal distribution p(x) or from any of the\nclass-conditional densities p(x|Ck).\nHowever, an alternative approach is to use the functional form of the general-\nized linear model explicitly and to determine its parameters directly by using maxi-\nmum likelihood. In this direct approach, we maximize a likelihood function deﬁned\nthrough the conditional distribution p(Ck|x), which represents a form ofdiscrimina-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 850, "text": "tive probabilistic modelling. One advantage of the discriminative approach is that\nthere will typically be fewer learnable parameters to be determined, as we will see\nshortly. It may also lead to improved predictive performance, particularly when the\nassumed forms for the class-conditional densities represent a poor approximation to\nthe true distributions.\n158 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\n5.4.1 Activation functions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 851, "text": "5.4.1 Activation functions\nIn linear regression, the model prediction y(x;w) is given by a linear functionChapter 4\nof the parameters\ny(x;w) = wTx + w0; (5.69)\nwhich gives a continuous-valued output in the range (−∞; ∞). For classiﬁcation\nproblems, however, we wish to predict discrete class labels, or more generally pos-\nterior probabilities that lie in the range (0;1). To achieve this, we consider a gener-\nalization of this model in which we transform the linear function of w and w0 using"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 852, "text": "a nonlinear function f( -) so that\ny(x;w) = f\n(\nwTw + w+ 0\n)\n: (5.70)\nIn the machine learning literature, f( -) is known as an activation function, whereas\nits inverse is called a link function in the statistics literature. The decision surfaces\ncorrespond to y(x) = constant , so that wTx = constant, and hence the decision\nsurfaces are linear functions of x, even if the function f(-)is nonlinear. For this\nreason, the class of models described by (5.70) are called generalized linear models"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 853, "text": "(McCullagh and Nelder, 1989). However, in contrast to the models used for regres-\nsion, they are no longer linear in the parameters due to the nonlinear function f(-).\nThis will lead to more complex analytical and computational properties than for\nlinear regression models. Nevertheless, these models are still relatively simple com-\npared to the much more ﬂexible nonlinear models that will be studied in subsequent\nchapters.\n5.4.2 Fixed basis functions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 854, "text": "chapters.\n5.4.2 Fixed basis functions\nSo far in this chapter, we have considered classiﬁcation models that work di-\nrectly with the original input vector x. However, all the algorithms are equally ap-\nplicable if we ﬁrst make a ﬁxed nonlinear transformation of the inputs using a vector\nof basis functions \u001e(x). The resulting decision boundaries will be linear in the fea-\nture space \u001e, and these correspond to nonlinear decision boundaries in the originalx"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 855, "text": "space, as illustrated in Figure 5.15. Classes that are linearly separable in the feature\nspace \u001e(x) need not be linearly separable in the original observation space x.\nNote that as in our discussion of linear models for regression, one of the basis\nfunctions is typically set to a constant, say \u001e0(x) = 1 , so that the corresponding\nparameter w0 plays the role of a bias.\nFor many problems of practical interest, there is signiﬁcant overlap in x-space"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 856, "text": "between the class-conditional densities p(x|Ck). This corresponds to posterior prob-\nabilities p(Ck|x), which, for at least some values ofx, are not 0 or 1. In such cases,\nthe optimal solution is obtained by modelling the posterior probabilities accurately\nand then applying standard decision theory. Note that nonlinear transformationsSection 5.2\n\u001e(x) cannot remove such a class overlap, although they can increase the level of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 857, "text": "overlap or create an overlap where none existed in the original observation space.\nHowever, suitable choices of nonlinearity can make the process of modelling the\nposterior probabilities easier. However, such ﬁxed basis function models have im-\nportant limitations, and these will be resolved in later chapters by allowing the basisSection 6.1\nfunctions themselves to adapt to the data.\n5.4. Discriminative Classiﬁers 159\nx1\nx2\n−1 0 1\n−1\n0\n1\nφ1\nφ2\n0 0.5 1\n0\n0.5\n1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 858, "text": "x1\nx2\n−1 0 1\n−1\n0\n1\nφ1\nφ2\n0 0.5 1\n0\n0.5\n1\nFigure 5.15 Illustration of the role of nonlinear basis functions in linear classiﬁcation models. The left-hand\nplot shows the original input space (x1;x2) together with data points from two classes labelled red and blue.\nTwo ‘Gaussian’ basis functions\u001e1(x) and \u001e2(x) are deﬁned in this space with centres shown by the green\ncrosses and with contours shown by the green circles. The right-hand plot shows the corresponding feature"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 859, "text": "space (\u001e1;\u001e2) together with the linear decision boundary obtained given by a logistic regression model of the\nform discussed in Section 5.4.3. This corresponds to a nonlinear decision boundary in the original input space,\nshown by the black curve in the left-hand plot.\n5.4.3 Logistic regression\nWe ﬁrst consider the problem of two-class classiﬁcation. In our discussion of\ngenerative approaches in Section 5.3, we saw that under rather general assumptions,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 860, "text": "the posterior probability of class C1 can be written as a logistic sigmoid acting on a\nlinear function of the feature vector \u001eso that\np(C1|\u001e) =y(\u001e) = \u001b\n(\nwT\u001e\n)\n(5.71)\nwith p(C2|\u001e) = 1 −p(C1|\u001e). Here \u001b(-)is the logistic sigmoid function deﬁned by\n(5.42). In the terminology of statistics, this model is known as logistic regression,\nalthough it should be emphasized that this is a model for classiﬁcation rather than\nfor continuous variable."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 861, "text": "for continuous variable.\nFor an M-dimensional feature space\u001e, this model hasMadjustable parameters.\nBy contrast, if we had ﬁtted Gaussian class-conditional densities using maximum\nlikelihood, we would have used 2M parameters for the means and M(M + 1)=2\nparameters for the (shared) covariance matrix. Together with the class prior p(C1),\nthis gives a total ofM(M+5)=2+1 parameters, which grows quadratically withM,\nin contrast to the linear dependence on M of the number of parameters in logistic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 862, "text": "regression. For large values of M, there is a clear advantage in working with the\nlogistic regression model directly.\n160 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nWe now use maximum likelihood to determine the parameters of the logistic\nregression model. To do this, we will make use of the derivative of the logistic sig-\nmoid function, which can conveniently be expressed in terms of the sigmoid function\nitself:Exercise 5.18\nd\u001b\nda = \u001b(1 −\u001b): (5.72)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 863, "text": "itself:Exercise 5.18\nd\u001b\nda = \u001b(1 −\u001b): (5.72)\nFor a data set {\u001en;tn}, where \u001en = \u001e(xn) and tn ∈{0;1}, with n = 1;:::;N ,\nthe likelihood function can be written\np(t|w) =\nN∏\nn=1\nytn\nn {1−yn}1−tn\n(5.73)\nwhere t = (t1;:::;t N)T and yn = p(C1|\u001en). As usual, we can deﬁne an error\nfunction by taking the negative logarithm of the likelihood, which gives the cross-\nentropy error function:\nE(w) = −ln p(t|w) = −\nN∑\nn=1\n{tnln yn + (1 −tn) ln(1−yn)} (5.74)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 864, "text": "N∑\nn=1\n{tnln yn + (1 −tn) ln(1−yn)} (5.74)\nwhere yn = \u001b(an) and an = wT\u001en. Taking the gradient of the error function with\nrespect to w, we obtainExercise 5.19\n∇E(w) =\nN∑\nn=1\n(yn −tn)\u001en (5.75)\nwhere we have made use of (5.72). We see that the factor involving the derivative\nof the logistic sigmoid has cancelled, leading to a simpliﬁed form for the gradient\nof the log likelihood. In particular, the contribution to the gradient from data point"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 865, "text": "n is given by the ‘error’ yn −tn between the target value and the prediction of\nthe model times the basis function vector \u001en. Furthermore, comparison with (4.12)\nshows that this takes precisely the same form as the gradient of the sum-of-squares\nerror function for the linear regression model.Section 4.1.3\nThe maximum likelihood solution corresponds to ∇E(w) = 0. However, from\n(5.75) we see that this no longer corresponds to a set of linear equations, due to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 866, "text": "the nonlinearity in y(-), and so this equation does not have a closed-form solution.\nOne approach to ﬁnding a maximum likelihood solution would be to use stochastic\ngradient descent, in which ∇En is the nth term on the right-hand side of (5.75).Chapter 7\nStochastic gradient descent will be the principal approach to training the highly non-\nlinear neural networks discussed in later chapters. However, the maximum likelihood"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 867, "text": "equation is only ‘slightly’ nonlinear, and in fact the error function (5.74), in which the\nmodel is deﬁned by (5.71), is a convex function of the parameters, which allows the\nerror function to be minimized using a simple algorithm called iterative reweighted\nleast squares or IRLS (Bishop, 2006). However, this does not easily generalize to\nmore complex models such as deep neural networks.\n5.4. Discriminative Classiﬁers 161"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 868, "text": "5.4. Discriminative Classiﬁers 161\nNote that maximum likelihood can exhibit severe over-ﬁtting for data sets that\nare linearly separable. This arises because the maximum likelihood solution occurs\nwhen the hyperplane corresponding to \u001b = 0:5, equivalent to wT\u001e= 0, separates\nthe two classes and the magnitude of w goes to inﬁnity. In this case, the logis-\ntic sigmoid function becomes inﬁnitely steep in feature space, corresponding to a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 869, "text": "Heaviside step function, so that every training point from each class kis assigned a\nposterior probability p(Ck|x) = 1. Furthermore, there is typically a continuum ofExercise 5.20\nsuch solutions because any separating hyperplane will give rise to the same posterior\nprobabilities at the training data points. Maximum likelihood provides no way to\nfavour one such solution over another, and which solution is found in practice will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 870, "text": "depend on the choice of optimization algorithm and on the parameter initialization.\nNote that the problem will arise even if the number of data points is large compared\nwith the number of parameters in the model, so long as the training data set is lin-\nearly separable. The singularity can be avoided by adding a regularization term to\nthe error function.Chapter 9\n5.4.4 Multi-class logistic regression\nIn our discussion of generative models for multi-class classiﬁcation, we haveSection 5.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 871, "text": "seen that, for a large class of distributions from the exponential family, the posterior\nprobabilities are given by a softmax transformation of linear functions of the feature\nvariables, so that\np(Ck|\u001e) = yk(\u001e) = exp(ak)\n∑\njexp(aj) (5.76)\nwhere the pre-activations ak are given by\nak = wT\nk\u001e: (5.77)\nThere we used maximum likelihood to determine separately the class-conditional\ndensities and the class priors and then found the corresponding posterior probabilities"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 872, "text": "using Bayes’ theorem, thereby implicitly determining the parameters{wk}. Here we\nconsider the use of maximum likelihood to determine the parameters {wk}of this\nmodel directly. To do this, we will require the derivatives of yk with respect to all\nthe pre-activations aj. These are given byExercise 5.21\n@yk\n@aj\n= yk(Ikj −yj) (5.78)\nwhere Ikj are the elements of the identity matrix.\nNext we write down the likelihood function. This is most easily done using"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 873, "text": "the 1-of-K coding scheme in which the target vector tn for a feature vector \u001en\nbelonging to class Ck is a binary vector with all elements zero except for element k,\nwhich equals one. The likelihood function is then given by\np(T|w1;:::; wK) =\nN∏\nn=1\nK∏\nk=1\np(Ck|\u001en)tnk =\nN∏\nn=1\nK∏\nk=1\nytnk\nnk (5.79)\n162 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nFigure 5.16 Representation of a multi-class lin-\near classiﬁcation model as a neu-\nral network having a single layer\nof connections. Each basis func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 874, "text": "of connections. Each basis func-\ntion is represented by a node,\nwith the solid node represent-\ning the ‘bias’ basis function \u001e0,\nwhereas each output y1;:::;y N is\nalso represented by a node. The\nlinks between the nodes represent\nthe corresponding weight and bias\nparameters.\n\u001eM−1(x)\n …\n\u001e1(x)\n\u001e0(x)\nyK(x;w)\n …\ny1(x;w)\nwhere ynk = yk(\u001en), and T is an N ×K matrix of target variables with elements\ntnk. Taking the negative logarithm then gives\nE(w1;:::; wK) = −ln p(T|w1;:::; wK) = −\nN∑\nn=1\nK∑\nk=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 875, "text": "N∑\nn=1\nK∑\nk=1\ntnkln ynk; (5.80)\nwhich is known as the cross-entropy error function for the multi-class classiﬁcation\nproblem.\nWe now take the gradient of the error function with respect to one of the param-\neter vectors wj. Making use of the result (5.78) for the derivatives of the softmax\nfunction, we obtainExercise 5.22\n∇wj E(w1;:::; wK) =\nN∑\nn=1\n(ynj −tnj) \u001en (5.81)\nwhere we have made use of ∑\nktnk = 1. Again, we could optimize the parameters\nthrough stochastic gradient descent.Chapter 7"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 876, "text": "through stochastic gradient descent.Chapter 7\nOnce again, we see the same form arising for the gradient as was found for the\nsum-of-squares error function with the linear model and for the cross-entropy error\nwith the logistic regression model, namely the product of the error(ynj−tnj) times\nthe basis function activation \u001en. These are examples of a more general result that\nwe will explore later.Section 5.4.6\nLinear classiﬁcation models can be represented as single-layer neural networks"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 877, "text": "as shown in Figure 5.16. If we consider the derivative of the error function with\nrespect to a weight wik, which links basis function \u001ei(x) to output unit tk, we have\nfrom (5.81)\n@E(w1;:::; wK)\n@wij\n=\nN∑\nn=1\n(ynk −tnk) \u001ei(xn): (5.82)\nComparing this with Figure 5.16, we see that, for each data point n this gradient\ntakes the form of the output of the basis function at the input end of the weight link\nwith the ‘error’(ynk −tnk) at the output end.\n5.4. Discriminative Classiﬁers 163"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 878, "text": "5.4. Discriminative Classiﬁers 163\nFigure 5.17 Schematic example of a probability den-\nsity p(\u0012) shown by the blue curve, given in this example\nby a mixture of two Gaussians, along with its cumulative\ndistribution function f(a), shown by the red curve. Note\nthat the value of the blue curve at any point, such as\nthat indicated by the vertical green line, corresponds to\nthe slope of the red curve at the same point. Conversely,\nthe value of the red curve at this point corresponds to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 879, "text": "area under the blue curve indicated by the shaded green\nregion. In the stochastic threshold model, the class label\ntakes the value t = 1 if the value of a = wT\u001eexceeds\na threshold, otherwise it takes the value t = 0. This is\nequivalent to an activation function given by the cumula-\ntive distribution function f(a).\n0 1 2 3 4\n0\n0.2\n0.4\n0.6\n0.8\n1\n5.4.5 Probit regression\nWe have seen that, for a broad range of class-conditional distributions described"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 880, "text": "by the exponential family, the resulting posterior class probabilities are given by a\nlogistic (or softmax) transformation acting on a linear function of the feature vari-\nables. However, not all choices of class-conditional density give rise to such a simple\nform for the posterior probabilities, which suggests that it might be worth exploring\nother types of discriminative probabilistic model. Consider the two-class case, again"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 881, "text": "remaining within the framework of generalized linear models, so that\np(t= 1|a) =f(a) (5.83)\nwhere a= wT\u001e, and f(-)is the activation function.\nOne way to motivate an alternative choice for the link function is to consider a\nnoisy threshold model, as follows. For each input \u001en, we evaluate an = wT\u001en and\nthen we set the target value according to\n{\ntn = 1; if an > \u0012;\ntn = 0; otherwise: (5.84)\nIf the value of \u0012 is drawn from a probability density p(\u0012), then the corresponding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 882, "text": "activation function will be given by the cumulative distribution function\nf(a) =\n∫a\n−∞\np(\u0012) d\u0012 (5.85)\nas illustrated in Figure 5.17.\nAs a speciﬁc example, suppose that the density p(\u0012) is given by a zero-mean,\nunit-variance Gaussian. The corresponding cumulative distribution function is given\nby\nΦ(a) =\n∫a\n−∞\nN(\u0012|0;1) d\u0012; (5.86)\n164 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nwhich is known as the probit function. It has a sigmoidal shape and is compared"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 883, "text": "with the logistic sigmoid function in Figure 5.12. Note that the use of a Gaussian\ndistribution with general mean and variances does not change the model because this\nis equivalent to a re-scaling of the linear coefﬁcients w. Many numerical packages\ncan evaluate a closely related function deﬁned by\nerf(a) = 2√\u0019\n∫a\n0\nexp(−\u00122=2) d\u0012 (5.87)\nand known as the erf function or error function (not to be confused with the error"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 884, "text": "function of a machine learning model). It is related to the probit function byExercise 5.23\nΦ(a) = 1\n2\n{\n1 + 1√\n2erf(a)\n}\n: (5.88)\nThe generalized linear model based on a probit activation function is known asprobit\nregression. We can determine the parameters of this model using maximum likeli-\nhood by a straightforward extension of the ideas discussed earlier. In practice, the\nresults found using probit regression tend to be like those of logistic regression."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 885, "text": "One issue that can occur in practical applications is that of outliers, which can\narise for instance through errors in measuring the input vector x or through misla-\nbelling of the target valuet. Because such points can lie a long way to the wrong side\nof the ideal decision boundary, they can seriously distort the classiﬁer. The logistic\nand probit regression models behave differently in this respect because the tails of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 886, "text": "the logistic sigmoid decay asymptotically like exp(−x) for |x|→∞, whereas for\nthe probit activation function, they decay like exp(−x2), and so the probit model\ncan be signiﬁcantly more sensitive to outliers.\n5.4.6 Canonical link functions\nFor the linear regression model with a Gaussian noise distribution, the error\nfunction, corresponding to the negative log likelihood, is given by (4.11). If we\ntake the derivative with respect to the parameter vector w of the contribution to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 887, "text": "error function from a data pointn, this takes the form of the ‘error’yn−tn times the\nfeature vector \u001en, where yn = wT\u001en. Similarly, for the combination of the logistic-\nsigmoid activation function and the cross-entropy error function (5.74) and for the\nsoftmax activation function with the multi-class cross-entropy error function (5.80),\nwe again obtain this same simple form. We now show that this is a general result"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 888, "text": "of assuming a conditional distribution for the target variable from the exponential\nfamily along with a corresponding choice for the activation function known as the\ncanonical link function.\nWe again make use of the restricted form (3.169) of exponential family distri-\nbutions. Note that here we are applying the assumption of exponential family distri-\nbution to the target variable t, in contrast to Section 5.3.4 where we applied it to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 889, "text": "input vector x. We therefore consider conditional distributions of the target variable\nof the form\np(t|\u0011;s) = 1\nsh\n(t\ns\n)\ng(\u0011) exp\n{ \u0011t\ns\n}\n: (5.89)\n5.4. Discriminative Classiﬁers 165\nUsing the same line of argument as led to the derivation of the result (3.172), we see\nthat the conditional mean of t, which we denote by y, is given by\ny≡E[t|\u0011] = −s d\nd\u0011ln g(\u0011): (5.90)\nThus, yand \u0011must related, and we denote this relation through \u0011= (y)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 890, "text": "Following Nelder and Wedderburn (1972), we deﬁne ageneralized linear model\nto be one for which yis a nonlinear function of a linear combination of the input (or\nfeature) variables so that\ny= f(wT\u001e) (5.91)\nwhere f(-)is known as the activation function in the machine learning literature, and\nf−1 (-)is known as the link function in statistics.\nNow consider the log likelihood function for this model, which, as a function of\n\u0011, is given by\nln p(t|\u0011;s) =\nN∑\nn=1\nln p(tn|\u0011;s) =\nN∑\nn=1\n{\nln g(\u0011n) + \u0011ntn"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 891, "text": "N∑\nn=1\nln p(tn|\u0011;s) =\nN∑\nn=1\n{\nln g(\u0011n) + \u0011ntn\ns\n}\n+ const (5.92)\nwhere we are assuming that all observations share a common scale parameter (which\ncorresponds to the noise variance for a Gaussian distribution, for instance) and so s\nis independent of n. The derivative of the log likelihood with respect to the model\nparameters w is then given by\n∇w ln p(t|\u0011;s) =\nN∑\nn=1\n{ d\nd\u0011n\nln g(\u0011n) + tn\ns\n} d\u0011n\ndyn\ndyn\ndan\n∇wan\n=\nN∑\nn=1\n1\ns{tn −yn} ′(yn)f′(an)\u001en (5.93)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 892, "text": "∇wan\n=\nN∑\nn=1\n1\ns{tn −yn} ′(yn)f′(an)\u001en (5.93)\nwhere an = wT\u001en, and we have used yn = f(an) together with the result (5.90)\nfor E[t|\u0011]. We now see that there is a considerable simpliﬁcation if we choose a\nparticular form for the link function f−1 (y) given by\nf−1 (y) = (y); (5.94)\nwhich gives f( (y)) = y and hence f′( ) ′(y) = 1. Also, because a = f−1 (y),\nwe have a = and hence f′(a) ′(y) = 1. In this case, the gradient of the error\nfunction reduces to\n∇ln E(w) = 1\ns\nN∑\nn=1\n{yn −tn}\u001en: (5.95)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 893, "text": "∇ln E(w) = 1\ns\nN∑\nn=1\n{yn −tn}\u001en: (5.95)\nWe have seen that there is a natural pairing between the choice of error function\nand the choice of output-unit activation function. Although we have derived this\nresult in the context of single-layer network models, the same considerations apply\nto deep neural networks discussed in later chapters.\n166 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nExercises\n5.1 (?) Consider a classiﬁcation problem with Kclasses and a target vector t that uses a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 894, "text": "1-of-Kbinary coding scheme. Show that the conditional expectation E[t|x]is given\nby the posterior probability p(Ck|x).\n5.2 (??) Given a set of data points {xn}, we can deﬁne the convex hull to be the set of\nall points x given by\nx =\n∑\nn\n\u000bnxn (5.96)\nwhere \u000bn > 0 and ∑\nn\u000bn = 1. Consider a second set of points {yn}together with\ntheir corresponding convex hull. By deﬁnition, the two sets of points will be linearly\nseparable if there exists a vector ˆw and a scalar w0 such that ˆwTxn+ w0 >0 for all"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 895, "text": "xn and ˆwTyn+ w0 <0 for all yn. Show that if their convex hulls intersect, the two\nsets of points cannot be linearly separable, and conversely that if they are linearly\nseparable, their convex hulls do not intersect.\n5.3 (??) Consider the minimization of a sum-of-squares error function (5.14), and sup-\npose that all the target vectors in the training set satisfy a linear constraint\naTtn + b= 0 (5.97)\nwhere tn corresponds to the nth row of the matrix T in (5.14). Show that as a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 896, "text": "consequence of this constraint, the elements of the model prediction y(x) given by\nthe least-squares solution (5.16) also satisfy this constraint, so that\naTy(x) + b= 0: (5.98)\nTo do so, assume that one of the basis functions\u001e0(x) = 1 so that the corresponding\nparameter w0 plays the role of a bias.\n5.4 (??) Extend the result of Exercise 5.3 to show that if multiple linear constraints are\nsatisﬁed simultaneously by the target vectors, then the same constraints will also be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 897, "text": "satisﬁed by the least-squares prediction of a linear model.\n5.5 (?) Use the deﬁnition (5.38), along with (5.30) and (5.31) to derive the result (5.39)\nfor the F-score.\n5.6 (??) Consider two non-negative numbers a and b, and show that, if a 6 b, then\na 6 (ab)1=2. Use this result to show that, if the decision regions of a two-class\nclassiﬁcation problem are chosen to minimize the probability of misclassiﬁcation,\nthis probability will satisfy\np(mistake) 6\n∫\n{p(x;C1)p(x;C2)}1=2 dx: (5.99)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 898, "text": "p(mistake) 6\n∫\n{p(x;C1)p(x;C2)}1=2 dx: (5.99)\n5.7 (?) Given a loss matrix with elements Lkj, the expected risk is minimized if, for\neach x, we choose the class that minimizes (5.23). Verify that, when the loss matrix\nExercises 167\nis given by Lkj = 1 −Ikj, where Ikj are the elements of the identity matrix, this\nreduces to the criterion of choosing the class having the largest posterior probability.\nWhat is the interpretation of this form of loss matrix?"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 899, "text": "5.8 (?) Derive the criterion for minimizing the expected loss when there is a general loss\nmatrix and general prior probabilities for the classes.\n5.9 (?) Consider the average of the posterior probabilities over a set of N data points in\nthe form\n1\nN\nN∑\nN=1\np(Ck|xn): (5.100)\nBy taking the limit N → ∞, show that this quantity approaches the prior class\nprobability p(Ck).\n5.10 (??) Consider a classiﬁcation problem in which the loss incurred when an input"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 900, "text": "vector from class Ck is classiﬁed as belonging to class Cj is given by the loss matrix\nLkj and for which the loss incurred in selecting the reject option is \u0015. Find the\ndecision criterion that will give the minimum expected loss. Verify that this reduces\nto the reject criterion discussed in Section 5.2.3 when the loss matrix is given by\nLkj = 1 −Ikj. What is the relationship between \u0015and the rejection threshold \u0012?"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 901, "text": "5.11 (?) Show that the logistic sigmoid function (5.42) satisﬁes the property \u001b(−a) =\n1 −\u001b(a) and that its inverse is given by \u001b−1 (y) = ln {y=(1−y)}.\n5.12 (?) Using (5.40) and (5.41), derive the result (5.48) for the posterior class probability\nin the two-class generative model with Gaussian densities, and verify the results\n(5.49) and (5.50) for the parameters w and w0.\n5.13 (?) Consider a generative classiﬁcation model for K classes deﬁned by prior class"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 902, "text": "probabilities p(Ck) = \u0019k and general class-conditional densities p(\u001e|Ck) where \u001e\nis the input feature vector. Suppose we are given a training data set {\u001en;tn}where\nn = 1;:::;N , and tn is a binary target vector of length K that uses the 1-of-K\ncoding scheme, so that it has components tnj = Ijk if data point nis from class Ck.\nAssuming that the data points are drawn independently from this model, show that\nthe maximum-likelihood solution for the prior probabilities is given by\n\u0019k = Nk\nN (5.101)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 903, "text": "\u0019k = Nk\nN (5.101)\nwhere Nk is the number of data points assigned to class Ck.\n5.14 (??) Consider the classiﬁcation model of Exercise 5.13 and now suppose that the\nclass-conditional densities are given by Gaussian distributions with a shared covari-\nance matrix, so that\np(\u001e|Ck) = N(\u001e|\u0016k;\u0006): (5.102)\n168 5. SINGLE-LAYER NETWORKS: CLASSIFICATION\nShow that the maximum likelihood solution for the mean of the Gaussian distribution\nfor class Ck is given by\n\u0016k = 1\nNk\nN∑\nn=1\ntnk\u001en; (5.103)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 904, "text": "\u0016k = 1\nNk\nN∑\nn=1\ntnk\u001en; (5.103)\nwhich represents the mean of those feature vectors assigned to class Ck. Similarly,\nshow that the maximum likelihood solution for the shared covariance matrix is given\nby\n\u0006 =\nK∑\nk=1\nNk\nN Sk (5.104)\nwhere\nSk = 1\nNk\nN∑\nn=1\ntnk(\u001en −\u0016k)(\u001en −\u0016k)T: (5.105)\nThus, \u0006 is given by a weighted average of the covariances of the data associated with\neach class, in which the weighting coefﬁcients are given by the prior probabilities of\nthe classes."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 905, "text": "the classes.\n5.15 (??) Derive the maximum likelihood solution for the parameters{\u0016ki}of the proba-\nbilistic naive Bayes classiﬁer with discrete binary features described in Section 5.3.3.\n5.16 (??) Consider a classiﬁcation problem with K classes for which the feature vector\n\u001ehas M components each of which can take Ldiscrete states. Let the values of the\ncomponents be represented by a 1-of-Lbinary coding scheme. Further suppose that,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 906, "text": "conditioned on the class Ck, the M components of \u001eare independent, so that the\nclass-conditional density factorizes with respect to the feature vector components.\nShow that the quantities ak given by (5.46), which appear in the argument to the\nsoftmax function describing the posterior class probabilities, are linear functions of\nthe components of \u001e. Note that this represents an example of a naive Bayes model.Section 11.2.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 907, "text": "5.17 (??) Derive the maximum likelihood solution for the parameters of the probabilistic\nnaive Bayes classiﬁer described in Exercise 5.16.\n5.18 (?) Verify the relation (5.72) for the derivative of the logistic sigmoid function de-\nﬁned by (5.42).\n5.19 (?) By making use of the result (5.72) for the derivative of the logistic sigmoid, show\nthat the derivative of the error function (5.74) for the logistic regression model is\ngiven by (5.75)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 908, "text": "given by (5.75).\n5.20 (?) Show that for a linearly separable data set, the maximum likelihood solution\nfor the logistic regression model is obtained by ﬁnding a vector w whose decision\nboundary wT\u001e(x) = 0 separates the classes and then taking the magnitude of w to\ninﬁnity.\n5.21 (?) Show that the derivatives of the softmax activation function (5.76), where theak\nare deﬁned by (5.77), are given by (5.78).\nExercises 169"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 909, "text": "Exercises 169\n5.22 (?) Using the result (5.78) for the derivatives of the softmax activation function, show\nthat the gradients of the cross-entropy error (5.80) are given by (5.81).\n5.23 (?) Show that the probit function (5.86) and the erf function (5.87) are related by\n(5.88).\n5.24 (??) Suppose we wish to approximate the logistic sigmoid \u001b(a) deﬁned by (5.42)\nby a scaled probit functionΦ(\u0015a), where Φ(a) is deﬁned by (5.86). Show that if \u0015is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 910, "text": "chosen so that the derivatives of the two functions are equal ata= 0, then \u00152 = \u0019=8.\n6\nDeep Neural\nNetworks\nIn recent years, neural networks have emerged as, by far, the most important ma-\nchine learning technology for practical applications, and we therefore devote a large\nfraction of this book to studying them. Previous chapters have already laid many\nof the foundations we will need. In particular, we have seen that linear regression"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 911, "text": "models that comprise linear combinations of ﬁxed nonlinear basis functions can be\nexpressed as neural networks having a single layer of weight and bias parameters.Chapter 4\nLikewise, classiﬁcation models based on linear combinations of basis functions can\nalso be viewed as single-layer neural networks. These allowed us to introduce severalChapter 5\nimportant concepts before we embark on a discussion of more complex multilayered\nnetworks in this chapter."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 912, "text": "networks in this chapter.\nGiven a sufﬁcient number of suitably chosen basis functions, such linear models\ncan approximate any given nonlinear transformation from inputs to outputs to any\ndesired accuracy and might therefore appear to be sufﬁcient to tackle any practical\n171© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_6"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 913, "text": "172 6. DEEP NEURAL NETWORKS\napplication. However, these models have some severe limitations, and so we will\nbegin our discussion of neural networks by exploring these limitations and under-\nstanding why it is necessary to use basis functions that are themselves learned from\ndata. This leads naturally to a discussion of neural networks having more than one\nlayer of learnable parameters. These are known as feed-forward networks or multi-Section 6.3.6"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 914, "text": "layer perceptrons. We will also discuss the beneﬁts of having many such layers of\nprocessing, leading to the key concept of deep neural networks that now dominate\nthe ﬁeld of machine learning.\n6.1.\nLimitations of Fixed Basis Functions\nLinear basis function models for classiﬁcation are based on linear combinations ofChapter 5\nbasis functions \u001ej(x) and take the form\ny(x;w) = f\n\n\nM∑\nj=1\nwj\u001ej(x) + w0\n\n (6.1)\nwhere f(-)is a nonlinear output activation function. Linear models for regression"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 915, "text": "take the same form but with f(-)replaced by the identity. These models allow forChapter 4\nan arbitrary set of nonlinear basis functions {\u001ei(x)}, and because of the generality\nof these basis functions, such models can in principle provide a solution to any re-\ngression or classiﬁcation problem. This is true in a trivial sense in that if one of the\nbasis functions corresponds to the desired input-to-output transformation, then the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 916, "text": "learnable linear layer simply has to copy the value of this basis function to the output\nof the model.\nMore generally, we would expect that a sufﬁciently large and rich set of basis\nfunctions would allow any desired function to be approximated to arbitrary accu-\nracy. It would seem therefore that such linear models constitute a general purpose\nframework for solving problems in machine learning. Unfortunately, there are some"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 917, "text": "signiﬁcant shortcomings with linear models, which arise from the assumption that\nthe basis functions \u001ej(x) are ﬁxed and independent of the training data. To under-\nstand these limitations, we start by looking at the behaviour of linear models as the\nnumber of input variables is increased.\n6.1.1 The curse of dimensionality\nConsider a simple regression model for a single input variable given by a poly-\nnomial of order M in the formSection 1.2\ny(x;w) = w0 + w1x+ w2x2 + ::: + wMxM (6.2)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 918, "text": "y(x;w) = w0 + w1x+ w2x2 + ::: + wMxM (6.2)\nand let us see what happens if we increase the number of inputs. If we have Dinput\nvariables {x1;:::;x D}, then a general polynomial with coefﬁcients up to order 3\n6.1. Limitations of Fixed Basis Functions 173\nFigure 6.1 Plot of the Iris data in which red,\ngreen, and blue points denote\nthree species of iris ﬂower and the\naxes represent measurements of\nthe length and width of the sepal,\nrespectively. Our goal is to clas-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 919, "text": "respectively. Our goal is to clas-\nsify a new test point such as the\none denoted by \u0002.\nsepal length\nsepal width\nwould take the form\ny(x;w) = w0 +\nD∑\ni=1\nwixi +\nD∑\ni=1\nD∑\nj=1\nwijxixj +\nD∑\ni=1\nD∑\nj=1\nD∑\nk=1\nwijkxixjxk: (6.3)\nAs D increases, the growth in the number of independent coefﬁcients is O(D3),\nwhereas for a polynomial of order M, the growth in the number of coefﬁcients is\nO(DM) (Bishop, 2006). We see that in spaces of higher dimensionality, polynomials"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 920, "text": "can rapidly become unwieldy and of little practical utility.\nThe severe difﬁculties that can arise in spaces of many dimensions is sometimes\ncalled the curse of dimensionality (Bellman, 1961). It is not limited to polynomial\nregression but is in fact quite general. Consider the use of linear models for solv-\ning classiﬁcation problems. Figure 6.1 shows a plot of data from the Iris data set\ncomprising 50 observations taken from each of three species of iris ﬂowers. Each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 921, "text": "observation has four variables representing measurements of the sepal length, sepal\nwidth, petal length, and petal width. For this illustration, we consider only the sepal\nlength and sepal width variables. Given these 150 observations as training data, our\ngoal is to classify a new test point, such as the one denoted by the cross inFigure 6.1,\nby assigning it to one of the three species. We observe that the cross is close to sev-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 922, "text": "eral red points, and so we might suppose that it belongs to the red class. However,\nthere are also some green points nearby, so we might think that it could instead be-\nlong to the green class. It seems less likely that it belongs to the blue class. The\nintuition here is that the identity of the cross should be determined more strongly by\nnearby points from the training set and less strongly by more distant points, and this\nintuition turns out to be reasonable."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 923, "text": "intuition turns out to be reasonable.\nOne very simple way of converting this intuition into a learning algorithm would\nbe to divide the input space into regular cells, as indicated in Figure 6.2. When we\nare given a test point and we wish to predict its class, we ﬁrst decide which cell it\n174 6. DEEP NEURAL NETWORKS\nFigure 6.2 Illustration of a simple approach\nfor solving classiﬁcation problems\nin which the input space is di-\nvided into cells and any new test\npoint is assigned to the class that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 924, "text": "point is assigned to the class that\nhas the most representatives in\nthe same cell as the test point.\nAs we shall see shortly, this sim-\nplistic approach has some severe\nshortcomings.\nsepal length\nsepal width\nbelongs to, and then we ﬁnd all the training data points that fall in the same cell. The\nidentity of the test point is predicted to be the same as the class having the largest\nnumber of training points in the same cell as the test point (with ties being broken"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 925, "text": "at random). We can view this as a basis function model in which there is a basis\nfunction \u001ei(x) for each grid cell, which simply returns zero if x lies outside the\ngrid cell, and otherwise returns the majority class of the training data points that fall\ninside the cell. The output of the model is then given by the sum of the outputs of all\nthe basis functions.\nThere are numerous problems with this naive approach, but one of the most"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 926, "text": "severe becomes apparent when we consider its extension to problems having larger\nnumbers of input variables, corresponding to input spaces of higher dimensionality.\nThe origin of the problem is illustrated inFigure 6.3, which shows that, if we divide a\nregion of a space into regular cells, then the number of such cells grows exponentially\nwith the dimensionality of the space. The challenge with an exponentially large"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 927, "text": "number of cells is that we would need an exponentially large quantity of training\nFigure 6.3 Illustration of the curse\nof dimensionality, showing how the\nnumber of regions of a regular grid\ngrows exponentially with the dimen-\nsionality D of the space. For clarity,\nonly a subset of the cubical regions\nare shown for D= 3.\nx1\nD = 1\nx1\nx2\nD = 2\nx1\nx2\nx3\nD = 3\n6.1. Limitations of Fixed Basis Functions 175\nFigure 6.4 Plot of the fraction of the volume\nof a hypersphere of radius r = 1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 928, "text": "of a hypersphere of radius r = 1\nlying in the range r = 1 −\u000f to\nr = 1 for various values of the di-\nmensionality D.\nϵ\nvolume fraction\nD = 1\nD = 2\nD = 5\nD = 20\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\ndata to ensure that the cells are not empty. We have already seen in Figure 6.2 that\nsome cells contain no training points. Hence, a test point in such cells cannot be\nclassiﬁed. Clearly, we have no hope of applying such a technique in a space of more"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 929, "text": "than a few variables. The difﬁculties with both the polynomial regression example\nand the Iris data classiﬁcation example arise because the basis functions were chosen\nindependently of the problem being solved. We will need to be more sophisticated\nin our choice of basis functions if we are to circumvent the curse of dimensionality.Section 6.1.4\n6.1.2 High-dimensional spaces\nFirst, however, we will look more closely at the properties of spaces with higher"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 930, "text": "dimensionality where our geometrical intuitions, formed through a life spent in a\nspace of three dimensions, can fail badly. As a simple example, consider a hyper-\nsphere of radius r = 1 in a space of Ddimensions, and ask what is the fraction of\nthe volume of the hypersphere that lies between radius r= 1 −\u000fand r= 1. We can\nevaluate this fraction by noting that the volume VD(r) of a hypersphere of radius r\nin Ddimensions must scale as rD, and so we write\nVD(r) = KDrD (6.4)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 931, "text": "VD(r) = KDrD (6.4)\nwhere the constant KD depends only on D. Thus, the required fraction is given byExercise 6.1\nVD(1) −VD(1 −\u000f)\nVD(1) = 1 −(1 −\u000f)D; (6.5)\nwhich is plotted as a function of \u000ffor various values of Din Figure 6.4. We see that,\nfor large D, this fraction tends to 1 even for small values of \u000f. Thus, we arrive at\nthe remarkable result that, in spaces of high dimensionality, most of the volume of a\nhypersphere is concentrated in a thin shell near the surface!"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 932, "text": "176 6. DEEP NEURAL NETWORKS\nFigure 6.5 Plot of the probability density with\nrespect to radius r of a Gaussian\ndistribution for various values of\nthe dimensionality D. In a high-\ndimensional space, most of the\nprobability mass of a Gaussian\nis located within a thin shell at a\nspeciﬁc radius.\nD = 1\nD = 2\nD = 20\nr\np(r)\n0 2 4\n0\n1\n2\nAs a further example of direct relevance to machine learning, consider the be-\nhaviour of a Gaussian distribution in a high-dimensional space. If we transform from"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 933, "text": "Cartesian to polar coordinates and then integrate out the directional variables, we ob-\ntain an expression for the densityp(r) as a function of radiusrfrom the origin. Thus,Exercise 6.3\np(r)\u000er is the probability mass inside a thin shell of thickness \u000er located at radius r.\nThis distribution is plotted, for various values of D, in Figure 6.5, and we see that\nfor large D, the probability mass of the Gaussian is concentrated in a thin shell at a\nspeciﬁc radius."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 934, "text": "speciﬁc radius.\nIn this book, we make extensive use of illustrative examples involving one or two\nvariables, because this makes it particularly easy to visualize these spaces graphi-\ncally. The reader should be warned, however, that not all intuitions developed in\nspaces of low dimensionality will generalize to situations involving many dimen-\nsions.\nFinally, although we have talked about the curse of dimensionality, there can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 935, "text": "also be advantages to working in high-dimensional spaces. Consider the situation\nshown in Figure 6.6. We see that this data set, in which each data point consists\nof a pair of values (x1;x2), is linearly separable, but when only the value of x1 is\nobserved, the classes have a strong overlap. The classiﬁcation problem is therefore\nmuch easier in the higher-dimensional space.\n6.1.3 Data manifolds\nWith both the polynomial regression model and the grid-based classiﬁer in Fig-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 936, "text": "ure 6.2, we saw that the number of basis functions grows rapidly with dimensionality,\nmaking such methods impractical for applications involving even a few dozen vari-\nables, never mind the millions of inputs that often arise with, say, image processing.\nThe problem is that the basis functions are ﬁxed ahead of time and do not depend on\nthe data, or indeed even on the speciﬁc problem being solved. We need to ﬁnd a way\nto create basis functions that are tuned to the particular application."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 937, "text": "Although the curse of dimensionality certainly raises important issues for ma-\nchine learning applications, it does not prevent us from ﬁnding effective techniques\napplicable to high-dimensional spaces. One reason for this is that real data will\ngenerally be conﬁned to a region of the data space having lower effective dimen-\n6.1. Limitations of Fixed Basis Functions 177\nx1\nx2\n(a)\nx1 (b)\nFigure 6.6 Illustration of a data set in two dimensions (x1;x2) in which data points from the two classes de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 938, "text": "picted using green and red circles can be separated by a linear decision surface, as seen in (a). If, however, only\nthe variable x1 is measured then the classes are no longer separable, as seen in (b).\nsionality. Consider the images shown in Figure 6.7. Each image is a point in a\nhigh-dimensional space whose dimensionality is determined by the number of pix-\nels. Because the objects can occur at different vertical and horizontal positions within"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 939, "text": "the image and in different orientations, there are three degrees of freedom of vari-\nability between images, and a set of images will, to a ﬁrst approximation, live on\na three-dimensional manifold embedded within the high-dimensional space. Due to\nthe complex relationships between the object position or orientation and the pixel\nintensities, this manifold will be highly nonlinear.\nIn fact, the number of pixels is really an artefact of the image generation pro-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 940, "text": "cess since they represent measurements of a continuous world. Capturing the same\nimage at a higher resolution increases the dimensionality Dof the data space with-\nout changing the fact that the images still live on a three-dimensional manifold. If\nwe can associate localized basis functions with the data manifold, rather than with\nthe entire high-dimensional data space, we might expect that the number of required\nbasis functions would grow exponentially with the dimensionality of the manifold"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 941, "text": "rather than with the dimensionality of the data space. Since the manifold will typi-\ncally have a much lower dimensionality than the data space, this represents a huge\nFigure 6.7 Examples of images of a hand-\nwritten digit that differ in the\nlocation of the digit within\nthe images as well as in\ntheir orientation. This data\nlives on a nonlinear three-\ndimensional manifold within the\nhigh-dimensional image space."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 942, "text": "178 6. DEEP NEURAL NETWORKS\nFigure 6.8 The top row shows examples of natural images of size64 ×64 pixels, whereas the bottom\nrow shows randomly generated images of the same size obtained by drawing pixel values\nfrom a uniform probability distribution over the possible pixel colours.\nimprovement. Effectively, neural networks learn a set of basis functions that are\nadapted to data manifolds. Moreover, for a particular application, not all directions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 943, "text": "within the manifold may be signiﬁcant. For example, if we wish to determine only\nthe orientation, and not the position, of the object inFigure 6.7, then there is only one\nrelevant degree of freedom on the manifold and not three. Neural networks are also\nable to learn which directions on the manifold are relevant to predicting the desired\noutputs.\nAnother way to see that real data is conﬁned to low-dimensional manifolds is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 944, "text": "to consider the task of generating random images. In Figure 6.8 we see examples\nof natural images along with examples of synthetic images of the same resolution\ngenerated by sampling each of the red, green, and blue intensities at each pixel inde-\npendently at random from a uniform distribution. We see that none of the synthetic\nimages look at all like natural images. The reason is that these random images lack"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 945, "text": "the very strong correlations between pixels that natural images exhibit. For example,\ntwo adjacent pixels in a natural image have a much higher probability of having the\nsame, or very similar, colour, than would two adjacent images in the random exam-\nples. Each of the images in Figure 6.8 corresponds to a point in a high-dimensional\nspace, yet natural images cover only a tiny fraction of this space.\n6.1.4 Data-dependent basis functions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 946, "text": "6.1.4 Data-dependent basis functions\nWe have seen that simple basis functions that are chosen independently of the\nproblem being solved can run into signiﬁcant limitations, particularly in spaces of\nhigh dimensionality. If we want to use basis functions in such situations, then one\napproach would be to use expert knowledge to hand-craft the basis functions in a\n6.1. Limitations of Fixed Basis Functions 179\nway that is speciﬁc to each application. For many years, this was the mainstream"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 947, "text": "approach in machine learning. Basis functions, often called features, would be de-\ntermined by a combination of domain knowledge and trial-and-error. However, this\napproach met with limited success and was superseded by data-driven approaches\nin which basis functions are learned from the training data. Domain knowledge still\nplays a role in modern machine learning, but at a more qualitative level in designing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 948, "text": "network architectures where it can capture appropriate inductive bias, as we will seeSection 9.1\nin later chapters.\nSince data in a high-dimensional space may be conﬁned to a low-dimensional\nmanifold, we do not need basis functions that densely ﬁll the whole input space,\nbut instead we can use basis functions that are themselves associated with the data\nmanifold. One way to do this is to have one basis function associated with each data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 949, "text": "point in the training set, which ensures that the basis functions are automatically\nadapted to the underlying data manifold. An example of such a model is that of\nradial basis functions (Broomhead and Lowe, 1988), which have the property that\neach basis function depends only on the radial distance (typically Euclidean) from a\ncentral vector. If the basis centres are chosen to be the input data values {xn}then\nthere is one basis function \u001en(x) for each data point, which will therefore capture"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 950, "text": "the whole of the data manifold. A typical choice for a radial basis function is\n\u001en(x) = exp\n(\n−∥x−xn∥2\ns2\n)\n(6.6)\nwhere sis a parameter controlling the width of the basis function. Although it can be\nquick to set up such a model, a major problem with this technique is that it becomes\ncomputationally unwieldy for large data sets. Moreover, the model needs careful\nregularization to avoid severe over-ﬁtting.\nA related approach, called a support vector machine or SVM (Vapnik, 1995;"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 951, "text": "Sch¨olkopf and Smola, 2002; Bishop, 2006), addresses this by again deﬁning basis\nfunctions that are centred on each of the training data points and then selecting a\nsubset of these automatically during training. As a result, the effective number of\nbasis functions in the resulting models is generally much smaller than the number of\ntraining points, although it is often still relatively large and typically increases with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 952, "text": "the size of the training set. Support vector machines also do not produce probabilistic\noutputs, and they do not naturally generalize to more than two classes. Methods such\nas radial basis functions and support vector machines have been superseded by deep\nneural networks, which are much better at exploiting very large data sets efﬁciently.\nMoreover, as we will see later, neural networks are able to learn deep hierarchical"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 953, "text": "representations, which are crucial to achieving high prediction accuracy in more\ncomplex applications.\n180 6. DEEP NEURAL NETWORKS\n6.2. Multilayer Networks\nIn the previous section, we saw that to apply linear models of the form (6.1) to prob-\nlems involving large-scale data sets and high-dimensional spaces, we need to ﬁnd a\nset of basis functions that is tuned to the problem being solved. The key idea behind\nneural networks is to choose basis functions \u001ej(x) that themselves have learnable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 954, "text": "parameters and then allow these parameters to be adjusted, along with the coefﬁ-\ncients {wj}, during training. We then optimize the whole model by minimizing an\nerror function using gradient-based optimization methods, such as stochastic gradi-\nent descent, where the error function is deﬁned jointly across all the parameters inChapter 7\nthe model.\nThere are, of course, many ways to construct parametric nonlinear basis func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 955, "text": "tions. One key requirement is that they must be differentiable functions of their\nlearnable parameters so that we can apply gradient-based optimization. The most\nsuccessful choice has been to use basis functions that follow the same form as (6.1),\nso that each basis function is itself a nonlinear function of a linear combination of\nthe inputs, where the coefﬁcients in the linear combination are learnable parameters."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 956, "text": "Note that this construction can clearly be extended recursively to give a hierarchical\nmodel with many layers, which forms the basis for deep neural networks.Section 6.3\nConsider a basic neural network model having two layers of learnable parame-\nters. First, we construct M linear combinations of the input variables x1;:::;x D in\nthe form\na(1)\nj =\nD∑\ni=1\nw(1)\nji xi + w(1)\nj0 (6.7)\nwhere j = 1;:::;M , and the superscript (1) indicates that the corresponding pa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 957, "text": "rameters are in the ﬁrst ‘layer’ of the network. We will refer to the parameters w(1)\nji\nas weights and the parameters w(1)\nj0 as biases, while the quantities a(1)\nj are calledChapter 4\npre-activations. Each of the quantities aj is then transformed using a differentiable,\nnonlinear activation function h(-)to give\nz(1)\nj = h(a(1)\nj ); (6.8)\nwhich represent the outputs of the basis functions in (6.1). In the context of neu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 958, "text": "ral networks, these basis functions are called hidden units. We will explore various\nchoices for the nonlinear function h(-)shortly, but here we note that provided the\nderivative h′(-)can be evaluated, then the overall network function will be differen-\ntiable. Following (6.1), these values are again linearly combined to give\na(2)\nk =\nM∑\nj=1\nw(2)\nkj z(1)\nj + w(2)\nk0 (6.9)\nwhere k = 1;:::;K , and K is the total number of outputs. This transformation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 959, "text": "corresponds to the second layer of the network, and again the w(2)\nk0 are bias parame-\nters. Finally, the {a(2)\nk }are transformed using an appropriate output-unit activation\n6.2. Multilayer Networks 181\nFigure 6.9 Network diagram for a two-layer\nneural network. The input, hid-\nden, and output variables are\nrepresented by nodes, and the\nweight parameters are repre-\nsented by links between the\nnodes. The bias parame-\nters are denoted by links com-\ning from additional input and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 960, "text": "ing from additional input and\nhidden variables x0 and z0\nwhich are themselves denoted\nby solid nodes. Arrows denote\nthe direction of information ﬂow\nthrough the network during for-\nward propagation.\nxD\nInputs\n …\nx1\nx0\nzM\nHidden units …\nz1\nz0\nyK\nOutputs\n …\ny1\nw(1)\nMD\nw(1)\n10\nw(2)\nKM\nw(2)\n10\nfunction f(-)to give a set of network outputsyk. A two-layer neural network can be\nrepresented in diagram form as shown in Figure 6.9.\n6.2.1 Parameter matrices"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 961, "text": "6.2.1 Parameter matrices\nAs we discussed in the context of linear regression models, the bias parametersSection 4.1.1\nin (6.7) can be absorbed into the set of weight parameters by deﬁning an additional\ninput variable x0 whose value is clamped at x0 = 1, so that (6.7) takes the form\naj =\nD∑\ni=0\nw(1)\nji xi: (6.10)\nWe can similarly absorb the second-layer biases into the second-layer weights, so\nthat the overall network function becomes\nyk(x;w) = f\n\n\nM∑\nj=0\nw(2)\nkj h\n(D∑\ni=0\nw(1)\nji xi\n)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 962, "text": "\n\nM∑\nj=0\nw(2)\nkj h\n(D∑\ni=0\nw(1)\nji xi\n)\n: (6.11)\nAnother notation that will prove convenient at various points in the book is to repre-\nsent the inputs as a column vector x = (x1;:::;x N)T and then to gather the weight\nand bias parameters in (6.11) into matrices to give\ny(x;w) = f\n(\nW(2)h\n(\nW(1)x\n))\n(6.12)\nwhere f(-)and h(-)are evaluated on each vector element separately.\n6.2.2 Universal approximation\nThe capability of a two-layer network to model a broad range of functions is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 963, "text": "illustrated in Figure 6.10. This ﬁgure also shows how individual hidden units work\ncollaboratively to approximate the ﬁnal function. The role of hidden units in a simple\nclassiﬁcation problem is illustrated in Figure 6.11.\n182 6. DEEP NEURAL NETWORKS\nFigure 6.10 Illustration of the ca-\npability of a two-layer neural network\nto approximate four different func-\ntions: (a) f(x) = x2, (b) f(x) =\nsin(x), (c), f(x) = |x|, and (d)\nf(x) = H(x) where H(x) is the\nHeaviside step function. In each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 964, "text": "Heaviside step function. In each\ncase, N = 50 data points, shown as\nblue dots, have been sampled uni-\nformly in x over the interval (−1;1)\nand the corresponding values of\nf(x) evaluated. These data points\nare then used to train a two-layer\nnetwork having three hidden units\nwith tanh activation functions and\nlinear output units. The resulting\nnetwork functions are shown by the\nred curves, and the outputs of the\nthree hidden units are shown by the\nthree dashed curves.\n(a)\n (b)\n(c)\n (d)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 965, "text": "three dashed curves.\n(a)\n (b)\n(c)\n (d)\nThe approximation properties of two-layer feed-forward networks were widely\nstudied in the 1980s, with various theorems showing that, for a wide range of activa-\ntion functions, such networks can approximate any function deﬁned over a continu-\nous subset of RD to arbitrary accuracy (Funahashi, 1989; Cybenko, 1989; Hornik,\nStinchcombe, and White, 1989; Leshnoet al., 1993). A similar result holds for func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 966, "text": "tions from any ﬁnite-dimensional discrete space to any another. Neural networks are\ntherefore said to be universal approximators.\nAlthough such theorems are reassuring, they tell us only that there exists a net-\nwork that can represent the required function. In some cases, they may require net-\nworks that have an exponentially large number of hidden units. Moreover, they say\nnothing about whether such a network can be found by a learning algorithm. Fur-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 967, "text": "thermore, we will see later that the no free lunch theoremsays that we can never ﬁndSection 9.1.2\na truly universal machine learning algorithm. Finally, although networks having two\nlayers of weights are universal approximators, in a practical application, there can\nbe huge beneﬁts in considering networks having many more than two layers that can\nlearn hierarchical internal representations. All these points support the drive towards\ndeep learning.\n6.2.3 Hidden unit activation functions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 968, "text": "6.2.3 Hidden unit activation functions\nWe have seen that the activation functions for the output units are determined\nby the kind of distribution being modelled. For the hidden units, however, the only\nrequirement is that they need to be differentiable, which leaves a wide range of pos-\n6.2. Multilayer Networks 183\nFigure 6.11 Example of the solution of a sim-\nple two-class classiﬁcation prob-\nlem involving synthetic data us-\ning a neural network having two\ninputs, two hidden units with tanh"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 969, "text": "inputs, two hidden units with tanh\nactivation functions, and a single\noutput having a logistic-sigmoid\nactivation function. The dashed\nblue lines show the z = 0:5 con-\ntours for each of the hidden units,\nand the red line shows the y =\n0:5 decision surface for the net-\nwork. For comparison, the green\nlines denote the optimal decision\nboundary computed from the dis-\ntributions used to generate the\ndata.\n−2 −1 0 1 2\n−2\n−1\n0\n1\n2\n3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 970, "text": "data.\n−2 −1 0 1 2\n−2\n−1\n0\n1\n2\n3\nsibilities. In most cases, all the hidden units in a network will be given the same\nactivation function, although in principle there is no reason why different choices\ncould not be applied in different parts of the network.\nThe simplest option for a hidden unit activation function is the identity function,\nwhich means that all the hidden units become linear. However, for any such network,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 971, "text": "we can always ﬁnd an equivalent network without hidden units. This follows from\nthe fact that the composition of successive linear transformations is itself a linear\ntransformation, and so its representational capability is no greater than that of a sin-\ngle linear layer. However, if the number of hidden units is smaller than either the\nnumber of input or output units, then the transformations that such a network can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 972, "text": "generate are not the most general possible linear transformation from inputs to out-\nputs because information is lost in the dimensionality reduction at the hidden units.\nConsider a network withNinputs, Mhidden units, and Koutputs, and where all ac-\ntivation functions are linear. Such a network has M(N + K) parameters, whereas a\nlinear transformation of inputs directly to outputs would haveNK parameters. If M\nis small relative to N or K, or both, this leads to a two-layer linear network having"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 973, "text": "fewer parameters than the direct linear mapping, corresponding to a rank-deﬁcient\ntransformation. Such ‘bottleneck’ networks of linear units corresponds to a standard\ndata analysis technique called principal component analysis. In general, however,Chapter 16\nthere is limited interest in using multilayer networks of linear units since the overall\nfunction computed by such a network is still linear.\nA simple, nonlinear differentiable function is the logistic sigmoid given by\n\u001b(a) = 1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 974, "text": "\u001b(a) = 1\n1 + exp(−a); (6.13)\nwhich is plotted in Figure 5.12. This was widely used in the early years of work on\nmultilayer neural networks and was partly inspired by studies of the properties of\n184 6. DEEP NEURAL NETWORKS\n−2.5 0.0 2.5\n−2.5\n0.0\n2.5\ntanh\n(a)\n−2.5 0.0 2.5\n−2.5\n0.0\n2.5\nhard tanh (b)\n−2.5 0.0 2.5\n−2.5\n0.0\n2.5\nsoftplus (c)\n−2.5 0.0 2.5\n−2.5\n0.0\n2.5\nReLU\n(d)\n−2.5 0.0 2.5\n−2.5\n0.0\n2.5\nleaky ReLU (e)\n−2.5 0.0 2.5\n−2.5\n0.0\n2.5\nabsolute (f)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 975, "text": "−2.5 0.0 2.5\n−2.5\n0.0\n2.5\nabsolute (f)\nFigure 6.12 A variety of nonlinear activation functions.\nbiological neurons. A closely related function is tanh, which is deﬁned by\ntanh(a) = ea −e−a\nea + e−a; (6.14)\nwhich is plotted in Figure 6.12(a). This function differs from the logistic sigmoid\nby a linear transformation of its input and its output values, and so for any network\nwith logistic-sigmoid hidden-unit activation functions, there is an equivalent network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 976, "text": "with tanh activation functions. However, when training a network, these are notExercise 6.4\nnecessarily equivalent because for gradient-based optimization, the network weights\nand biases need to be initialized, and so if the activation functions are changed, then\nthe initialization scheme must be adjusted accordingly. A ‘hard’ version of the tanh\nfunction (Collobert, 2004) is given by\nh(a) = max (−1;min(1;a)) (6.15)\nand is plotted in Figure 6.12(b)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 977, "text": "and is plotted in Figure 6.12(b).\nA major drawback of both the logistic sigmoid and the tanh activation functions\nis that the gradients go to zero exponentially when the inputs have either large pos-\nitive or large negative values. We will discuss this ‘vanishing gradients’ issue later,Section 7.4.2\n6.2. Multilayer Networks 185\nbut for the moment, we note that it will generally be better to use activation func-\ntions with non-zero gradients, at least when the input takes a large positive value."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 978, "text": "One such choice is the softplus activation function given byExercise 6.7\nh(a) = ln (1 + exp(a)); (6.16)\nwhich is plotted inFigure 6.12(c). For a≫ 1, we haveh(a) ≃a, and so the gradient\nremains non-zero even when the input to the activation function is large and positive,\nthereby helping to alleviate the vanishing gradients problem.\nAn even simpler choice of activation function is therectiﬁed linear unitor ReLU,\nwhich is deﬁned by\nh(a) = max(0;a) (6.17)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 979, "text": "which is deﬁned by\nh(a) = max(0;a) (6.17)\nand which is plotted inFigure 6.12(d). Empirically, this is one of the best-performing\nactivation functions, and it is in widespread use. Note that strictly speaking, the\nderivative of the ReLU function is not deﬁned when a = 0, but in practice this can\nbe safely ignored. The softplus function (6.16) can be viewed as a smoothed version\nof the ReLU and is therefore also sometimes called soft ReLU.Exercise 6.5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 980, "text": "Although the ReLU has a non-zero gradient for positive input values, this is\nnot the case for negative inputs, which can mean that some hidden units receive no\n‘error signal’ during training. A modiﬁcation of ReLU that seeks to avoid this issue\nis called a leaky ReLU and is deﬁned by\nh(a) = max(0;a) + \u000bmin(0;a); (6.18)\nwhere 0 <\u000b< 1. This function is plotted inFigure 6.12(e). Unlike ReLU, this has a\nnonzero gradient for input values a< 0, which ensures that there is a signal to drive"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 981, "text": "training. A variant of this activation function uses\u000b= −1, in which case h(a) = |a|,\nwhich is plotted inFigure 6.12(f). Another variant allows each hidden unit to have its\nown value \u000bj, which can be learned during network training by evaluating gradients\nwith respect to the {\u000bj}along with the gradients with respect to the weights and\nbiases.\nThe introduction of ReLU gave a big improvement in training efﬁciency over"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 982, "text": "previous sigmoidal activation functions (Krizhevsky, Sutskever, and Hinton, 2012).\nAs well as allowing deeper networks to be trained efﬁciently, it is much less sensitive\nto the random initialization of the weights. It is also well suited to a low-precision\nimplementation, such as 8-bit ﬁxed versus 64-bit ﬂoating point, and it is computa-\ntionally cheap to evaluate. Many practical applications simply use ReLU units as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 983, "text": "the default unless the goal is explicitly to explore the effects of different choices of\nactivation function.\n6.2.4 Weight-space symmetries\nOne property of feed-forward networks is that multiple distinct choices for the\nweight vector w can all give rise to the same mapping function from inputs to outputs\n(Chen, Lu, and Hecht-Nielsen, 1993). Consider a two-layer network of the form\nshown in Figure 6.9 with M hidden units having tanh activation functions and full"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 984, "text": "connectivity in both layers. If we change the sign of all the weights and the bias\n186 6. DEEP NEURAL NETWORKS\nfeeding into a particular hidden unit, then, for a given input data point, the sign\nof the pre-activation of the hidden unit will be reversed, and therefore so too will\nthe activation, because tanh is an odd function, so that tanh(−a) = −tanh(a).\nThis transformation can be exactly compensated for by changing the sign of all the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 985, "text": "weights leading out of that hidden unit. Thus, by changing the signs of a particular\ngroup of weights (and a bias), the input–output mapping function represented by\nthe network is unchanged, and so we have found two different weight vectors that\ngive rise to the same mapping function. For M hidden units, there will be M such\n‘sign-ﬂip’ symmetries, and thus, any given weight vector will be one of a set 2M\nequivalent weight vectors ."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 986, "text": "equivalent weight vectors .\nSimilarly, imagine that we interchange the values of all of the weights (and the\nbias) leading both into and out of a particular hidden unit with the corresponding\nvalues of the weights (and bias) associated with a different hidden unit. Again, this\nclearly leaves the network input–output mapping function unchanged, but it cor-\nresponds to a different choice of weight vector. For M hidden units, any given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 987, "text": "weight vector will belong to a set of M ×(M −1) ×---×2 ×1 = M! equivalent\nweight vectors associated with this interchange symmetry, corresponding to the M!\ndifferent orderings of the hidden units. The network will therefore have an overall\nweight-space symmetry factor of M! 2M. For networks with more than two layers\nof weights, the total level of symmetry will be given by the product of such factors,\none for each layer of hidden units."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 988, "text": "one for each layer of hidden units.\nIt turns out that these factors account for all the symmetries in weight space\n(except for possible accidental symmetries due to speciﬁc choices for the weight\nvalues). Furthermore, the existence of these symmetries is not a particular property\nof the tanh function but applies to a wide range of activation functions (Kurkov´a and\nKainen, 1994). In general, these symmetries in weight space are of little practical"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 989, "text": "consequence, since network training aims to ﬁnd a speciﬁc setting for the parameters,\nand the existence of other, equivalent, settings is of little consequence. However,\nweight-space symmetries do play a role when Bayesian methods are used to evaluate\nthe probability distribution over networks of different sizes (Bishop, 2006).\n6.3.\nDeep Networks\nWe have motivated the development of neural networks by making the basis func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 990, "text": "tions of a linear regression or classiﬁcation model themselves be governed by learn-\nable parameters, giving rise to the two-layer network model shown inFigure 6.9. For\nmany years, this was the most widely used architecture, primarily because it proved\ndifﬁcult to train networks with more than two layers effectively. However, extend-\ning neural networks to have more than two layers, known as deep neural networks,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 991, "text": "brings many advantages as we will discuss shortly, and recent advances in techniques\nfor training neural networks are effective for networks with many layers.Chapter 7\nWe can easily extend the two-layer network architecture (6.12) to any ﬁnite num-\nber Lof layers, in which layer l= 1;:::;L computes the following function:\nz(l) = h(l) (\nW(l)z(l−1) )\n(6.19)\n6.3. Deep Networks 187\nwhere h(l) denotes the activation function associated with layer l, and W(l) denotes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 992, "text": "the corresponding matrix of weight and bias parameters. Also, z(0) = x represents\nthe input vector and z(L) = y represents the output vector.\nNote that there has been some confusion in the literature regarding the termi-\nnology for counting the number of layers in such networks. Thus, the network in\nFigure 6.9 is sometimes described as a three-layer network (which counts the num-\nber of layers of units and treats the inputs as units) or sometimes as a single-hidden-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 993, "text": "layer network (which counts the number of layers of hidden units). We recommend\na terminology in which Figure 6.9 is called a two-layer network, because it is the\nnumber of layers of learnable weights that is important for determining the network\nproperties.\nWe have seen that a network of the form shown inFigure 6.9, having two layers\nof learnable parameters, has universal approximation capabilities. However, net-\nworks with more than two layers can sometimes represent a given function with far"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 994, "text": "fewer parameters than a two-layer network. Mont ´ufar et al. (2014) show that the\nnetwork function divides the input space into a number of regions that is exponential\nin the depth of the network, but which is only polynomial in the width of the hidden\nlayers. To represent the same function using a two-layer network would require an\nexponential number of hidden units.\n6.3.1 Hierarchical representations\nAlthough this is an interesting result, a more compelling reason to explore deep"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 995, "text": "neural networks is that the network architecture encodes a particular form of induc-\ntive bias, namely that the outputs are related to the input space through a hierarchical\nrepresentation. A good example is the task of recognizing objects in images. TheChapter 10\nrelationship between the pixels of an image and a high-level concept such as ‘cat’ is\nhighly complex and nonlinear, and would be an extremely challenging problem for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 996, "text": "a two-layer network. However, a deep neural network can learn to detect low-level\nfeatures, such as edges, in the early layers, and can then combine these in subse-\nquent layers to make higher-level features such as eyes or whiskers, which in turn\ncan be combined in later layers to detect the presence of a cat. This can be viewed\nas a compositional inductive bias, in which higher-level objects, such as a cat, are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 997, "text": "composed of lower-level objects, such as eyes, which in turn have yet lower-level el-\nements such as edges. We can also think of this in reverse by considering the process\nof generating an image starting with low-level features such as edges, then combin-\ning these to form simple shapes such as circles, and then combining those in turn to\nform higher-level objects such as cats. At each stage there are many ways to com-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 998, "text": "bine different components, giving an exponential gain in the number of possibilities\nwith increasing depth.\n6.3.2 Distributed representations\nNeural networks can take advantage of another form of compositionality called\na distributed representation. Conceptually, each unit in a hidden layer can be thought\nof as representing a ‘feature’ at that level of the network, with a high value of the\n188 6. DEEP NEURAL NETWORKS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 999, "text": "188 6. DEEP NEURAL NETWORKS\nactivation indicating that the corresponding feature is present and a low value indi-\ncating its absence. With M units in a given layer, such a network can represent M\ndifferent features. However, the network could potentially learn a different represen-\ntation in which combinations of hidden units represent features, thereby potentially\nallowing a hidden layer with M units to represent 2M different features, growing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1000, "text": "exponentially with the number of units. Consider, for example, a network designed\nto process images of faces. Each particular face image may or may not have glasses,\nit may or may not have a hat, and it may or may not have a beard, leading to eight\ndifferent combinations. Although this could be represented by eight units each of\nwhich ‘turns on’ when it detects the corresponding combination, it could also be\nrepresented more compactly by just three units, one for each attribute. These can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1001, "text": "present independently of each other (although statistically their presence is likely to\nbe correlated to some degree). Later, we will explore in detail the kinds of internal\nrepresentations that deep learning networks discover for themselves during training.Chapter 10\n6.3.3 Representation learning\nWe can view the successive layers of a deep neural network as performing trans-\nformations of the data, that make it easier to solve the desired task or tasks. For"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1002, "text": "example, a neural network that successfully learns to classify skin lesions as benign\nor malignant must have learned to transform the original image data into a new space,Section 1.1.1\nrepresented by the outputs of the ﬁnal layer of hidden units, such that the ﬁnal layer\nof the network can distinguish the two classes. This ﬁnal layer can be viewed as a\nsimple linear classiﬁer, and so in the representation of the last hidden layer, the two"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1003, "text": "classes must be well separated by a linear surface. This ability to discover a nonlin-\near transformation of the data that makes subsequent tasks easier to solve is called\nrepresentation learning (Bengio, Courville, and Vincent, 2012). The learned repre-\nsentation, sometimes called the embedding space, is given by the outputs of one of\nthe hidden layers of the network, so that any input vector, either from the training set"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1004, "text": "or from some new data set, can be transformed into this representation by forward\npropagation through the network.\nRepresentation learning is especially powerful because it allows us to exploit\nunlabelled data. Often it is easy to collect a large quantity of unlabelled data, but\nacquiring the associated labels may be more difﬁcult. For example, a video camera\non a vehicle can gather large numbers of images of urban scenes as the vehicle is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1005, "text": "driven around a city, but taking those images and identifying relevant objects, such\nas pedestrians and road signs, would require expensive and time-consuming human\nlabelling.\nLearning from unlabelled data is called unsupervised learning, and many differ-\nent algorithms have been developed to do this. For example, a neural network can be\ntrained to take images as input and to create the same images as the output. To make"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1006, "text": "this into a non-trivial task, the network may use hidden layers with fewer units than\nthe number of pixels in the image, thereby forcing the network to learn some kind of\ncompression of the images. Only unlabelled data is needed because each image in\nthe training set acts as both the input vector and the target vector. Such networks are\nknown as autoencoders. The goal is that this type of training will force the networkSection 19.1\n6.3. Deep Networks 189"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1007, "text": "6.3. Deep Networks 189\nto discover some internal representation for the data that is useful for solving other\ntasks, such as image classiﬁcation.\nHistorically, unsupervised learning played an important role in enabling the ﬁrst\ndeep networks (apart from convolutional networks) to be successfully trained. Each\nlayer of the network was ﬁrst pre-trained using unsupervised learning and then the\nentire network was trained further using gradient-based supervised training. It was"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1008, "text": "later discovered that the pre-training phase could be omitted and a deep network\ncould be trained from scratch purely using supervised learning given appropriate\nconditions.\nHowever, pre-training and representation learning remain central to deep learn-\ning in other contexts. The most notable example of pre-training is in natural language\nprocessing in which transformer models are trained on large quantities of text and areChapter 12"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1009, "text": "able to learn highly sophisticated internal representations of language that facilitates\nan impressive range of capabilities at human level and beyond.\n6.3.4 Transfer learning\nThe internal representation learned for one particular task might also be useful\nfor related tasks. For example, a network trained on a large labelled data set of\neveryday objects can learn how to transform an image representation into one that is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1010, "text": "much better suited for classifying objects. Then, the ﬁnal classiﬁcation layer of the\nnetwork can be retrained using a smaller labelled data set of skin lesion images to\ncreate a lesion classiﬁer. This is an example of transfer learning (Hospedales et al.,Section 1.1.1\n2021), which allows higher accuracy to be achieved than if only the lesion image\ndata were used for training, because the network can exploit commonalities shared"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1011, "text": "by natural images in general. Transfer learning is illustrated in Figure 6.13.\nIn general, transfer learning can be used to improve performance on some task\nA, for which training data is in short supply, by using data from a related task B, for\nwhich data is more plentiful. The two tasks should have the same kind of inputs,\nand there should be some commonality between the tasks so that low-level features,\nor internal representations, learned from task B will be useful for task A. When we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1012, "text": "look at convolutional networks we will see that many image processing tasks requireChapter 10\nsimilar low-level features corresponding to the early layers of a deep neural network,\nwhereas later layers are more specialized to a particular task, making such networks\nwell suited to transfer learning applications.\nWhen data for task A is very scarce, we might simply re-train the ﬁnal layer of\nthe network. In contrast, if there are more data points, it is feasible to retrain several"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1013, "text": "layers. The process of learning parameters using one task that are then applied to\none or more other tasks is called pre-training. Note that for the new task, instead\nof applying stochastic gradient descent to the whole network, it is much more ef-\nﬁcient to send the new training data once through the ﬁxed pre-trained network so\nas to evaluate the training inputs in the new representation. Iterative gradient-based"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1014, "text": "optimization can then be applied just to the smaller network consisting of the ﬁnal\nlayers. As well as using a pre-trained network as a ﬁxed pre-processor for a different\ntask, it is also possible to apply ﬁne-tuning in which the whole network is adapted to\nthe data for task A. This is generally done with a very small learning rate for a lim-\n190 6. DEEP NEURAL NETWORKS\ntree\ncat\ndog. ..\n(a)\ncancer\nnormal\n(b)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1015, "text": "tree\ncat\ndog. ..\n(a)\ncancer\nnormal\n(b)\nFigure 6.13 Schematic illustration of transfer learning. (a) A network is ﬁrst trained on a task with abundant\ndata, such as object classiﬁcation of natural images. (b) The early layers of the network (shown in red) are\ncopied from the ﬁrst task and the ﬁnal few layers of the network (shown in blue) are then retrained on a new task\nsuch as skin lesion classiﬁcation for which training data is more scarce."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1016, "text": "ited number of iterations to ensure that the network does not over-ﬁt to the relatively\nsmall data set available for the new task.\nA related approach is multitask learning (Caruana, 1997) in which a network\njointly learns more than one related task at the same time. For example, we might\nwish to construct a spam email ﬁlter that allows different users to have different\nclassiﬁers tuned to their particular preferences. The training data may comprise ex-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1017, "text": "amples of spam email and non-spam email for many different users, but the number\nof examples for any one user may be quite limited, and therefore training a separate\nclassiﬁer for each user would give poor results. Instead, we can combine the data\nsets to train a single larger network that might, for example, share early layers but\nhave separate learnable parameters for the different users in later layers. Sharing\ndata across tasks allows the network to exploit commonalities amongst the tasks,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1018, "text": "thereby improving the accuracy for all users. With a large number of training exam-\nples, a deeper network with more parameters can be used, again leading to improved\nperformance.\nLearning across multiple tasks can be extended to meta-learning, which is also\ncalled learning to learn . Whereas multitask learning aims to make predictions for\na ﬁxed set of tasks, the aim of meta-learning is to make predictions for future tasks"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1019, "text": "that were not seen during training. This can be done by not only learning a shared\n6.3. Deep Networks 191\ninternal representation across tasks but also by learning the learning algorithm itself\n(Hospedales et al., 2021). Meta-learning can be used to facilitate generalization of,\nfor example, a classiﬁcation model to new classes when there are very few labelled\nexamples of the new classes. This is referred to as few-shot learning. When only a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1020, "text": "single labelled example is used it is called one-shot learning.\n6.3.5 Contrastive learning\nOne of the most common and powerful representation learning methods is con-\ntrastive learning (Gutmann and Hyv ¨arinen, 2010; Oord, Li, and Vinyals, 2018;\nChen, Kornblith, et al., 2020). The idea is to learn a representation such that cer-\ntain pairs of inputs, referred to as positive pairs, are close in the embedding space,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1021, "text": "and other pairs of inputs, called negative pairs, are far apart. The intuition is that\nif we choose our positive pairs in such a way that they are semantically similar and\nchoose negative pairs that are semantically dissimilar, then we will learn a represen-\ntation space in which similar inputs are close, making downstream tasks, such as\nclassiﬁcation, much easier. As with other forms of representation learning, the out-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1022, "text": "puts of the trained network are typically not used directly, and instead the activations\nat some earlier layer are used to form the embedding space. Contrastive learning is\nunlike most other machine learning tasks, in that the error function for a given input\nis deﬁned only with respect to other inputs, instead of having a per-input label or\ntarget output.\nSuppose we have a given data pointx called the anchor, for which we have spec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1023, "text": "iﬁed another data point x+ that together with x makes up a positive pair. We must\nalso specify a set of data points {x−\n1 ;:::; x−\nN}each of which makes up a negative\npair with x. We now need a loss function that will reward close proximity between\nthe representations ofx and x+ while encouraging a large distance between each pair\n{x;x−\nn}. One example of such a function, and the most commonly used loss func-\ntion for contrastive learning, is called the InfoNCE loss (Gutmann and Hyv ¨arinen,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1024, "text": "2010; Oord, Li, and Vinyals, 2018), where NCE denotes ‘noise contrastive estima-\ntion’. Suppose we have a neural network function fw(x) that maps points from the\ninput space x to a representation space, governed by learnable parameters w. This\nrepresentation is normalized so that ||fw(x)||= 1. Then, for a data point x, the\nInfoNCE loss is deﬁned by\nE(w) = −ln exp{fw(x)Tfw(x+)}\nexp{fw(x)Tfw(x+)}+ ∑N\nn=1 exp{fw(x)Tfw(x−n)}\n: (6.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1025, "text": "n=1 exp{fw(x)Tfw(x−n)}\n: (6.20)\nWe can see that in this function, the cosine similarity fw(x)Tfw(x+) between the\nrepresentation fw(x) of the anchor and the representation fw(x+) of the positive\nexample provides our measure of how close the positive pair examples are in the\nlearned space, and the same measure is used to assess how close the anchor is to the\nnegative examples. Note that the function resembles a classiﬁcation cross-entropy"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1026, "text": "error function in which the cosine similarity of the positive pair gives the logit for\nthe label class and the cosine similarities for the negative pairs give the logits for the\nincorrect classes. Also note that the negative pairs are crucial as without them the\n192 6. DEEP NEURAL NETWORKS\nembedding would simply learn the degenerate solution of mapping every point to the\nsame representation.\nA particular contrastive learning algorithm is deﬁned predominantly by how the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1027, "text": "positive and negative pairs are chosen, which is how we use our prior knowledge to\nspecify what a good representation should be. For example, consider the problem of\nlearning representations of images. Here, a common choice is to create positive pairs\nby corrupting the input images in ways that should preserve the semantic information\nof the image while greatly altering the image in the pixel space (Wu et al., 2018; He"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1028, "text": "et al., 2019; Chen, Kornblith, et al., 2020). Corruptions are closely related to data\naugmentations, and examples include rotation, translation, and colour shifts. OtherSection 9.1.3\nimages from the data set can then be used to create the negative pairs. This approach\nto contrastive learning is known as instance discrimination.\nIf, however, we have access to class labels, then we can use images of the same\nclass as positive pairs and images of different classes as negative pairs. This re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1029, "text": "laxes the reliance on specifying the augmentations that the representation should be\ninvariant to and also avoids treating two semantically similar images as a negative\npair. This is referred to as supervised contrastive learning (Khosla et al., 2020) be-\ncause of the reliance on the class labels, and it can often yield better results than\nsimply learning the representation using cross-entropy classiﬁcation.\nThe members of positive and negative pairs do not necessarily have to come"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1030, "text": "from the same data modality. In contrastive-language image pretraining, or CLIP\n(Radford et al., 2021), a positive pair consists of an image and its corresponding\ntext caption, and two separate functions, one for each modality, are used to map the\ninputs to the same representation space. Negative pairs are then mismatched images\nand captions. This is often referred to as weakly supervised, as it relies on captioned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1031, "text": "images, which are often easier to obtain by scraping data from the internet than by\nmanually labelling images with their classes. The loss function in this case is given\nby\nE(w) = −1\n2 ln exp{fw(x+)Tg\u0012(y+)}\nexp{fw(x+)Tg\u0012(y+)}+ ∑N\nn=1 exp{fw(x−n)Tg\u0012(y+)}\n−1\n2 ln exp{fw(x+)Tg\u0012(y+)}\nexp{fw(x+)Tg\u0012(y+)}+ ∑M\nm=1 exp{fw(x+)Tg\u0012(y−m)}\n(6.21)\nwhere x+ and y+ represent a positive pair in which x is an image and y is its corre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1032, "text": "sponding text caption, fw represents the mapping from images to the representation\nspace, and g\u0012 is the mapping from text input to the representation space. We also\nrequire a set {x−\n1 ;:::; x−\nN}of other images from the data set, for which we can\nassume the text caption y+ is inappropriate, and a set {y−\n1 ;:::; y−\nM}of text cap-\ntions that are similarly mismatched to the input image x. The two terms in the loss"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1033, "text": "function ensure that (a) the representation of the image is close to its text caption\nrepresentation relative to other image representations and (b) the text caption rep-\nresentation is close to the representation of the image it describes relative to other\nrepresentations of text captions. Although CLIP uses text and image pairs, any data\n6.3. Deep Networks 193\nx x + x−\nfw(x)\nfw(x+)\nfw(x−)\n(a)\nx x+ x−\nfw(x)\nfw(x+)\nfw(x−)\n(b)\n‘a ginger\ncat sat\non a wall’\n‘a bike\nleaning\non a\nbrick wall’"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1034, "text": "on a wall’\n‘a bike\nleaning\non a\nbrick wall’\nx+ y+ y−\nfw(x+)\ng\u0012(y+)\ng\u0012(y−)\n(c)\nFigure 6.14 Illustration of three different contrastive learning paradigms. (a) The instance discrimination ap-\nproach, where the positive pair is made up of the anchor and an augmented version of the same image. These\nare mapped to points in a normalized space that can be thought of as a unit hypersphere. The coloured arrows"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1035, "text": "show that the loss encourages the representations of the positive pair to be closer together but pushes negative\npairs further apart. (b) Supervised contrastive learning in which the positive pair consists of two different images\nfrom the same class. (c) The CLIP model in which the positive pair is made up of an image and an associated\ntext snippet.\nset with paired modalities can be used to learn representations. A comparison of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1036, "text": "different contrastive learning methods we have discussed is shown in Figure 6.14.\n6.3.6 General network architectures\nSo far, we have explored neural network architectures that are organized into a\nsequence of fully-connected layers. However, because there is a direct correspon-\ndence between a network diagram and its mathematical function, we can develop\nmore general network mappings by considering more complex network diagrams."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1037, "text": "These must be restricted to afeed-forward architecture, in other words to one having\nno closed directed cycles, to ensure that the outputs are deterministic functions of\nthe inputs. This is illustrated with a simple example in Figure 6.15. Each (hidden or\noutput) unit in such a network computes a function given by\nzk = h\n\n ∑\nj∈A(k)\nwkjzj + bk\n\n (6.22)\nwhere A(k) denotes the set of ancestors of node k, in other words the set of units"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1038, "text": "that send connections to unit k, and bk denotes the associated bias parameter. For\na given set of values applied to the inputs of the network, successive application of\n(6.22) allows the activations of all units in the network to be evaluated including\nthose of the output units.\n194 6. DEEP NEURAL NETWORKS\nFigure 6.15 Example of a neural network\nhaving a general feed-forward topology. Note\nthat each hidden and output unit has an asso-\nciated bias parameter (omitted for clarity). x1\ninputs\nx2\nz1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1039, "text": "inputs\nx2\nz1\nz2\nz3\nz2\nz3\ny1\noutputs\ny2\n6.3.7 Tensors\nWe see that linear algebra plays a central role in neural networks, with quantities\nsuch as data sets, activations, and network parameters represented as scalars, vectors,\nand matrices. However, we also encounter variables of higher dimensionality. Con-\nsider, for example, a data set of N colour images each of which is I pixels high and\nJ pixels wide. Each pixel is indexed by its row and column within the image and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1040, "text": "has red, green, and blue values. We have one such value for each image in the data\nset, and so we can represent a particular intensity value by a four-dimensional array\nX with elements xijkn where i ∈ {1;:::;I }and j ∈ {1;:::;J }index the row\nand column within the image, k ∈{1;2;3}indexes the red, green, and blue inten-\nsities, and n ∈{1;:::;N }indexes the particular image within the data set. These\nhigher-dimensional arrays are called tensors and include scalars, vectors, and matri-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1041, "text": "ces as special cases. We will see many examples of such tensors when we discuss\nmore sophisticated neural network architectures later in the book. Massively parallel\nprocessors such as GPUs are especially well suited to processing tensors.\n6.4.\nError Functions\nIn earlier chapters, we explored linear models for regression and classiﬁcation, andChapter 4\nChapter 5 in the process we derived suitable forms for the error functions along with corre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1042, "text": "sponding choices for the output-unit activation function. The same considerations\nfor choosing an error function apply for multilayer neural networks, and so for con-\nvenience, we will summarize the key points here.\n6.4.1 Regression\nWe start by discussing regression problems, and for the moment we consider\na single target variable tthat can take any real value. Following the discussion of\nregression in single-layer networks, we assume that t has a Gaussian distributionSection 2.3.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1043, "text": "with an x-dependent mean, which is given by the output of the neural network, so\nthat\np(t|x;w) = N\n(\nt|y(x;w);\u001b2)\n(6.23)\n6.4. Error Functions 195\nwhere \u001b2 is the variance of the Gaussian noise. Of course this is a somewhat restric-\ntive assumption, and in some applications we will need to extend this approach to\nallow for more general distributions. For the conditional distribution given by (6.23),Section 6.5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1044, "text": "it is sufﬁcient to take the output-unit activation function to be the identity, because\nsuch a network can approximate any continuous function from x to y. Given a data\nset of N i.i.d. observations X = {x1;:::; xN}, along with corresponding target\nvalues t = {t1;:::;t N}, we can construct the corresponding likelihood function:\np(t|X;w;\u001b2) =\nN∏\nn=1\np(tn|y(xn;w);\u001b2): (6.24)\nNote that in the machine learning literature, it is usual to consider the minimization"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1045, "text": "of an error function rather than the maximization of the likelihood, and so here we\nwill follow this convention. Taking the negative logarithm of the likelihood function\n(6.24), we obtain the error function\n1\n2\u001b2\nN∑\nn=1\n{y(xn;w) −tn}2 + N\n2 ln \u001b2 + N\n2 ln(2\u0019); (6.25)\nwhich can be used to learn the parametersw and \u001b2. Consider ﬁrst the determination\nof w. Maximizing the likelihood function is equivalent to minimizing the sum-of-\nsquares error function given by\nE(w) = 1\n2\nN∑\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1046, "text": "squares error function given by\nE(w) = 1\n2\nN∑\nn=1\n{y(xn;w) −tn}2 (6.26)\nwhere we have discarded additive and multiplicative constants. The value of w\nfound by minimizing E(w) will be denoted w?. Note that this will typically not\ncorrespond to the global maximum of the likelihood function because the nonlin-\nearity of the network function y(xn;w) causes the error E(w) to be non-convex,\nand so ﬁnding the global optimum is generally infeasible. Moreover, regularization"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1047, "text": "terms may be added to the error function and other modiﬁcations may be made toChapter 9\nthe training process, so that the resulting solution for the network parameters may\ndiffer signiﬁcantly from the maximum likelihood solution.\nHaving found w?, the value of\u001b2 can be found by minimizing the error function\n(6.25) to giveExercise 6.8\n\u001b2? = 1\nN\nN∑\nn=1\n{y(xn;w?) −tn}2: (6.27)\nNote that this can be evaluated once the iterative optimization required to ﬁnd w? is\ncompleted."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1048, "text": "completed.\nIf we have multiple target variables, and we assume that they are independent,\nconditional on x and w, with shared noise variance \u001b2, then the conditional distri-\nbution of the target values is given by\np(t|x;w) = N\n(\nt|y(x;w);\u001b2I\n)\n: (6.28)\n196 6. DEEP NEURAL NETWORKS\nFollowing the same argument as for a single target variable, we see that maximizing\nthe likelihood function with respect to the weights is equivalent to minimizing the\nsum-of-squares error function:Exercise 6.9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1049, "text": "sum-of-squares error function:Exercise 6.9\nE(w) = 1\n2\nN∑\nn=1\n∥y(xn;w) −tn∥2: (6.29)\nThe noise variance is then given by\n\u001b2? = 1\nNK\nN∑\nn=1\n∥y(xn;w?) −tn∥2 (6.30)\nwhere K is the dimensionality of the target variable. The assumption of conditional\nindependence of the target variables can be dropped at the expense of a slightly more\ncomplex optimization problem.Exercise 6.10\nRecall that there is a natural pairing of the error function (given by the negative"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1050, "text": "log likelihood) and the output-unit activation function. In regression, we can view theSection 5.4.6\nnetwork as having an output activation function that is the identity, so that yk = ak.\nThe corresponding sum-of-squares error function then has the property\n@E\n@ak\n= yk −tk: (6.31)\n6.4.2 Binary classiﬁcation\nNow consider binary classiﬁcation in which we have a single target variable\nt such that t = 1 denotes class C1 and t = 0 denotes class C2. Following the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1051, "text": "discussion of canonical link functions, we consider a network having a single outputSection 5.4.6\nwhose activation function is a logistic sigmoid (6.13) so that 0 6 y(x;w) 6 1. We\ncan interpret y(x;w) as the conditional probability p(C1|x), withp(C2|x)given by\n1 −y(x;w). The conditional distribution of targets given inputs is then a Bernoulli\ndistribution of the form\np(t|x;w) = y(x;w)t{1−y(x;w)}1−t : (6.32)\nIf we consider a training set of independent observations, then the error function,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1052, "text": "which is given by the negative log likelihood, is then a cross-entropy error of the\nform\nE(w) = −\nN∑\nn=1\n{tnln yn + (1 −tn) ln(1−yn)} (6.33)\nwhere yn denotes y(xn;w). Simard, Steinkraus, and Platt (2003) found that using\nthe cross-entropy error function instead of the sum-of-squares for a classiﬁcation\nproblem leads to faster training as well as improved generalization.\nNote that there is no analogue of the noise variance \u001b2 in (6.32) because the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1053, "text": "target values are assumed to be correctly labelled. However, the model is easily\nextended to allow for labelling errors by introducing a probability \u000fthat the targetExercise 6.11\n6.4. Error Functions 197\nvalue thas been ﬂipped to the wrong value (Opper and Winther, 2000). Here \u000fmay\nbe set in advance, or it may be treated as a hyperparameter whose value is inferred\nfrom the data.\nIf we have K separate binary classiﬁcations to perform, then we can use a net-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1054, "text": "work having K outputs each of which has a logistic-sigmoid activation function.\nAssociated with each output is a binary class labeltk ∈{0;1}, wherek= 1;:::;K .\nIf we assume that the class labels are independent, given the input vector, then the\nconditional distribution of the targets is\np(t|x;w) =\nK∏\nk=1\nyk(x;w)tk [1 −yk(x;w)]1−tk : (6.34)\nTaking the negative logarithm of the corresponding likelihood function then gives\nthe following error function:Exercise 6.13\nE(w) = −\nN∑\nn=1\nK∑\nk=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1055, "text": "E(w) = −\nN∑\nn=1\nK∑\nk=1\n{tnkln ynk + (1 −tnk) ln(1−ynk)} (6.35)\nwhere ynk denotes yk(xn;w). Again, the derivative of the error function with re-\nspect to the pre-activation for a particular output unit takes the form (6.31), just as inExercise 6.14\nthe regression case.\n6.4.3 multiclass classiﬁcation\nFinally, we consider the standard multiclass classiﬁcation problem in which each\ninput is assigned to one of Kmutually exclusive classes. The binary target variables"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1056, "text": "tk ∈ {0;1}have a 1-of- K coding scheme indicating the class, and the networkSection 5.1.3\noutputs are interpreted as yk(x;w) = p(tk = 1|x), leading to the error function\n(5.80), which we reproduce here:\nE(w) = −\nN∑\nn=1\nK∑\nk=1\ntknln yk(xn;w): (6.36)\nThe output-unit activation function, which corresponds to the canonical link, is given\nby the softmax function:Section 5.4.4\nyk(x;w) = exp(ak(x;w))\n∑\nj\nexp(aj(x;w))\n; (6.37)\nwhich satisﬁes 0 6 yk 6 1 and ∑\nkyk = 1. Note that the yk(x;w) are unchanged"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1057, "text": "kyk = 1. Note that the yk(x;w) are unchanged\nif a constant is added to all of theak(x;w), causing the error function to be constant\nfor some directions in weight space. This degeneracy is removed if an appropriate\nregularization term is added to the error function. Once again, the derivative of theChapter 9\nerror function with respect to the pre-activation for a particular output unit takes the\nfamiliar form (6.31).Exercise 6.15\n198 6. DEEP NEURAL NETWORKS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1058, "text": "198 6. DEEP NEURAL NETWORKS\nIn summary, there is a natural choice of both output-unit activation function\nand matching error function according to the type of problem being solved. For\nregression, we use linear outputs and a sum-of-squares error, for multiple indepen-\ndent binary classiﬁcations, we use logistic sigmoid outputs and a cross-entropy error\nfunction, and for multi-class classiﬁcation, we use softmax outputs with the corre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1059, "text": "sponding multi-class cross-entropy error function. For classiﬁcation problems in-\nvolving two classes, we can use a single logistic sigmoid output, or alternatively, we\ncan use a network with two outputs having a softmax output activation function.\nThis procedure is quite general, and by considering other forms of conditional\ndistribution, we can derive the associated error functions as the corresponding neg-\native log likelihood. We will see an example of this in the next section when we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1060, "text": "consider multimodal network outputs.\n6.5.\nMixture Density Networks\nSo far in this chapter we have discussed neural networks whose outputs represent\nsimple probability distributions comprising either a Gaussian for continuous vari-\nables or a binary distribution for discrete variables. We close the chapter by showing\nhow a neural network can represent more general conditional probabilities by treat-\ning the outputs of the network as the parameters of a more complex distribution, in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1061, "text": "this case a Gaussian mixture model. This is known as a mixture density network,\nand we will see how to deﬁne the associated error function and the corresponding\noutput-unit activation functions.\n6.5.1 Robot kinematics example\nThe goal of supervised learning is to model a conditional distribution p(t|x),\nwhich for many simple regression problems is chosen to be Gaussian. However,\npractical machine learning problems can often have signiﬁcantly non-Gaussian dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1062, "text": "tributions. These can arise, for example, with inverse problems in which the distri-\nbution can be multimodal, in which case the Gaussian assumption can lead to very\npoor predictions.\nAs a simple illustration of an inverse problem, consider the kinematics of a robot\narm, as illustrated in Figure 6.16. The forward problem involves ﬁnding the end ef-Exercise 6.16\nfector position given the joint angles and has a unique solution. However, in practice"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1063, "text": "we wish to move the end effector of the robot to a speciﬁc position, and to do this we\nmust set appropriate joint angles. We therefore need to solve the inverse problem,\nwhich has two solutions, as seen in Figure 6.16.\nForward problems often correspond to causality in a physical system and gen-\nerally have a unique solution. For instance, a speciﬁc pattern of symptoms in the\nhuman body may be caused by the presence of a particular disease. In machine"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1064, "text": "learning, however, we typically have to solve an inverse problem, such as trying to\npredict the presence of a disease given a set of symptoms. If the forward problem\ninvolves a many-to-one mapping, then the inverse problem will have multiple solu-\ntions. For instance, several different diseases may result in the same symptoms.\n6.5. Mixture Density Networks 199\nFigure 6.16 (a) A two-link robot\narm, in which the Cartesian coor-\ndinates (x1;x2) of the end effector"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1065, "text": "dinates (x1;x2) of the end effector\nare determined uniquely by the two\njoint angles \u00121 and \u00122 and the (ﬁxed)\nlengths L1 and L2 of the arms. This\nis known as the forward kinematics\nof the arm. (b) In practice, we have\nto ﬁnd the joint angles that will give\nrise to a desired end effector posi-\ntion. This inverse kinematics has\ntwo solutions corresponding to ‘el-\nbow up’ and ‘elbow down’.\nL1\nL2\n\u00121\n\u00122\n(x1;x2)\n(a)\n(x1; x2)\nelbow\ndown\nelbow\nup (b)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1066, "text": "\u00122\n(x1;x2)\n(a)\n(x1; x2)\nelbow\ndown\nelbow\nup (b)\nIn the robotics example, the kinematics is deﬁned by geometrical equations, and\nthe multimodality is readily apparent. However, in many machine learning problems\nthe presence of multimodality, particularly in problems involving spaces of high di-\nmensionality, can be less obvious. For tutorial purposes, however, we will consider\na simple toy problem for which we can easily visualize the multimodality. The data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1067, "text": "for this problem is generated by sampling a variable xuniformly over the interval\n(0;1), to give a set of values {xn}, and the corresponding target values tn are ob-\ntained by computing the functionxn+0:3 sin(2\u0019xn) and then adding uniform noise\nover the interval (−0:1;0:1). The inverse problem is then obtained by keeping the\nsame data points but exchanging the roles ofxand t. Figure 6.17 shows the data sets\nfor the forward and inverse problems, along with the results of ﬁtting two-layer neu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1068, "text": "ral networks having six hidden units and a single linear output unit by minimizing\na sum-of-squares error function. Least squares corresponds to maximum likelihood\nunder a Gaussian assumption. We see that this leads to a good model for the forward\nproblem but a very poor model for the highly non-Gaussian inverse problem.\n6.5.2 Conditional mixture distributions\nWe therefore seek a general framework for modelling conditional probability"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1069, "text": "distributions. This can be achieved by using a mixture model for p(t|x)in which\nFigure 6.17 On the left is the data\nset for a simple forward problem in\nwhich the red curve shows the result\nof ﬁtting a two-layer neural network\nby minimizing the sum-of-squares\nerror function. The corresponding\ninverse problem, shown on the right,\nis obtained by exchanging the roles\nof xand t. Here the same network,\nagain trained by minimizing the sum-\nof-squares error function, gives a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1070, "text": "of-squares error function, gives a\npoor ﬁt to the data due to the mul-\ntimodality of the data set.\n0 1\n0\n1\n0 1\n0\n1\n200 6. DEEP NEURAL NETWORKS\nFigure 6.18 The mixture density net-\nwork can represent general conditional\nprobability densities p(t|x)by consid-\nering a parametric mixture model for\nthe distribution of t whose parameters\nare determined by the outputs of a neu-\nral network that takes x as its input\nvector.\nxD …\nx1\n …\n\u0012K …\n\u00121\nt\np(t|x)\n\u0012"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1071, "text": "vector.\nxD …\nx1\n …\n\u0012K …\n\u00121\nt\np(t|x)\n\u0012\nboth the mixing coefﬁcients as well as the component densities are ﬂexible functions\nof the input vectorx, giving rise to amixture density network. For any given value of\nx, the mixture model provides a general formalism for modelling an arbitrary condi-\ntional density function p(t|x). Provided we consider a sufﬁciently ﬂexible network,\nwe then have a framework for approximating arbitrary conditional distributions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1072, "text": "Here we will develop the model explicitly for Gaussian components, so that\np(t|x) =\nK∑\nk=1\n\u0019k(x)N\n(\nt|\u0016k(x);\u001b2\nk(x)\n)\n: (6.38)\nThis is an example of a heteroscedastic model in which the noise variance on the\ndata is a function of the input vector x. Instead of Gaussians, we can use other dis-\ntributions for the components, such as Bernoulli distributions if the target variables\nare binary rather than continuous. We have also specialized to the case of isotropic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1073, "text": "covariances for the components, although the mixture density network can readily\nbe extended to allow for general covariance matrices by representing the covariances\nusing a Cholesky factorization (Williams, 1996). Even with isotropic components,\nthe conditional distribution p(t|x)does not assume factorization with respect to the\ncomponents of t (in contrast to the standard sum-of-squares regression model) as a\nconsequence of the mixture distribution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1074, "text": "consequence of the mixture distribution.\nWe now take the various parameters of the mixture model, namely the mix-\ning coefﬁcients \u0019k(x), the means \u0016k(x), and the variances \u001b2\nk(x), to be governed\nby the outputs of a neural network that takes x as its input. The structure of this\nmixture density network is illustrated in Figure 6.18. The mixture density network\nis closely related to the mixture-of-experts model (Jacobs et al., 1991). The prin-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1075, "text": "cipal difference is that a mixture of experts has independent parameters for each\ncomponent model in the mixture, whereas in a mixture density network, the same\nfunction is used to predict the parameters of all the component densities as well as\nthe mixing coefﬁcients, and so the nonlinear hidden units are shared amongst the\ninput-dependent functions.\nThe neural network in Figure 6.18 can, for example, be a two-layer network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1076, "text": "having sigmoidal (tanh) hidden units. If there are K components in the mixture\nmodel (6.38), and if t has L components, then the network will have K output-\n6.5. Mixture Density Networks 201\nunit pre-activations denoted by a\u0019\nk that determine the mixing coefﬁcients \u0019k(x), K\noutputs denoted by a\u001b\nk that determine the Gaussian standard deviations \u001bk(x), and\nK×Loutputs denoted bya\u0016\nkj that determine the components\u0016kj(x) of the Gaussian"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1077, "text": "means \u0016k(x). The total number of network outputs is given by (L+ 2)K, unlike\nthe usual Loutputs for a network that simply predicts the conditional means of the\ntarget variables.\nThe mixing coefﬁcients must satisfy the constraints\nK∑\nk=1\n\u0019k(x) = 1; 0 6 \u0019k(x) 6 1; (6.39)\nwhich can be achieved using a set of softmax outputs:\n\u0019k(x) = exp(a\u0019\nk)\n∑K\nl=1 exp(a\u0019\nl )\n: (6.40)\nSimilarly, the variances must satisfy \u001b2\nk(x) > 0 and so can be represented in terms"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1078, "text": "k(x) > 0 and so can be represented in terms\nof the exponentials of the corresponding network pre-activations using\n\u001bk(x) = exp(a\u001b\nk): (6.41)\nFinally, because the means \u0016k(x) have real components, they can be represented\ndirectly by the network outputs:\n\u0016kj(x) = a\u0016\nkj (6.42)\nin which the output-unit activation functions are given by the identity f(a) = a.\nThe learnable parameters of the mixture density network comprise the vector w"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1079, "text": "of weights and biases in the neural network, which can be set by maximum likelihood\nor equivalently by minimizing an error function deﬁned to be the negative logarithm\nof the likelihood. For independent data, this error function takes the form\nE(w) = −\nN∑\nn=1\nln\n{ K∑\nk=1\n\u0019k(xn;w)N\n(\ntn|\u0016k(xn;w);\u001b2\nk(xn;w)\n)\n}\n(6.43)\nwhere we have made the dependencies on w explicit.\n6.5.3 Gradient optimization\nTo minimize the error function, we need to calculate the derivatives of the error"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1080, "text": "E(w) with respect to the components of w. We will see later how to compute these\nderivatives automatically. It is instructive, however, to derive suitable expressions forChapter 8\nthe derivatives of the error with respect to the output-unit pre-activations explicitly as\nthis highlights the probabilistic interpretation of these quantities. Because the error\nfunction (6.43) is composed of a sum of terms, one for each training data point, we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1081, "text": "can consider the derivatives for a particular input vector xn with associated target\nvector tn. The derivatives of the total error E are obtained by summing over all\n202 6. DEEP NEURAL NETWORKS\ndata points, or the individual gradients for each data point can be used directly in\ngradient-based optimization algorithms.Chapter 7\nIt is convenient to introduce the following variables:\n\rnk = \rk(tn|xn) = \u0019kNnk\n∑K\nl=1 \u0019lNnl\n(6.44)\nwhere Nnk denotes N(tn|\u0016k(xn);\u001b2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1082, "text": "l=1 \u0019lNnl\n(6.44)\nwhere Nnk denotes N(tn|\u0016k(xn);\u001b2\nk(xn)). These quantities have a natural inter-\npretation as posterior probabilities for the components of the mixture in which the\nmixing coefﬁcients \u0019k(x) are viewed as x-dependent prior probabilities.Exercise 6.17\nThe derivatives of the error function with respect to the network output pre-\nactivations governing the mixing coefﬁcients are given byExercise 6.18\n@En\n@a\u0019\nk\n= \u0019k −\rnk: (6.45)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1083, "text": "@En\n@a\u0019\nk\n= \u0019k −\rnk: (6.45)\nSimilarly, the derivatives with respect to the output pre-activations controlling the\ncomponent means are given byExercise 6.19\n@En\n@a\u0016\nkl\n= \rnk\n{ \u0016kl −tnl\n\u001b2\nk\n}\n: (6.46)\nFinally, the derivatives with respect to the output pre-activations controlling the com-\nponent variances are given byExercise 6.20\n@En\n@a\u001b\nk\n= \rnk\n{\nL−∥tn −\u0016k∥2\n\u001b2\nk\n}\n: (6.47)\n6.5.4 Predictive distribution\nWe illustrate the use of a mixture density network by returning to the toy ex-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1084, "text": "ample of an inverse problem shown in Figure 6.17. Plots of the mixing coefﬁ-\ncients \u0019k(x), the means \u0016k(x), and the conditional density contours corresponding\nto p(t|x), are shown inFigure 6.19. The outputs of the neural network, and hence the\nparameters in the mixture model, are necessarily continuous single-valued functions\nof the input variables. However, we see fromFigure 6.19(c) that the model is able to\nproduce a conditional density that is unimodal for some values ofxand trimodal for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1085, "text": "other values by modulating the amplitudes of the mixing components \u0019k(x).\nOnce a mixture density network has been trained, it can predict the conditional\ndensity function of the target data for any given value of the input vector. This\nconditional density represents a complete description of the generator of the data, so\nfar as the problem of predicting the value of the output vector is concerned. From this\ndensity function, we can calculate more speciﬁc quantities that may be of interest in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1086, "text": "different applications. One of the simplest of these is the mean, corresponding to the\nconditional average of the target data, and is given by\nE[t|x] =\n∫\ntp(t|x) dt =\nK∑\nk=1\n\u0019k(x)\u0016k(x) (6.48)\n6.5. Mixture Density Networks 203\nFigure 6.19 (a) Plot of the mixing\ncoefﬁcients \u0019k(x) as a function of\nx for the three mixture components\nin a mixture density network trained\non the data shown in Figure 6.17.\nThe model has three Gaussian com-\nponents and uses a two-layer neu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1087, "text": "ponents and uses a two-layer neu-\nral network with ﬁve tanh sigmoidal\nunits in the hidden layer and nine\noutputs (corresponding to the three\nmeans and three variances of the\nGaussian components and the three\nmixing coefﬁcients). At both small\nand large values of x, where the\nconditional probability density of the\ntarget data is unimodal, only one\nof the Gaussian components has\na high value for its prior probabil-\nity, whereas at intermediate values\nof x, where the conditional density"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1088, "text": "of x, where the conditional density\nis trimodal, the three mixing coefﬁ-\ncients have comparable values. (b)\nPlots of the means \u0016k(x) using the\nsame colour coding as for the mix-\ning coefﬁcients. (c) Plot of the con-\ntours of the corresponding condi-\ntional probability density of the tar-\nget data for the same mixture den-\nsity network. (d) Plot of the ap-\nproximate conditional mode, shown\nby the red points, of the conditional\ndensity.\n(a)\n0 1\n0\n1\n(b)\n0 1\n0\n1\n(c)\n0 1\n0\n1\n(d)\n0 1\n0\n1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1089, "text": "(a)\n0 1\n0\n1\n(b)\n0 1\n0\n1\n(c)\n0 1\n0\n1\n(d)\n0 1\n0\n1\nwhere we have used (6.38). Because a standard network trained by least squares\napproximates the conditional mean, we see that a mixture density network can re-\nproduce the conventional least-squares result as a special case. Of course, as we have\nalready noted, for a multimodal distribution the conditional mean is of limited value.\nWe can similarly evaluate the variance of the density function about the condi-\ntional average, to giveExercise 6.21"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1090, "text": "tional average, to giveExercise 6.21\ns2(x) = E\n[\n∥t−E[t|x]∥2 |x\n]\n(6.49)\n=\nK∑\nk=1\n\u0019k(x)\n\n\n\u001b2\nk(x) +\n\u0016k(x) −\nK∑\nl=1\n\u0019l(x)\u0016l(x)\n\n2\n\n (6.50)\nwhere we have used (6.38) and (6.48). This is more general than the corresponding\nleast-squares result because the variance is a function of x.\nWe have seen that for multimodal distributions, the conditional mean can give\na poor representation of the data. For instance, in controlling the simple robot arm"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1091, "text": "shown in Figure 6.16, we need to pick one of the two possible joint angle settings\n204 6. DEEP NEURAL NETWORKS\nto achieve the desired end-effector location, but the average of the two solutions is\nnot itself a solution. In such cases, the conditional mode may be of more value.\nBecause the conditional mode for the mixture density network does not have a sim-\nple analytical solution, a numerical iteration is required. A simple alternative is to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1092, "text": "take the mean of the most probable component (i.e., the one with the largest mixing\ncoefﬁcient) at each value of x. This is shown for the toy data set in Figure 6.19(d).\nExercises\n6.1 (???) Use the result (2.126) to derive an expression for the surface area SD and\nthe volume VD of a hypersphere of unit radius in D dimensions. To do this, con-\nsider the following result, which is obtained by transforming from Cartesian to polar\ncoordinates:\nD∏\ni=1\n∫∞\n−∞\ne−x2\ni dxi = SD\n∫∞\n0\ne−r2\nrD−1 dr: (6.51)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1093, "text": "∫∞\n−∞\ne−x2\ni dxi = SD\n∫∞\n0\ne−r2\nrD−1 dr: (6.51)\nUsing the gamma function, deﬁned by\nΓ(x) =\n∫∞\n0\ntx−1 e−t dt (6.52)\ntogether with (2.126), evaluate both sides of this equation, and hence show that\nSD = 2\u0019D=2\nΓ(D=2) : (6.53)\nNext, by integrating with respect to the radius from 0 to 1, show that the volume of\nthe unit hypersphere in Ddimensions is given by\nVD = SD\nD : (6.54)\nFinally, use the results Γ(1) = 1 and Γ(3=2) = √\u0019=2 to show that (6.53) and (6.54)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1094, "text": "reduce to the usual expressions for D= 2 and D= 3.\n6.2 (???) Consider a hypersphere of radius ain D dimensions together with the con-\ncentric hypercube of side 2a, so that the hypersphere touches the hypercube at the\ncentres of each of its sides. By using the results of Exercise 6.1, show that the ratio\nof the volume of the hypersphere to the volume of the cube is given by\nvolume of hypersphere\nvolume of cube = \u0019D=2\nD2D−1Γ(D=2) : (6.55)\nNow make use of Stirling’s formula in the form"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1095, "text": "Now make use of Stirling’s formula in the form\nΓ(x+ 1) ≃(2\u0019)1=2e−xxx+1=2; (6.56)\nwhich is valid for x ≫ 1, to show that, as D →∞, the ratio (6.55) goes to zero.\nShow also that the distance from the centre of the hypercube to one of the corners\nExercises 205\ndivided by the perpendicular distance to one of the sides is\n√\nD, which therefore goes\nto ∞ as D→∞. From these results, we see that, in a space of high dimensionality,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1096, "text": "most of the volume of a cube is concentrated in the large number of corners, which\nthemselves become very long ‘spikes’!\n6.3 (???) In this exercise, we explore the behaviour of the Gaussian distribution in high-\ndimensional spaces. Consider a Gaussian distribution in Ddimensions given by\np(x) = 1\n(2\u0019\u001b2)D=2 exp\n(\n−∥x∥2\n2\u001b2\n)\n: (6.57)\nWe wish to ﬁnd the density as a function of the radius in polar coordinates in which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1097, "text": "the direction variables have been integrated out. To do this, show that the integral of\nthe probability density over a thin shell of radius rand thickness \u000f, where \u000f≪ 1, is\ngiven by p(r)\u000fwhere\np(r) = SDrD−1\n(2\u0019\u001b2)D=2 exp\n(\n−r2\n2\u001b2\n)\n(6.58)\nwhere SD is the surface area of a unit hypersphere in Ddimensions. Show that the\nfunction p(r) has a single stationary point located, for large D, at ˆr ≃\n√\nD\u001b. By\nconsidering p(ˆr+ \u000f) where \u000f≪ ˆr, show that for large D,\np(ˆr+ \u000f) = p(ˆr) exp\n(\n−3\u000f2\n2\u001b2\n)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1098, "text": "p(ˆr+ \u000f) = p(ˆr) exp\n(\n−3\u000f2\n2\u001b2\n)\n; (6.59)\nwhich shows that ˆris a maximum of the radial probability density and also thatp(r)\ndecays exponentially away from its maximum at ˆr with length scale \u001b. We have\nalready seen that \u001b ≪ ˆr for large D, and so we see that most of the probability\nmass is concentrated in a thin shell at large radius. Finally, show that the probability\ndensity p(x) is larger at the origin than at the radius ˆr by a factor of exp(D=2)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1099, "text": "We therefore see that most of the probability mass in a high-dimensional Gaussian\ndistribution is located at a different radius from the region of high probability density.\n6.4 (??) Consider a two-layer network function of the form (6.11) in which the hidden-\nunit nonlinear activation functionsh(-)are given by logistic sigmoid functions of the\nform\n\u001b(a) = {1 + exp(−a)}−1 : (6.60)\nShow that there exists an equivalent network, which computes exactly the same func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1100, "text": "tion, but with hidden-unit activation functions given bytanh(a) where the tanh func-\ntion is deﬁned by (6.14). Hint: ﬁrst ﬁnd the relation between \u001b(a) and tanh(a), and\nthen show that the parameters of the two networks differ by linear transformations.\n6.5 (??) The swish activation function (Ramachandran, Zoph, and Le, 2017) is deﬁned\nby\nh(x) = x\u001b(\fx) (6.61)\n206 6. DEEP NEURAL NETWORKS\nwhere \u001b(x) is the logistic-sigmoid activation function deﬁned by (6.13). When used"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1101, "text": "in a neural network, \fcan be treated as a learnable parameter. Either sketch or plot\nusing software graphs of the swish activation function as well as its ﬁrst derivative\nfor \f = 0:1, \f = 1:0, and \f = 10. Show that when \f →∞, the swish function\nbecomes the ReLU function.\n6.6 (?) We saw in (5.72) that the derivative of the logistic-sigmoid activation function\ncan be expressed in terms of the function value itself. Derive the corresponding\nresult for the tanh activation function deﬁned by (6.14)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1102, "text": "6.7 (??) Show that the softplus activation function \u0010(a) given by (6.16) satisﬁes the\nproperties:\n\u0010(a) −\u0010(−a) = a (6.62)\nln \u001b(a) = −\u0010(−a) (6.63)\nd\u0010(a)\nda = \u001b(a) (6.64)\n\u0010−1 (a) = ln (exp(a) −1) (6.65)\nwhere \u001b(a) is the logistic-sigmoid activation function given by (6.13).\n6.8 (?) Show that minimization of the error function (6.25) with respect to the variance\n\u001b2 gives the result (6.27).\n6.9 (?) Show that maximizing the likelihood function under the conditional distribu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1103, "text": "tion (6.28) for a multioutput neural network is equivalent to minimizing the sum-of-\nsquares error function (6.29). Also, show that the noise variance that minimizes this\nerror function is given by (6.30).\n6.10 (??) Consider a regression problem involving multiple target variables in which it is\nassumed that the distribution of the targets, conditioned on the input vector x, is a\nGaussian of the form\np(t|x;w) = N(t|y(x;w);\u0006) (6.66)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1104, "text": "p(t|x;w) = N(t|y(x;w);\u0006) (6.66)\nwhere y(x;w) is the output of a neural network with input vector x and weight\nvector w, and \u0006 is the covariance of the assumed Gaussian noise on the targets.\nGiven a set of independent observations of x and t, write down the error function\nthat must be minimized to ﬁnd the maximum likelihood solution forw, if we assume\nthat \u0006 is ﬁxed and known. Now assume that\u0006 is also to be determined from the data,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1105, "text": "and write down an expression for the maximum likelihood solution for \u0006. Note that\nthe optimizations of w and \u0006 are now coupled, in contrast to the case of independent\ntarget variables discussed in Section 6.4.1.\n6.11 (??) Consider a binary classiﬁcation problem in which the target values are t ∈\n{0;1}, with a network output y(x;w) that represents p(t = 1|x), and suppose that\nthere is a probability\u000fthat the class label on a training data point has been incorrectly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1106, "text": "set. Assuming i.i.d. data, write down the error function corresponding to the negative\nlog likelihood. Verify that the error function (6.33) is obtained when\u000f= 0. Note that\nExercises 207\nthis error function makes the model robust to incorrectly labelled data, in contrast to\nthe usual cross-entropy error function.\n6.12 (??) The error function (6.33) for binary classiﬁcation problems was derived for a\nnetwork having a logistic-sigmoid output activation function, so that0 6 y(x;w) 6"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1107, "text": "1, and data having target values t∈{0;1}. Derive the corresponding error function\nif we consider a network having an output −1 6 y(x;w) 6 1 and target values\nt= 1 for class C1 and t= −1 for class C2. What would be the appropriate choice of\noutput-unit activation function?\n6.13 (?) Show that maximizing the likelihood for a multi-class neural network model\nin which the network outputs have the interpretation yk(x;w) = p(tk = 1|x)is\nequivalent to minimizing the cross-entropy error function (6.36)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1108, "text": "6.14 (?) Show that the derivative of the error function (6.33) with respect to the pre-\nactivation ak for an output unit having a logistic-sigmoid activation function yk =\n\u001b(ak), where \u001b(a) is given by (6.13), satisﬁes (6.31).\n6.15 (?) Show that the derivative of the error function (6.36) with respect to the pre-\nactivation ak for output units having a softmax activation function (6.37) satisﬁes\n(6.31).\n6.16 (??) Write down a pair of equations that express the Cartesian coordinates (x1;x2)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1109, "text": "for the robot arm shown in Figure 6.16 in terms of the joint angles \u00121 and \u00122 and\nthe lengths L1 and L2 of the links. Assume the origin of the coordinate system is\ngiven by the attachment point of the lower arm. These equations deﬁne the forward\nkinematics of the robot arm.\n6.17 (??) Show that the variable \rnk deﬁned by (6.44) can be viewed as the posterior\nprobabilities p(k|t)for the components of the mixture distribution (6.38) in which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1110, "text": "the mixing coefﬁcients \u0019k(x) are viewed as x-dependent prior probabilities p(k).\n6.18 (??) Derive the result (6.45) for the derivative of the error function with respect to\nthe network output pre-activations controlling the mixing coefﬁcients in the mixture\ndensity network.\n6.19 (??) Derive the result (6.46) for the derivative of the error function with respect to\nthe network output pre-activations controlling the component means in the mixture\ndensity network."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1111, "text": "density network.\n6.20 (??) Derive the result (6.47) for the derivative of the error function with respect to the\nnetwork output pre-activations controlling the component variances in the mixture\ndensity network.\n6.21 (???) Verify the results (6.48) and (6.50) for the conditional mean and variance of\nthe mixture density network model.\n7\nGradient\nDescent\nIn the previous chapter we saw that neural networks are a very broad and ﬂexible"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1112, "text": "class of functions and are able in principle to approximate any desired function to\narbitrarily high accuracy given a sufﬁciently large number of hidden units. More-\nover, we saw that deep neural networks can encode inductive biases corresponding\nto hierarchical representations, which prove valuable in a wide range of practical\napplications. We now turn to the task of ﬁnding a suitable setting for the network\nparameters (weights and biases), based on a set of training data."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1113, "text": "As with the regression and classiﬁcation models discussed in earlier chapters,\nwe choose the model parameters by optimizing an error function. We have seen how\nto deﬁne a suitable error function for a particular application by using maximum\nlikelihood. Although in principle the error function could be minimized numericallySection 6.4\nthrough a series of direct error function evaluations, this turns out to be very inefﬁ-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1114, "text": "cient. Instead, we turn to another core concept that is used in deep learning, which\n209© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_7"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1115, "text": "210 7. GRADIENT DESCENT\nis that optimizing the error function can be done much more efﬁciently by making\nuse of gradient information, in other words by evaluating the derivatives of the error\nfunction with respect to the network parameters. This is why we took care to en-\nsure that the function represented by the neural network is differentiable by design.\nLikewise, the error function itself also needs to be differentiable."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1116, "text": "The required derivatives of the error function with respect to each of the pa-\nrameters in the network can be evaluated efﬁciently using a technique called back-\npropagation, which involves successive computations that ﬂow backwards throughChapter 8\nthe network in a way that is analogous to the forward ﬂow of function computations\nduring the evaluation of the network outputs.\nAlthough the likelihood is used to deﬁne an error function, the goal when op-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1117, "text": "timizing the error function in a neural network is to achieve good generalization on\ntest data. In classical statistics, maximum likelihood is used to ﬁt a parametric model\nto a ﬁnite data set, in which the number of data points typically far exceeds the num-\nber of parameters in the model. The optimal solution has the maximum value of the\nlikelihood function, and the values found for the ﬁtted parameters are of direct inter-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1118, "text": "est. By contrast, modern deep learning works with very rich models containing huge\nnumbers of learnable parameters, and the goal is never simply exact optimization.Section 9.3.2\nInstead, the properties and behaviour of the learning algorithm itself, along with var-\nious methods for regularization, are important in determining how well the solutionChapter 9\ngeneralizes to new data.\n7.1.\nError Surfaces\nOur goal during training is to ﬁnd values for the weights and biases in the neural"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1119, "text": "network that will allow it to make effective predictions. For convenience we will\ngroup these parameters into a single vector w, and we will optimize w by using a\nchosen error function E(w). At this point, it is useful to have a geometrical picture\nof the error function, which we can view as a surface sitting over ‘weight space’, as\nshown in Figure 7.1.\nFirst note that if we make a small step in weight space from w to w + \u000ew then\nthe change in the error function is given by\n\u000eE ≃\u000ewT∇E(w) (7.1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1120, "text": "\u000eE ≃\u000ewT∇E(w) (7.1)\nwhere the vector ∇E(w) points in the direction of the greatest rate of increase of\nthe error function. Provided the error E(w) is a smooth, continuous function of w,\nits smallest value will occur at a point in weight space such that the gradient of the\nerror function vanishes, so that\n∇E(w) = 0 (7.2)\nas otherwise we could make a small step in the direction of −∇E(w) and thereby\nfurther reduce the error. Points at which the gradient vanishes are called stationary"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1121, "text": "points and may be further classiﬁed into minima, maxima, and saddle points.Section 7.1.1\n7.1. Error Surfaces 211\nFigure 7.1 Geometrical view of the error function E(w) as a\nsurface sitting over weight space. Point wA is a\nlocal minimum and wB is the global minimum, so\nthat E(wA) >E (wB). At any point wC, the local\ngradient of the error surface is given by the vector\n∇E.\nw1\nw2\nE(w)\nwA wB wC\nrE\nWe will aim to ﬁnd a vector w such that E(w) takes its smallest value. How-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1122, "text": "ever, the error function typically has a highly nonlinear dependence on the weights\nand bias parameters, and so there will be many points in weight space at which the\ngradient vanishes (or is numerically very small). Indeed, for any point w that is a\nlocal minimum, there will generally be other points in weight space that are equiva-\nlent minima. For instance, in a two-layer network of the kind shown in Figure 6.9,\nwith M hidden units, each point in weight space is a member of a family of M! 2M"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1123, "text": "equivalent points.Section 6.2.4\nFurthermore, there may be multiple non-equivalent stationary points and in par-\nticular multiple non-equivalent minima. A minimum that corresponds to the smallest\nvalue of the error function across the whole of w-space is said to be a global min-\nimum. Any other minima corresponding to higher values of the error function are\nsaid to be local minima. The error surfaces for deep neural networks can be very"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1124, "text": "complex, and it was thought that gradient-based methods might become trapped in\npoor local minima. In practice, this seems not to be the case, and large networks can\nreach solutions with similar performance under a variety of initial conditions.Section 9.3.2\n7.1.1 Local quadratic approximation\nInsight into the optimization problem and into the various techniques for solving\nit can be obtained by considering a local quadratic approximation to the error func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1125, "text": "tion. The Taylor expansion of E(w) around some point ˆw in weight space is given\nby\nE(w) ≃E( ˆw) + (w−ˆw)Tb + 1\n2(w −ˆw)TH(w −ˆw) (7.3)\nwhere cubic and higher terms have been omitted. Hereb is deﬁned to be the gradient\nof Eevaluated at ˆw\nb ≡∇E |w=bw : (7.4)\nThe Hessian is deﬁned to be the corresponding matrix of second derivatives\nH( ˆw) = ∇∇E(w)|w=bw : (7.5)\n212 7. GRADIENT DESCENT\nIf there is a total of W weights and biases in the network, then w and b have length"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1126, "text": "W and H has dimensionality W ×W. From (7.3), the corresponding local approx-\nimation to the gradient is given by\n∇E(w) = b + H(w −ˆw): (7.6)\nFor points w that are sufﬁciently close to ˆw, these expressions will give reasonable\napproximations for the error and its gradient.\nConsider the particular case of a local quadratic approximation around a point\nw? that is a minimum of the error function. In this case there is no linear term,\nbecause ∇E = 0 at w?, and (7.3) becomes\nE(w) = E(w?) + 1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1127, "text": "E(w) = E(w?) + 1\n2(w −w?)TH(w −w?) (7.7)\nwhere the Hessian H is evaluated atw?. To interpret this geometrically, consider the\neigenvalue equation for the Hessian matrix:\nHui = \u0015iui (7.8)\nwhere the eigenvectors ui form a complete orthonormal set so thatAppendix A\nuT\ni uj = \u000eij: (7.9)\nWe now expand (w −w?) as a linear combination of the eigenvectors in the form\nw −w? =\n∑\ni\n\u000biui: (7.10)\nThis can be regarded as a transformation of the coordinate system in which the origin"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1128, "text": "is translated to the point w? and the axes are rotated to align with the eigenvectors\nthrough the orthogonal matrix whose columns are {u1;:::; uW}. By substitutingAppendix A\n(7.10) into (7.7) and using (7.8) and (7.9), the error function can be written in the\nformExercise 7.1\nE(w) = E(w?) + 1\n2\n∑\ni\n\u0015i\u000b2\ni: (7.11)\nSuppose we set all \u000bi = 0 for i ̸=j and then vary \u000bj, corresponding to moving\nw away from w? in the direction of uj. We see from (7.11) that the error function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1129, "text": "will increase if the corresponding eigenvalue \u0015j is positive and will decrease if it is\nnegative. If all eigenvalues are positive then w? corresponds to a local minimum of\nthe error function, whereas if they are all negative then w? corresponds to a local\nmaximum. If we have a mix of positive and negative eigenvalues thenw? represents\na saddle point.\nA matrix H is said to be positive deﬁnite if, and only if,\nvTHv >0; for all v: (7.12)\n7.2. Gradient Descent Optimization 213"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1130, "text": "7.2. Gradient Descent Optimization 213\nFigure 7.2 In the neighbourhood of a mini-\nmum w?, the error function can\nbe approximated by a quadratic.\nContours of constant error are\nthen ellipses whose axes are\naligned with the eigenvectors\nui of the Hessian matrix, with\nlengths that are inversely pro-\nportional to the square roots of\nthe corresponding eigenvectors\n\u0015i.\nw1\nw2\n\u0015−1=2\n1\n\u0015−1=2\n2\nu1\nw?\nu2\nBecause the eigenvectors {ui}form a complete set, an arbitrary vector v can be\nwritten in the form\nv ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1131, "text": "written in the form\nv =\n∑\ni\nciui: (7.13)\nFrom (7.8) and (7.9), we then have\nvTHv =\n∑\ni\nc2\ni\u0015i (7.14)\nand so H will be positive deﬁnite if, and only if, all its eigenvalues are positive.Exercise 7.2\nThus, a necessary and sufﬁcient condition for w? to be a local minimum is that the\ngradient of the error function should vanish at w? and the Hessian matrix evaluated\nat w? should be positive deﬁnite. In the new coordinate system, whose basis vectorsExercise 7.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1132, "text": "are given by the eigenvectors {ui}, the contours of constant E(w) are axis-aligned\nellipses centred on the origin, as illustrated in Figure 7.2.Exercise 7.6\n7.2. Gradient Descent Optimization\nThere is little hope of ﬁnding an analytical solution to the equation ∇E(w) = 0 for\nan error function as complex as one deﬁned by a neural network, and so we resort to\niterative numerical procedures. The optimization of continuous nonlinear functions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1133, "text": "is a widely studied problem, and there exists an extensive literature on how to solve it\nefﬁciently. Most techniques involve choosing some initial value w(0) for the weight\nvector and then moving through weight space in a succession of steps of the form\nw(\u001c) = w(\u001c−1) + ∆w (\u001c−1) (7.15)\nwhere \u001c labels the iteration step. Different algorithms involve different choices for\nthe weight vector update ∆w (\u001c).\nBecause of the complex shape of the error surface for all but the simplest neu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1134, "text": "ral networks, the solution found will depend, among other things, on the particular\nchoice of initial parameter values w(0). To ﬁnd a sufﬁciently good solution, it may\n214 7. GRADIENT DESCENT\nbe necessary to run a gradient-based algorithm multiple times, each time using a dif-\nferent randomly chosen starting point, and comparing the resulting performance on\nan independent validation set.\n7.2.1 Use of gradient information"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1135, "text": "7.2.1 Use of gradient information\nThe gradient of an error function for a deep neural network can be evaluated\nefﬁciently using the technique of error backpropagation, and applying this gradientChapter 8\ninformation can lead to signiﬁcant improvements in the speed of network training.\nWe can see why this is so, as follows.\nIn the quadratic approximation to the error function given by (7.3), the error\nsurface is speciﬁed by the quantities b and H, which contain a total of W(W +"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1136, "text": "3)=2 independent elements (because the matrix H is symmetric), where W is theExercise 7.7\ndimensionality of w (i.e., the total number of learnable parameters in the network).\nThe location of the minimum of this quadratic approximation therefore depends on\nO(W2) parameters, and we should not expect to be able to locate the minimum until\nwe have gathered O(W2) independent pieces of information. If we do not make\nuse of gradient information, we would expect to have to perform O(W2) function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1137, "text": "evaluations, each of which would require O(W) steps. Thus, the computational\neffort needed to ﬁnd the minimum using such an approach would be O(W3).\nNow compare this with an algorithm that makes use of the gradient information.\nBecause ∇E is a vector of length W, each evaluation of ∇E brings W pieces of\ninformation, and so we might hope to ﬁnd the minimum of the function in O(W)\ngradient evaluations. As we shall see, by using error backpropagation, each suchChapter 8"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1138, "text": "evaluation takes only O(W) steps and so the minimum can now be found inO(W2)\nsteps. Although the quadratic approximation only holds in the neighbourhood of\na minimum, the efﬁciency gains are generic. For this reason, the use of gradient\ninformation forms the basis of all practical algorithms for training neural networks.\n7.2.2 Batch gradient descent\nThe simplest approach to using gradient information is to choose the weight up-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1139, "text": "date in (7.15) such that there is a small step in the direction of the negative gradient,\nso that\nw(\u001c) = w(\u001c−1) −\u0011∇E(w(\u001c−1) ) (7.16)\nwhere the parameter\u0011 >0 is known as thelearning rate. After each such update, the\ngradient is re-evaluated for the new weight vector w(\u001c+1) and the process repeated.\nAt each step, the weight vector is moved in the direction of the greatest rate of\ndecrease of the error function, and so this approach is known as gradient descent or"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1140, "text": "steepest descent. Note that the error function is deﬁned with respect to a training set,\nand so to evaluate ∇E, each step requires that the entire training set be processed.\nTechniques that use the whole data set at once are called batch methods.\n7.2.3 Stochastic gradient descent\nDeep learning methods beneﬁt greatly from very large data sets. However, batch\nmethods can become extremely inefﬁcient if there are many data points in the train-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1141, "text": "ing set because each error function or gradient evaluation requires the entire data set\n7.2. Gradient Descent Optimization 215\nAlgorithm 7.1: Stochastic gradient descent\nInput: Training set of data points indexed by n∈{1;:::;N }\nError function per data point En(w)\nLearning rate parameter \u0011\nInitial weight vector w\nOutput: Final weight vector w\nn← 1\nrepeat\nw ← w −\u0011∇En(w) // update weight vector\nn← n+ 1(mod N) // iterate over data\nuntil convergence\nreturn w"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1142, "text": "until convergence\nreturn w\nto be processed. To ﬁnd a more efﬁcient approach, note that error functions based on\nmaximum likelihood for a set of independent observations comprise a sum of terms,\none for each data point:\nE(w) =\nN∑\nn=1\nEn(w): (7.17)\nThe most widely used training algorithms for large data sets are based on a sequential\nversion of gradient descent known as stochastic gradient descent (Bottou, 2010), or\nSGD, which updates the weight vector based on one data point at a time, so that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1143, "text": "w(\u001c) = w(\u001c−1) −\u0011∇En(w(\u001c−1) ): (7.18)\nThis update is repeated by cycling through the data. A complete pass through the\nwhole training set is known as a training epoch. This technique is also known as\nonline gradient descent, especially if the data arises from a continuous stream of\nnew data points. Stochastic gradient descent is summarized in Algorithm 7.1.\nA further advantage of stochastic gradient descent, compared to batch gradient"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1144, "text": "descent, is that it handles redundancy in the data much more efﬁciently. To see this,\nconsider an extreme example in which we take a data set and double its size by\nduplicating every data point. Note that this simply multiplies the error function by\na factor of 2 and so is equivalent to using the original error function, if the value of\nthe learning rate is adjusted to compensate. Batch methods will require double the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1145, "text": "computational effort to evaluate the batch error function gradient, whereas stochastic\ngradient descent will be unaffected. Another property of stochastic gradient descent\nis the possibility of escaping from local minima, since a stationary point with respect\nto the error function for the whole data set will generally not be a stationary point\nfor each data point individually.\n216 7. GRADIENT DESCENT\n7.2.4 Mini-batches"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1146, "text": "216 7. GRADIENT DESCENT\n7.2.4 Mini-batches\nA downside of stochastic gradient descent is that the gradient of the error func-\ntion computed from a single data point provides a very noisy estimate of the gradient\nof the error function computed on the full data set. We can consider an interme-\ndiate approach in which a small subset of data points, called a mini-batch, is used\nto evaluate the gradient at each iteration. In determining the optimum size for the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1147, "text": "mini-batch, note that the error in computing the mean from N samples is given by\n\u001b=\n√\nN where \u001bis the standard deviation of the distribution generating the data. ThisExercise 7.8\nindicates that there are diminishing returns in estimating the true gradient from in-\ncreasing the batch size. If we increase the size of the mini-batch by a factor of 100\nthen the error only reduces by a factor of 10. Another consideration in choosing the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1148, "text": "mini-batch size is the desire to make efﬁcient use of the hardware architecture on\nwhich the code is running. For example, on some hardware platforms, mini-batch\nsizes that are powers of 2 (for example, 64, 128, 256, . . . ) work well.\nOne important consideration when using mini-batches is that the constituent data\npoints should be chosen randomly from the data set, since in raw data sets there\nmay be correlations between successive data points arising from the way the data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1149, "text": "was collected (for example, if the data points have been ordered alphabetically or\nby date). This is often handled by randomly shufﬂing the entire data set and then\nsubsequently drawing mini-batches as successive blocks of data. The data set can\nalso be reshufﬂed between iterations through the data set, so that each mini-batch is\nunlikely to have been used before, which can help escape local minima. The variant\nof stochastic gradient descent with mini-batches is summarized in Algorithm 7.2."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1150, "text": "Note that the learning algorithm is often still called ‘stochastic gradient descent’\neven when mini-batches are used.\n7.2.5 Parameter initialization\nIterative algorithms such as gradient descent require that we choose some ini-\ntial setting for the parameters being learned. The speciﬁc initialization can have a\nsigniﬁcant effect on how long it takes to reach a solution and on the generalization\nperformance of the resulting trained network. Unfortunately, there is relatively little"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1151, "text": "theory to guide the initialization strategy.\nOne key consideration, however, issymmetry breaking. Consider a set of hidden\nunits or output units that take the same inputs. If the parameters were all initialized\nwith the same value, for example if they were all set to zero, the parameters of these\nunits would all be updated in unison and the units would each compute the same\nfunction and hence be redundant. This problem can be addressed by initializing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1152, "text": "parameters randomly from some distribution to break symmetry. If computational\nresources permit, the network might be trained multiple times starting from different\nrandom initializations and the results compared on held-out data.\nThe distribution used to initialize the weights is typically either a uniform distri-\nbution in the range[−\u000f;\u000f] or a zero-mean Gaussian of the formN(0;\u000f2). The choice\nof the value of \u000fis important, and various heuristics to select it have been proposed."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1153, "text": "One widely used approach is called He initialization (He et al., 2015b). Consider a\n7.2. Gradient Descent Optimization 217\nAlgorithm 7.2: Mini-batch stochastic gradient descent\nInput: Training set of data points indexed by n∈{1;:::;N }\nBatch size B\nError function per mini-batch En:n+B−1 (w)\nLearning rate parameter \u0011\nInitial weight vector w\nOutput: Final weight vector w\nn← 1\nrepeat\nw ← w −\u0011∇En:n+B−1 (w) // weight vector update\nn← n+ B\nif n>N then\nshufﬂe data\nn← 1\nend if\nuntil convergence"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1154, "text": "shufﬂe data\nn← 1\nend if\nuntil convergence\nreturn w\nnetwork in which layer levaluates the following transformations\na(l)\ni =\nM∑\nj=1\nwijz(l−1)\nj (7.19)\nz(l)\ni = ReLU(a(l)\ni ) (7.20)\nwhere M is the number of units that send connections to uniti, and the ReLU activa-\ntion function is given by (6.17). Suppose we initialize the weights using a Gaussian\nN(0;\u000f2), and suppose that the outputsz(l−1)\nj of the units in layer l−1 have variance\n\u00152. Then we can easily show thatExercise 7.9\nE[a(l)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1155, "text": "E[a(l)\ni ] = 0 (7.21)\nvar[z(l)\nj ] = M\n2 \u000f2\u00152 (7.22)\nwhere the factor of 1=2 arises from the ReLU activation function. Ideally we want\nto ensure that the variance of the pre-activations neither decays to zero nor grows\nsigniﬁcantly as we propagate from one layer to the next. If we therefore require that\nthe units at layer lalso have variance \u00152 then we arrive at the following choice for\nthe standard deviation of the Gaussian used to initialize the weights that feed into a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1156, "text": "218 7. GRADIENT DESCENT\nFigure 7.3 Schematic illustration of ﬁxed-step gradient\ndescent for an error function that has substantially differ-\nent curvatures along different directions. The error sur-\nface E has the form of a long valley, as depicted by the\nellipses. Note that, for most points in weight space, the lo-\ncal negative gradient vector −∇E does not point towards\nthe minimum of the error function. Successive steps of\ngradient descent can therefore oscillate across the valley,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1157, "text": "leading to very slow progress along the valley towards the\nminimum. The vectors u1 and u2 are the eigenvectors of\nthe Hessian matrix.\nu1\nu2\nunit with M inputs:\n\u000f=\n√\n2\nM: (7.23)\nIt is also possible to treat the scale \u000fof the initialization distribution as a hyper-\nparameter and to explore different values across multiple training runs. The bias pa-\nrameters are typically set to small positive values to ensure that most pre-activations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1158, "text": "are initially active during learning. This is particularly helpful with ReLU units,\nwhere we want the pre-activations to be positive so that there is a non-zero gradient\nto drive learning.\nAnother important class of techniques for initializing the parameters of a neural\nnetwork is by using the values that result from training the network on a different\ntask or by exploiting various forms of unsupervised training. These techniques fall"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1159, "text": "into the broad class of transfer learning techniques.Section 6.3.4\n7.3.\nConvergence\nWhen applying gradient descent in practice, we need to choose a value for the learn-\ning rate parameter \u0011. Consider the simple error surface depicted in Figure 7.3 for a\nhypothetical two-dimensional weight space in which the curvature of E varies sig-\nniﬁcantly with direction, creating a ‘valley’. At most points on the error surface, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1160, "text": "local gradient vector for batch gradient descent, which is perpendicular to the local\ncontour, does not point directly towards the minimum. Intuitively we might expect\nthat increasing the value of \u0011 should lead to bigger steps through weight space and\nhence faster convergence. However, the successive steps oscillate back and forth\nacross the valley, and if we increase \u0011too much, those oscillations will become di-\nvergent. Because \u0011 must be kept sufﬁciently small to avoid divergent oscillations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1161, "text": "across the valley, progress along the valley is very slow. Gradient descent then takes\nmany small steps to reach the minimum and is a very inefﬁcient procedure.\nWe can gain deeper insight into the nature of this problem by considering the\nquadratic approximation to the error function in the neighbourhood of the minimum.Section 7.1.1\nFrom (7.7), (7.8), and (7.10), the gradient of the error function in this approximation\n7.3. Convergence 219\ncan be written as\n∇E =\n∑\ni\n\u000bi\u0015iui: (7.24)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1162, "text": "can be written as\n∇E =\n∑\ni\n\u000bi\u0015iui: (7.24)\nAgain using (7.10) we can express the change in the weight vector in terms of corre-\nsponding changes in the coefﬁcients {\u000bi}:\n∆w =\n∑\ni\n∆\u000biui: (7.25)\nCombining (7.24) with (7.25) and the gradient descent formula (7.16) and using the\northonormality relation (7.9) for the eigenvectors of the Hessian, we obtain the fol-\nlowing expression for the change in\u000biat each step of the gradient descent algorithm:\n∆\u000bi = −\u0011\u0015i\u000bi (7.26)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1163, "text": "∆\u000bi = −\u0011\u0015i\u000bi (7.26)\nfrom which it follows thatExercise 7.10\n\u000bnew\ni = (1 −\u0011\u0015i)\u000bold\ni (7.27)\nwhere ‘old’ and ‘new’ denote values before and after a weight update. Using the\northonormality relation (7.9) for the eigenvectors together with (7.10), we have\nuT\ni (w −w?) = \u000bi (7.28)\nand so \u000bi can be interpreted as the distance to the minimum along the direction ui.\nFrom (7.27) we see that these distances evolve independently such that, at each step,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1164, "text": "the distance along the direction ofui is multiplied by a factor(1−\u0011\u0015i). After a total\nof T steps we have\n\u000b(T)\ni = (1 −\u0011\u0015i)T\u000b(0)\ni : (7.29)\nIt follows that, provided |1−\u0011\u0015i|< 1, the limit T →∞ leads to \u000bi = 0, which\nfrom (7.28) shows that w = w? and so the weight vector has reached the minimum\nof the error.\nNote that (7.29) demonstrates that gradient descent leads to linear convergence\nin the neighbourhood of a minimum. Also, convergence to the stationary point re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1165, "text": "quires that all the \u0015i be positive, which in turn implies that the stationary point is\nindeed a minimum. By making \u0011 larger we can make the factor (1 −\u0011\u0015i) smaller\nand hence improve the speed of convergence. There is a limit to how large \u0011 can\nbe made, however. We can permit (1 −\u0011\u0015i) to go negative (which gives oscillating\nvalues of \u000bi), but we must ensure that |1−\u0011\u0015i|< 1 otherwise the \u000bi values will\ndiverge. This limits the value of \u0011 to \u0011 <2=\u0015max where \u0015max is the largest of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1166, "text": "eigenvalues. The rate of convergence, however, is dominated by the smallest eigen-\nvalue, so with \u0011set to its largest permitted value, the convergence along the direction\ncorresponding to the smallest eigenvalue (the long axis of the ellipse in Figure 7.3)\nwill be governed by (\n1 −2\u0015min\n\u0015max\n)\n(7.30)\n220 7. GRADIENT DESCENT\nw\nE\n∆w (1)\n∆w (2)\n∆w (3)\nFigure 7.4 With a ﬁxed learning rate parameter, gradient descent down a surface with low curvature"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1167, "text": "leads to successively smaller steps corresponding to linear convergence. In such a sit-\nuation, the effect of a momentum term is like an increase in the effective learning rate\nparameter.\nwhere \u0015min is the smallest eigenvalue. If the ratio \u0015min=\u0015max (whose reciprocal\nis known as the condition number of the Hessian) is very small, corresponding to\nhighly elongated elliptical error contours as in Figure 7.3, then progress towards the\nminimum will be extremely slow.\n7.3.1 Momentum"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1168, "text": "minimum will be extremely slow.\n7.3.1 Momentum\nOne simple technique for dealing with the problem of widely differing eigenval-\nues is to add amomentum term to the gradient descent formula. This effectively adds\ninertia to the motion through weight space and smooths out the oscillations depicted\nin Figure 7.3. The modiﬁed gradient descent formula is given by\n∆w (\u001c−1) = −\u0011∇E\n(\nw(\u001c−1) )\n+ \u0016∆w (\u001c−2) (7.31)\nwhere \u0016is called the momentum parameter. The weight vector is then updated using\n(7.15)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1169, "text": "(7.15).\nTo understand the effect of the momentum term, consider ﬁrst the motion through\na region of weight space for which the error surface has relatively low curvature, as\nindicated in Figure 7.4. If we make the approximation that the gradient is unchang-\ning, then we can apply (7.31) iteratively to a long series of weight updates, and then\nsum the resulting arithmetic series to give\n∆w = −\u0011∇E{1 +\u0016+ \u00162 + :::} (7.32)\n= − \u0011\n1 −\u0016∇E (7.33)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1170, "text": "= − \u0011\n1 −\u0016∇E (7.33)\nand we see that the result of the momentum term is to increase the effective learning\nrate from \u0011to \u0011=(1 −\u0016).\nBy contrast, in a region of high curvature in which gradient descent is oscillatory,\nas indicated in Figure 7.5, successive contributions from the momentum term will\n7.3. Convergence 221\nFigure 7.5 For a situation in which successive\nsteps of gradient descent are oscilla-\ntory, a momentum term has little in-\nﬂuence on the effective value of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1171, "text": "ﬂuence on the effective value of the\nlearning rate parameter.\n∆w (1)\n∆w (2)\n∆w (3)\ntend to cancel and the effective learning rate will be close to\u0011. Thus, the momentum\nterm can lead to faster convergence towards the minimum without causing divergent\noscillations. A schematic illustration of the effect of a momentum term is shown in\nFigure 7.6.\nAlthough the inclusion of momentum can lead to an improvement in the per-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1172, "text": "formance of gradient descent, it also introduces a second parameter \u0016whose value\nneeds to be chosen, in addition to that of the learning rate parameter \u0011. From (7.33)\nwe see that \u0016should be in the range 0 6 \u0016 6 1. A typical value used in practice\nis \u0016 = 0 :9. Stochastic gradient descent with momentum is summarized in Algo-\nrithm 7.3.\nThe convergence can be further accelerated using a modiﬁed version of momen-\ntum called Nesterov momentum (Nesterov, 2004; Sutskever et al., 2013). In con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1173, "text": "ventional stochastic gradient descent with momentum, we ﬁrst compute the gradient\nFigure 7.6 Illustration of the effect of adding\na momentum term to the gradient\ndescent algorithm, showing the\nmore rapid progress along the val-\nley of the error function, compared\nwith the unmodiﬁed gradient de-\nscent shown in Figure 7.3.\nu1\nu2\nat the current location then take a step that is ampliﬁed by adding momentum from\nthe previous step. With the Nesterov method, we change the order of these and ﬁrst"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1174, "text": "compute a step based on the previous momentum, then calculate the gradient at this\n222 7. GRADIENT DESCENT\nAlgorithm 7.3: Stochastic gradient descent with momentum\nInput: Training set of data points indexed by n∈{1;:::;N }\nBatch size B\nError function per mini-batch En:n+B−1 (w)\nLearning rate parameter \u0011\nMomentum parameter \u0016\nInitial weight vector w\nOutput: Final weight vector w\nn← 1\n∆w ← 0\nrepeat\n∆w ←−\u0011 ∇En:n+B−1 (w) + \u0016∆w // calculate update term\nw ← w + ∆w // weight vector update\nn← n+ B"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1175, "text": "w ← w + ∆w // weight vector update\nn← n+ B\nif n>N then\nshufﬂe data\nn← 1\nend if\nuntil convergence\nreturn w\nnew location to ﬁnd the update, so that\n∆w (\u001c−1) = −\u0011∇E\n(\nw(\u001c−1) + \u0016∆w (\u001c−2) )\n+ \u0016∆w (\u001c−2) : (7.34)\nFor batch gradient descent, Nesterov momentum can improve the rate of conver-\ngence, although for stochastic gradient descent it can be less effective.\n7.3.2 Learning rate schedule\nIn the stochastic gradient descent learning algorithm (7.18), we need to specify"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1176, "text": "a value for the learning rate parameter\u0011. If \u0011is very small then learning will proceed\nslowly. However, if\u0011is increased too much it can lead to instability. Although someSection 7.3.1\noscillation can be tolerated, it should not be divergent. In practice, the best results\nare obtained by using a larger value for\u0011at the start of training and then reducing the\nlearning rate over time, so that the value of\u0011becomes a function of the step index\u001c:\nw(\u001c) = w(\u001c−1) −\u0011(\u001c−1) ∇En(w(\u001c−1) ): (7.35)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1177, "text": "w(\u001c) = w(\u001c−1) −\u0011(\u001c−1) ∇En(w(\u001c−1) ): (7.35)\n7.3. Convergence 223\nExamples of learning rate schedules include linear, power law, and exponential de-\ncay:\n\u0011(\u001c) = (1 −\u001c=K) \u0011(0) + (\u001c=K)\u0011(K) (7.36)\n\u0011(\u001c) = \u0011(0) (1 + \u001c=s)c (7.37)\n\u0011(\u001c) = \u0011(0)c\u001c=s (7.38)\nwhere in (7.36) the value of \u0011reduces linearly over K steps, after which its value is\nheld constant at \u0011(K). Good values for the hyperparameters \u0011(0), \u0011(K), K, S, and c\nmust be found empirically. It can be very helpful in practice to monitor the learning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1178, "text": "curve showing how the error function evolves during the gradient descent iteration\nto ensure that it is decreasing at a suitable rate.\n7.3.3 RMSProp and Adam\nWe saw that the optimal learning rate depends on the local curvature of the er-\nror surface, and moreover that this curvature can vary according to the direction inSection 7.3\nparameter space. This motivates several algorithms that use different learning rates"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1179, "text": "for each parameter in the network. The values of these learning rates are adjusted\nautomatically during training. Here we review some of the most widely used exam-\nples. Note, however, that this intuition really applies only if the principal curvature\ndirections are aligned with the axes in weight space, corresponding to a locally diag-\nonal Hessian matrix, which is unlikely to be the case in practice. Nevertheless, these\ntypes of algorithms can be effective and are widely used."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1180, "text": "The key idea behind AdaGrad, short for ‘adaptive gradient’, is to reduce each\nlearning rate parameter over time by using the accumulated sum of squares of all the\nderivatives calculated for that parameter (Duchi, Hazan, and Singer, 2011). Thus,\nparameters associated with high curvature are reduced most rapidly. Speciﬁcally,\nr(\u001c)\ni = r(\u001c−1)\ni +\n(@E(w)\n@wi\n)2\n(7.39)\nw(\u001c)\ni = w(\u001c−1)\ni − \u0011√r\u001c\ni + \u000e\n(@E(w)\n@wi\n)\n(7.40)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1181, "text": "i = w(\u001c−1)\ni − \u0011√r\u001c\ni + \u000e\n(@E(w)\n@wi\n)\n(7.40)\nwhere \u0011 is the learning rate parameter, and \u000e is a small constant, say 10−8 , that\nensures numerical stability in the event that ri is close to zero. The algorithm is\ninitialized with r(0)\ni = 0. Here E(w) is the error function for a particular mini-batch,\nand the update (7.40) is standard stochastic gradient descent but with a modiﬁed\nlearning rate that is speciﬁc to each parameter."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1182, "text": "learning rate that is speciﬁc to each parameter.\nOne problem with AdaGrad is that it accumulates the squared gradients from the\nvery start of training, and so the associated weight updates can become very small,\nwhich can slow down training too much in the later phases. The idea behind the\nRMSProp algorithm, which is short for ‘root mean square propagation’, is to replace\nthe sum of squared gradients of AdaGrad with an exponentially weighted average\n224 7. GRADIENT DESCENT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1183, "text": "224 7. GRADIENT DESCENT\n(Hinton, 2012), giving\nr(\u001c)\ni = \fr(\u001c−1)\ni + (1 −\f)\n(@E(w)\n@wi\n)2\n(7.41)\nw(\u001c)\ni = w(\u001c−1)\ni − \u0011√r\u001c\ni + \u000e\n(@E(w)\n@wi\n)\n(7.42)\nwhere 0 <\f <1 and a typical value is \f = 0:9.\nIf we combine RMSProp with momentum, we obtain the Adam optimization\nmethod (Kingma and Ba, 2014) where the name is derived from ‘adaptive moments’.\nAdam stores the momentum for each parameter separately using update equations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1184, "text": "that consist of exponentially weighted moving averages for both the gradients and\nthe squared gradients in the form\ns(\u001c)\ni = \f1s(\u001c−1)\ni + (1 −\f1)\n(@E(w)\n@wi\n)\n(7.43)\nr(\u001c)\ni = \f2r(\u001c−1)\ni + (1 −\f2)\n(@E(w)\n@wi\n)2\n(7.44)\nˆsi\n(\u001c) = s(\u001c)\ni\n1 −\f\u001c\n1\n(7.45)\nˆri\n\u001c = r\u001c\ni\n1 −\f\u001c\n2\n(7.46)\nw(\u001c)\ni = w(\u001c−1)\ni −\u0011 ˆsi\n\u001c\n√\nˆr\u001c\ni + \u000e: (7.47)\nHere the factors1=(1−\f\u001c\n1 ) and 1=(1−\f\u001c\n2 ) correct for a bias introduced by initializing\ns(0)\ni and r(0)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1185, "text": "s(0)\ni and r(0)\ni to zero. Note that the bias goes to zero as\u001cbecomes large, since\fi <1,Exercise 7.12\nand so in practice this bias correction is sometimes omitted. Typical values for the\nweighting parameters are\f1 = 0:9 and \f2 = 0:99. Adam is the most widely adopted\nlearning algorithm in deep learning and is summarized in Algorithm 7.4.\n7.4. Normalization\nNormalization of the variables computed during the forward pass through a neural"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1186, "text": "network removes the need for the network to deal with extremely large or extremely\nsmall values. Although in principle the weights and biases in a neural network can\nadapt to whatever values the input and hidden variables take, in practice normaliza-\ntion can be crucial for ensuring effective training. Here we consider three kinds of\nnormalization according to whether we are normalizing across the input data, across\nmini-batches, or across layers.\n7.4. Normalization 225"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1187, "text": "7.4. Normalization 225\nAlgorithm 7.4: Adam optimization\nInput: Training set of data points indexed by n∈{1;:::;N }\nBatch size B\nError function per mini-batch En:n+B−1 (w)\nLearning rate parameter \u0011\nDecay parameters \f1 and \f2\nStabilization parameter \u000e\nOutput: Final weight vector w\nn← 1\ns ← 0\nr ← 0\nrepeat\nChoose a mini-batch at random from D\ng = −∇En:n+B−1 (w) // evaluate gradient vector\ns ← \f1s + (1 −\f1)g\nr ← \f2r + (1 −\f2)g ⊙g // element-wise multiply\nˆs ← s=(1 −\f\u001c\n1 ) // bias correction"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1188, "text": "ˆs ← s=(1 −\f\u001c\n1 ) // bias correction\nˆr ← r=(1 −\f\u001c\n2 ) // bias correction\n∆w ←−\u0011 ˆs√\nˆr + \u000e\n// element-wise operations\nw ← w + ∆w // weight vector update\nn← n+ B\nif n+ B >Nthen\nshufﬂe data\nn← 1\nend if\nuntil convergence\nreturn w\n226 7. GRADIENT DESCENT\n7.4.1 Data normalization\nSometimes we encounter data sets in which different input variables span very\ndifferent ranges. For example, in health data, a patient’s height might be measured"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1189, "text": "in meters, such as 1.8m, whereas their blood platelet count might be measured in\nplatelets per microliter, such as 300,000 platelets per \u0016L. Such variations can make\ngradient descent training much more challenging. Consider a single-layer regression\nnetwork with two weights in which the two corresponding input variables have very\ndifferent ranges. Changes in the value of one of the weights produce much larger\nchanges in the output, and hence in the error function, than would similar changes in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1190, "text": "the other weight. This corresponds to an error surface with very different curvatures\nalong different axes as illustrated in Figure 7.3.\nFor continuous input variables, it can therefore be very beneﬁcial to re-scale the\ninput values so that they span similar ranges. This is easily done by ﬁrst evaluating\nthe mean and variance of each input:\n\u0016i = 1\nN\nN∑\nn=1\nxni (7.48)\n\u001b2\ni = 1\nN\nN∑\nn=1\n(xni −\u0016i)2; (7.49)\nwhich is a calculation that is performed once, before any training is started. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1191, "text": "input values are then re-scaled using\n˜xni = xni −\u0016i\n\u001bi\n(7.50)\nso that the re-scaled values {˜xni}have zero mean and unit variance. Note that theExercise 7.14\nsame values of\u0016iand \u001bimust be used to pre-process any development, validation, or\ntest data to ensure that all inputs are scaled in the same way. Input data normalization\nis illustrated in Figure 7.7.\nFigure 7.7 Illustration of the effect of input data normal-\nization. The red circles show the original data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1192, "text": "ization. The red circles show the original data\npoints for a data set with two variables. The\nblue crosses show the data set after normal-\nization such that each variable now has zero\nmean and unit variance across the data set.\n−10 0 10x1\n−10\n0\n10\nx2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1193, "text": "7.4. Normalization 227\n7.4.2 Batch normalization\nWe have seen the importance of normalizing the input data, and we can apply\nsimilar reasoning to the variables in each hidden layer of a deep network. If there is\nwide variation in the range of activation values in a particular hidden layer, then nor-\nmalizing those values to have zero mean and unit variance should make the learning\nproblem easier for the next layer. However, unlike normalization of the input values,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1194, "text": "which can be done once prior to the start of training, normalization of the hidden-\nunit values will need to be repeated during training every time the weight values are\nupdated. This is called batch normalization (Ioffe and Szegedy, 2015).\nA further motivation for batch normalization arises from the phenomena ofvan-\nishing gradients and exploding gradients, which occur when we try to train very deep\nneural networks. From the chain rule of calculus, the gradient of an error functionE"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1195, "text": "with respect to a parameter in the ﬁrst layer of the network is given bySection 8.1.5\n@E\n@wi\n=\n∑\nm\n---\n∑\nl\n∑\nj\n@z(1)\nm\n@wi\n---\n@z(K)\nj\n@z(K−1)\nl\n@E\n@z(K)\nj\n(7.51)\nwhere z(k)\nj denotes the activation of node jin layer k, and each of the partial deriva-\ntives on the right-hand side of (7.51) represents the elements of the Jacobian matrixSection 8.1.5\nfor that layer. The product of a large number of such terms will tend towards 0 if"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1196, "text": "most of them have a magnitude <1 and will tend towards ∞ if most of them have\na magnitude >1. Consequently, as the depth of a network increases, error function\ngradients can tend to become either very large or very small. Batch normalization\nlargely resolves this issue.\nTo see how batch normalization is deﬁned, consider a speciﬁc layer within a\nmulti-layer network. Each hidden unit in that layer computes a nonlinear function of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1197, "text": "its input pre-activation zi = h(ai), and so we have a choice of whether to normalize\nthe pre-activation values ai or the activation values zi. In practice, either approach\nmay be used, and here we illustrate the procedure by normalizing the pre-activations.\nBecause weight values are updated after each mini-batch of examples, we apply the\nnormalization to each mini-batch. Speciﬁcally, for a mini-batch of sizeK, we deﬁne\n\u0016i = 1\nK\nK∑\nn=1\nani (7.52)\n\u001b2\ni = 1\nK\nK∑\nn=1\n(ani −\u0016i)2 (7.53)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1198, "text": "ani (7.52)\n\u001b2\ni = 1\nK\nK∑\nn=1\n(ani −\u0016i)2 (7.53)\nˆani = ani −\u0016i√\n\u001b2\ni + \u000e\n(7.54)\nwhere the summations over n = 1;:::;K are taken over the elements of the mini-\nbatch. Here \u000eis a small constant, introduced to avoid numerical issues in situations\nwhere \u001b2\ni is small.\nBy normalizing the pre-activations in a given layer of the network, we reduce\nthe number of degrees of freedom in the parameters of that layer and hence we\n228 7. GRADIENT DESCENT\n\u0016 \u001b\nHidden units\nMini-batch\n(a)\n\u0016\n\u001b\nHidden units"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1199, "text": "\u0016 \u001b\nHidden units\nMini-batch\n(a)\n\u0016\n\u001b\nHidden units\nMini-batch\n(b)\nFigure 7.8 Illustration of batch normalization and layer normalization in a neural network. In batch normaliza-\ntion, shown in (a), the mean and variance are computed across the mini-batch separately for each hidden unit.\nIn layer normalization, shown in (b), the mean and variance are computed across the hidden units separately for\neach data point.\nreduce its representational capability. We can compensate for this by re-scaling the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1200, "text": "pre-activations of the batch to have mean \fi and standard deviation \ri using\n˜ani = \riˆani + \fi (7.55)\nwhere \fi and \ri are adaptive parameters that are learned by gradient descent jointly\nwith the weights and biases of the network. These learnable parameters represent a\nkey difference compared to input data normalization.Section 7.4.1\nIt might appear that the transformation (7.55) has simply undone the effect of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1201, "text": "batch normalization since the mean and variance can now adapt to arbitrary values\nagain. However, the crucial difference is in the way the parameters evolve during\ntraining. For the original network, the mean and variance across a mini-batch are\ndetermined by a complex function of all the weights and biases in the layer, whereas\nin the representation given by (7.55), they are determined directly by independent\nparameters \fi and \ri, which turn out to be much easier to learn during gradient"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1202, "text": "descent.\nEquations (7.52) – (7.55) describe a transformation of the variables that is dif-\nferentiable with respect to the learnable parameters \fi and \ri. This can be viewed\nas an additional layer in the neural network, and so each standard hidden layer can\nbe followed by a batch normalization layer. The structure of the batch-normalization\nprocess is illustrated in Figure 7.8.\nOnce the network is trained and we want to make predictions on new data, we\n7.4. Normalization 229"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1203, "text": "7.4. Normalization 229\nno longer have the training mini-batches available, and we cannot determine a mean\nand variance from just one data example. To solve this, we could in principle eval-\nuate \u0016i and \u001b2\ni for each layer across the whole training set after we have made the\nﬁnal update to the weights and biases. However, this would involve processing the\nwhole data set just to evaluate these quantities and is therefore usually too expensive."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1204, "text": "Instead, we compute moving averages throughout the training phase:\n\u0016(\u001c)\ni = \u000b\u0016(\u001c−1)\ni + (1 −\u000b)\u0016i (7.56)\n\u001b(\u001c)\ni = \u000b\u001b(\u001c−1)\ni + (1 −\u000b)\u001bi (7.57)\nwhere 0 6 \u000b6 1. These moving averages play no role during training but are used\nto process new data points during the inference phase.\nAlthough batch normalization is very effective in practice, there is uncertainty\nas to why it works so well. Batch normalization was originally motivated by noting"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1205, "text": "that updates to weights in earlier layers of the network change the distribution of\nvalues seen by later layers, a phenomenon called internal covariate shift. However,\nlater studies (Santurkar et al., 2018) suggest that covariate shift is not a signiﬁcant\nfactor and that the improved training results from an improvement in the smoothness\nof the error function landscape.\n7.4.3 Layer normalization\nWith batch normalization, if the batch size is too small then the estimates of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1206, "text": "mean and variance become too noisy. Also, for very large training sets, the mini-\nbatches may be split across different GPUs, making global normalization across the\nmini-batch inefﬁcient. An alternative to normalizing across examples within a mini-\nbatch for each hidden unit separately is to normalize across the hidden-unit values\nfor each data point separately. This is known as layer normalization (Ba, Kiros, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1207, "text": "Hinton, 2016). It was introduced in the context of recurrent neural networks whereSection 12.2.5\nthe distributions change after each time step making batch normalization infeasible.\nHowever, it is useful in other architectures such as transformer networks.Chapter 12\nBy analogy with batch normalization, we therefore make the following transfor-\nmation:\n\u0016n = 1\nM\nM∑\ni=1\nani (7.58)\n\u001b2\nn = 1\nM\nM∑\ni=1\n(ani −\u0016i)2 (7.59)\nˆani = ani −\u0016n√\n\u001b2\nn + \u000e\n(7.60)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1208, "text": "(ani −\u0016i)2 (7.59)\nˆani = ani −\u0016n√\n\u001b2\nn + \u000e\n(7.60)\nwhere the sums i = 1;:::;M are taken over all hidden units in the layer. As with\nbatch normalization, additional learnable mean and standard deviation parameters\nare introduced for each hidden unit separately in the form (7.55). Note that the same\nnormalization function can be employed during training and during inference, and\n230 7. GRADIENT DESCENT\nso there is no need to store moving averages. Layer normalization is compared with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1209, "text": "batch normalization in Figure 7.8.\nExercises\n7.1 (?) By substituting (7.10) into (7.7) and using (7.8) and (7.9), show that the error\nfunction (7.7) can be written in the form (7.11).\n7.2 (?) Consider a Hessian matrix H with eigenvector equation (7.8). By setting the\nvector v in (7.14) equal to each of the eigenvectorsuiin turn, show thatH is positive\ndeﬁnite if, and only if, all its eigenvalues are positive.\n7.3 (??) By considering the local Taylor expansion (7.7) of an error function about a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1210, "text": "stationary point w?, show that the necessary and sufﬁcient condition for the station-\nary point to be a local minimum of the error function is that the Hessian matrix H,\ndeﬁned by (7.5) with ˆw = w?, is positive deﬁnite.\n7.4 (??) Consider a linear regression model with a single input variable xand a single\noutput variable yof the form\ny(x;w;b) = wx+ b (7.61)\ntogether with a sum-of-squares error function given by\nE(w;b) = 1\n2\nN∑\nn=1\n{y(xn;w;b) −tn}2 : (7.62)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1211, "text": "E(w;b) = 1\n2\nN∑\nn=1\n{y(xn;w;b) −tn}2 : (7.62)\nDerive expressions for the elements of the2 ×2 Hessian matrix given by the second\nderivatives of the error function with respect to the weight parameter wand bias pa-\nrameter b. Show that the trace and the determinant of this Hessian are both positive.\nSince the trace represents the sum of the eigenvalue and the determinant corresponds\nto the product of the eigenvalues, then both eigenvalues are positive and hence theAppendix A"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1212, "text": "stationary point of the error function is a minimum.\n7.5 (??) Consider a single-layer classiﬁcation model with a single input variable xand\na single output variable yof the form\ny(x;w;b) = \u001b(wx+ b) (7.63)\nwhere \u001b(-)is the logistic sigmoid function deﬁned by (5.42) together with a cross-\nentropy error function given by\nE(w;b) =\nN∑\nn=1\n{tnln y(xn;w;b) + (1 −tn) ln(1−y(xn;w;b))}: (7.64)\nDerive expressions for the elements of the2 ×2 Hessian matrix given by the second"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1213, "text": "derivatives of the error function with respect to the weight parameter wand bias pa-\nrameter b. Show that the trace and the determinant of this Hessian are both positive.\nExercises 231\nSince the trace represents the sum of the eigenvalue and the determinant corresponds\nto the product of the eigenvalues, then both eigenvalues are positive and hence theAppendix A\nstationary point of the error function is a minimum."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1214, "text": "7.6 (??) Consider a quadratic error function deﬁned by (7.7) in which the Hessian matrix\nH has an eigenvalue equation given by (7.8). Show that the contours of constant\nerror are ellipses whose axes are aligned with the eigenvectors ui with lengths that\nare inversely proportional to the square roots of the corresponding eigenvalues \u0015i.\n7.7 (?) Show that, as a consequence of the symmetry of the Hessian matrix H, the\nnumber of independent elements in the quadratic error function (7.3) is given by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1215, "text": "W(W + 3)=2.\n7.8 (?) Consider a set of values x1;:::;x N drawn from a distribution with mean \u0016and\nvariance \u001b2, and deﬁne the sample mean to be\nx= 1\nN\nN∑\nn=1\nxn: (7.65)\nShow that the expectation of the squared error (x −\u0016)2 with respect to the distri-\nbution from which the data is drawn is given by \u001b2=N. This shows that the RMS\nerror in the sample mean is given by \u001b=\n√\nN, which decreases relatively slowly as\nthe sample size N increases."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1216, "text": "the sample size N increases.\n7.9 (??) Consider a layered network that computes the functions (7.19) and (7.20) in\nlayer l. Suppose we initialize the weights using a Gaussian N(0;\u000f2), and suppose\nthat the outputs z(l−1)\nj of the units in layer l−1 have variance \u00152. By using the form\nof the ReLU activation function, show that the mean and variance of the outputs in\nlayer l are given by (7.21) and (7.22), respectively. Hence, show that if we want"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1217, "text": "the units in layer lalso to have pre-activations with variance \u00152 then the value of \u000f\nshould be given by (7.23).\n7.10 (??) By making use of (7.7), (7.8), and (7.10), derive the results (7.24) and (7.25),\nwhich express the gradient vector and a general weight update, as expansions in the\neigenvectors of the Hessian matrix. Use these results, together with the eigenvector\northonormality relation (7.9) and the batch gradient descent formula (7.16), to de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1218, "text": "rive the result (7.26) for the batch gradient descent update expressed in terms of the\ncoefﬁcients {\u000bi}.\n7.11 (?) Consider a smoothly varying error surface with low curvature such that the gra-\ndient varies only slowly with position. Show that, for small values of the learning\nrate and momentum parameters, the Nesterov momentum gradient update deﬁned\nby (7.34) is equivalent to the standard gradient descent with momentum deﬁned by\n(7.31)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1219, "text": "(7.31).\n7.12 (??) Consider a sequence of values {x1;:::;x N}of some variable x, and suppose\nwe compute an exponentially weighted moving average using the formula\n\u0016n = \f\u0016n−1 + (1 −\f)xn (7.66)\n232 7. GRADIENT DESCENT\nwhere 0 6 \f 6 1. By making use of the following result for the sum of a ﬁnite\ngeometric series\nn∑\nk=1\n\fk−1 = 1 −\fn\n1 −\f (7.67)\nshow that if the sequence of averages is initialized using \u00160 = 0, then the estimators\nare biased and that the bias can be corrected using\nˆ\u0016n = \u0016n"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1220, "text": "ˆ\u0016n = \u0016n\n1 −\fn: (7.68)\n7.13 (?) In gradient descent, the weight vector w is updated by taking a step in weight\nspace in the direction of the negative gradient governed by a learning rate parameter\n\u0011. Suppose instead that we choose a direction d in weight space along which we\nminimize the error function, given the current weight vector w(\u001c). This involves\nminimizing the quantity\nE(w(\u001c) + \u0015d) (7.69)\nas a function of \u0015to give a value \u0015? corresponding to a new weight vector w(\u001c+1)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1221, "text": "Show that the gradient of E(w) at w(\u001c+1) is orthogonal to the vector d. This is\nknown as a ‘line search’ method and it forms the basis for a variety of numerical\noptimization algorithms (Bishop, 1995b).\n7.14 (?) Show that the renormalized input variables deﬁned by (7.50), where\u0016i is deﬁned\nby (7.48) and \u001b2\ni is deﬁned by (7.49), have zero mean and unit variance.\n8\nBackpropagation\nOur goal in this chapter is to ﬁnd an efﬁcient technique for evaluating the gradient"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1222, "text": "of an error function E(w) for a feed-forward neural network. We will see that this\ncan be achieved using a local message-passing scheme in which information is sent\nbackwards through the network and is known as error backpropagation, or some-\ntimes simply as backprop.\nHistorically, the backpropagation equations would have been derived by hand\nand then implemented in software alongside the forward propagation equations, with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1223, "text": "both steps taking time and being prone to mistakes. Modern neural network software\nenvironments, however, allow virtually any derivatives of interest to be calculated\nefﬁciently with only minimal effort beyond that of coding up the original network\nfunction. This idea, called automatic differentiation, plays a key role in modern deepSection 8.2\nlearning. However, it is valuable to understand how the calculations are performed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1224, "text": "so that we are not relying on ‘black box’ software solutions. In this chapter we\n233© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_8"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1225, "text": "234 8. BACKPROPAGATION\ntherefore explain the key concepts of backpropagation, and explore the framework\nof automatic differentiation in detail.\nNote that the term ‘backpropagation’ is used in the neural computing literature\nin a variety of different ways. For instance, a feed-forward architecture may be\ncalled a backpropagation network. Also the term ‘backpropagation’ is sometimes\nused to describe the end-to-end training procedure for a neural network including"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1226, "text": "the gradient descent parameter updates. In this book we will use ‘backpropagation’\nspeciﬁcally to describe the computational procedure used in the numerical evaluation\nof derivatives such as the gradient of the error function with respect to the weights\nand biases of a network. This procedure can also be applied to the evaluation of other\nimportant derivatives such as the Jacobian and Hessian matrices.\n8.1.\nEvaluation of Gradients"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1227, "text": "8.1.\nEvaluation of Gradients\nWe now derive the backpropagation algorithm for a general network having arbitrary\nfeed-forward topology, arbitrary differentiable nonlinear activation functions, and a\nbroad class of error function. The resulting formulae will then be illustrated using\na simple layered network structure having a single layer of sigmoidal hidden units\ntogether with a sum-of-squares error.\nMany error functions of practical interest, for instance those deﬁned by maxi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1228, "text": "mum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data\npoint in the training set, so that\nE(w) =\nN∑\nn=1\nEn(w): (8.1)\nHere we will consider the problem of evaluating ∇En(w) for one such term in the\nerror function. This may be used directly for stochastic gradient descent, or the\nresults could be accumulated over a set of training data points for batch or mini-\nbatch methods.\n8.1.1 Single-layer networks"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1229, "text": "batch methods.\n8.1.1 Single-layer networks\nConsider ﬁrst a simple linear model in which the outputs yk are linear combina-\ntions of the input variables xi so that\nyk =\n∑\ni\nwkixi (8.2)\ntogether with a sum-of-squares error function that, for a particular input data point\nn, takes the form\nEn = 1\n2\n∑\nk\n(ynk −tnk)2 (8.3)\n8.1. Evaluation of Gradients 235\nwhere ynk = yk(xn;w), and tnk is the associated target value. The gradient of this\nerror function with respect to a weight wji is given by\n@En\n@wji"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1230, "text": "@En\n@wji\n= (ynj −tnj)xni: (8.4)\nThis can be interpreted as a ‘local’ computation involving the product of an ‘error\nsignal’ynj −tnj associated with the output end of the link wji and the variable xni\nassociated with the input end of the link. In Section 5.4.3, we saw how a similar\nformula arises with the logistic-sigmoid activation function together with the cross-\nentropy error function and similarly for the softmax activation function together with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1231, "text": "its matching multivariate cross-entropy error function. We will now see how this sim-\nple result extends to the more complex setting of multilayer feed-forward networks.\n8.1.2 General feed-forward networks\nIn general, a feed-forward network consists of a set of units each of which com-\nputes a weighted sum of its inputs:\naj =\n∑\ni\nwjizi (8.5)\nwhere zi is either the activation of another unit or an input unit that sends a connec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1232, "text": "tion to unit j, and wji is the weight associated with that connection. Biases can be\nincluded in this sum by introducing an extra unit, or input, with activation ﬁxed at\n+1, and so we do not need to deal with biases explicitly. The sum in (8.5), knownSection 6.2\nas a pre-activation, is transformed by a nonlinear activation function h(-)to give the\nactivation zj of unit jin the form\nzj = h(aj): (8.6)\nNote that one or more of the variables zi in the sum in (8.5) could be an input, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1233, "text": "similarly, the unit jin (8.6) could be an output.\nFor each data point in the training set, we will suppose that we have supplied the\ncorresponding input vector to the network and calculated the activations of all the\nhidden and output units in the network by successive application of (8.5) and (8.6).\nThis process is called forward propagation because it can be regarded as a forward\nﬂow of information through the network."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1234, "text": "ﬂow of information through the network.\nNow consider the evaluation of the derivative of En with respect to a weight\nwji. The outputs of the various units will depend on the particular input data point\nn. However, to keep the notation uncluttered, we will omit the subscript nfrom the\nnetwork variables. First note thatEndepends on the weightwji only via the summed\ninput aj to unit j. We can therefore apply the chain rule for partial derivatives to give\n@En\n@wji\n= @En\n@aj\n@aj\n@wji\n: (8.7)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1235, "text": "@En\n@wji\n= @En\n@aj\n@aj\n@wji\n: (8.7)\nWe now introduce a useful notation:\n\u000ej ≡@En\n@aj\n(8.8)\n236 8. BACKPROPAGATION\nFigure 8.1 Illustration of the calculation of \u000ej for hidden\nunit j by backpropagation of the \u000e’s from those\nunits k to which unit j sends connections. The\nblack arrows denote the direction of information\nﬂow during forward propagation, and the red ar-\nrows indicate the backward propagation of error\ninformation.\nzi\nzj\n\u000ej\n\u000ek\n\u000e1\n …\nwji wkj"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1236, "text": "information.\nzi\nzj\n\u000ej\n\u000ek\n\u000e1\n …\nwji wkj\nwhere the \u000e’s are often referred to as errors for reasons we will see shortly. Using\n(8.5), we can write\n@aj\n@wji\n= zi: (8.9)\nSubstituting (8.8) and (8.9) into (8.7), we then obtain\n@En\n@wji\n= \u000ejzi: (8.10)\nEquation (8.10) tells us that the required derivative is obtained simply by multiplying\nthe value of \u000e for the unit at the output end of the weight by the value of z for the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1237, "text": "unit at the input end of the weight (where z = 1 for a bias). Note that this takes the\nsame form as that found for the simple linear model in (8.4). Thus, to evaluate the\nderivatives, we need calculate only the value of\u000ej for each hidden and output unit in\nthe network and then apply (8.10).\nAs we have seen already, for the output units, we have\n\u000ek = yk −tk (8.11)\nprovided we are using the canonical link as the output-unit activation function. ToSection 5.4.6"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1238, "text": "evaluate the \u000e’s for hidden units, we again make use of the chain rule for partial\nderivatives:\n\u000ej ≡@En\n@aj\n=\n∑\nk\n@En\n@ak\n@ak\n@aj\n(8.12)\nwhere the sum runs over all units kto which unit jsends connections. The arrange-\nment of units and weights is illustrated in Figure 8.1. Note that the units labelled k\ninclude other hidden units and/or output units. In writing down (8.12), we are mak-\ning use of the fact that variations in aj give rise to variations in the error function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1239, "text": "only through variations in the variables ak.\nIf we now substitute the deﬁnition of \u000ej given by (8.8) into (8.12) and make use\nof (8.5) and (8.6), we obtain the following backpropagation formula:Exercise 8.1\n\u000ej = h′(aj)\n∑\nk\nwkj\u000ek; (8.13)\nwhich tells us that the value of \u000e for a particular hidden unit can be obtained by\npropagating the \u000e’s backwards from units higher up in the network, as illustrated\n8.1. Evaluation of Gradients 237\nAlgorithm 8.1: Backpropagation\nInput: Input vector xn"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1240, "text": "Input: Input vector xn\nNetwork parameters w\nError function En(w) for input xn\nActivation function h(a)\nOutput: Error function derivatives {@En=@wji}\n// Forward propagation\nfor j ∈all hidden and output units do\naj ← ∑\niwjizi // {zi}includes inputs {xi}\nzj ← h(aj) // activation function\nend for\n// Error evaluation\nfor k∈all output units do\n\u000ek ← @En\n@ak\n// compute errors\nend for\n// Backward propagation, in reverse order\nfor j ∈all hidden units do\n\u000ej ← h′(aj) ∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1241, "text": "for j ∈all hidden units do\n\u000ej ← h′(aj) ∑\nkwkj\u000ek // recursive backward evaluation\n@En\n@wji\n← \u000ejzi // evaluate derivatives\nend for\nreturn\n{ @En\n@wji\n}\nin Figure 8.1. Note that the summation in (8.13) is taken over the ﬁrst index on\nwkj (corresponding to backward propagation of information through the network),\nwhereas in the forward propagation equation (8.5), it is taken over the second index.Exercise 8.2\nBecause we already know the values of the \u000e’s for the output units, it follows that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1242, "text": "by recursively applying (8.13), we can evaluate the \u000e’s for all the hidden units in a\nfeed-forward network, regardless of its topology. The backpropagation procedure is\nsummarized in Algorithm 8.1.\nFor batch methods, the derivative of the total error E can then be obtained by\nrepeating the above steps for each data point in the training set and then summing\nover all data points in the batch or mini-batch:\n@E\n@wji\n=\n∑\nn\n@En\n@wji\n: (8.14)\n238 8. BACKPROPAGATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1243, "text": "=\n∑\nn\n@En\n@wji\n: (8.14)\n238 8. BACKPROPAGATION\nIn the above derivation we have implicitly assumed that each hidden or output unit in\nthe network has the same activation function h(-). However, the derivation is easily\ngeneralized to allow different units to have individual activation functions, simply\nby keeping track of which form of h(-)goes with which unit.\n8.1.3 A simple example\nThe above derivation of the backpropagation procedure allowed for general"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1244, "text": "forms for the error function, the activation functions, and the network topology. To\nillustrate the application of this algorithm, we consider a two-layer network of the\nform illustrated in Figure 6.9, together with a sum-of-squares error. The output units\nhave linear activation functions, so thatyk = ak, and the hidden units have sigmoidal\nactivation functions given by\nh(a) ≡tanh(a) (8.15)\nwhere tanh(a) is deﬁned by (6.14). A useful feature of this function is that its"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1245, "text": "derivative can be expressed in a particularly simple form:\nh′(a) = 1 −h(a)2: (8.16)\nWe also consider a sum-of-squares error function, so that for data point nthe error\nis given by\nEn = 1\n2\nK∑\nk=1\n(yk −tk)2 (8.17)\nwhere yk is the activation of output unit k, and tk is the corresponding target value\nfor a particular input vector xn.\nFor each data point in the training set in turn, we ﬁrst perform a forward propa-\ngation using\naj =\nD∑\ni=0\nw(1)\nji xi (8.18)\nzj = tanh(a j) (8.19)\nyk =\nM∑\nj=0\nw(2)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1246, "text": "zj = tanh(a j) (8.19)\nyk =\nM∑\nj=0\nw(2)\nkj zj (8.20)\nwhere D is the dimensionality of the input vector x and M is the total number of\nhidden units. Also we have usedx0 = z0 = 1 to allow bias parameters to be included\nin the weights. Next we compute the \u000e’s for each output unit using\n\u000ek = yk −tk: (8.21)\nThen, we backpropagate these errors to obtain \u000e’s for the hidden units using\n\u000ej = (1 −z2\nj)\nK∑\nk=1\nw(2)\nkj \u000ek; (8.22)\n8.1. Evaluation of Gradients 239"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1247, "text": "kj \u000ek; (8.22)\n8.1. Evaluation of Gradients 239\nwhich follows from (8.13) and (8.16). Finally, the derivatives with respect to the\nﬁrst-layer and second-layer weights are given by\n@En\n@w(1)\nji\n= \u000ejxi; @En\n@w(2)\nkj\n= \u000ekzj: (8.23)\n8.1.4 Numerical differentiation\nOne of the most important aspects of backpropagation is its computational efﬁ-\nciency. To understand this, let us examine how the number of compute operations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1248, "text": "required to evaluate the derivatives of the error function scales with the total number\nW of weights and biases in the network.\nA single evaluation of the error function (for a given input data point) would\nrequire O(W) operations, for sufﬁciently large W. This follows because, except for\na network with very sparse connections, the number of weights is typically much\ngreater than the number of units, and so the bulk of the computational effort in for-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1249, "text": "ward propagation arises from evaluation of the sums in (8.5), with the evaluation of\nthe activation functions representing a small overhead. Each term in the sum in (8.5)\nrequires one multiplication and one addition, leading to an overall computational\ncost that is O(W).\nAn alternative approach to backpropagation for computing the derivatives of the\nerror function is to use ﬁnite differences. This can be done by perturbing each weight"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1250, "text": "in turn and approximating the derivatives by using the expression\n@En\n@wji\n= En(wji + \u000f) −En(wji)\n\u000f + O(\u000f) (8.24)\nwhere \u000f ≪ 1. In a software simulation, the accuracy of the approximation to the\nderivatives can be improved by making\u000fsmaller, until numerical round-off problems\narise. The accuracy of the ﬁnite differences method can be improved signiﬁcantly\nby using symmetrical central differences of the form\n@En\n@wji\n= En(wji + \u000f) −En(wji −\u000f)\n2\u000f + O(\u000f2): (8.25)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1251, "text": "@wji\n= En(wji + \u000f) −En(wji −\u000f)\n2\u000f + O(\u000f2): (8.25)\nIn this case, the O(\u000f) corrections cancel, as can be veriﬁed by a Taylor expansionExercise 8.3\nof the right-hand side of (8.25), and so the residual corrections are O(\u000f2). Note,\nhowever, that the number of computational steps is roughly doubled compared with\n(8.24). Figure 8.2 shows a plot of the error between a numerical evaluation of a\ngradient using both ﬁnite differences (8.24) and central differences (8.25) versus the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1252, "text": "analytical result, as a function of the value of the step size \u000f.\nThe main problem with numerical differentiation is that the highly desirable\nO(W) scaling has been lost. Each forward propagation requires O(W) steps, and\nthere are W weights in the network each of which must be perturbed individually, so\nthat the overall computational cost is O(W2).\nHowever, numerical differentiation can play a useful role in practice, because"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1253, "text": "a comparison of the derivatives calculated from a direct implementation of back-\npropagation, or from automatic differentiation, with those obtained using central\ndifferences provides a powerful check on the correctness of the software.\n240 8. BACKPROPAGATION\nFigure 8.2 The red curve shows a\nplot of the error between the numer-\nical evaluation of a gradient using ﬁ-\nnite differences (8.24) and the analyti-\ncal result, as a function of \u000f. As \u000f de-\ncreases, the plot initially shows a lin-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1254, "text": "creases, the plot initially shows a lin-\near decrease in error, and this repre-\nsents a power law behaviour since the\naxes are logarithmic. The slope of this\nline is 1 which shows that this error be-\nhaves like O(\u000f). At some point the eval-\nuated gradient reaches the limit of nu-\nmerical round-off and further reduction\nin \u000f leads to a noisy line, which again\nfollows a power law but where the error\nnow increases with decreasing \u000f. The\nblue curve shows the corresponding re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1255, "text": "blue curve shows the corresponding re-\nsult for central differences (8.25). We\nsee a much smaller error compared to\nﬁnite differences, and the slope of the\nline is 2 which shows that the error is\nO(\u000f2).\n10−13 10−11 10−9 10−7\nϵ\n10−20\n10−18\n10−16\n10−14\n10−12\nError\nﬁnite differences\ncentral differences\n8.1.5 The Jacobian matrix\nWe have seen how the derivatives of an error function with respect to the weights\ncan be obtained by propagating errors backwards through the network. Backprop-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1256, "text": "agation can also be used to calculate other derivatives. Here we consider the eval-\nuation of the Jacobian matrix, whose elements are given by the derivatives of the\nnetwork outputs with respect to the inputs:\nJki ≡@yk\n@xi\n(8.26)\nwhere each such derivative is evaluated with all other inputs held ﬁxed. Jacobian\nmatrices play a useful role in systems built from a number of distinct modules, as\nillustrated in Figure 8.3. Each module can comprise a ﬁxed or learnable function,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1257, "text": "which can be linear or nonlinear, so long as it is differentiable.\nSuppose we wish to minimize an error function Ewith respect to the parameter\nwin Figure 8.3. The derivative of the error function is given by\n@E\n@w =\n∑\nk;j\n@E\n@yk\n@yk\n@zj\n@zj\n@w (8.27)\nin which the Jacobian matrix for the red module in Figure 8.3 appears as the middle\nterm on the right-hand side.\nBecause the Jacobian matrix provides a measure of the local sensitivity of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1258, "text": "outputs to changes in each of the input variables, it also allows any known errors∆ xi\n8.1. Evaluation of Gradients 241\nFigure 8.3 Illustration of a modular\ndeep learning architecture in which the\nJacobian matrix can be used to back-\npropagate error signals from the out-\nputs through to earlier modules in the\nsystem.\nu\nwx @yk\n@zj\ny\nv\nz @E\n@yk\n@E\n@zj\nassociated with the inputs to be propagated through the trained network to estimate"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1259, "text": "their contribution ∆yk to the errors at the outputs, through the relation\n∆yk ≃\n∑\ni\n@yk\n@xi\n∆xi; (8.28)\nwhich assumes that the|∆xi|are small. In general, the network mapping represented\nby a trained neural network will be nonlinear, and so the elements of the Jacobian\nmatrix will not be constants but will depend on the particular input vector used. Thus,\n(8.28) is valid only for small perturbations of the inputs, and the Jacobian itself must\nbe re-evaluated for each new input vector."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1260, "text": "be re-evaluated for each new input vector.\nThe Jacobian matrix can be evaluated using a backpropagation procedure that is\nlike the one derived earlier for evaluating the derivatives of an error function with\nrespect to the weights. We start by writing the element Jki in the form\nJki = @yk\n@xi\n=\n∑\nj\n@yk\n@aj\n@aj\n@xi\n=\n∑\nj\nwji\n@yk\n@aj\n(8.29)\nwhere we have made use of (8.5). The sum in (8.29) runs over all units j to which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1261, "text": "the input unit i sends connections (for example, over all units in the ﬁrst hidden\nlayer in the layered topology considered earlier). We now write down a recursive\nbackpropagation formula for the derivatives @yk=@aj:\n@yk\n@aj\n=\n∑\nl\n@yk\n@al\n@al\n@aj\n= h′(aj)\n∑\nl\nwlj\n@yk\n@al\n(8.30)\nwhere the sum runs over all unitslto which unit jsends connections (corresponding\nto the ﬁrst index of wlj). Again, we have made use of (8.5) and (8.6). This back-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1262, "text": "propagation starts at the output units, for which the required derivatives can be found\ndirectly from the functional form of the output-unit activation function. For linear\noutput units, we have\n@yk\n@al\n= \u000ekl (8.31)\n242 8. BACKPROPAGATION\nwhere \u000ekl are the elements of the identity matrix and are deﬁned by\n\u000ekl =\n{\n1; if k= l;\n0; otherwise: (8.32)\nIf we have individual logistic sigmoid activation functions at each output unit, thenSection 3.4\n@yk\n@al\n= \u000ekl\u001b′(al) (8.33)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1263, "text": "@yk\n@al\n= \u000ekl\u001b′(al) (8.33)\nwhereas for softmax outputs, we haveSection 3.4\n@yk\n@al\n= \u000eklyk −ykyl: (8.34)\nWe can summarize the procedure for calculating the Jacobian matrix as follows.\nApply the input vector corresponding to the point in input space at which the Jaco-\nbian matrix is to be evaluated, and forward propagate in the usual way to obtain the\nstates of all the hidden and output units in the network. Next, for each row kof the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1264, "text": "Jacobian matrix, corresponding to the output unit k, backpropagate using the recur-\nsive relation (8.30), starting with (8.31), (8.33) or (8.34), for all the hidden units in\nthe network. Finally, use (8.29) for the backpropagation to the inputs. The Jacobian\ncan also be evaluated using an alternativeforward propagation formalism, which can\nbe derived in an analogous way to the backpropagation approach given here.Exercise 8.5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1265, "text": "Again, the implementation of such algorithms can be checked using numerical\ndifferentiation in the form\n@yk\n@xi\n= yk(xi + \u000f) −yk(xi −\u000f)\n2\u000f + O(\u000f2); (8.35)\nwhich involves 2D forward propagation passes for a network having D inputs and\ntherefore requires O(DW) steps in total.\n8.1.6 The Hessian matrix\nWe have shown how backpropagation can be used to obtain the ﬁrst derivatives\nof an error function with respect to the weights in the network. Backpropagation can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1266, "text": "also be used to evaluate the second derivatives of the error, which are given by\n@2E\n@wji@wlk\n: (8.36)\nIt is often convenient to consider all the weight and bias parameters as elements wi\nof a single vector, denotedw, in which case the second derivatives form the elements\nHij of the Hessian matrix H:\nHij = @2E\n@wi@wj\n(8.37)\n8.1. Evaluation of Gradients 243\nwhere i;j ∈{1;:::;W }and W is the total number of weights and biases. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1267, "text": "Hessian matrix arises in several nonlinear optimization algorithms used for training\nneural networks based on considerations of the second-order properties of the error\nsurface (Bishop, 2006). It also plays a role in some Bayesian treatments of neural\nnetworks (MacKay, 1992; Bishop, 2006) and has been used to reduce the precision\nof the weights in large language models to lessen their memory footprint (Shenet al.,\n2019)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1268, "text": "2019).\nAn important consideration for many applications of the Hessian is the efﬁciency\nwith which it can be evaluated. If there are W parameters (weights and biases) in\nthe network, then the Hessian matrix has dimensions W ×W and so the compu-\ntational effort needed to evaluate the Hessian will scale like O(W2) for each point\nin the data set. Extension of the backpropagation procedure (Bishop, 1992) allows"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1269, "text": "the Hessian matrix to be evaluated efﬁciently with a scaling that is indeed O(W2).Exercise 8.6\nSometimes, we do not need the Hessian matrix explicitly but only the product vTH\nof the Hessian with some vector v, and this product can be calculated efﬁciently\nin O(W) steps using an extension of backpropagation (Møller, 1993; Pearlmutter,\n1994).\nSince neural networks may contain millions or even billions of parameters, eval-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1270, "text": "uating, or even just storing, the full Hessian matrix for many models is infeasible.\nEvaluating the inverse of the Hessian is even more demanding as this has O(W3)\ncomputational scaling. Consequently there is interest in ﬁnding effective approxi-\nmations to the full Hessian.\nOne approximation involves simply evaluating only the diagonal elements of\nthe Hessian and implicitly setting the off-diagonal elements to zero. This requires"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1271, "text": "O(W) storage and allows the inverse to be evaluated inO(W) steps but still requires\nO(W2) computation (Ricotti, Ragazzini, and Martinelli, 1988), although with fur-\nther approximation this can be reduced to O(W) steps (Becker and LeCun, 1989;\nLeCun, Denker, and Solla, 1990). In practice, however, the Hessian generally has\nsigniﬁcant off-diagonal terms, and so this approximation must be treated with care.\nA more convincing approach, known as the outer product approximation, is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1272, "text": "obtained as follows. Consider a regression application using a sum-of-squares error\nfunction of the form\nE = 1\n2\nN∑\nn=1\n(yn −tn)2 (8.38)\nwhere we have considered a single output to keep the notation simple (the extension\nto several outputs is straightforward). We can then write the Hessian matrix in theExercise 8.8\nform\nH = ∇∇E =\nN∑\nn=1\n∇yn(∇yn)T +\nN∑\nn=1\n(yn −tn)∇∇yn (8.39)\nwhere ∇denotes the gradient with respect to w. If the network has been trained"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1273, "text": "on the data set and its outputs yn are very close to the target values tn, then the\nﬁnal term in (8.39) will be small and can be neglected. More generally, however,\nit may be appropriate to neglect this term based on the following argument. Recall\nfrom Section 4.2 that the optimal function that minimizes a sum-of-squares loss is\n244 8. BACKPROPAGATION\nthe conditional average of the target data. The quantity (yn −tn) is then a random"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1274, "text": "variable with zero mean. If we assume that its value is uncorrelated with the value\nof the second derivative term on the right-hand side of (8.39), then the whole term\nwill average to zero in the summation over n.Exercise 8.9\nBy neglecting the second term in (8.39), we arrive at the Levenberg–Marquardt\napproximation, also known as the outer product approximation because the Hessian\nmatrix is built up from a sum of outer products of vectors, given by\nH ≃\nN∑\nn=1\n∇an∇aT\nn: (8.40)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1275, "text": "H ≃\nN∑\nn=1\n∇an∇aT\nn: (8.40)\nEvaluating the outer product approximation for the Hessian is straightforward as it\ninvolves only ﬁrst derivatives of the error function, which can be evaluated efﬁciently\nin O(W) steps using standard backpropagation. The elements of the matrix can then\nbe found in O(W2) steps by simple multiplication. It is important to emphasize\nthat this approximation is likely to be valid only for a network that has been trained"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1276, "text": "appropriately, and that for a general network mapping, the second derivative terms\non the right-hand side of (8.39) will typically not be negligible.\nFor a cross-entropy error function for a network with logistic-sigmoid output-\nunit activation functions, the corresponding approximation is given byExercise 8.10\nH ≃\nN∑\nn=1\nyn(1 −yn)∇an∇aT\nn: (8.41)\nAn analogous result can be obtained for multi-class networks having softmax output-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1277, "text": "unit activation functions. The outer product approximation can also be used to de-Exercise 8.11\nvelop an efﬁcient sequential procedure for approximating the inverse of a HessianExercise 8.12\n(Hassibi and Stork, 1993).\n8.2.\nAutomatic Differentiation\nWe have seen the importance of using gradient information to train neural networks\nefﬁciently. There are essentially four ways in which the gradient of a neural networkSection 7.2.1\nerror function can be evaluated."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1278, "text": "error function can be evaluated.\nThe ﬁrst approach, which formed the mainstay of neural networks for many\nyears, is to derive the backpropagation equations by hand and then to implement\nthem explicitly in software. If this is done carefully it results in efﬁcient code that\ngives precise results that are accurate to numerical precision. However, the process\nof deriving the equations as well as the process of coding them both take time and are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1279, "text": "prone to errors. It also results in some redundancy in the code because the forward\npropagation equations are coded separately from the backpropagation equations. As\nthese often involve duplicated calculations, then if the model is altered, both the\nforward and backward implementations need to be changed in unison. This effort\n8.2. Automatic Differentiation 245\ncan easily become a limitation on how quickly and effectively different architectures\ncan be explored empirically."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1280, "text": "can be explored empirically.\nA second approach is to evaluate the gradients numerically using ﬁnite differ-\nences. This requires only a software implementation of the forward propagationSection 8.1.4\nequations. One problem with numerical differentiation is that it has limited com-\nputational accuracy, although this is unlikely to be an issue for network training as\nwe may be using stochastic gradient descent in which each evaluation is only a very"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1281, "text": "noisy estimate of the local gradient. The main drawback of this approach is that it\nscales poorly with the size of the network. However, the technique is useful for de-\nbugging other approaches, because the gradients are evaluated using only the forward\npropagation code and so can be used to conﬁrm the correctness of backpropagation\nor other code used to evaluate gradients.\nA third approach is called symbolic differentiation and makes use of specialist"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1282, "text": "software to automate the analytical manipulations that are done by hand in the ﬁrst\napproach. This process is an example of computer algebra or symbolic computation\nand involves the automatic application of the rules of calculus, such as the chain\nrule, in a completely mechanistic process. The resulting expressions are then imple-\nmented in standard software. An obvious advantage of this approach is that it avoids\nhuman error in the manual derivation of the backpropagation equations. Moreover,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1283, "text": "the gradients are again calculated to machine precision, and the poor scaling seen\nwith numerical differentiation is avoided. The major downside of symbolic differ-\nentiation, however, is that the resulting expressions for derivatives can become ex-\nponentially longer than the original function, with correspondingly long evaluation\ntimes. Consider a function f(x) given by the product ofu(x) and v(x). The function\nand its derivative are given by\nf(x) = u(x)v(x) (8.42)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1284, "text": "f(x) = u(x)v(x) (8.42)\nf′(x) = u′(x)v(x) + u(x)v′(x): (8.43)\nWe see that there is redundant computation in that u(x) and v(x) must be evalu-\nated both for the calculation of f(x) and for f′(x). If the factors u(x) and v(x)\nthemselves involve factors, then we end up with a nested duplication of expressions,\nwhich rapidly grow in complexity. This problem is called expression swell.\nAs a further illustration, consider a function that is structured like two layers of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1285, "text": "a neural network (Grosse, 2018) with a single input x, a hidden unit with activation\nz, and an output yin which\nz= h(w1x+ b1) (8.44)\ny= h(w2z+ b2) (8.45)\nwhere h(a) is the soft ReLU:\n\u0010(a) = ln (1 + exp(a)) : (8.46)\nThe overall function is therefore given by\ny(x) = h(w2h(w1x+ b1) + b2) (8.47)\n246 8. BACKPROPAGATION\nand the derivative of the network output with respect to w1, evaluated symbolically,\nis given byExercise 8.13\n@y\n@w1\n= w2xexp\n(\nw1x+ b1 + b2 + w2 ln[1 + ew1x+b1 ]\n)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1286, "text": "= w2xexp\n(\nw1x+ b1 + b2 + w2 ln[1 + ew1x+b1 ]\n)\n(1 + ew1x+b1 ) (1 + exp(b2 + w2 ln[1 + ew1x+b1 ])): (8.48)\nAs well as being signiﬁcantly more complex than the original function, we also see\nredundant computation where expressions such as w1x+ b1 occur in several places.\nA further major drawback with symbolic differentiation is that it requires that\nthe expression to be differentiated is expressed in closed form. It therefore excludes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1287, "text": "important control ﬂow operations such as loops, recursions, conditional execution,\nand procedure calls, which are valuable constructs that we might wish to use when\ndeﬁning the network function.\nWe therefore turn to the fourth technique for evaluating derivatives in neural\nnetworks called automatic differentiation, also known as ‘autodiff’ or ‘algorithmic\ndifferentiation’ (Baydin et al., 2018). Unlike symbolic differentiation, the goal of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1288, "text": "automatic differentiation is not to ﬁnd a mathematical expression for the derivatives\nbut to have the computer automatically generate the code that implements the gra-\ndient calculations given only the code for the forward propagation equations. It\nis accurate to machine precision, just as with symbolic differentiation, but is more\nefﬁcient because it is able to exploit intermediate variables used in the deﬁnition\nof the forward propagation equations and thereby avoid redundant evaluations. It"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1289, "text": "is important to note that not only can automatic differentiation handle conventional\nclosed-form mathematical expressions but it can also deal with ﬂow control elements\nsuch as branches, loops, recursion, and procedure calls, and is therefore signiﬁcantly\nmore powerful than symbolic differentiation. Automatic differentiation is a well-\nestablished ﬁeld with broad applicability that was developed largely outside of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1290, "text": "machine learning community. Modern deep learning is a largely empirical process,\ninvolving evaluating and comparing different architectures, and automatic differenti-\nation therefore plays a key role in enabling this experimentation to be done accurately\nand efﬁciently.\nThe key idea of automatic differentiation is to take the code that evaluates a func-\ntion, for example the forward propagation equations that evaluate the error function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1291, "text": "for a neural network, and augment the code with additional variables whose values\nare accumulated during code execution to obtain the required derivatives. There are\ntwo principal forms of automatic differentiation, known as forward mode and re-\nverse mode. We start by looking at forward mode, which is conceptually somewhat\nsimpler.\n8.2.1 Forward-mode automatic differentiation\nIn forward-mode automatic differentiation, we augment each intermediate vari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1292, "text": "able zi, known as a ‘primal’ variable, involved in the evaluation of a function, such\nas the error function of a neural network, with an additional variable representing\nthe value of some derivative of that variable, which we can denote ˙zi, known as a\n‘tangent’ variable. The tangent variables and their associated code are generated\n8.2. Automatic Differentiation 247\nFigure 8.4 Evaluation trace diagram showing\nthe steps involved in the numerical evaluation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1293, "text": "the steps involved in the numerical evaluation\nof the function (8.49) using the primal equations\n(8.50) to (8.56).\nx1 v1\nx1\nv3\nv1v2\nv5\nexp(v3)\nv7\nv5 + v6\nf\nx2 v2\nx2\nv4\nsin(v2)\nv6\nv3 −v4\nautomatically by the software environment. Instead of simply doing forward prop-\nagation to compute {zi}, the code now propagates tuples (zi;˙zi) so that variables\nand derivatives are evaluated in parallel. The original function is generally deﬁned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1294, "text": "in terms of elementary operators consisting of arithmetic operations and negation\nas well as transcendental functions such as exponential, logarithm, and trigonomet-\nric functions, all of which have simple formulae for their derivatives. Using these\nderivatives in combination with the chain rule of calculus allows the code used to\nevaluate gradients to be constructed automatically.\nAs an example, consider the following function, which has two input variables:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1295, "text": "f(x1;x2) = x1x2 + exp(x1x2) −sin(x2): (8.49)\nWhen implemented in software, the code consists of a sequence of operations that\ncan be expressed as an evaluation trace of the underlying elementary operations.\nThis trace can be visualized in the form of a graph, as shown in Figure 8.4. Here we\nhave deﬁned the following primal variables\nv1 = x1 (8.50)\nv2 = x2 (8.51)\nv3 = v1v2 (8.52)\nv4 = sin(v2) (8.53)\nv5 = exp(v3) (8.54)\nv6 = v3 −v4 (8.55)\nv7 = v5 + v6: (8.56)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1296, "text": "v6 = v3 −v4 (8.55)\nv7 = v5 + v6: (8.56)\nNow suppose we wish to evaluate the derivative @f=@x1. We deﬁne the tangent\nvariables by ˙vi = @vi=@x1. Expressions for evaluating these can be constructed\nautomatically using the chain rule of calculus:\n˙vi = @vi\n@x1\n=\n∑\nj∈pa(i)\n@vj\n@x1\n@vi\n@vj\n=\n∑\nj∈pa(i)\n˙vj\n@vi\n@vj\n: (8.57)\nwhere pa(i) denotes the set of parents of the node iin the evaluation trace diagram,\nthat is the set of variables with arrows pointing to nodei. For example, in Figure 8.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1297, "text": "the parents of node v3 are nodes v1 and v2. Applying (8.57) to the evaluation trace\n248 8. BACKPROPAGATION\nFigure 8.5 Extension of the example shown in\nFigure 8.4 to a function with two outputs f1 and\nf2. x1 v1\nx1\nv3\nv1v2\nv5\nexp(v3)\nv7\nv5 + v6\nf1\nx2 v2\nx2\nv4\nsin(v2)\nv6\nv3 −v4\nv8\nv5v6\nf2\nequations (8.50) to (8.56), we obtain the following evaluation trace equations for the\ntangent variables\n˙v1 = 1 (8.58)\n˙v2 = 0 (8.59)\n˙v3 = v1 ˙v2 + ˙v1v2 (8.60)\n˙v4 = ˙v2 cos(v2) (8.61)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1298, "text": "˙v4 = ˙v2 cos(v2) (8.61)\n˙v5 = ˙v3 exp(v3) (8.62)\n˙v6 = ˙v3 −˙v4 (8.63)\n˙v7 = ˙v5 + ˙v6: (8.64)\nWe can summarize automatic differentiation for this example as follows. We\nﬁrst write code to implement the evaluation of the primal variables, given by (8.50) to\n(8.56). The associated equations and corresponding code for evaluating the tangent\nvariables (8.58) to (8.64) are generated automatically. To evaluate the derivative"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1299, "text": "@f=@x1, we input speciﬁc values of x1 and x2 and the code then executes the primal\nand tangent equations, numerically evaluating the tuples (vi;˙vi) in sequence until\nwe obtain ˙v5, which is the required derivative.Exercise 8.17\nNow consider an example with two outputs f1(x1;x2) and f2(x1;x2) where\nf1(x1;x2) is deﬁned by (8.49) and\nf2(x1;x2) = (x1x2 −sin(x2)) exp(x1x2) (8.65)\nas illustrated by the evaluation trace diagram in Figure 8.5. We see that this in-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1300, "text": "volves only a small extension to the evaluation equations for the primal and tangent\nvariables, and so both @f1=@x1 and @f2=@x1 can be evaluated together in a single\nforward pass. The downside, however, is that if we wish to evaluate derivatives with\nrespect to a different input variable x2 then we have to run a separate forward pass.\nIn general, if we have a function with D inputs and K outputs then a single pass\nof forward-mode automatic differentiation produces a single column of the K×D"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1301, "text": "Jacobian matrix:\nJ =\n\n\n@f1\n@x1\n::: @f1\n@xD\n..\n. … .\n.\n.\n@fK\n@x1\n::: @fK\n@xD\n\n\n: (8.66)\n8.2. Automatic Differentiation 249\nTo compute column j of the Jacobian, we need to initialize the forward pass of the\ntangent equations by setting ˙xj = 1 and ˙xi = 0 for i̸=j. We can write this in vector\nform as ˙x = ei where ei is the ith unit vector. To compute the full Jacobian matrix\nwe need Dforward-mode passes. However, if we wish to evaluate the product of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1302, "text": "Jacobian with a vector r = (r1;:::;r D)T:\nJ =\n\n\n@f1\n@x1\n::: @f1\n@xD\n..\n. … .\n.\n.\n@fK\n@x1\n::: @fK\n@xD\n\n\n\n\n\n\nr1\n..\n.\nrD\n\n\n(8.67)\nthen this can be done in single forward pass by setting ˙x = r.Exercise 8.18\nWe see that forward-mode automatic differentiation can evaluate the fullK×D\nJacobian matrix of derivatives usingDforward passes. This is very efﬁcient for net-\nworks with a few inputs and many outputs, such that K ≫ D. However, we often"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1303, "text": "operate in a regime where we often have just one function, namely the error function\nused for training, and large numbers of variables that we want to differentiate with\nrespect to, comprising the weights and biases in the network, of which there may\nbe millions or billions. In such situations, forward-mode automatic differentiation is\nextremely inefﬁcient. We therefore turn to an alternative version of automatic dif-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1304, "text": "ferentiation based on the a backwards ﬂow of derivative data through the evaluation\ntrace graph.\n8.2.2 Reverse-mode automatic differentiation\nWe can think of reverse-mode automatic differentiation as a generalization of\nthe error backpropagation procedure. As with forward mode, we augment each in-\ntermediate variable vi with additional variables, in this case called adjoint variables,\ndenoted\nvi. Consider again a situation with a single output function f for which the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1305, "text": "adjoint variables are deﬁned by\nvi = @f\n@vi\n: (8.68)\nThese can be evaluated sequentially starting with the output and working backwards\nby using the chain rule of calculus:\nvi = @f\n@vi\n=\n∑\nj∈ch(i)\n@f\n@vj\n@vj\n@vi\n=\n∑\nj∈ch(i)\nvj\n@vj\n@vi\n: (8.69)\nHere ch(i) denotes the children of node i in the evaluation trace graph, in other\nwords the set of nodes that have arrows pointing into them from nodei. The succes-\nsive evaluation of the adjoint variables represents a ﬂow of information backwards"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1306, "text": "through the graph, as we saw previously.Figure 8.1\nIf we again consider the speciﬁc example function given by (8.50) to (8.56), we\nobtain the following evaluation equations for the evaluation of the adjoint variablesExercise 8.16\n250 8. BACKPROPAGATION\nv7 = 1 (8.70)\nv6 = v7 (8.71)\nv5 = v7 (8.72)\nv4 = −v6 (8.73)\nv3 = v5v5 + v6 (8.74)\nv2 = v2v1 + v4 cos(v2) (8.75)\nv1 = v3v2: (8.76)\nNote that these start at the output and then ﬂow backwards through the graph to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1307, "text": "inputs. Even with multiple inputs, only a single backward pass is required to evaluate\nthe derivatives. For a neural network error function, the derivatives ofEwith respect\nto the weight and biases are obtained as the corresponding adjoint variables. How-\never, if we now have more than one output then we need to run a separate backwardFigure 8.5\npass for each output variable.\nReverse mode is often more memory intensive than forward mode because all"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1308, "text": "of the intermediate primal variables must be stored so that they will be available as\nneeded when evaluating the adjoint variables during the backward pass. By contrast,\nwith forward mode, the primal and tangent variables are computed together during\nthe forward pass, and therefore variables can be discarded once they have been used.\nIt is therefore also generally easier to implement forward mode compared to reverse\nmode."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1309, "text": "mode.\nFor both forward-mode and reverse-mode automatic differentiation, a single\npass through the network is guaranteed to take no more than 6 times the compu-\ntational cost of a single function evaluation. In practice, the overhead is typically\ncloser to a factor of 2 or 3 (Griewank and Walther, 2008). Hybrids of forward and\nreverse modes are also of interest. One situation in which this arises is in the eval-\nuation of the product of a Hessian matrix with a vector, which can be calculated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1310, "text": "without explicit evaluation of the full Hessian (Pearlmutter, 1994). Here we can use\nreverse mode to calculate the gradient of code, which itself has been generated by the\nforward model. We start from a vector b and a point x at which the Hessian–vector\nproduct is to be evaluated. By setting ˙x = v and using forward mode, we obtain\nthe directional derivative vT∇f. This is then differentiated using reverse mode to\nobtain ∇2fv = Hv. If W is the number of parameters in the neural network then"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1311, "text": "this evaluation has O(W) complexity even though the Hessian is of size W ×W.\nThe Hessian itself can also be evaluated explicitly using automatic differentiation\nbut this has O(W2) complexity.\nExercises\n8.1 (?) By making use of (8.5), (8.6), (8.8), and (8.12), verify the backpropagation for-\nmula (8.13) for evaluating the derivatives of an error function.\n8.2 (??) Consider a network that consists of layers and rewrite the backpropagation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1312, "text": "formula (8.13) in matrix notation by starting with the forward propagation equation\n(6.19). Note that the result involves multiplication by the transposes of the matrices.\nExercises 251\n8.3 (?) By using a Taylor expansion, verify that the terms that are O(\u000f) cancel on the\nright-hand side of (8.25).\n8.4 (??) Consider a two-layer network of the form shown inFigure 6.9 with the addition\nof extra parameters corresponding to skip-layer connections that go directly from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1313, "text": "inputs to the outputs. By extending the discussion of Section 8.1.3, write down the\nequations for the derivatives of the error function with respect to these additional\nparameters.\n8.5 (???) In Section 8.1.5, we derived a procedure for evaluating the Jacobian matrix of\na neural network using a backpropagation procedure. Derive an alternative formal-\nism for ﬁnding the Jacobian based on forward propagation equations.\n8.6 (???) Consider a two-layer neural network, and deﬁne the quantities"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1314, "text": "\u000ek = @En\n@ak\n; M kk′≡ @2En\n@ak@ak′\n: (8.77)\nDerive expressions for the elements of the Hessian matrix expressed in terms of \u000ek\nand Mkk′for elements in which (i) both weights are in the second layer, (ii) both\nweights are in the ﬁrst layer, and (iii) one weight is in each layer.\n8.7 (???) Extend the results of Exercise 8.6 for the exact Hessian of a two-layer network\nto include skip-layer connections that go directly from inputs to outputs."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1315, "text": "8.8 (??) The outer product approximation to the Hessian matrix for a neural network us-\ning a sum-of-squares error function is given by (8.40). Extend this result for multiple\noutputs.\n8.9 (??) Consider a squared-loss function of the form\nE(w) = 1\n2\n∫∫\n{y(x;w) −t}2 p(x;t) dx dt (8.78)\nwhere y(x;w) is a parametric function such as a neural network. The result (4.37)\nshows that the function y(x;w) that minimizes this error is given by the conditional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1316, "text": "expectation of tgiven x. Use this result to show that the second derivative ofEwith\nrespect to two elements wr and ws of the vector w, is given by\n@2E\n@wr@ws\n=\n∫ @y\n@wr\n@y\n@ws\np(x) dx: (8.79)\nNote that, for a ﬁnite sample from p(x), we obtain (8.40).\n8.10 (??) Derive the expression (8.41) for the outer product approximation of a Hessian\nmatrix for a network having a single output with a logistic-sigmoid output-unit acti-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1317, "text": "vation function and a cross-entropy error function, corresponding to the result (8.40)\nfor the sum-of-squares error function.\n252 8. BACKPROPAGATION\n8.11 (??) Derive an expression for the outer product approximation of a Hessian matrix\nfor a network having K outputs with a softmax output-unit activation function and\na cross-entropy error function, corresponding to the result (8.40) for the sum-of-\nsquares error function.\n8.12 (???) Consider the matrix identity\n(\nM + vvT)−1\n= M−1 −(M−1 v)\n("}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1318, "text": "(\nM + vvT)−1\n= M−1 −(M−1 v)\n(\nvTM−1 )\n1 + vTM−1 v ; (8.80)\nwhich is simply a special case of the Woodbury identity (A.7). By applying (8.80)\nto the outer product approximation (8.40) for a Hessian matrix, derive a formula that\nallows the inverse of the Hessian matrix to be computed by making one pass through\nthe training data and updating the inverse Hessian with each data point. Note that\nthe algorithm can initialized using H = \u000bI where \u000bis a small constant, and that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1319, "text": "results are not particularly sensitive to the precise value of \u000b.\n8.13 (??) Verify that the derivative of (8.47) is given by (8.48).\n8.14 (??) The logistic map is a function deﬁned by the iterative relation Ln+1(x) =\n4Ln(x) (1−Ln(x)) with L1(x) = x. Write down the evaluation trace equations for\nL2(x), L3(x), and L4(x), and then write down expressions for the corresponding\nderivatives L′\n1(x), L′\n2(x), L′\n3(x), and L′\n4(x). Do not simplify the expressions but"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1320, "text": "4(x). Do not simplify the expressions but\ninstead simply note how the complexity of the formulae for the derivatives grows\nmuch more rapidly than the expressions for the functions themselves.\n8.15 (??) Starting from the evaluation trace equations (8.50) to (8.56) for the example\nfunction (8.49), use (8.57) to derive the forward-mode tangent variable evaluation\nequations (8.58) to (8.64).\n8.16 (??) Starting from the evaluation trace equations (8.50) to (8.56) for the example"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1321, "text": "function (8.49), and referring to Figure 8.4, use (8.69) to derive the reverse-mode\nadjoint variable evaluation equations (8.70) to (8.76).\n8.17 (???) Consider the example function (8.49). Write down an expression for @f=@x1\nand evaluate this function forx1 = 1 and x2 = 2. Now use the evaluation trace equa-\ntions (8.50) to (8.56) to evaluate the variables v1 to v7 and then use the evaluation\ntrace equations of forward-mode automatic differentiation to evaluate the tangent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1322, "text": "variables ˙v1 to ˙v7 and to conﬁrm that the resulting value of @f=@x1 agrees with that\nfound directly. Similarly, use the evaluation trace equations of reverse-mode auto-\nmatic differentiation (8.70) to (8.76) to evaluate the adjoint variables\nv7 to v1 and\nagain conﬁrm that the resulting value of @f=@x1 agrees with that found directly.\n8.18 (??) By expressing an arbitrary vector r = (r1;:::;r D)T as a linear combination of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1323, "text": "the unit vectors ei, where i = 1;:::;D , show that the product of the Jacobian of a\nfunction with r in the form (8.67) can be evaluated using a single pass of forward-\nmode automatic differentiation by setting ˙x = r.\n9\nRegularization\nWe introduced the concept of regularization when discussing polynomial curve ﬁt-\nting as a way to reduce over-ﬁtting by discouraging the parameters of the model fromSection 1.2\ntaking values with a large magnitude. This involved adding a simple penalty term to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1324, "text": "the error function to give a regularized error function in the form\n˜E(w) = E(w) + \u0015\n2 wTw (9.1)\nwhere w is the vector of model parameters,E(w) is the unregularized error function,\nand the regularization hyperparameter \u0015controls the strength of the regularization\neffect. An improvement in predictive accuracy with such a regularizer can be under-\nstood in terms of the bias–variance trade-off through the reduction in the variance ofSection 4.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1325, "text": "the solution at the expense of some increase in bias. In this chapter we will explore\nregularization in depth and will discuss several different approaches to regulariza-\n253© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1326, "text": "254 9. REGULARIZATION\ntion. We will also look more broadly at the important role of bias in achieving good\ngeneralization from ﬁnite training data sets.\nIn a practical application, it is very unlikely that the process that generates the\ndata will correspond precisely to a particular neural network architecture, and so\nany given neural network will only ever represent an approximation to the true data\ngenerator. Larger networks can provide closer approximations, but this comes at the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1327, "text": "risk of over-ﬁtting. In practice, we ﬁnd that the best generalization results are almost\nalways obtained by using a larger network combined with some form of regular-\nization. In this chapter we explore several alternative regularization techniques in-\ncluding early stopping, model averaging, dropout, data augmentation, and parameter\nsharing. Multiple forms of regularization can be used together if desired. For exam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1328, "text": "ple, error function regularization of the form (9.1) is often used alongside dropout.\n9.1.\nInductive Bias\nWhen we compared the predictive error of polynomials of various orders for the si-\nnusoidal synthetic data problem, we saw that the smallest generalization error wasSection 1.2.4\nachieved using a polynomial of intermediate complexity, being neither too simple\nnor too ﬂexible. A similar result was found when we used a regularization term of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1329, "text": "the form (9.1) to control model complexity, as an intermediate value of the regular-Section 1.2.5\nization coefﬁcient \u0015gave the best predictions for new input values. Insight into this\nresult came from the bias–variance decomposition, where we saw that an appropriateSection 4.3\nlevel of bias in the model was important to allow generalization from ﬁnite data sets.\nSimple models with high bias are unable to capture the variation in the underlying"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1330, "text": "data generation process, whereas highly ﬂexible models with low bias are prone to\nover-ﬁtting leading to poor generalization. As the size of the data set grows, we can\nafford to use more ﬂexible models having less bias without incurring excessive vari-\nance, thereby leading to improved generalization. Note that in a practical setting, our\nchoice of model might also be inﬂuenced by factors such as memory usage or speed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1331, "text": "of execution. Here we ignore such ancillary considerations and focus on the core\ngoal of achieving good predictive performance, in other words good generalization.\n9.1.1 Inverse problems\nThis issue of model choice lies at the heart of machine learning and can be\ntraced to the fact that most machine learning tasks are examples of inverse prob-\nlems. Given a conditional distribution p(t|x)along with a ﬁnite set of input points"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1332, "text": "{x1;:::; xN}, it is straightforward, at least in principle, to sample corresponding\nvalues {t1;:::;t N}from that distribution. In machine learning, however, we have\nto solve the inverse of this problem, namely to infer an entire distribution given only a\nﬁnite number of samples. This is intrinsically ill-posed, as there are inﬁnitely many\ndistributions which could potentially have been responsible for generating the ob-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1333, "text": "served training data. In fact any distribution that has a non-zero probability density\nat the observed target values is a candidate.\n9.1. Inductive Bias 255\nFor machine learning to be useful, however, we need to make predictions for\nnew values of x, and therefore we need a way to choose a speciﬁc distribution from\namongst the inﬁnitely many possibilities. The preference for one choice over oth-\ners is called inductive bias, or prior knowledge, and plays a central role in machine"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1334, "text": "learning. Prior knowledge may come from background information that helps con-\nstrain the space of solutions. For many applications, small changes in the input\nvalues should lead to small changes in the output values, and so we should bias our\nsolutions towards those with smoothly varying functions. Regularization terms of\nthe form (9.1) encourage the model weights to have a smaller magnitude and hence\nintroduce a bias towards functions that vary more slowly with changes in the inputs."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1335, "text": "Likewise, when detecting objects in images, we can introduce prior knowledge that\nthe identity of an object is generally independent of its location within the image.Chapter 10\nThis is known as translation invariance, and incorporating this into our solution can\ngreatly simplify the task of building a system with good generalization. However,\ncare must be taken not to incorporate biases or constraints that are inconsistent with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1336, "text": "the underlying process that generates the data. For example, assuming that the re-\nlationship between outputs and inputs is linear when in fact there are signiﬁcant\nnonlinearities can lead to a system that produces inaccurate answers.\nTechniques such as transfer learning and multi-task learning can also be viewedSection 6.3.4\nfrom the perspective of regularization. When training data for a particular task is\nlimited, additional data from a different, but related, task can be used to help de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1337, "text": "termine the learnable parameters in a neural network. The assumption of similarity\nbetween the tasks represents a more sophisticated form of inductive bias compared\nto simple regularization, and this explains the improved performance resulting from\nthe use of the additional data.\n9.1.2 No free lunch theorem\nThe core focus of this book is on the important class of machine learning mod-\nels called deep neural networks. These are highly ﬂexible models and have revo-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1338, "text": "lutionized many ﬁelds including computer vision, speech recognition, and natural\nlanguage processing. In fact, they have become the framework of choice for the\ngreat majority of machine learning applications. It might appear, therefore, that they\nrepresent a ‘universal’ learning algorithm able to solve all tasks. However, even very\nﬂexible neural networks contain important inductive biases. For example, convolu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1339, "text": "tional neural networks encode speciﬁc forms of inductive bias, including translationChapter 10\nequivariance, that are especially useful in applications involving images.\nThe no free lunch theorem(Wolpert, 1996), named from the expression ‘There’s\nno such thing as a free lunch,’ states that every learning algorithm is as good as any\nother when averaged over all possible problems. If a particular model or algorithm\nis better than average on some problems, it must be worse than average on others."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1340, "text": "However, this is a rather theoretical notion as the space of possible problems here\nincludes relationships between input and output that would be very uncharacteristic\nof any plausible practical application. For example, we have already noted that most\nexamples of practical interest exhibit some degree of smoothness, in which small\nchanges in the input values are associated, for the most part, with small changes in\n256 9. REGULARIZATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1341, "text": "256 9. REGULARIZATION\nthe target values. Models such as neural networks, and indeed most widely used\nmachine learning techniques, exhibit this form of inductive bias, and therefore to\nsome degree, they have broad applicability.\nAlthough the no free lunch theorem is somewhat theoretical, it does highlight\nthe central importance of bias in determining the performance of a machine learning\nalgorithm. It is not possible to learn ‘purely from data’ in the absence of any bias. In"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1342, "text": "practice, bias may be implicit. For example, every neural network has a ﬁnite number\nof parameters, which therefore limits the functions that it can represent. However,\nbias may also be encoded explicitly as a reﬂection of prior knowledge relating to the\nspeciﬁc type of problem being solved.\nIn trying to ﬁnd general-purpose learning algorithms, we are really seeking in-\nductive biases that are appropriate to the broad classes of applications that will be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1343, "text": "encountered in practice. For any given application, however, better results can be ob-\ntained if it is possible to incorporate stronger inductive biases that are speciﬁc to that\napplication. The perspective of model-based machine learning (Winn et al., 2023)\nadvocates making all the assumptions in machine learning models explicit so that\nthe appropriate choices can be made for inductive biases.\nWe have seen that inductive bias can be incorporated through the form of dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1344, "text": "tribution, for example by specifying that the output is a linear function of a ﬁxed\nset of speciﬁc basis functions. It can also be incorporated through the addition of\na regularization term to the error function used during training. Yet another way to\ncontrol the complexity of a neural network is through the training process itself. WeChapter 7\nwill see that deep neural networks can give good generalization even when the num-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1345, "text": "ber of adjustable parameters exceeds the number of training data points, provided\nthe training process is set up correctly. Part of the skill in applying deep learning to\nreal-world problems is in the careful design of inductive bias and the incorporation\nof prior knowledge.\n9.1.3 Symmetry and invariance\nIn many applications of machine learning, the predictions should be unchanged,\nor invariant, under one or more transformations of the input variables. For example,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1346, "text": "when classifying an object in two-dimensional images, such as a cat or dog, a par-\nticular object should be assigned the same classiﬁcation irrespective of its position\nwithin the image. This is known as translation invariance. Likewise changes to the\nsize of the object within the image should also leave its classiﬁcation unchanged.\nThis is called scale invariance. Exploiting such symmetries to create inductive bi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1347, "text": "ases can greatly improve the performance of machine learning models and forms the\nsubject of geometric deep learning (Bronstein et al., 2021).\nTransformations, such as a translation or scaling, that leave particular properties\nunchanged, represent symmetries. The set of all transformations corresponding to a\nparticular symmetry form a mathematical structure called a group. A group consists\nof a set of elementsA;B;C;::: together with a binary operation for composing pairs"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1348, "text": "of elements together, which we denote using the notation A-B. The following four\naxioms hold for a group:\n9.1. Inductive Bias 257\n1. Closed: For any two elements A;Bin the set, A-B must also be in the set.\n2. Associative: For any three elementsA;B;Cin the set,(A-B)-C= A-(B-C).\n3. Identity: There exists an element Iof the set, called the identity, with the\nproperty: A-I = I-A = Afor every element Ain the set.\n4. Inverse: For each element Ain the set, there exists another element in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1349, "text": "set, which we denote by A−1 , called the inverse, which has the property: A-\nA−1 = A−1 -A= I.\nSimple examples of groups include the set of rotations of a square through multiples\nof 90-or the set of continuous translations of an object in a two-dimensional plane.Exercise 9.1\nIn principle, invariance of the predictions made by a neural network to trans-\nformations of the input space could be learned from data, without any special mod-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1350, "text": "iﬁcations to the network or the training procedure. In practice, however, this can\nprove to be extremely challenging because such transformations can produce sub-\nstantial changes in the raw data. For example, relatively small translations of an\nobject within an image, even by a few pixels, can cause pixel values to change sig-\nniﬁcantly. Furthermore, multiple invariances must often hold at the same time, for\nexample invariance to translations in two dimensions as well as scaling, rotation,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1351, "text": "changes of intensity, changes of colour balance, and many others. There are expo-\nnentially many possible combinations of such transformations, making the size of\nthe required training set needed to learn all of these invariances prohibitive.\nWe therefore seek more efﬁcient approaches for encouraging an adaptive model\nto exhibit the required invariances. These can broadly be divided into four categories:\n1. Pre-processing. Invariances are built into a pre-processing stage by computing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1352, "text": "features of the data that are invariant under the required transformations. Any\nsubsequent regression or classiﬁcation system that uses such features as inputs\nwill necessarily also respect these invariances.\n2. Regularized error function. A regularization term is added to the error func-\ntion to penalize changes in the model output when the input is subject to one\nof the invariant transformations.\n3. Data augmentation. The training set is expanded using replicas of the training"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1353, "text": "data points, transformed according to the desired invariances and carrying the\nsame output target values as the untransformed examples.\n4. Network architecture. The invariance properties are built into the structure\nof a neural network through an appropriate choice of network architecture.\nOne challenge with approach 1 is to design features that exhibit the required\ninvariances without also discarding information that can be useful for determining"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1354, "text": "the network outputs. We have already seen that ﬁxed, hand-crafted features have\nlimited capabilities and have been superseded by learned representations obtainedChapter 6\nusing deep neural networks.\n258 9. REGULARIZATION\n(a)\n (b)\n (c)\n (d)\n(e)\n (f)\n (g)\n (h)\nFigure 9.1 Illustration of data set augmentation, showing (a) the original image, (b) horizontal inversion, (c)\nscaling, (d) translation, (e) rotation, (f) brightness and contrast change, (g) additive noise, and (h) colour shift."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1355, "text": "An example of approach 2 is the technique oftangent propagation(Simard et al.,\n1992) in which a regularisation term is added to the error function during training.\nThis term directly penalizes changes in the output resulting from changes in the input\nvariables that correspond to one of the invariant transformations. A limitation of this\ntechnique, in addition to the extra complexity of training, is can only cope with small\ntransformations (e.g., translations by less than a pixel)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1356, "text": "Approach 3 is known as data set augmentation. It is often relatively easy to\nimplement and can prove to be very effective in practice. It is often applied in the\ncontext of image analysis as it straightforward to create the transformed training data.\nFigure 9.1 shows examples of such transformations applied to an image of a cat.\nFor medical images of soft tissue, data augmentation could also include continuous\n‘rubber sheet’ deformations (Ronneberger, Fischer, and Brox, 2015)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1357, "text": "For sequential training algorithms, such as stochastic gradient descent, the data\nset can be augmented by transforming each input data point before it is presented\nto the model so that, if the data points are being recycled, a different transformation\n(drawn from an appropriate distribution) is applied each time. For batch methods, a\nsimilar effect can be achieved by replicating each data point a number of times and\ntransforming each copy independently."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1358, "text": "transforming each copy independently.\nWe can analyse the effect of using augmented data by considering transforma-\ntions that represent small changes to the original examples and then making a Taylor\nexpansion of the error function in powers of the magnitude of the transformation\n(Bishop, 1995c; Leen, 1995; Bishop, 2006). This leads to a regularized error func-\ntion in which the regularizer penalizes the gradient of the network output with respect\n9.1. Inductive Bias 259"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1359, "text": "9.1. Inductive Bias 259\nto the input variables projected onto the direction of transformation. This is related\nto the technique of tangent propagation discussed above. A special case arises when\nthe transformation of the input variables consists simply of the addition of random\nnoise, in which case the regularizer penalizes the derivatives of the network outputs\nwith respect to the inputs. Again, this is intuitively reasonable, since we are encour-Exercise 9.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1360, "text": "aging the network outputs to remain unchanged despite the addition of noise to the\ninput variables.\nFinally, approach 4, in which we build invariances into the structure of the net-\nwork, has proven to be very powerful and effective and leads to other key beneﬁts.\nWe will discuss this approach at length in the context of convolutional neural net-Chapter 10\nworks for computer vision.\n9.1.4 Equivariance\nAn important generalization of invariance is called equivariance in which the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1361, "text": "output of the network, instead of remaining constant when the input is transformed,\nis itself transformed in a speciﬁc way. For example, consider a network that takes\nan image as input and returns a segmentation of that image in which each pixel\nis classiﬁed as belonging either to a foreground object or to the background. In\nthis case, if the location of the object within the image is translated, we want the\ncorresponding segmentation of the object to be similarly translated. Suppose we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1362, "text": "denote the image by I, and the operation of the segmentation network by S, then for\na translation operation T we have\nS(T(I)) = T(S(I)); (9.2)\nwhich says that the segmentation of the translated image is given by the translation\nof the segmentation of the original image. This is illustrated in Figure 9.2\nMore generally, equivariance can hold if the transformation applied to the output\nis different to that applied to the input:\nS(T(I)) = ˜T(S(I)): (9.3)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1363, "text": "S(T(I)) = ˜T(S(I)): (9.3)\nFor example, if the segmented image has a lower resolution than the original image,\nthen if Tis a translation in the original image space, ˜Trepresents the corresponding\ntranslation in the lower-dimensional segmentation space. Similarly, if Sis an oper-\nator that measures the orientation of an object within an image, and T represents a\nrotation (which is a complex nonlinear transformation of all of the pixel values in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1364, "text": "image) then ˜Twill increment or decrement the scalar orientation value generated by\nS.\nWe also see that invariance is a special case of equivariance in which the output\ntransformation is simply the identity. For example, if Cis a neural network that\nclassiﬁes an object within an image and T is a translation operator then\nC(T(I)) = C(I); (9.4)\nwhich says that the class of the object does not depend on its position within the\nimage.\n260 9. REGULARIZATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1365, "text": "image.\n260 9. REGULARIZATION\nFigure 9.2 Illustration of equivariance, cor-\nresponding to (9.2). If an image\n(a) is ﬁrst translated to give (b)\nand then segmented to give (d),\nthe result is the same as if the\nimage is ﬁrst segmented to give\n(c) and then translated to give\n(d).\n(a)\n (b)\n(c)\n (d)\nS S\nT\nT\n9.2. Weight Decay\nWe introduced regularization in the context of linear regression to control modelSection 1.2.5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1366, "text": "complexity, as an alternative to limiting the number of parameters in the model. The\nsimplest regularizer comprises the sum of the squares of the model parameters to\ngive a regularized error function of the form (9.1), which penalizes parameter values\nwith large magnitude. The effective model complexity is then determined by the\nchoice of the regularization coefﬁcient \u0015.\nWe have also seen that this additive regularization term can be interpreted as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1367, "text": "the negative logarithm of a zero-mean Gaussian prior distribution over the weight\nvector w. This provides a probabilistic perspective on the inclusion of prior knowl-Section 2.6.2\nedge into the model training process. Unfortunately, this prior is expressed over the\nmodel parameters, whereas any domain knowledge we might possess regarding the\nproblem to be solved is more likely to be expressed in terms of the network function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1368, "text": "from inputs to outputs. The relationship between the parameters and the network\nfunction is, however, extremely complex, and therefore only very limited kinds of\nprior knowledge can easily be expressed directly as priors over model parameters.\nThe sum-of-squares regularizer in (9.1) is known in the machine learning litera-\nture as weight decay because in sequential learning algorithms, it encourages weight"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1369, "text": "values to decay towards zero, unless supported by the data. One advantage of thisExercise 9.3\nkind of regularizer is that it is trivial to evaluate its derivatives for use in gradient\ndescent training. Speciﬁcally for (9.1) the gradient is given by\n∇˜E(w) = ∇E(w) + \u0015w: (9.5)\n9.2. Weight Decay 261\nu1\nu2\nw1\nw2 E(w)\nE(w) + λ(w2\n1 + w2\n2)\nw2\n1 + w2\n2\nw⋆\nˆw\nFigure 9.3 Contours of the error function (red), the regularization term (green), and a linear combination of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1370, "text": "the two (blue) for a quadratic error function and a sum-of-squares regularizer \u0015(w2\n1 + w2\n2). Here the axes in\nparameter space have been rotated to align with the axes of the elliptical contour of the unregularized error\nfunction. For \u0015 = 0, the minimum error is indicated by w?. When \u0015 >0, the minimum of the regularized error\nfunction E(w) + \u0015(w2\n1 + w2\n2) is shifted towards the origin. This shift is greater in the direction of w1 because"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1371, "text": "the unregularized error is relatively insensitive to the parameter value, and less in directionw2 where the error is\nmore strongly dependent on the parameter value. The regularization term is effectively suppressing parameters\nthat have only a small effect on the accuracy of the network predictions.\nWe see that the factor of 1=2 in (9.1), which is often included by convention, disap-\npears when we take the derivative."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1372, "text": "pears when we take the derivative.\nThe general effect of a quadratic regularizer can be seen by considering a two-\ndimensional parameter space along with an unregularized error function E(w) that\nis a quadratic function of w (corresponding to a simple linear regression model with\na sum-of-squares error function), as illustrated in Figure 9.3. The axes in param-Section 4.1.2\neter space have been rotated to align with the eigenvectors of the Hessian matrix,Section 8.1.6"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1373, "text": "corresponding to the axes of the elliptical error function contours. We see that the\neffect of the regularization term is to shrink the magnitudes of the weight parameters.\nHowever, the effect is much larger for parameterw1 because the unregularized error\nis much less sensitive to the value of w1 compared to that of w2. Intuitively only\nthe parameter w2 is ‘active’ because the output is relatively insensitive to w1, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1374, "text": "hence the regularizer pushes w1 close to zero. The effective number of parameters\nis the number that remain active after regularization, and this concept can be formal-\nized either from a Bayesian or from a frequentist perspective (Bishop, 2006; Hastie,\nTibshirani, and Friedman, 2009). For \u0015→∞, all the parameters are driven to zero\nand the effective number of parameters is then zero. As \u0015is reduced, the number\nof parameters increases until for \u0015 = 0 it equals the total number of actual param-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1375, "text": "262 9. REGULARIZATION\neters in the model. We see that controlling model complexity by regularization has\nsimilarities to controlling model complexity by limiting the number of parameters.\n9.2.1 Consistent regularizers\nOne of the limitations of simple weight decay in the form (9.1) is that it breaks\ncertain desirable transformation properties of network mappings. To illustrate this,\nconsider a multilayer perceptron network having two layers of weights and linear"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1376, "text": "output units that performs a mapping from a set of input variables {xi}to a set of\noutput variables {yk}. The activations of the hidden units in the ﬁrst hidden layer\ntake the form\nzj = h\n(∑\ni\nwjixi + wj0\n)\n(9.6)\nwhereas the activations of the output units are given by\nyk =\n∑\nj\nwkjzj + wk0: (9.7)\nSuppose we perform a linear transformation of the input data:\nxi → ˜xi = axi + b: (9.8)\nThen we can arrange for the mapping performed by the network to be unchanged"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1377, "text": "by making a corresponding linear transformation of the weights and biases from the\ninputs to the units in the hidden layer:Exercise 9.4\nwji → ˜wji = 1\nawji (9.9)\nwj0 → ˜wj0 = wj0 −b\na\n∑\ni\nwji: (9.10)\nSimilarly, a linear transformation of the output variables of the network of the form\nyk → ˜yk = cyk + d (9.11)\ncan be achieved transforming the second-layer weights and biases using\nwkj → ˜wkj = cwkj (9.12)\nwk0 → ˜wk0 = cwk0 + d: (9.13)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1378, "text": "wk0 → ˜wk0 = cwk0 + d: (9.13)\nIf we train one network using the original data and one network using data for which\nthe input and/or target variables have been transformed by one of the above linear\ntransformations, then consistency requires that we should obtain equivalent networks\nthat differ only by the linear transformation of the weights as given. Any regularizer\nshould be consistent with this property, otherwise it would arbitrarily favour one"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1379, "text": "solution over another, equivalent one. Clearly, simple weight decay (9.1), which\ntreats all weights and biases on an equal footing, does not satisfy this property.\n9.2. Weight Decay 263\nWe therefore look for a regularizer that is invariant under the linear transforma-\ntions (9.9), (9.10), (9.12), and (9.13). These require that the regularizer should be\ninvariant to re-scaling of the weights and to shifts of the biases. Such a regularizer is\ngiven by\n\u00151\n2\n∑\nw∈W1\nw2 + \u00152\n2\n∑\nw∈W2\nw2 (9.14)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1380, "text": "given by\n\u00151\n2\n∑\nw∈W1\nw2 + \u00152\n2\n∑\nw∈W2\nw2 (9.14)\nwhere W1 denotes the set of weights in the ﬁrst layer, W2 denotes the set of weights\nin the second layer, and biases are excluded from the summations. This regularizer\nwill remain unchanged under the weight transformations provided the regularization\nparameters are re-scaled using \u00151 → a1=2\u00151 and \u00152 → c−1=2\u00152.\nThe regularizer (9.14) corresponds to a prior distribution over the parameters of\nthe form:\np(w|\u000b1;\u000b2) ∝exp\n(\n−\u000b1\n2\n∑\nw∈W1\nw2 −\u000b2\n2\n∑\nw∈W2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1381, "text": "p(w|\u000b1;\u000b2) ∝exp\n(\n−\u000b1\n2\n∑\nw∈W1\nw2 −\u000b2\n2\n∑\nw∈W2\nw2\n)\n: (9.15)\nNote that priors of this form are improper (they cannot be normalized) because the\nbias parameters are unconstrained. Using improper priors can lead to difﬁculties in\nselecting regularization coefﬁcients and in model comparison within the Bayesian\nframework. It is therefore common to include separate priors for the biases (which\nthen break the shift invariance) that have their own hyperparameters."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1382, "text": "We can illustrate the effect of the resulting four hyperparameters by drawing\nsamples from the prior and plotting the corresponding network functions, as shown\nin Figure 9.4. The priors are governed by four hyperparameters,\u000bb\n1, \u000bw\n1 , \u000bb\n2, and \u000bw\n2 ,\nwhich represent the precisions of the Gaussian distributions of the ﬁrst-layer biases,\nﬁrst-layer weights, second-layer biases, and second-layer weights, respectively. We\nsee that the parameter \u000bw"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1383, "text": "see that the parameter \u000bw\n2 governs the vertical scale of the functions (note the dif-\nferent vertical axis ranges on the top two diagrams), \u000bw\n1 governs the horizontal scale\nof variations in the function values, and \u000bb\n1 governs the horizontal range over which\nvariations occur. The parameter \u000bb\n2, whose effect is not illustrated here, governs the\nrange of the vertical offsets of the functions\nMore generally, we can consider regularizers in which the weights are divided"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1384, "text": "into any number of groups Wk so that\nΩ(w ) = 1\n2\n∑\nk\n\u000bk∥w∥2\nk (9.16)\nwhere\n∥w∥2\nk =\n∑\nj∈Wk\nw2\nj: (9.17)\nFor example, we could use a different regularizer for each layer in a multilayer net-\nwork.\n264 9. REGULARIZATION\nαw\n1 = 1,αb\n1 = 1, αw\n2 = 1,αb\n2 = 1\n−1 −0.5 0 0.5 1\n−6\n−4\n−2\n0\n2\n4\nαw\n1 = 1,αb\n1 = 1, αw\n2 = 10,αb\n2 = 1\n−1 −0.5 0 0.5 1\n−60\n−40\n−20\n0\n20\n40\nαw\n1 = 1000,αb\n1 = 100, αw\n2 = 1,αb\n2 = 1\n−1 −0.5 0 0.5 1\n−10\n−5\n0\n5\nαw\n1 = 1000,αb\n1 = 1000, αw\n2 = 1,αb\n2 = 1\n−1 −0.5 0 0.5 1\n−10\n−5\n0\n5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1385, "text": "2 = 1,αb\n2 = 1\n−1 −0.5 0 0.5 1\n−10\n−5\n0\n5\nFigure 9.4 Illustration of the effect of the hyperparameters governing the prior distribution over weights and\nbiases in a two-layer network having a single input, a single linear output, and12 hidden units withtanh activation\nfunctions.\n9.2.2 Generalized weight decay\nA generalization of the simple quadratic regularizer is sometimes used:\nΩ(w ) = \u0015\n2\nM∑\nj=1\n|wj|q (9.18)\nwhere q = 2 corresponds to the quadratic regularizer in (9.1). Figure 9.5 shows"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1386, "text": "contours of the regularization function for different values of q.\nA regularizer of the form (9.18) with q = 1 is known as a lasso in the statistics\nliterature (Tibshirani, 1996). For quadratic error functions, it has the property that\nif \u0015is sufﬁciently large, some of the coefﬁcients wj are driven to zero, leading to a\nsparse model in which the corresponding basis functions play no role. To see this,\n9.2. Weight Decay 265\nFigure 9.5 Contours of the regu-\nlarization term in (9.18) for various"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1387, "text": "larization term in (9.18) for various\nvalues of the parameter q.\nw1\nw2\nq = 0.5\nw1\nw2\nq = 1\nw1\nw2\nq = 2\nw1\nw2\nq = 4\nwe ﬁrst note that minimizing the regularized error function given by\nE(w) + \u0015\n2\nM∑\nj=1\n|wj|q (9.19)\nis equivalent to minimizing the unregularized error function E(w) subject to the\nconstraintExercise 9.5\nM∑\nj=1\n|wj|q 6 \u0011 (9.20)\nfor an appropriate value of the parameter\u0011, where the two approaches can be related"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1388, "text": "using Lagrange multipliers. The origin of the sparsity can be seen in Figure 9.6,Appendix C\nwhich shows the minimum of the error function, subject to the constraint (9.20). As\n\u0015is increased, more parameters will be driven to zero. By comparison, a quadratic\nregularizer leaves both weight parameters with non-zero values.\nRegularization allows complex models to be trained on data sets of limited size\nwithout severe over-ﬁtting, essentially by limiting the effective model complexity."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1389, "text": "However, the problem of determining the optimal model complexity is then shifted\nfrom one of ﬁnding the appropriate number of learnable parameters to one of deter-\nmining a suitable value of the regularization coefﬁcient \u0015. We will discuss the issue\nof model complexity in the next section.\n266 9. REGULARIZATION\nw1\nw2\nˆw\n|w1|+ |w2|⩽ η\nE(w)\nw1\nw2\nˆw\nw2\n1 + w2\n2 ⩽ η\nE(w)\nFigure 9.6 Plot of the contours of the unregularized error function (red) along with the constraint region (9.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1390, "text": "for the lasso regularizer q = 1 on the left, and the quadratic regularizer q = 2 on the right, in which the optimum\nvalue for the parameter vector w is denoted by bw. The lasso gives a sparse solution in which bw1 = 0, whereas\nthe quadratic regularizer simply reduces w1 to a smaller value.\n9.3. Learning Curves\nWe have already explored how the generalization performance of a model varies as\nwe change the number of parameters in the model, the size of the data set, and the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1391, "text": "coefﬁcient of a weight-decay regularization term. Each of these allows for a trade-\noff between bias and variance to minimize the generalization error. Another factor\nthat inﬂuences this trade-off is the learning process itself. During optimization of\nthe error function through gradient descent, the training error typically decreases\nas the model parameters are updated, whereas the error for hold-out data may be\nnon-monotonic. This behaviour can be visualized using learning curves, which plot"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1392, "text": "performance measures such as training set and validation set error as a function of\niteration number during an iterative learning process such as stochastic gradient de-\nscent. These curves provide insight into the progress of training and also offer a\npractical methodology for controlling the effective model complexity.\n9.3.1 Early stopping\nAn alternative to regularization as a way of controlling the effective complexity"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1393, "text": "of a network is early stopping. The training of deep learning models involves an\niterative reduction of the error function deﬁned with respect to a set of training data.\nAlthough the error function evaluated using the training set often shows a broadly\nmonotonic decrease as a function of the iteration number, the error measured with\nrespect to held-out data, generally called a validation set, often shows a decrease at\n9.3. Learning Curves 267\n0 10 20 30 40 50\n0.15\n0.2\n0.25\n0 10 20 30 40 50\n0.35"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1394, "text": "0.15\n0.2\n0.25\n0 10 20 30 40 50\n0.35\n0.4\n0.45\nFigure 9.7 An illustration of the behaviour of training set error (left) and validation set error (right) during a\ntypical training session, as a function of the iteration step, for the sinusoidal data set. To achieve the best\ngeneralization performance , the training should be stopped at the point shown by the vertical dashed lines,\ncorresponding to the minimum of the validation set error."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1395, "text": "ﬁrst, followed by an increase as the network starts to over-ﬁt. Therefore, to obtain\na network with good generalization performance, training should be stopped at the\npoint of smallest error with respect to the validation data set, as indicated in Fig-\nure 9.7.\nThis behaviour of the learning curves is sometimes explained qualitatively in\nterms of the effective number of parameters in the network. This number starts out"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1396, "text": "small and then grows during training, corresponding to a steady increase in the ef-\nfective complexity of the model. Stopping training before a minimum of the training\nerror has been reached is a way to limit the effective network complexity.\nWe can verify this insight for a quadratic error function and show that early stop-\nping should exhibit similar behaviour to regularization using a simple weight-decay\nterm (Bishop, 1995a). This can be understood from Figure 9.8, in which the axes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1397, "text": "in weight space have been rotated to be parallel to the eigenvectors of the Hessian\nmatrix. If, in the absence of weight decay, the weight vector starts at the origin andSection 7.1.1\nproceeds during training along a path that follows the local negative gradient vec-\ntor, then the weight vector will move initially parallel to the w2 axis through a point\ncorresponding roughly to ˆw and then move towards the minimum of the error func-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1398, "text": "tion wML. This follows from the shape of the error surface and the widely differing\neigenvalues of the Hessian. Stopping at a point near ˆw is therefore similar to weight\ndecay. The relationship between early stopping and weight decay can be made quan-\ntitative, thereby showing that the quantity \u001c\u0011 (where \u001c is the iteration index and \u0011isExercise 9.6\nthe learning rate parameter) acts like the reciprocal of the regularization parameter\u0015."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1399, "text": "The effective number of parameters in the network therefore grows during training.\n268 9. REGULARIZATION\nFigure 9.8 A schematic illustration of why\nearly stopping can give similar\nresults to weight decay for a\nquadratic error function. The\nellipses show contours of con-\nstant error, and w? denotes the\nmaximum likelihood solution cor-\nresponding to the minimum of the\nunregularized error function. If the\nweight vector starts at the origin\nand moves according to the local"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1400, "text": "and moves according to the local\nnegative gradient direction, then it\nwill follow the path shown by the\ncurve. By stopping training early,\na weight vector bw is found that is\nqualitatively like that obtained with\na simple weight-decay regularizer\nalong with training to the minimum\nof the regularized error, as can\nbe seen by comparing with Fig-\nure 9.3.\nw⋆\nˆw\nw1\nw2\n9.3.2 Double descent\nThe bias–variance trade-off provides insight into the generalization performanceSection 4.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1401, "text": "of a learnable model as the number of parameters in the model is varied. Models with\ntoo few parameters will have a high test set error due to the limited representational\ncapacity (high bias), and as the number of parameters increases, the test error is ex-\npected to fall. However, as the number of parameters is increased further, the test\nerror increases again due to over-ﬁtting (high variance). This leads to the conven-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1402, "text": "tional belief, widespread in classical statistics, that the number of parameters in the\nmodel needs to be limited according to the size of the data set and that for a given\ntraining data set, very large models are expected to have poor performance.\nContrary to this expectation, however, modern deep neural networks can have\nexcellent performance even when the number of parameters far exceeds that required\nto achieve a perfect ﬁt to the training data (Zhang et al. , 2016), and the general"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1403, "text": "wisdom in the deep learning community is that bigger models are better. Although\nearly stopping is sometimes used, models may also be trained to zero error and yet\nstill have good performance on test data.\nThese seemingly contradictory perspectives can be reconciled by examining\nlearning curves and other plots of generalization performance versus model com-\nplexity, which reveal a more subtle phenomenon called double descent (Belkin et"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1404, "text": "al., 2019). This is illustrated in Figure 9.9, which shows training set and test set er-\nrors versus model complexity, as determined by the number of learnable parameters,\nfor a large neural network called ResNet18 (He et al., 2015a), which has 18 layers\nof parameters trained on an image classiﬁcation task. The number of weights and\nbiases in the network is varied by changing the ‘width parameter’, which governs\nthe number of hidden units in each layer. We see that the training error decreases"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1405, "text": "monotonically with increasing complexity of the model, as expected. However, the\n9.3. Learning Curves 269\nFigure 9.9 Plot of training set and test set errors for a large neural network model called ResNet18 trained on an\nimage classiﬁcation problem versus the complexity of a model. The horizontal axis represents a hyperparameter\ngoverning the number of hidden units and hence the overall number of weights and biases in the network. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1406, "text": "vertical dashed line, labelled ‘interpolation threshold’ indicates the level of model complexity at which the model\nis capable, in principle, of achieving zero error on the training set. [From Nakkiran et al. (2019) with permission.]\ntest set error decreases at ﬁrst then increases again and then ﬁnally decreases again.\nThis reduction in test set error for very large models continues even after the training\nset error has reached zero."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1407, "text": "set error has reached zero.\nThis surprising behaviour is more complex than we would expect from the usual\nbias–variance discussion of classical statistics and exhibits two different regimes of\nmodel ﬁtting, as shown schematically in Figure 9.9, corresponding to the classical\nbias–variance trade-off for small to medium complexity, followed by a further re-\nduction in test set error as we enter a regime of very large models. The transition"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1408, "text": "between the two regimes occurs roughly when the number of parameters in the model\nis sufﬁciently large that the model is able to ﬁt the training data exactly (Belkinet al.,\n2019). Nakkiran et al. (2019) deﬁne the effective model complexity to be the maxi-\nmum size of training data set on which a model can achieve zero training error, and\nso double descent arises when the effective model complexity exceeds the number\nof data points in the training set."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1409, "text": "of data points in the training set.\nWe see similar behaviour if we control model complexity using early stopping,\nas seen in Figure 9.10. Increasing the number of training epochs increases the ef-\nfective model complexity, and for a sufﬁciently large model, double descent is again\nobserved. For such models there are many possible solutions including those that\nover-ﬁt to the data. It therefore seems to be a property of stochastic gradient descent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1410, "text": "that the implicit biases that it introduces lead to good generalization performance.\nAnalogous results are also obtained when a regularization term in the error func-\n270 9. REGULARIZATION\nFigure 9.10 Plot of test set error versus number of epochs of gradient descent training for ResNet18 models\nof various sizes. The effective model complexity increases with the number of training epochs, and the double"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1411, "text": "descent phenomenon is observed for a sufﬁciently large model. [From Nakkiran et al. (2019) with permission.]\ntion is used to control complexity. Here the test set error of a large model trained\nto convergence shows double descent with respect to 1=\u0015, the inverse regularization\nparameter, since high \u0015corresponds to low complexity (Yilmaz and Heckel, 2022).\nOne ironic consequence of double descent is that it possible to operate in a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1412, "text": "regime where increasing the size of the training data set could actually reduce per-\nformance, contrary to the conventional view that more data is always a good thing.\nFor a model in the critical regime shown in Figure 9.9, an increase in the size of the\ntraining set can push the interpolation threshold to the right, leading to a higher test\nset error. This is conﬁrmed in Figure 9.11, which shows the test set error for a trans-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1413, "text": "former model as a function of the dimensionality of the input space, known as theChapter 12\nembedding dimension. Increasing the embedding dimension increases the number\nof weights and biases in the model and hence increases the model complexity. We\nsee that increasing the training set size from 4,000 to 18,000 data points leads to a\ncurve that is overall much lower. However, for a range of embedding dimensions\nthat correspond to models in the critical complexity regime, increasing the size of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1414, "text": "the data set can actually reduce generalization performance.\n9.4.\nParameter Sharing\nRegularization terms, such as the L2 regularizer ∥w∥2, help to reduce over-ﬁtting by\nencouraging weight values to be close to zero. Another way to reduce network com-\nplexity is to impose hard constraints on the weights by forming them into groups and\nrequiring that all weights within each group share the same value, in which the shared\n9.4. Parameter Sharing 271"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1415, "text": "9.4. Parameter Sharing 271\nFigure 9.11 Plot of test set error for a large transformer model versus the embedding dimension, which controls\nthe number of parameters in the model. Increasing the size of the training set from 4,000 to 18,000 samples\ngenerally leads to a lower test set error, but for some intermediate values of model complexity, there can be an\nincrease in the error, as shown by the vertical red arrows. [From Nakkiran et al. (2019) with permission.]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1416, "text": "value is learned from data. This is known as weight sharing or parameter sharing or\nparameter tying. It means that the number of degrees of freedom is smaller than the\nnumber of connections in the network. Usually this is introduced as a way to encode\ninductive bias into a network to express some known invariances. Evaluating the\nerror function gradients for such networks can be done using a small modiﬁcation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1417, "text": "to backpropagation although in practice this is handled implicitly through automaticExercise 9.7\ndifferentiation. We will make extensive use of parameter sharing when we discuss\nconvolutional neural networks. Parameter sharing is applicable, however, only toChapter 10\nparticular problems in which the form of the constraints can be speciﬁed in advance.\n9.4.1 Soft weight sharing\nInstead of using a hard constraint that forces sets of model parameters to be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1418, "text": "equal, Nowlan and Hinton (1992) introduced a form of soft weight sharing in which\na regularization term encourages groups of weights to have similar values. Further-\nmore, the division of weights into groups, the mean weight value for each group,\nand the spread of values within the groups are all determined as part of the learning\nprocess.\nRecall that the simple-weight decay regularizer in (9.1) can be viewed as the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1419, "text": "negative log of a Gaussian prior distribution over the weights. This encourages all\nthe weights to converge towards a single value of zero. We can instead encourage the\nweight values to form several groups, rather than just one group, by considering a\nprobability distribution that is amixture of Gaussians. The means {\u0016j}and variancesSection 3.2.9\n{\u001b2\nj}of the Gaussian components, as well as the mixing coefﬁcients {\u0019j}, will be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1420, "text": "considered as adjustable parameters to be determined as part of the learning process.\n272 9. REGULARIZATION\nThus, we have a probability density of the form\np(w) =\n∏\ni\n\n\n\nK∑\nj=1\n\u0019jN(wi|\u0016j;\u001b2\nj)\n\n\n (9.21)\nwhere Kis the number of components in the mixture. Taking the negative logarithm\nthen leads to a regularization function of the form\nΩ(w ) = −\n∑\ni\nln\n\n\nK∑\nj=1\n\u0019jN(wi|\u0016j;\u001b2\nj)\n\n: (9.22)\nThe total error function is then given by\n˜E(w) = E(w) + \u0015Ω(w ) (9.23)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1421, "text": "˜E(w) = E(w) + \u0015Ω(w ) (9.23)\nwhere \u0015is the regularization coefﬁcient.\nThis error is minimized jointly with respect to the weights{wi}and with respect\nto the parameters {\u0019j;\u0016j;\u001bj}of the mixture model. This can be done using gradient\ndescent, which requires that we evaluate the derivatives of Ω(w ) with respect to all\nthe learnable parameters. To do this, it is convenient to regard the {\u0019j}as prior\nprobabilities for each component to have generated a weight value, and to introduce"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1422, "text": "the corresponding posterior probabilities, which are given by Bayes’ theorem:Exercise 9.8\n\rj(wi) = \u0019jN(wi|\u0016j;\u001b2\nj)\n∑\nk\u0019kN(wi|\u0016k;\u001b2\nk): (9.24)\nThe derivatives of the total error function with respect to the weights are then given\nbyExercise 9.9\n@˜E\n@wi\n= @E\n@wi\n+ \u0015\n∑\nj\n\rj(wi)(wi −\u0016j)\n\u001b2\nj\n: (9.25)\nThe effect of the regularization term is therefore to pull each weight towards the\ncentre of the jth Gaussian, with a force proportional to the posterior probability of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1423, "text": "that Gaussian for the given weight. This is precisely the kind of effect that we are\nseeking.\nDerivatives of the error with respect to the centres of the Gaussians are also\neasily computed to giveExercise 9.10\n@˜E\n@\u0016j\n= \u0015\n∑\ni\n\rj(wi)(\u0016j −wi)\n\u001b2\nj\n(9.26)\nwhich has a simple intuitive interpretation, because it pushes \u0016j towards an aver-\nage of the weight values, weighted by the posterior probabilities that the respective\nweight parameters were generated by component j.\n9.4. Parameter Sharing 273"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1424, "text": "9.4. Parameter Sharing 273\nTo ensure that the variances {\u001b2\nj}remain positive, we introduce new variables\n{\u0018j}deﬁned by\n\u001b2\nj = exp(\u0018j) (9.27)\nand an unconstrained minimization is performed with respect to the {\u0018j}. The asso-\nciated derivatives are then given byExercise 9.11\n@˜E\n@\u0018 = \u0015\n2\n∑\ni\n\rj(wi)\n(\n1 −(wi −\u0016j)2\n\u001b2\nj\n)\n: (9.28)\nThis process drives \u001bj towards a weighted average of the squared deviations of the\nweights around the corresponding centre \u0016j, where the weighting coefﬁcients are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1425, "text": "again given by the posterior probability that each weight is generated by component\nj.\nFor the derivatives with respect to the mixing coefﬁcients \u0019j, we need to take\naccount of the constraints\n∑\nj\n\u0019j = 1; 0 6 \u0019i 6 1; (9.29)\nwhich follow from the interpretation of the \u0019j as prior probabilities. This can be\ndone by expressing the mixing coefﬁcients in terms of a set of auxiliary variables\n{\u0011j}using the softmax function given by\n\u0019j = exp(\u0011j)\n∑K\nk=1 exp(\u0011k)\n: (9.30)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1426, "text": "\u0019j = exp(\u0011j)\n∑K\nk=1 exp(\u0011k)\n: (9.30)\nThe derivatives of the regularized error function with respect to the {\u0011j}then take\nthe formExercise 9.12\n@˜E\n@\u0011j\n= \u0015\n∑\ni\n{\u0019j −\rj(wi)}: (9.31)\nWe see that \u0019j is therefore driven towards the average posterior probability for mix-\nture component j.\nA different application of soft weight sharing (Lasserre, Bishop, and Minka,\n2006) introduces a principled approach that combines the unsupervised training of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1427, "text": "a generative model with the supervised training of a corresponding discriminative\nmodel. This is useful in situations where we have a signiﬁcant amount of unlabelled\ndata but where labelled data is in short supply. The generative model has the advan-\ntage that all of the data can be used to determine its parameters, whereas only the\nlabelled examples directly inform the parameters of the discriminative model. How-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1428, "text": "ever, a discriminative model can achieve better generalization when there is model\nmis-speciﬁcation, in other words when the model does not exactly describe the true\ndistribution that generates the data, as is typically the case. By introducing a soft\ntying of the parameters of the two models, we obtain a well-deﬁned hybrid of gen-\nerative and discriminative approaches that can be robust to model mis-speciﬁcation\nwhile also beneﬁting from being trained on unlabelled data.\n274 9. REGULARIZATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1429, "text": "274 9. REGULARIZATION\ninput\ngradient\ninput\ngradient\ninput\ngradient\n(a) (b) (c)\nFigure 9.12 Plots of the Jacobian for networks with a single input and a single output, showing (a) a network\nwith two layers of weights, (b) a network with 25 layers of weights, and (c) a network with 51 layers of weights\ntogether with residual connections. [From Balduzzi et al. (2017) with permission.]\n9.5. Residual Connections\nThe representational power of deep neural networks stems in large part from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1430, "text": "use of multiple layers of processing, and it has been observed that increasing the\nnumber of layers in a network can increase generalization performance signiﬁcantly.\nWe have also seen how batch normalization, along with careful initialization of theSection 7.4.2\nweights and biases, can help address the problem of vanishing or exploding gradientsSection 7.2.5\nin deep networks. However, even with batch normalization, it becomes increasingly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1431, "text": "difﬁcult to train networks with a large number of layers.\nOne explanation for this phenomenon is called shattered gradients (Balduzzi et\nal., 2017). We have seen that the representational capabilities of neural networks\nincrease exponentially with depth. With ReLU activation functions, there is an ex-\nponential increase in the number of linear regions that the network can represent.Section 6.3\nHowever, a consequence of this is a proliferation of discontinuities in the gradient"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1432, "text": "of the error function. This is illustrated for networks with a single input variable\nand a single output variable in Figure 9.12. Here the derivative of the output vari-\nable with respect to the input variable (the Jacobian of the network) is plotted as\na function of the input variable. From the chain rule of calculus, these derivatives\ndetermine the gradients of the error function surface. We see that for deep networks,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1433, "text": "extremely small changes in the weight parameters in the early layers of the network\ncan produce signiﬁcant changes in the gradient. Iterative gradient-based optimiza-\ntion algorithms assume that the gradient varies smoothly across parameter space,\nand hence this ‘shattered gradient’ effect can render training ineffective in very deep\nnetworks.\nAn important modiﬁcation to the architecture of neural networks that greatly as-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1434, "text": "sists in training very deep networks is that ofresidual connections (He et al., 2015a),\nwhich are a particular form of skip-layer connections. Consider a neural network\n9.5. Residual Connections 275\nx F1 + F2 + F3 + y\nz1 z2\nFigure 9.13 A residual network consisting of three residual blocks, corresponding to the sequence of transfor-\nmations (9.35) to (9.37).\nthat consists of a sequence of three layers of the form\nz1 = F1(x) (9.32)\nz2 = F2(z1) (9.33)\ny = F3(z2): (9.34)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1435, "text": "z2 = F2(z1) (9.33)\ny = F3(z2): (9.34)\nHere the functions Fl(-)might simply consist of a linear transformation followed\nby a ReLU activation function or they might be more complex with multiple linear,\nactivation function, and normalization layers. A residual connection consists simply\nof adding the input to each function back onto the output to give\nz1 = F1(x) + x (9.35)\nz2 = F2(z1) + z1 (9.36)\ny = F3(z2) + z2: (9.37)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1436, "text": "z2 = F2(z1) + z1 (9.36)\ny = F3(z2) + z2: (9.37)\nEach combination of a function and a residual connection, such as F1(x) + x, is\ncalled a residual block. A residual network, also known as a ResNet, consists of\nmultiple layers of such blocks in sequence. A modiﬁed network with residual con-\nnections is illustrated inFigure 9.13. A residual block can easily generate the identity\ntransformation, if the parameters in the nonlinear function are small enough for the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1437, "text": "function outputs to become close to zero.\nThe term ‘residual’ refers to the fact that in each block the function learns the\nresidual between the identity map and the desired output, which we can see by rear-\nranging the residual transformation:\nFl(zl−1 ) = zl −zl−1 : (9.38)\nThe gradients in a network with residual connections are much less sensitive to input\nvalues compared to a standard deep network, as seen in Figure 9.12(c)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1438, "text": "Li et al. (2017) developed a way to visualize error surfaces directly, which\nshowed that the effect of the residual connections is to create smoother error function\nsurfaces, as shown in Figure 9.14. It is usual to include batch normalization layers\nin a residual network, as together they signiﬁcantly reduce the issue of vanishing\nand exploding gradients. He et al. (2015a) showed that including residual connec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1439, "text": "tions allows very deep networks, potentially having hundreds of layers, to be trained\neffectively.\nFurther insight into the way residual connections encourage smooth error sur-\nfaces can be obtained if we combine (9.35), (9.36), and (9.37) to give a single overall\nequation for the whole network:\ny = F3(F2(F1(x) + x) + z1) + z2: (9.39)\n276 9. REGULARIZATION\n(a) (b)\nFigure 9.14 (a) A visualization of the error surface for a network with 56 layers. (b) The same network with the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1440, "text": "inclusion of residual connections, showing the smoothing effect that comes from the residual connections. [From\nLi et al. (2017) with permission.]\nWe can now substitute for the intermediate variablesz1 and z2 to give an expression\nfor the network output as a function of the input x:Exercise 9.13\ny = F3(F2(F1(x) + x) + F1(x) + x)\n+ F2(F1(x) + x))\n+ F1(x) + x: (9.40)\nThis expanded form of the residual network is depicted in Figure 9.15. We see that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1441, "text": "the overall function consists of multiple networks acting in parallel and that these\ninclude networks with fewer layers. The network has the representational capability\nof a deep network, since it contains such a network as a special case. However, the\nerror surface is moderated by a combination of shallow and deep sub-networks.\nNote that the skip-layer connections deﬁned by (9.40) require the input and all\nthe intermediate variables to have the same dimensionality so that they can be added."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1442, "text": "We can change the dimensionality at some point in the network by including a non-\nsquare matrix W of learnable parameters in the form\nzl = Fl(zl−1) + Wzl−1: (9.41)\nSo far we have not been speciﬁc about the form of the learnable nonlinear func-\ntions Fl(-). The simplest choice would be a standard neural network that alternates\nbetween layers consisting of a learnable linear transformation and a ﬁxed nonlinear\nactivation function such as ReLU. This opens two possibilities for placing the resid-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1443, "text": "ual connections, as shown in Figure 9.16. In version (a) the quantities being added\nare always non-negative since they are given by the outputs of ReLU layers, and so\nto allow for both positive and negative values, version (b) is more commonly used.\n9.6. Model Averaging 277\nF1 + F2 + F3\nF1 +\nF1 + F2 +\nF1 +\n+ yx\nFigure 9.15 The same network as in Figure 9.13, shown here in expanded form.\n9.6. Model Averaging\nIf we have several different models trained to solve the same problem then instead"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1444, "text": "of trying to select the single best model, we can often improve generalization by\naveraging the predictions made by the individual models. Such combinations of\nmodels are sometimes called committees or ensembles. For models that produce\nprobabilistic outputs, the predicted distribution is the average of the predictions from\neach model:\np(y|x) =1\nL\nL∑\nl=1\npl(y|x) (9.42)\nwhere pl(y|x)is the output of model land Lis the total number of models.\nLinear ReLU + Linear ReLU +\n(a)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1445, "text": "Linear ReLU + Linear ReLU +\n(a)\nReLU Linear + ReLU ReLU +\n(b)\nFigure 9.16 Two alternative ways to include residual network connections into a standard feed-forward network\nthat alternates between learnable linear layers and nonlinear ReLU activation functions.\n278 9. REGULARIZATION\nThis averaging process can be motivated by considering the trade-off between\nbias and variance. Recall fromFigure 4.7 that when we trained multiple polynomialsSection 4.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1446, "text": "using the sinusoidal data and then averaged the resulting functions, the contribution\narising from the variance term tended to cancel, leading to improved predictions.\nIn practice, of course, we have only a single data set, and so we have to ﬁnd\na way to introduce variability between the different models within the committee.\nOne approach is to use bootstrap data sets, in which multiple data sets are created as\nfollows. Suppose our original data set consists of N data points X = {x1;:::; xN}."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1447, "text": "We can create a new data set XB by drawing N points at random from X, with\nreplacement, so that some points inX may be replicated inXB, whereas other points\nin X may be absent from XB. This process can be repeated Ltimes to generate L\ndata sets each of size N and each obtained by sampling from the original data setX.\nEach data set can then be used to train a model, and the predictions of the resulting\nmodels are averaged. This procedure is known as bootstrap aggregation or bagging"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1448, "text": "(Breiman, 1996). An alternative approach to forming an ensemble is to use the\noriginal data set to train multiple different models having different architectures.\nWe can analyse the beneﬁts of ensemble predictions by considering a regression\nproblem with an input vector x and a single output variable y. Suppose we have a\nset of trained models y1(x);:::;y M(x), and we form a committee prediction given\nby\nyCOM(x) = 1\nM\nM∑\nm=1\nym(x): (9.43)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1449, "text": "by\nyCOM(x) = 1\nM\nM∑\nm=1\nym(x): (9.43)\nIf the true function that we are trying to predict is given by h(x), then the output of\neach of the models can be written as the true value plus an error:\nym(x) = h(x) + \u000fm(x): (9.44)\nThe average sum-of-squares error then takes the form\nEx\n[\n{ym(x) −h(x)}2\n]\n= Ex\n[\n\u000fm(x)2]\n(9.45)\nwhere Ex[-]denotes a frequentist expectation with respect to the distribution of the\ninput vector x. The average error made by the models acting individually is therefore\nEAV = 1\nM"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1450, "text": "EAV = 1\nM\nM∑\nm=1\nEx\n[\n\u000fm(x)2]\n: (9.46)\nSimilarly, the expected error from the committee (9.43) is given by\nECOM = Ex\n\n\n{\n1\nM\nM∑\nm=1\nym(x) −h(x)\n}2\n\n= Ex\n\n\n{\n1\nM\nM∑\nm=1\n\u000fm(x)\n}2\n: (9.47)\n9.6. Model Averaging 279\nIf we assume that the errors have zero mean and are uncorrelated, so that\nEx [\u000fm(x)] = 0 (9.48)\nEx [\u000fm(x)\u000fl(x)] = 0 ; m ̸=l (9.49)\nthen we obtainExercise 9.14\nECOM = 1\nMEAV: (9.50)\nThis apparently dramatic result suggests that the average error of a model can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1451, "text": "reduced by a factor of M simply by averaging M versions of the model. Unfortu-\nnately, it depends on the key assumption that the errors due to the individual models\nare uncorrelated. In practice, the errors are typically highly correlated, and the re-\nduction in the overall error is generally much smaller. It can, however, be shown that\nthe expected committee error will not exceed the expected error of the constituent\nmodels, so that ECOM 6 EAV.Exercise 9.15"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1452, "text": "models, so that ECOM 6 EAV.Exercise 9.15\nA somewhat different approach to model combination, known as boosting (Fre-\nund and Schapire, 1996), combines multiple ‘base’ classiﬁers to produce a form of\ncommittee whose performance can be signiﬁcantly better than that of any of the base\nclassiﬁers. Boosting can give good results even if the base classiﬁers perform only\nslightly better than random. The principal difference between boosting and the com-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1453, "text": "mittee methods, such as bagging as discussed above, is that the base classiﬁers are\ntrained in sequence and each base classiﬁer is trained using a weighted form of the\ndata set in which the weighting coefﬁcient associated with each data point depends\non the performance of the previous classiﬁers. In particular, points that are misclas-\nsiﬁed by one of the base classiﬁers are given a greater weight when used to train"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1454, "text": "the next classiﬁer in the sequence. Once all the classiﬁers have been trained, their\npredictions are then combined through a weighted majority voting scheme.\nIn practice, the major drawback of all model combination methods is that mul-\ntiple models have to be trained and then predictions have to be evaluated for all the\nmodels, thereby increasing the computational cost of both training and inference.\nHow signiﬁcant this depends on the speciﬁc application scenario.\n9.6.1 Dropout"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1455, "text": "9.6.1 Dropout\nA widely used and very effective form of regularization known as dropout (Sri-\nvastava et al., 2014) can be viewed as an implicit way to perform approximate model\naveraging over exponentially many models without having to train multiple models\nindividually. It has broad applicability and is computationally cheap. Dropout is one\nof the most effective forms of regularization and is widely used in applications."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1456, "text": "The central idea of dropout is to delete nodes from the network, including their\nconnections, at random during training. Each time a data point is presented to the\nnetwork, a new random choice is made for which nodes to omit. Figure 9.17 shows\na simple network along with examples of pruned networks in which subsets of nodes\nhave been omitted.\nDropout is applied to both hidden nodes and input nodes, but not outputs, and is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1457, "text": "equivalent to setting the output of a dropped node to zero. It can be implemented by\ndeﬁning a mask vector Ri ∈{0;1}which multiplies the activation of the non-output\n280 9. REGULARIZATION\ninputs\nhidden units\noutputs\nFigure 9.17 A neural network on the left along with two examples of pruned networks in which a random subset\nof nodes have been omitted.\nnode i for data point n, whose values are set to 1 with probability \u001a. A value of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1458, "text": "\u001a= 0:5 seems to work well for the hidden nodes, whereas for the inputs a value of\n\u001a= 0:8 is typically used.\nDuring training, as each data point is presented to the network, a new mask is\ncreated, and the forward and backward propagation steps are applied on that pruned\nnetwork to create error function gradients, which are then used to update the weights,\nfor example by stochastic gradient descent. If the data points are grouped into mini-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1459, "text": "batches then the gradients are averaged over the data points in each mini-batch beforeSection 7.2.4\napplying the weight update. For a network with M non-output nodes, there are 2M\npruned networks, and so only a small fraction of these networks will ever be con-\nsidered during training. This differs from conventional ensemble methods in which\neach of the networks in the ensemble is independently trained to convergence. An-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1460, "text": "other difference is that the exponentially many networks that are implicitly being\ntrained with dropout are not independent but share their parameter values with the\nfull network, and hence with each other. Note that training can take longer with\ndropout since the individual parameter updates are very noisy. Also, because the\nerror function is intrinsically noisy, it is harder to conﬁrm that the optimization al-\ngorithm is working correctly just by looking for a decreasing error function during"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1461, "text": "training.\nOnce training is complete, predictions can in principle be made by applying the\nensemble rule (9.42), which in this case takes the form\np(y|x) =\n∑\nR\np(R)p(y|x;R) (9.51)\nwhere the sum is over the exponentially large space of masks, and p(y|x;R) is the\npredictive distribution from the network with mask R. Because this summation is\nintractable, it can be approximated by sampling a small number of masks, and in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1462, "text": "practice, as few as 10 or 20 masks can be sufﬁcient to obtain good results. This\nprocedure is known as Monte Carlo dropout.\nExercises 281\nAn even simpler approach is to make predictions using the trained network with\nno nodes masked out, and to re-scale the weights in the network so that the expected\ninput to each node is roughly the same during testing as it would be during training,\ncompensating for the fact that in training a proportion of the nodes would be missing."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1463, "text": "Thus, if a node is present with probability \u001aduring training, then during testing the\noutput weights from that node would be multiplied by \u001abefore using the network to\nmake predictions.\nA different motivation for dropout comes from the Bayesian perspective. In a\nfully Bayesian treatment, we would make predictions by averaging over all possibleSection 2.6\n2M network models, with each network weighted by its posterior probability. Com-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1464, "text": "putationally, this would be prohibitively expensive, both during training when eval-\nuating the posterior probabilities and during testing when computing the weighted\npredictions. Dropout approximates this model averaging by giving an equal weight\nto each possible model.\nFurther intuition behind dropout comes from its role in reducing over-ﬁtting. In\na standard network, the parameters can become tuned to noise on individual data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1465, "text": "points, with hidden nodes becoming over-specialized. Each node adjusts its weights\nto minimize the error, given the outputs of other nodes, leading to co-adaptation of\nnodes in a way that might not generalize to new data. With dropout, each node\ncannot rely on the presence of other speciﬁc nodes and must instead make useful\ncontributions in a broad range of contexts, thereby reducing co-adaptation and spe-\ncialization. For a simple linear regression model trained using least squares, dropout"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1466, "text": "regularization is equivalent to a modiﬁed form of quadratic regularization.Exercise 9.18\nExercises\n9.1 (?) By considering each of the four group axioms in turn, show that the set of all pos-\nsible rotations of a square through (positive or negative) multiples of 90-, together\nwith the binary operation of composing rotations, forms a group. Similarly, showSection 9.1.3\nthat the set of all continuous translations of an object in a two-dimensional plane\nalso forms a group."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1467, "text": "also forms a group.\n9.2 (??) Consider a linear model of the form\ny(x;w) = w0 +\nD∑\ni=1\nwixi (9.52)\ntogether with a sum-of-squares error function of the form\nED(w) = 1\n2\nN∑\nn=1\n{y(xn;w) −tn}2 : (9.53)\nNow suppose that Gaussian noise \u000fi with zero mean and variance \u001b2 is added in-\ndependently to each of the input variables xi. By making use of E[\u000fi] = 0 and\nE[\u000fi\u000fj] = \u000eij\u001b2, show that minimizing ED averaged over the noise distribution is\n282 9. REGULARIZATION"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1468, "text": "282 9. REGULARIZATION\nequivalent to minimizing the sum-of-squares error for noise-free input variables with\nthe addition of a weight-decay regularization term, in which the bias parameter w0\nis omitted from the regularizer.\n9.3 (??) Consider an error function that consists simply of the quadratic regularizer\nΩ(w ) = −1\n2wTw (9.54)\ntogether with the gradient descent update formula\nw(\u001c+1) = w(\u001c+1) −\u0011∇Ω(w ): (9.55)\nBy considering the limit of inﬁnitesimal updates, write down a corresponding dif-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1469, "text": "ferential equation for the evolution of w. Write down the solution of this equation\nstarting from an initial value w0, and show that the elements of w decay exponen-\ntially to zero.\n9.4 (?) Verify that the network function deﬁned by (9.6) and (9.7) is invariant under\nthe transformation (9.8) applied to the inputs, provided the weights and biases are\nsimultaneously transformed using (9.9) and (9.10). Similarly, show that the network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1470, "text": "outputs can be transformed according to (9.11) by applying the transformation (9.12)\nand (9.13) to the second-layer weights and biases.\n9.5 (??) By using Lagrange multipliers, show that minimizing the regularized errorAppendix C\nfunction given by (9.19) is equivalent to minimizing the unregularized error func-\ntion E(w) subject to the constraint (9.20). Discuss the relationship between the\nparameters \u0011and \u0015.\n9.6 (???) Consider a quadratic error function of the form\nE = E0 + 1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1471, "text": "E = E0 + 1\n2(w −w?)TH(w −w?) (9.56)\nwhere w?represents the minimum, and the Hessian matrixH is positive deﬁnite and\nconstant. Suppose the initial weight vector w(0) is chosen to be at the origin and is\nupdated using simple gradient descent:\nw(\u001c) = w(\u001c−1) −\u001a∇E (9.57)\nwhere \u001c denotes the step number, and \u001ais the learning rate (which is assumed to be\nsmall). Show that, after \u001c steps, the components of the weight vector parallel to the\neigenvectors of H can be written\nw(\u001c)\nj = {1−(1 −\u001a\u0011j)\u001c}w?"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1472, "text": "w(\u001c)\nj = {1−(1 −\u001a\u0011j)\u001c}w?\nj (9.58)\nwhere wj = wTuj, and uj and \u0011j are the eigenvectors and eigenvalues of H, re-\nspectively, deﬁned by\nHuj = \u0011juj: (9.59)\nExercises 283\nShow that as \u001c →∞, this gives w(\u001c) → w? as expected, provided |1−\u001a\u0011j|< 1.\nNow suppose that training is halted after a ﬁnite number \u001c of steps. Show that the\ncomponents of the weight vector parallel to the eigenvectors of the Hessian satisfy\nw(\u001c)\nj ≃w?\nj when \u0011j ≫ (\u001a\u001c)−1 (9.60)\n|w(\u001c)\nj |≪|w?\nj| when \u0011j ≪ (\u001a\u001c)−1 : (9.61)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1473, "text": "|w(\u001c)\nj |≪|w?\nj| when \u0011j ≪ (\u001a\u001c)−1 : (9.61)\nThis result shows that(\u001a\u001c)−1 plays an analogous role to the regularization parameter\n\u0015in weight decay.\n9.7 (??) Consider a neural network in which multiple weights are constrained to have the\nsame value. Discuss how the standard backpropagation algorithm must be modiﬁed\nto ensure that such constraints are satisﬁed when evaluating the derivatives of an\nerror function with respect to the adjustable parameters in the network."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1474, "text": "9.8 (?) Consider a mixture distribution deﬁned by\np(w) =\nM∑\nj=1\n\u0019jN(w|\u0016j;\u001b2\nj) (9.62)\nin which {\u0019j}can be viewed as prior probabilities p(j) for the corresponding Gaus-\nsian components. Using Bayes’ theorem, show that the corresponding posterior\nprobabilities p(j|w) are given by (9.24).\n9.9 (??) Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.25).\n9.10 (??) Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.26)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1475, "text": "9.11 (??) Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.28).\n9.12 (??) Show that the derivatives of the mixing coefﬁcients{\u0019k}deﬁned by (9.30) with\nrespect to the auxiliary parameters {\u0011j}are given by\n@\u0019k\n@\u0011j\n= \u000ejk\u0019j −\u0019j\u0019k: (9.63)\nHence, by making use of the constraint ∑\nk\rk(wi) = 1 for all i, derive the result\n(9.31).\n9.13 (?) Verify that combining (9.35), (9.36), and (9.37) gives a single overall equation\nfor the whole network in the form (9.40)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1476, "text": "for the whole network in the form (9.40).\n9.14 (??) The expected sum-of-squares error EAV for a simple committee model can be\ndeﬁned by (9.46), and the expected error of the committee itself is given by (9.47).\nAssuming that the individual errors satisfy (9.48) and (9.49), derive the result (9.50).\n284 9. REGULARIZATION\n9.15 (??) By making use of Jensen’s inequality (2.102) for the special case of the convex\nfunction f(x) = x2, show that the average expected sum-of-squares error EAV of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1477, "text": "the members of a simple committee model, given by (9.46), and the expected error\nECOM of the committee itself, given by (9.47), satisfy\nECOM 6 EAV: (9.64)\n9.16 (??) By making use of Jensen’s in equality (2.102), show that the result (9.64) de-\nrived in the previous exercise holds for any error function E(y), not just sum-of-\nsquares, provided it is a convex function of y.\n9.17 (??) Consider a committee in which we allow unequal weighting of the constituent\nmodels, so that\nyCOM(x) =\nM∑\nm=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1478, "text": "models, so that\nyCOM(x) =\nM∑\nm=1\n\u000bmym(x): (9.65)\nTo ensure that the predictions yCOM(x) remain within sensible limits, suppose that\nwe require that they be bounded at each value of x by the minimum and maximum\nvalues given by any of the members of the committee, so that\nymin(x) 6 yCOM(x) 6 ymax(x): (9.66)\nShow that a necessary and sufﬁcient condition for this constraint is that the coefﬁ-\ncients \u000bm satisfy\n\u000bm > 0;\nM∑\nm=1\n\u000bm = 1: (9.67)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1479, "text": "cients \u000bm satisfy\n\u000bm > 0;\nM∑\nm=1\n\u000bm = 1: (9.67)\n9.18 (???) Here we explore the effect of dropout regularization on a simple linear regres-\nsion model trained using least squares. Consider a model of the form\nyk =\nD∑\ni=1\nwkixi (9.68)\nalong with a sum-of-squares error function given by\nE(W) =\nN∑\nn=1\nK∑\nk=1\n{\ntnk −\nD∑\ni=1\nwkiRnixni\n}2\n(9.69)\nwhere the elements Rni ∈{0;1}of the dropout matrix are chosen randomly from\na Bernoulli distribution with parameter \u001a. We now take an expectation over the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1480, "text": "distribution of random dropout parameters. Show that\nE[Rni] = \u001a (9.70)\nE[RniRnj] = \u000eij\u001a+ (1 −\u000eij)\u001a2: (9.71)\nExercises 285\nHence, show that the expected error function for this dropout model is given by\nE[E(W)] =\nN∑\nn=1\nK∑\nk=1\n{\nynk −\u001a\nD∑\ni=1\nwkixni\n}2\n(9.72)\n+ \u001a(1 −\u001a)\nN∑\nn=1\nK∑\nk=1\nD∑\ni=1\nw2\nkix2\nni: (9.73)\nThus, we see that the expected error function corresponds to a sum-of-squares error\nwith a quadratic regularizer in which the regularization coefﬁcient is scaled sepa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1481, "text": "rately for each input variable according to the data values seen by that input. Finally,\nwrite down a closed-form solution for the weight matrix that minimizes this regular-\nized error function.\n10\nConvolutional\nNetworks\nThe simplest machine learning models assume that the observed data values are un-\nstructured, meaning that the elements of the data vectors x = ( x1;:::;x D) are\ntreated as if we do not know anything in advance about how the individual elements"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1482, "text": "might relate to each other. If we were to make a random permutation of the ordering\nof these variables and apply this ﬁxed permutation consistently on all training and\ntest data, there would be no difference in the performance for the models considered\nso far.\nMany applications of machine learning, however, involve structured data in\nwhich there are additional relationships between input variables. For example, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1483, "text": "words in natural language form a sequence, and if we were to model language as aChapter 12\ngenerative autoregressive process then we would expect each word to depend more\nstrongly on the immediately preceding words and less so on words much earlier in\nthe sequence. Likewise, the pixels of an image have a well-deﬁned spatial relation-\n287© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1484, "text": "C. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_10"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1485, "text": "288 10. CONVOLUTIONAL NETWORKS\nship to each other in which the input variables are arranged in a two-dimensional\ngrid, and nearby pixels have highly correlated values.\nWe have already seen that our knowledge of the structure of speciﬁc data modal-\nities can be utilized through the addition of a regularization term to the error functionSection 9.1\nin the training objective, through data augmentation, or through modiﬁcations to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1486, "text": "model architecture. These approaches can help guide the model to respect certain\nproperties such as invariance and equivariance with respect to transformations of theSection 9.1.4\ninput data. In this chapter we will take a look at an architectural approach called a\nconvolutional neural network (CNN), which we will see can be viewed as a sparsely\nconnected multilayer network with parameter sharing, and designed to encode in-\nvariances and equivariances speciﬁc to image data.\n10.1."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1487, "text": "10.1.\nComputer Vision\nThe automatic analysis and interpretation of image data form the focus of the ﬁeld\nof computer vision and represent a major application area for machine learning\n(Szeliski, 2022). Historically, computer vision was based largely on 3-dimensional\nprojective geometry. Hand-crafted features were constructed and used as input to\nsimple learning algorithms (Hartley and Zisserman, 2004). However, it was one"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1488, "text": "of the ﬁrst ﬁelds to be transformed by the deep learning revolution, predominantly\nthanks to the CNN architecture. Although the architecture was originally developed\nin the context of image analysis, it has also been applied in other domains such as the\nanalysis of sequential data. Recently alternative architectures based on transformersChapter 12\nhave become competitive with convolutional networks in some applications."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1489, "text": "There are many applications for machine learning in computer vision, of which\nsome of the most commonly encountered are the following:\n1. Classiﬁcation of images, for example classifying an image of a skin lesion asFigure 1.1\nbenign or malignant. This is sometimes called ‘image recognition’.\n2. Detection of objects in an image and determining their locations within theFigure 10.19\nimage, for example detecting pedestrians from camera data collected by an\nautonomous vehicle."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1490, "text": "autonomous vehicle.\n3. Segmentation of images, in which each pixel is classiﬁed individually therebyFigure 10.26\ndividing the image into regions sharing a common label. For example, a nat-\nural scene might be segmented into sky, grass, trees, and buildings, whereas\na medical scan image could be segmented into cancerous tissue and normal\ntissue.\n4. Caption generation in which a textual description is generated automaticallyFigure 12.27\nfrom an image."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1491, "text": "from an image.\n5. Synthesis of new images, for example generating images of human faces. Im-Figure 1.3\nages can also be synthesized based on a text input describing the desired image\ncontent.\n10.1. Computer Vision 289\n6. Inpainting in which a region of an image is replaced with synthesized pixelsFigure 20.9\nthat are consistent with the rest of the image. This is used, for example, to\nremove unwanted objects during image editing."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1492, "text": "remove unwanted objects during image editing.\n7. Style transfer in which an input image in one style, for example a photograph,Figure 10.32\nis transformed into a corresponding image in a different style, for example an\noil painting.\n8. Super-resolution in which the resolution of an image is improved by increas-Figure 20.8\ning the number of pixels and synthesizing associated high-frequency informa-\ntion.\n9. Depth prediction in which one or more views are used to predict the distance"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1493, "text": "of the scene from the camera at each pixel in a target image.\n10. Scene reconstruction in which one or more two-dimensional images of a\nscene are used to reconstruct a three-dimensional representation.\n10.1.1 Image data\nAn image comprises a rectangular array of pixels, in which each pixel has either\na grey-scale intensity or more commonly a triplet of red, green, and blue channels\neach with its own intensity value. These intensities are non-negative numbers that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1494, "text": "also have some maximum value corresponding to the limits of the camera or other\nhardware device used to capture the image. For the most part, we will view the in-\ntensities as continuous variables, but in practice they are represented with ﬁnite pre-\ncision, for example as 8-bit numbers represented as integers in the range 0;:::; 255.\nSome images, such as the magnetic resonance imaging (MRI) scans used in medical\ndiagnosis, comprise three-dimensional grids of voxels. Similarly, videos comprise"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1495, "text": "a sequence of two-dimensional images and therefore can also be viewed as three-\ndimensional structures in which successive frames are stacked through time.\nNow consider the challenge of applying neural networks to image data to ad-\ndress some of the applications highlighted above. Images generally have a high\ndimensionality, with typical cameras capturing images comprising tens of megapix-\nels. Treating the image data as unstructured may therefore require a model with a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1496, "text": "vast number of parameters that would be infeasible to train. More signiﬁcantly, such\nan approach fails to take account of the highly structured nature of image data, in\nwhich the relative positions of different pixels play a crucial role. We can see this\nbecause if we take the pixels of an image and randomly permute them, then the result\nno longer looks like a natural image. Similarly, if we generate a synthetic image by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1497, "text": "drawing random values for the pixel intensities independently for each pixel, there\nis essentially zero chance of generating something that looks like a natural image.Figure 6.8\nLocal correlations are important, and in a natural image there is a much higher prob-\nability that two nearby pixels will have similar colours and intensities compared to\ntwo pixels that are far apart. This represents powerful prior knowledge and can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1498, "text": "used to encode strong inductive biases into a neural network, leading to models with\nfar fewer parameters and with much better generalization accuracy.\n290 10. CONVOLUTIONAL NETWORKS\n10.2. Convolutional Filters\nOne motivation for the introduction of convolutional networks is that for image data,\nwhich is the modality for which CNNs were designed, a standard fully connected\narchitecture would require vast numbers of parameters due to the high-dimensional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1499, "text": "nature of images. To see this, consider a colour image with 103 ×103 pixels, each\nwith three values corresponding to red, green, and blue intensities. If the ﬁrst hidden\nlayer of the network has, say, 1,000 hidden units, then we already have 3 ×109\nweights in the ﬁrst layer. Furthermore, such a network would have to learn any\ninvariances and equivariances by example, which would require huge data sets. By\ndesigning an architecture that incorporates our inductive bias about the structure"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1500, "text": "of images, we can reduce the data set requirements dramatically and also improve\ngeneralization with respect to symmetries in the image space.\nTo exploit the two-dimensional structure of image data to create inductive bi-\nases, we can use four interrelated concepts: hierarchy, locality, equivariance, and\ninvariance. Consider the task of detecting faces in images. There is a natural hier-\narchical structure because one image may contain several faces, and each face in-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1501, "text": "cludes elements such as eyes, and each eye has structure such as an iris, which itself\nhas structure such as edges. At the lowest level of the hierarchy, a node in a neural\nnetwork could detect the presence of a feature such as an edge using information\nthat is local to a small region of an image, and therefore it would only need to see\na small subset of the image pixels. More complex structures further up the hierar-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1502, "text": "chy can be detected by composing multiple features found at previous levels. A key\npoint, however, is that although we want to build the general concept of hierarchy\ninto the model, we want the details of the hierarchy, including the type of features\nextracted at each level, to be learned from data, not hand-coded. Hierarchical models\nﬁt naturally within the deep learning framework, which already allows very complex"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1503, "text": "concepts to be extracted from raw data through a succession of, possibly very many,\n‘layers’ of processing, in which the whole system is trained end-to-end.\n10.2.1 Feature detectors\nFor simplicity we will initially restrict our attention to grey-scale images (i.e.,\nones having a single channel). Consider a single unit in the ﬁrst layer of a neural\nnetwork that takes as input just the pixel values from a small rectangular region,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1504, "text": "or patch, from the image, as illustrated in Figure 10.1(a). This patch is referred to\nas the receptive ﬁeld of that unit, and it captures the notion of locality. We would\nlike weight values associated with this unit to learn to detect some useful low-level\nfeature. The output of this unit is given by the usual functional form comprising a\nweighted linear combination of the input values, which is subsequently transformed\nusing a nonlinear activation function:\nz= ReLU(wTx + w0) (10.1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1505, "text": "z= ReLU(wTx + w0) (10.1)\nwhere x is a vector of pixel values for the receptive ﬁeld, and we have assumed a\nReLU activation function. Because there is one weight associated with each input\n10.2. Convolutional Filters 291\nFigure 10.1 (a) Illustration of a re-\nceptive ﬁeld, showing a unit in a hid-\nden layer of a network that receives\ninput from pixels in a 3 ×3 patch of\nthe image. Pixels in this patch form\nthe receptive ﬁeld for this unit. (b)\nThe weight values associated with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1506, "text": "The weight values associated with\nthis hidden unit can be visualized as\na small 3×3 matrix, known as a ker-\nnel. There is also an additional bias\nparameter that is not shown here.\nimage\nhidden units\n(a)\n0:4 1 :7 0 :9\n2:3 −2:1 4:0\n−1:4 0:7 2 :1\n(b)\npixel, the weights themselves form a small two-dimensional grid known as a ﬁlter,\nsometimes also called a kernel, which itself can be visualized as an image. This is\nillustrated in Figure 10.1(b)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1507, "text": "illustrated in Figure 10.1(b).\nSuppose that w and w0 in (10.1) are ﬁxed and we ask for which value of the\ninput image patch x will this hidden unit give the largest output response. To answer\nthis we need to constrainx in some way, so let us suppose that its norm∥x∥2 is ﬁxed.\nThen the solution for x that maximizes wTx, and hence maximizes the response of\nthe hidden unit, is of the form x = \u000bw for some coefﬁcient \u000b. This says thatExercise 10.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1508, "text": "the maximum output response from this hidden unit occurs when it detects a patch\nof image that, up to an overall scaling, looks like the kernel image. Note that the\nReLU generates a non-zero output only when wTx exceeds a threshold of −w0, and\ntherefore the unit acts as a feature detector that signals when it ﬁnds a sufﬁciently\ngood match to its kernel.\n10.2.2 Translation equivariance\nNext note that if a small patch in a face image represents, for example, an eye"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1509, "text": "at that location, then the same set of pixel values in a different part of the image\nmust represent an eye at the new location. Our neural network needs to be able to\ngeneralize what it has learned in one location to all possible locations in the image,\nwithout needing to see examples in the training set of the corresponding feature at\nevery possible location. To achieve this, we can simply replicate the same hidden-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1510, "text": "unit weight values at multiple locations across the image, as illustrated for a one-\ndimensional input space in Figure 10.2.\nThe units of the hidden layer form a feature map in which all the units share\nthe same weights. Consequently if a local patch of an image produces a particular\n292 10. CONVOLUTIONAL NETWORKS\nFigure 10.2 Illustration of convolution for a one-dimensional array of\ninput values and a kernel of width 2. The connections\nare sparse and are shared by the hidden units, as shown"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1511, "text": "by the red and blue arrows in which links with the same\ncolour have the same weight values. This network there-\nfore has six connections but only two independent learn-\nable parameters.\nresponse in the unit connected to that patch, then the same set of pixel values at\na different location will produce the same response in the corresponding translated\nlocation in the feature map. This is an example of equivariance. We see that theSection 9.1.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1512, "text": "connections in this network are sparse in that most connections are absent. Also, the\nvalues of the weights are shared by all the hidden units, as indicated by the colours\nof the connections. This transformation is an example of a convolution.\nWe can extend the idea of convolution to two-dimensional images as follows\n(Dumoulin and Visin, 2016). For an image I with pixel intensities I(j;k), and a\nﬁlter K with pixel values K(l;m), the feature map C has activation values given by\nC(j;k) =\n∑\nl\n∑\nm"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1513, "text": "C(j;k) =\n∑\nl\n∑\nm\nI(j+ l;k + m)K(l;m) (10.2)\nwhere we have omitted the nonlinear activation function for clarity. This again is\nan example of a convolution and is sometimes expressed as C = I ∗K. Note that\nstrictly speaking (10.2) is called a cross-correlation, which differs slightly from the\nconventional mathematical deﬁnition of convolution, but here we will follow com-Exercise 10.4\nmon practice in the machine learning literature and refer to (10.2) as a convolution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1514, "text": "The relationship (10.2) is illustrated in Figure 10.3 for a 3 ×3 image and a 2 ×2\nﬁlter. Importantly, when using batch normalization in a convolutional network, theSection 7.4.2\nsame value of mean and variance must be used at every spatial location within a fea-\nture map when normalizing the states of the units to ensure that the statistics of the\nfeature map are independent of location.\nAs an example of the application of convolution, we consider the problem of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1515, "text": "detecting edges in images using a ﬁxed, hand-crafted convolutional ﬁlter. Intuitively,\nwe can think of a vertical edge as occurring when there is a signiﬁcant local change\nin the intensity between pixels as we move horizontally across the image. We can\nmeasure this by convolving the image with a 3 ×3 ﬁlter of the form\n−1 0 1\n−1 0 1\n−1 0 1 (10.3)\n10.2. Convolutional Filters 293\na b c\nd e f\ng h i\n∗\nj k\nl m\n=\naj+ bk+\ndl+ em\nbj+ ck+\nel+ fm\ndj+ ek+\ngl+ hm\nej+ fk+\nhl+ im\nI K C"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1516, "text": "el+ fm\ndj+ ek+\ngl+ hm\nej+ fk+\nhl+ im\nI K C\nFigure 10.3 Example of a 3×3 image convolved with a2×2 ﬁlter to give a resulting2×2 feature map.\nSimilarly we can detect horizontal edges by convolving with the transpose of this\nﬁlter:\n−1 −1 −1\n0 0 0\n1 1 1 (10.4)\nFigure 10.4 shows the results of applying these two convolutional ﬁlters to a sample\nimage. Note that in Figure 10.4(b) if a vertical edge corresponds to an increase in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1517, "text": "pixel intensity, the corresponding point on the feature map is positive (indicated by a\nlight colour), whereas if the vertical edge corresponds to a decrease in pixel intensity,\nthe corresponding point on the feature map is negative (indicated by a dark colour),\nwith analogous properties for Figure 10.4(c) for horizontal edges.\nComparing this convolutional structure with a standard fully connected net-\nwork, we see several advantages: (i) the connections are sparse, leading to far fewer"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1518, "text": "weights even with large images, (ii) the weight values are shared, greatly reducing\nthe number of independent parameters and consequently reducing the required size\n(a)\n (b)\n (c)\nFigure 10.4 Illustration of edge detection using convolutional ﬁlters showing (a) the original image, (b) the result\nof convolving with the ﬁlter (10.3) that detects vertical edges, and (c) the result of convolving with the ﬁlter (10.4)\nthat detects horizontal edges.\n294 10. CONVOLUTIONAL NETWORKS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1519, "text": "294 10. CONVOLUTIONAL NETWORKS\nof the training set needed to learn those parameters, and (iii) the same network can\nbe applied to images of different sizes without the need for retraining. We will re-\nturn to this ﬁnal point later, but for the moment, simply note that changing the sizeSection 10.4.3\nof the input image simply changes the size of the feature map but does not change\nthe number of weights, or the number of independent learnable parameters, in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1520, "text": "model. One ﬁnal observation regarding convolutional networks is that, by exploiting\nthe massive parallelism of graphics processing units (GPUs) to achieve high compu-\ntational throughput, convolutions can be implemented very efﬁciently.\n10.2.3 Padding\nWe see from Figure 10.3 that the convolution map is smaller than the original\nimage. If the image has dimensionality J×Kpixels and we convolve with a kernel\nof dimensionality M ×M (ﬁlters are typically chosen to be square) the resulting"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1521, "text": "feature map has dimensionality (J −M + 1) ×(K −M + 1). In some cases\nwe want the feature map to have the same dimensions as the original image. This\ncan be achieved by padding the original image with additional pixels around the\noutside, as illustrated in Figure 10.5. If we pad with P pixels then the output map has\ndimensionality (J+2P−M+1) ×(K+2P−M+1). If there is no padding, so that\nP = 0, this is called avalid convolution. When the value ofP is chosen such that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1522, "text": "output array has the same size as the input, corresponding to P = (M −1)=2, thisExercise 10.6\nis called a same convolution, because the image and the feature map have the same\ndimensions. In computer vision, ﬁlters generally use odd values of M, so that the\npadding can be symmetric on all sides of the image and that there is a well-deﬁned\ncentral pixel associated with the location of the ﬁlter. Finally, we have to choose a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1523, "text": "suitable value for the intensities associated with the padding pixels. A typical choice\nis to set the padding values to zero, after ﬁrst subtracting the mean from each image\nso that zero represents the average value of the pixel intensity. Padding can also be\napplied to feature maps for processing by subsequent convolutional layers.\n10.2.4 Strided convolutions\nIn typical image processing applications, the images can have very large num-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1524, "text": "bers of pixels, and since the kernels are often relatively small, so thatM ≪ J;K, the\nconvolutional feature map will be of a similar size to the original image and will be\nthe same size if same padding is used. Sometimes we wish to use feature maps that\nare signiﬁcantly smaller than the original image to provide ﬂexibility in the design\nof convolutional network architectures. One way to achieve this is to use strided"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1525, "text": "convolutions in which, instead of stepping the ﬁlter over the image one pixel at a\ntime, it is moved in larger steps of sizeS, called the stride. If we use the same stride\nhorizontally and vertically, then the number of elements in the feature map will beExercise 10.7\n⌊J+ 2P−M\nS −1\n⌋\n×\n⌊K+ 2P −M\nS −1\n⌋\n(10.5)\nwhere ⌊x⌋denotes the ‘ﬂoor’ ofx, i.e., the largest integer that is less than or equal\nto x. For large images and small ﬁlter sizes, the image map will be roughly a factor"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1526, "text": "of 1=Ssmaller than the original image.\n10.2. Convolutional Filters 295\nFigure 10.5 Illustration of a 4 ×4 image that has been padded\nwith additional pixels to create a 6 ×6 image. 0 0 0 0 0 0\n0 X11 X12 X13 X14 0\n0 X21 X22 X23 X24 0\n0 X31 X32 X33 X34 0\n0 X41 X42 X43 X44 0\n0 0 0 0 0 0\n10.2.5 Multi-dimensional convolutions\nSo far we have considered convolutions over a single grey-scale image. For a\ncolour image there will be three channels corresponding to the red, green, and blue"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1527, "text": "colours. We can easily extend convolutions to cover multiple channels by extending\nthe dimensionality of the ﬁlter. An image with J ×K pixels and C channels will\nbe described by a tensor of dimensionality J ×K ×C. We can introduce a ﬁlterSection 6.3.7\ndescribed by a tensor of dimensionality M×M×Ccomprising a separate M×M\nﬁlter for each of the C channels. Assuming no padding and a stride of 1, this again\ngives a feature map of size(J−M+1)×(K−M+1), as is illustrated inFigure 10.6.\nimage"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1528, "text": "image\nhidden units\n(a)\n1:2 0 :8 −3:7\n−3:6 −2:1 4:0\n2:4 0 :7 2 :1\n−3:2 0:7 1 :3\n−1:4 −2:1 4:0\n4:2 0 :7 2 :1\n0:4 1 :7 0 :9\n2:3 −2:1 4:0\n−1:0 0:7 2 :1\n(b)\nFigure 10.6 (a) Illustration of a multi-dimensional ﬁlter that takes input from across the R, G, and B channels.\n(b) The kernel here has 27 weights (plus a bias parameter not shown) and can be visualized as a 3 ×3 ×3\ntensor.\n296 10. CONVOLUTIONAL NETWORKS\nFigure 10.7 The multi-dimensional convolu-\ntional ﬁlter layer shown in Figure 10.6 can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1529, "text": "tional ﬁlter layer shown in Figure 10.6 can be\nextended to include multiple independent ﬁlter\nchannels.\nWe now make a further important extension to convolutions. Up to now we\nhave created a single feature map in which all the points in the feature map share the\nsame set of learnable parameters. For a ﬁlter of dimensionality M ×M ×C, this\nwill have M2C weight parameters, irrespective of the size of the image. In addition"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1530, "text": "there will be a bias parameter associated with this unit. Such a ﬁlter is analogous\nto a single hidden node in a fully connected network, and it can learn to detect only\none kind of feature and is therefore very limited. To build more ﬂexible models,\nwe simply include multiple such ﬁlters, in which each ﬁlter has its own independent\nset of parameters giving rise to its own independent feature map, as illustrated in\nFigure 10.7. We will again refer to these separate feature maps as channels. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1531, "text": "ﬁlter tensor now has dimensionality M ×M ×C×COUT where C is the number\nof input channels and COUT is the number of output channels. Each output channel\nwill have its own associated bias parameter, so the total number of parameters will\nbe (M2C+ 1)COUT.\nA useful concept in designing convolutional networks is the 1 ×1 convolution\n(Lin, Chen, and Yan, 2013), which is simply a convolutional layer in which the ﬁlter\nsize is a single pixel. The ﬁlters have C weights, one for each input channel, plus"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1532, "text": "a bias. One application for 1 ×1 convolutions is simply to change the number of\nchannels (typically to reduce the number of channels) without changing the size of\nthe feature maps, by setting the number of output channels to be different to the\nnumber of input channels. It is therefore complementary to strided convolutions or\npooling in that it reduces the number of channels rather than the dimensionality of\nthe channels.\n10.2.6 Pooling"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1533, "text": "the channels.\n10.2.6 Pooling\nA convolutional layer encodes translationequivariance, whereby if a small patch\nof pixels, representing the receptive ﬁeld of a hidden unit, is moved to a different\n10.2. Convolutional Filters 297\nFigure 10.8 Illustration of max-pooling\nin which blocks of 2 ×2 pixels in a fea-\nture map are combined using the ‘max’\noperator to generate a new feature map\nof smaller dimensionality.\n3 5 4 6\n1 1 9 4\n7 10 9 5\n12 2 9 4\n5 9\n12 9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1534, "text": "3 5 4 6\n1 1 9 4\n7 10 9 5\n12 2 9 4\n5 9\n12 9\nlocation in the image, the associated outputs of the feature map will move to the\ncorresponding location in the feature map. This is valuable for applications such as\nﬁnding the location of an object within an image. For other applications, such as\nclassifying an image, we want the output to be invariant to translations of the input.\nIn all cases, however, we want the network to be able to learn hierarchical structure"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1535, "text": "in which complex features at a particular level are built up from simpler features\nat the previous level. In many cases the spatial relationship between those simpler\nfeatures will be important. For example, it is the relative positions of the eyes, nose,\nand mouth that help determine the presence of a face and not just the presence of\nthese features in arbitrary locations within the image. However, small changes in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1536, "text": "relative locations do not affect the classiﬁcation, and we want to be invariant to such\nsmall translations of individual features. This can be achieved using pooling applied\nto the output of the convolutional layer.\nPooling has similarities to using a convolutional layer in that an array of units is\narranged in a grid, with each unit taking input from a receptive ﬁeld in the previous\nfeature map layer. Again, there is a choice of ﬁlter size and of stride length. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1537, "text": "difference, however, is that the output of a pooling unit is a simple, ﬁxed function of\nits inputs, and so there are no learnable parameters in pooling. A common example\nof a pooling function is max-pooling (Zhou and Chellappa, 1988) in which each unit\nsimply outputs the max function applied to the input values. This is illustrated with\na simple example in Figure 10.8. Here the stride length is equal to the ﬁlter width,\nand so there is no overlap of the receptive ﬁelds."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1538, "text": "As well as building in some local translation invariance, pooling can also be\nused to reduce the dimensionality of the representation by down-sampling the feature\nmap. Note that using strides greater than1 in a convolutional layer also has the effect\nof down-sampling the feature maps.\nWe can interpret the activation of a unit in a feature map as a measure of the\nstrength of detection of a corresponding feature, so that the max-pooling preserves"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1539, "text": "information on whether the feature is present and with what strength but discards\nsome positional information. There are many other choices of pooling function, for\nexample average pooling in which the pooling function computes the average of the\nvalues from the corresponding receptive ﬁeld in the feature map. These all introduce\nsome degree of local translation invariance.\nPooling is usually applied to each channel of a feature map independently. For\n298 10. CONVOLUTIONAL NETWORKS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1540, "text": "298 10. CONVOLUTIONAL NETWORKS\nexample, if we have a feature map with 8 channels, each of dimensionality 64 ×64,\nand we apply max-pooling with a receptive ﬁeld of size 2 ×2 and a stride of 2, the\noutput of the pooling operation will be a tensor of dimensionality 32 ×32 ×8.\nWe can also apply pooling across multiple channels of a feature map, which\ngives the network the potential to learn other invariances beyond simple translation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1541, "text": "invariance. For example, if several channels in a convolutional layer learn to detect\nthe same feature but at different orientations, then max-pooling across those feature\nmaps will be approximately invariant to rotations.\nPooling also allows a convolutional network to process images of varying sizes.\nUltimately, the output, and generally some of the intermediate layers, of a convolu-\ntional network must have a ﬁxed size. Variable-sized inputs can be accommodated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1542, "text": "by varying the stride length of the pooling according to the size of the image such\nthat the number of pooled outputs remains constant.\n10.2.7 Multilayer convolutions\nThe convolutional network structure described so far is analogous to a single\nlayer in a standard fully connected neural network. To allow the network to discover\nand represent hierarchical structure in the data, we now extend the architecture by\nconsidering multiple layers of the kind described above. Each convolutional layer is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1543, "text": "described by a ﬁlter tensor of dimensionality M ×M ×CIN ×COUT in which the\nnumber of independent weight and bias parameters is(M2CIN+1)COUT. Each such\nconvolutional layer can optionally be followed by a pooling layer. We can now apply\nmultiple such layers of convolution and pooling in succession, in which the COUT\noutput channels of a particular layer, analogous to the RGB channels of the input\nimage, form the input channels of the next layer. Note that the number of channels"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1544, "text": "in a feature map is sometimes called the ‘depth’ of the feature map, but we prefer to\nreserve the term depth to mean the number of layers in a multilayer network.\nA key property that we built into the convolutional framework is that of locality,\nin which a given unit in a feature map takes information only from a small patch, the\nreceptive ﬁeld, in the previous layer. When we construct a deep neural network in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1545, "text": "which each layer is convolutional then the effective receptive ﬁeld of a unit in later\nlayers in the network becomes much larger than those in earlier layers, as seen in\nFigure 10.9.\nIn many applications, the output units of the network need to make predictions\nabout the image as a whole, for example in a classiﬁcation task, and so they need\nto combine information from across the whole of the input image. This is typically"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1546, "text": "achieved by introducing one or two standard fully connected layers as the ﬁnal stages\nof the network, in which each unit is connected to every unit in the previous layer.\nThe number of parameters in such an architecture can be manageable because the ﬁ-\nnal convolutional layer generally has much lower dimensionality than the input layer\ndue to the intermediate pooling layers. Nevertheless, the ﬁnal fully connected layers"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1547, "text": "may contain the majority of the independent degrees of freedom in the network even\nif the number of (shared) connections in the network is larger in the convolutional\nlayers.\nA complete CNN therefore comprises multiple layers of convolutions inter-\n10.2. Convolutional Filters 299\nFigure 10.9 Illustration of how the effective re-\nceptive ﬁeld grows with depth in\na multilayer convolutional network.\nHere we see that the red unit at\nthe top of the output layer takes\ninputs from a receptive ﬁeld in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1548, "text": "inputs from a receptive ﬁeld in\nthe middle layer of units, each of\nwhich has a receptive ﬁeld in the\nﬁrst layer of units. Thus, the ac-\ntivation of the red unit in the out-\nput layer depends on the outputs\nof 3 units in the middle layer and 5\nunits in the input layer.\nreceptive ﬁeld\nspersed with pooling operations, and often with conventional fully connected layers\nin the ﬁnal stages of the network. There are many choices to be made in designing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1549, "text": "such an architecture including the number of layers, the number of channels in each\nlayer, the ﬁlter sizes, the stride widths, and multiple other such hyperparameters. A\nwide variety of different architectures have been explored, although in practice it is\ndifﬁcult to make a systematic comparison of hyperparameter values using hold-out\ndata due to the high computational cost of training each candidate conﬁguration.\n10.2.8 Example network architectures"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1550, "text": "10.2.8 Example network architectures\nConvolutional networks were the ﬁrst deep neural networks (i.e., ones with\nmore than two learnable layers of parameters) to be successfully deployed in ap-\nplications. An early example was LeNet, which was used to classify low-resolution\nmonochrome images of handwritten digits (LeCun et al., 1989; LeCun et al., 1998).\nThe development of more powerful convolutional networks was accelerated through"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1551, "text": "the introduction of a large-scale benchmark data set called ImageNet (Deng et al.,\n2009) comprising some 14 million natural images each of which has been hand la-\nbelled into one of nearly 22,000 categories. This was a much larger data set than had\nbeen used previously, and the advances in the ﬁeld driven by ImageNet served to em-\nphasize the importance of large-scale data, alongside well-designed models having\nappropriate inductive biases, in building successful deep learning solutions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1552, "text": "A subset of images comprising 1,000 non-overlapping categories formed the ba-\nsis for the annual ImageNet Large Scale Visual Recognition Challenge. Again, this\nwas a much larger number of categories than the typically few dozen classes previ-\nously considered. Having so many categories made the problem much more chal-\nlenging because, if the classes were distributed uniformly, random guessing would\n300 10. CONVOLUTIONAL NETWORKS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1553, "text": "300 10. CONVOLUTIONAL NETWORKS\nhave an error rate of 99.9%. The data set has just over 1.28 million training images,\n50,000 validation images, and 100,000 test images. The classiﬁers are designed to\nproduce a ranked list of predicted output classes on test images, and results are re-\nported in terms of top-1 and top-5 error rates, meaning an image is deemed to be\ncorrectly classiﬁed if the true class appears at the top of the list or if it is in one of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1554, "text": "ﬁve highest-ranked class predictions. Early results with this data set achieved a top-5\nerror rate of around 25.5%. An important advance was made by the AlexNet convo-\nlutional network architecture (Krizhevsky, Sutskever, and Hinton, 2012), which won\nthe 2012 competition and reduced the top-5 error rate to a new record of 15.3%. Key\naspects of this model were the use of the ReLU activation function, the application of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1555, "text": "GPUs to train the network, and the use of dropout regularization. Subsequent yearsSection 9.6.1\nsaw further advances, leading to error rates of around 3%, which is somewhat better\nthan human-level performance for the same data, which is around 5% (Dodge and\nKaram, 2017). This can be attributed to the difﬁculty humans have in distinguishing\nsubtly different classes (for example multiple varieties of mushrooms).\nAs an example of a typical convolutional network architecture, we look in detail"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1556, "text": "at the VGG-16 model (Simonyan and Zisserman, 2014), where VGG stands for the\nVisual Geometry Group, who developed the model, and 16 refers to the number of\nlearnable layers in the model. VGG-16 has some simple design principles leading to\na relatively uniform architecture, shown in Figure 10.10, that minimizes the number\nof hyperparameter choices that need to be made. It takes an input image having\n224 ×224 pixels and three colour channels, followed by sets of convolutional layers"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1557, "text": "interspersed with down-sampling. All convolutional layers have ﬁlters of size 3 ×3\nwith a stride of 1, same padding, and a ReLU activation function, whereas the max-\npooling operations all use stride 2 and ﬁlter size 2 ×2 thereby down-sampling the\nnumber of units by a factor of 4. The ﬁrst learnable layer is a convolutional layer in\nwhich each unit takes input from a 3 ×3 ×3 ‘cube’ from the stack of input channels\nand so has 28 parameters including the bias. These parameters are shared across"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1558, "text": "all units in the feature map for that channel. There are 64 such feature channels in\nthe ﬁrst layer, giving an output tensor of size 224 ×224 ×64. The second layer\nis also convolutional and again has 64 channels. This is followed by max-pooling\ngiving feature maps of size 112 ×112. Layers 3 and 4 are again convolutional, of\ndimensionality 112 ×112, and each was chosen to have 128 channels. This increase\nin the number of channels offsets to some extent the down-sampling in the max-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1559, "text": "pooling layer to ensure that the number of variables in the representation at each\nlayer does not decrease too rapidly through the network. Again, this is followed by a\nmax-pooling operation to give a feature map size of 56 ×56. Next come three more\nconvolutional layers each with 256 channels, thereby again doubling the number of\nchannels in association with the down-sampling. This is followed by another max-\npooling to give feature maps of size 28 ×28 followed by three more convolutional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1560, "text": "layers each having 512 channels, followed by another max-pooling, which down-\nsamples to feature maps of size14×14. This is followed by three more convolutional\nlayers, although the number of feature maps in these layers is kept at 512, followed\nby another max-pooling, which brings the size of the feature maps down to 7 ×7.\nFinally there are three more layers that are fully connected meaning that they are\n10.2. Convolutional Filters 301\nconvolution → ReLU\nmax pooling\nfully connected → ReLU"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1561, "text": "max pooling\nfully connected → ReLU\nsoftmax activation\ninput image\n224 ×224 ×3\n← 224 ×224 ×64\n← 112 ×112 ×128\n← 56 ×56 ×256\n↑\n28 ×28 ×512\n↓\n14 ×14 ×512\n↑\n7 ×7 ×512\n↓\n1 ×1 ×4096\n↓\n1 ×1 ×1000\nFigure 10.10 The architecture of a typical convolutional network, in this case a model called VGG-16.\nstandard neural network layers with full connectivity and no sharing of parameters.\nThe ﬁnal max-pooling layer has 512 channels each of size 7 ×7 giving 25,088 units"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1562, "text": "in total. The ﬁrst fully connected layer has 4,096 units, each of which is connected\nto each of the max-pooling units. This is followed by a second fully connected layer\nagain with 4,096 units, and ﬁnally there is a third fully connected layer with 1,000\nunits so that the network can be applied to a classiﬁcation problem involving 1,000\nclasses. All the learnable layers in the network have nonlinear ReLU activation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1563, "text": "functions, except for the output layer, which has a softmax activation function. In\ntotal there are roughly 138 million independently learnable parameters in VGG-16,\nthe majority of which (nearly 103 million) are in the ﬁrst fully connected layer,\nwhereas most of the connections are in the ﬁrst convolutional layer.Exercise 10.8\nEarlier CNNs typically had fewer convolutional layers, as they had larger re-\nceptive ﬁelds. For example, Alexnet (Krizhevsky, Sutskever, and Hinton, 2012) has"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1564, "text": "11×11 receptive ﬁelds with a stride of 4. We saw inFigure 10.9 that larger receptive\nﬁelds can also be achieved implicitly by using multiple layers each having smaller\nreceptive ﬁelds. The advantage of the latter approach is that it requires signiﬁcantly\nfewer parameters, effectively imposing an inductive bias on the larger ﬁlters as they\nmust be composed of convolutional sub-ﬁlters. Although this is a highly complex"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1565, "text": "architecture, only the network function itself needs to be coded explicitly since the\nderivatives of the cost function can be evaluated using automatic differentiation andSection 8.2\nthe cost function optimized using stochastic gradient descent.\n302 10. CONVOLUTIONAL NETWORKS\n10.3. Visualizing Trained CNNs\nWe turn now to an exploration of the features learned by modern deep CNNs, and\nwe will see some remarkable similarities to the properties of the mammalian visual\ncortex.\n10.3.1 Visual cortex"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1566, "text": "cortex.\n10.3.1 Visual cortex\nHistorically, much of the motivation for CNNs came from pioneering research\nin neuroscience, which gave insights into the nature of visual processing in mam-\nmals including humans. Electrical signals from the retina are transformed through\na series of processing layers in the visual cortex, which is at the back of the brain,\nwhere the neurons are organized into two-dimensional sheets each of which forms a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1567, "text": "map of the two-dimensional visual ﬁeld. In their pioneering work, Hubel and Wiesel\n(1959) measured the electrical responses of individual neurons in the visual cortex\nof cats while presenting visual stimuli to the cats’ eyes. They discovered that some\nneurons, called ‘simple cells’, have a strong response to visual inputs with a simple\nedge oriented at a particular angle and located at a particular position within the vi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1568, "text": "sual ﬁeld, whereas other stimuli generated relatively little response in those neurons.\nMore detailed studies showed that the response of these simple cells can be modelled\nusing Gabor ﬁlters, which are two-dimensional functions deﬁned by\nG(x;y) = Aexp\n(\n−\u000b˜x2 −\f˜y2)\nsin (!˜x+ \u001e) (10.6)\nwhere\n˜x= (x−x0) cos(\u0012) + (y−y0) sin(\u0012) (10.7)\n˜y= −(x−x0) sin(\u0012) + (y−y0) cos(\u0012): (10.8)\nEquations (10.7) and (10.8) represent a rotation of the coordinate system through"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1569, "text": "an angle \u0012 and therefore the sin(-)term in (10.6) represents a sinusoidal spatial\noscillation oriented in a direction deﬁned by the polar angle \u0012, with frequency !\nand phase angle \u001e. The exponential factor in (10.6) creates a decay envelope that\nlocalizes the ﬁlter in the neighbourhood of position (x0;y0) and with decay rates\ngoverned by \u000band \f. Example Gabor ﬁlters are shown in Figure 10.11.\nHubel and Wiesel also discovered the presence of ‘complex cells’, which re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1570, "text": "spond to more complex stimuli and which seem to be derived by combining and\nprocessing the output of simple cells. These responses exhibit some degree of invari-\nance to small changes in the input such as shifts in location, analogous to the pooling\nunits in a convolutional deep network. Deeper levels of the mammalian visual pro-\ncessing system have even more speciﬁc responses and even greater invariance to\ntransformations of the visual input. Such cells have been termed ‘grandmother cells’"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1571, "text": "because such a cell could notionally respond if, and only if, the visual input corre-\nsponds to a person’s grandmother, irrespective of location, scale, lighting, or other\ntransformations of the scene. This work directly inspired an early form of deep neu-\nral network called the neocognitron (Fukushima, 1980), which was the forerunner of\n10.3. Visualizing Trained CNNs 303\nFigure 10.11 Examples of Gabor ﬁlters deﬁned\nby (10.6). The orientation angle\n\u0012 varies from 0 in the top row to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1572, "text": "\u0012 varies from 0 in the top row to\n\u0019=2 in the bottom row, whereas\nthe frequency varies from ! = 1\nin the left column to != 10 in the\nright column.\nconvolutional neural networks. The neocognitron had multiple layers of processing\ncomprising local receptive ﬁelds with shared weights followed by local averaging or\nmax-pooling to confer positional invariance. However, it lacked an end-to-end train-\ning procedure since it predated the development of backpropagation, relying instead"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1573, "text": "on greedy layer-wise learning through an unsupervised clustering algorithm.\n10.3.2 Visualizing trained ﬁlters\nSuppose we have a trained deep CNN and we wish to explore what the hidden\nunits have learned to detect. For the ﬁlters in the ﬁrst convolutional layer this is\nrelatively straightforward, as they correspond to small patches in the original input\nimage space, and so we can visualize the network weights associated with these"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1574, "text": "ﬁlters directly as small images. The ﬁrst convolutional layer computes inner products\nbetween the ﬁlters and the corresponding image patches, and so the unit will have a\nlarge activation when the inner product has a large magnitude.\nFigure 10.12 shows some example ﬁlters from the ﬁrst layer of a CNN trained\non the ImageNet data set. We see a remarkable similarity between these ﬁlters and\nthe Gabor ﬁlters of Figure 10.11. However, this does not imply that a convolutional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1575, "text": "neural network is a good model of how the brain works, because very similar results\ncan be obtained from a wide variety of statistical methods (Hyv ¨arinen, Hurri, and\nHoyer, 2009). This is because these characteristic ﬁlters are a general property of\nthe statistics of natural images and therefore prove useful for image understanding\nin both natural and artiﬁcial systems.\nAlthough we can visualize the ﬁlters in the ﬁrst layer directly, the subsequent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1576, "text": "layers in the network are harder to interpret because their inputs are not patches\nof images but groups of ﬁlter responses. One approach, analogous to that used by\nHubel and Wiesel, is to present a large number of image patches to the network and\n304 10. CONVOLUTIONAL NETWORKS\nFigure 10.12 Examples of learned ﬁlters from\nthe ﬁrst layer of AlexNet. Note\nthe remarkable similarity of many\nof the learned ﬁlters to the Gabor\nﬁlters in Figure 10.11, which cor-\nrespond to features detected by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1577, "text": "respond to features detected by\nliving neurons in the visual cortex\nof mammals.\nsee which produce the highest activation value in any particular hidden unit. Fig-\nure 10.13 shows examples obtained using a network with ﬁve convolutional layers,\nfollowed by two fully connected layers, trained on 1.3 million ImageNet data points\nspanning 1;000 classes. We see a natural hierarchical structure, with the ﬁrst layer re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1578, "text": "sponding to edges, the second layer responding to textures and simple shapes, layer 3\nshowing components of objects (such as wheels), and layer 5 showing entire objects.\nWe can extend this technique to go beyond simply selecting image patches from\nthe validation set and instead perform a numerical optimization over the input vari-\nables to maximize the activation of a particular unit (Zeiler and Fergus, 2013; Si-\nmonyan, Vedaldi, and Zisserman, 2013; Yosinski et al., 2015). If we chose the unit"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1579, "text": "to be one of the outputs then we can look for an image that is most representative\nof the corresponding class label. Because the output units generally have a softmax\nactivation function, it is better to maximise the pre-activation value that feeds into\nthe softmax rather than the class probability directly, as this ensures the optimization\ndepends on only one class. For example, if we seek the image that produces the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1580, "text": "strongest response to the class ‘dog’, then if we optimize the softmax output it could\ndrive the image to be, say, less like a cat because of the denominator in the softmax.\nThis approach is related to adversarial training. Unconstrained optimization of theChapter 17\noutput-unit activation, however, leads to individual pixel values being driven to inﬁn-\nity and also creates high-frequency structure that is difﬁcult to interpret, and so some"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1581, "text": "form of regularization is required to ﬁnd solutions that are closer to natural images.\nYosinski et al. (2015) used a regularization function comprising the sum of squares\nof the pixel values along with a procedure that alternates gradient-based updates to\nthe image pixel values with a blurring operation to remove high-frequency structure\nand a clipping operation that sets to zero those pixel values that make only small\ncontributions to the class label. Example results are shown in Figure 10.14."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1582, "text": "10.3. Visualizing Trained CNNs 305\nLayer 1 Layer 2 Layer 3 Layer 5\nFigure 10.13 Examples of image patches (taken from a validation set) that produce the strongest activation in\nthe hidden units in a network having ﬁve convolutional layers trained on ImageNet data. The top nine activations\nin each feature map are arranged as a3×3 grid for four randomly chosen channels in each of the corresponding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1583, "text": "layers. We see a steady progression in complexity with depth, from simple edges in layer 1 to complete objects\nin layer 5. [From Zeiler and Fergus (2013) with permission.]\n10.3.3 Saliency maps\nAnother way to gain insight into the features used by a convolutional network\nis through saliency maps, which aim to identify those regions of an image that are\nmost signiﬁcant in determining the class label. This is best done by investigating the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1584, "text": "ﬁnal convolutional layer because this still retains spatial localization, which becomes\nlost in the subsequent fully connected layers, and yet it has the highest level of se-\nmantic representation. The Grad-CAM (gradient class activation mapping) method\n(Selvaraju et al., 2016) ﬁrst computes, for a given input image, the derivatives of the\noutput-unit pre-activation a(c) for a given class c, before the softmax, with respect to\nthe pre-activations a(k)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1585, "text": "the pre-activations a(k)\nij of all the units in the ﬁnal convolutional layer for channel k.\nFor each channel in that layer, the average of those derivatives is evaluated to give\n\u000bk = 1\nMk\n∑\ni\n∑\nj\n@a(c)\n@a(k)\nij\n(10.9)\nwhere iand jindex the rows and columns of channel k, and Mk is the total number\nof units in that channel. These averages are then used to form a weighted sum of the\nform:\nL =\n∑\nk\n\u000bkA(k) (10.10)\nin which A(k) is a matrix with elements a(k)\nij . The resulting array has the same"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1586, "text": "ij . The resulting array has the same\ndimensionality as the ﬁnal convolutional layer, for example 14 ×14 for the VGG\nnetwork shown in Figure 10.10, and can be superimposed on the original image in\nthe form of a ‘heat map’ as seen inFigure 10.15.\n306 10. CONVOLUTIONAL NETWORKS\nFigure 10.14 Examples of synthetic images generated by maximizing the class probability with respect to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1587, "text": "image pixel channel values for a trained convolutional classiﬁer. Four different solutions, obtained with different\nsettings of the regularization parameters, are shown for each of four object classes. [From Y osinskiet al. (2015)\nwith permission.]\n10.3.4 Adversarial attacks\nGradients with respect to changes in the input image pixel values can also be\nused to create adversarial attacks against convolutional networks (Szegedy et al.,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1588, "text": "2013). These attacks involve making very small modiﬁcations to an image, at a level\nthat is imperceptible to a human, which cause the image to be misclassiﬁed by the\nneural network. One simple approach to creating adversarial images is called the\nfast gradient sign method (Goodfellow, Shlens, and Szegedy, 2014). This involves\nchanging each pixel value in an image x by a ﬁxed amount \u000fwith a sign determined\nby the gradient of an error function E(x;t) with respect to the pixel values. This"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1589, "text": "gives a modiﬁed image deﬁned by\nx′= x + \u000fsign(∇xE(x;t)): (10.11)\nHere tis the true label of x, and the error E(x;t) could, for example, be the neg-\native log likelihood of x. The required gradient can be computed efﬁciently using\nbackpropagation. During conventional training of a neural network, the network pa-Chapter 8\nrameters are adjusted to minimize this error, whereas the modiﬁcation deﬁned by\n(10.11) alters the image (while keeping the trained network parameters ﬁxed) so as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1590, "text": "Figure 10.15 Saliency maps for the\nVGG-16 network with respect to the\n‘dog’ and ‘cat’ categories. [From Sel-\nvaraju et al. (2016) with permission.]\nOriginal image Saliency map for ‘dog’ Saliency map for ‘cat’"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1591, "text": "10.3. Visualizing Trained CNNs 307\nFigure 10.16 Example of an adver-\nsarial attack against a trained convo-\nlutional network. The image on the\nleft is classiﬁed as a panda with conﬁ-\ndence 57:7%. The addition of a small\nlevel of a random-looking perturba-\ntion (that itself is classiﬁed as a ne-\nmatode with conﬁdence 8:2%) results\nin the image on the right, which is\nclassiﬁed as a gibbon with conﬁdence\n99:3%. [From Goodfellow, Shlens, and\nSzegedy (2014) with permission.]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1592, "text": "Szegedy (2014) with permission.]\npanda 57.7% gibbon 99.3%\nto increase the error. By keeping\u000fsmall, we ensure that the changes to the image are\nundetectable to the human eye. Remarkably, this can give images that are misclassi-\nﬁed by the network with high conﬁdence, as seen in the example in Figure 10.16.\nThe ability to fool neural networks in this way raises potential security concerns\nas it creates opportunities for attacking trained classiﬁers. It might appear that this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1593, "text": "issue arises from over-ﬁtting, in which a high-capacity model has adapted precisely\nto the speciﬁc image such that small changes in the input produce large changes\nin the predicted class probabilities. However, it turns out that an image that has\nbeen adapted to give a spurious output for a particular trained network can give\nsimilarly spurious outputs when fed to other networks (Goodfellow, Shlens, and\nSzegedy, 2014). Moreover, a similar adversarial result can be obtained with much"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1594, "text": "less ﬂexible linear models. It is even possible to create physical artefacts such that\na regular, uncorrupted image of the artefact will give erroneous predictions when\npresented to a trained neural network, as seen in Figure 10.17. Although these basic\nkinds of adversarial attacks can be addressed by simple modiﬁcations to the network\ntraining process, more sophisticated approaches are harder to defeat. Understanding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1595, "text": "the implications of these results and mitigating their potential pitfalls remain open\nareas of research.\nFigure 10.17 Two examples of physical stop\nsigns that have been modiﬁed. Images of these\nobjects are robustly classiﬁed as 45 mph speed-\nlimit signs by CNNs. [From Eykholt et al. (2018)\nwith permission.]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1596, "text": "308 10. CONVOLUTIONAL NETWORKS\n10.3.5 Synthetic images\nAs a ﬁnal example of image modiﬁcation that provides additional insights into\nthe operation of a trained convolutional network, we consider a technique called\nDeepDream (Mordvintsev, Olah, and Tyka, 2015). The goal is to generate a synthetic\nimage with exaggerated characteristics. We do this by determining which nodes in\na particular hidden layer of the network respond strongly to a particular image and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1597, "text": "then modifying the image to amplify those responses. For example, if we present\nan image of some clouds to a network trained on object recognition and a particular\nnode detects a cat-like pattern at a particular region of the image, then we modify\nthe image to be more ‘cat like’ in that region. To do this, we apply an image to the\ninput of the network and forward propagate through to some particular layer. We"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1598, "text": "then set the backpropagation \u000evariables for that layer equal to the pre-activations ofSection 8.1.2\nthe nodes and run backpropagation to the input layer to get a gradient vector over\nthe pixels of the image. Finally, we modify the image by taking a small step in the\ndirection of the gradient vector. This procedure can be viewed as a gradient-based\nmethod for increasing the functionExercise 10.10\nF(I) =\n∑\ni;j;k\naijk(I)2 (10.12)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1599, "text": "F(I) =\n∑\ni;j;k\naijk(I)2 (10.12)\nwhere aijk(I) is the pre-activation of the unit in row iand column jof channel kin\nthe chosen layer when the input image is I, and the sum is over all units and over\nall channels in that layer. To generate smooth-looking images, some regularization\nis applied in the form of spatial smoothing and pixel clipping. This process can\nthen be repeated multiple times if stronger enhancements are desired. Examples"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1600, "text": "of the resulting image are shown in Figure 10.18. It is interesting that even though\nconvolutional networks are trained to discriminate between object classes, they seem\nable to capture at least some of the information needed to generate images from those\nclasses.\nThis technique can be applied to a photograph, or we can start with inputs con-\nsisting of random noise to obtain an image generated entirely from the trained net-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1601, "text": "work. Although DeepDream provides some insights into the operation of the trained\nnetwork, it has primarily been used to generate interesting looking images as a form\nof artwork.\n10.4.\nObject Detection\nWe have motivated the design of CNNs primarily by the image classiﬁcation prob-\nlem, in which an entire image is assigned to a single class, for example ‘cat’ or\n‘bicycle’. This is reasonable for data sets such as ImageNet where, by design, each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1602, "text": "image is dominated by a single object. However, there are many other applications\nfor CNNs that are able to exploit the inbuilt inductive biases. More generally, the\nconvolutional layers of a CNN trained on a large image data base for a particular\ntask can learn internal representations that have broad applicability, and therefore a\n10.4. Object Detection 309\n5 iterations\n 30 iterations\nlayer 7\nlayer 10"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1603, "text": "5 iterations\n 30 iterations\nlayer 7\nlayer 10\nFigure 10.18 Examples of DeepDream applied to an image. The top row shows outputs when the algorithm\nis applied using the activations from the 7th convolutional layer of the VGG-16 network after ﬁve iterations and\nafter 30 iterations. Similarly, the bottom row shows examples using the 10th layer, again after ﬁve iterations and\nafter 30 iterations.\nCNN can be ﬁne-tuned for a wide range of speciﬁc tasks. We have already seenSection 6.3.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1604, "text": "an example of a convolutional network trained on ImageNet data, which through\ntransfer learning was able to achieve human-level performance on skin lesion classi-\nﬁcation.Section 1.1.1\n10.4.1 Bounding boxes\nMany images have multiple objects belonging to one or more classes, and we\nmay wish to detect the presence and class of each object. Moreover, in many appli-\ncations of computer vision we also need to determine the locations within the image"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1605, "text": "of any objects that are detected. For example, an autonomous vehicle that uses RGB\ncameras may need to detect the presence and location of pedestrians and also identify\nroad signs, other vehicles, etc.\nConsider the problem of specifying the location of an object in an image. A\nwidely used approach is to deﬁne a bounding box, which consists of a rectangle that\nﬁts closely to the boundary of the object, as illustrated inFigure 10.19. The bounding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1606, "text": "box can be deﬁned by the coordinates of its centre along with its width and height in\nthe form of a vector b = (bx;by;bW;bH). Here the elements of b can be speciﬁed\nin terms of pixels or as continuous numbers where, by convention, the top left of the\nimage is given coordinates (0;0) and the bottom right is given coordinates (1;1).\n310 10. CONVOLUTIONAL NETWORKS\nFigure 10.19 An image containing several objects from different classes in which the location of each"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1607, "text": "object is labelled by a close-ﬁtting rectangle known as a bounding box. Here blue boxes\ncorrespond to the class ‘car’, red boxes to the class ‘pedestrian’, and orange boxes to the\nclass ‘trafﬁc light’. [Original image courtesy of Wayve Technologies Ltd.]\nWhen images are assumed to contain one, and only one, object drawn from\na predeﬁned set of C classes, a CNN will generally have C output units whose\nactivation functions are deﬁned by the softmax function. An object can be localizedSection 5.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1608, "text": "by using an additional four outputs, with linear activation functions trained to predict\nthe bounding box coordinates(bx;by;bW;bH). Since these quantities are continuous,\na sum-of-squares error function over the corresponding outputs may be appropriate.\nThis is used for example by Redmon et al. (2015), who ﬁrst divide the image into a\n7 ×7 grid. For each grid cell, they use a convolutional network to output the class"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1609, "text": "and bounding box coordinates of any object associated with that grid cell, based on\nfeatures taken from the whole image.\n10.4.2 Intersection-over-union\nWe need a meaningful way to measure the performance of a trained network that\ncan predict bounding boxes. In image classiﬁcation the output of the network is a\nprobability distribution over class labels, and we can measure performance by look-\ning at the log likelihood for the true class labels on a test set. For object localization,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1610, "text": "however, we need some way to measure the accuracy of a bounding box relative to\nsome ground truth, where the latter could, for example, be obtained by human la-\nbelling. The extent to which the predicted and target boxes overlap can be used as\nthe basis for such a measure, but the area of the overlap will depend on the size of\nthe object within the image. Also a predicted bounding box should be be penalized\nfor the region of the prediction that lies outside the ground truth bounding box. A"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1611, "text": "better metric that addresses both of these issues is called intersection-over-union, or\nIoU, and is simply the ratio of the area of the intersection of the two bounding boxes\n10.4. Object Detection 311\nFigure 10.20 Illustration of the\nintersection-over-union metric for\nquantifying the accuracy of a bound-\ning box prediction. If the predicted\nbounding box is shown by the blue\nrectangle and the ground truth by the\nred rectangle, then the intersection-\nover-union is deﬁned as the ratio of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1612, "text": "over-union is deﬁned as the ratio of\nthe area of the intersection of the\nboxes, shown in green on the left,\ndivided by the area of their union,\nshown in green on the right.\narea of intersection area of union\nto that of their union, as illustrated in Figure 10.20. Note that the IoU measure lies\nin the range 0 to 1. Predictions can be labelled as correct if the IoU measure exceeds\na threshold, which is typically set at 0:5. Note that IoU is not generally used directly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1613, "text": "as a loss function for training as it is hard to optimize by gradient descent, and so\ntraining is typically performed using centred objects, and the IoU score is mainly\nused an evaluation metric.\n10.4.3 Sliding windows\nOne approach to object detection and object localization starts by creating a\ntraining set consisting of tightly cropped examples of the object to be detected, as\nwell as examples of similarly cropped sections of images that do not contain any"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1614, "text": "object (the ‘background’ class). This data set is used to train a classiﬁer, such as\na deep CNN, whose outputs represent the probability of there being an object of\neach particular class in the input window. The trained model is then used to detect\nobjects in a new image by ‘scanning’ an input window across the image and, for\neach location, taking the resulting subset of the image as input to the classiﬁer. This\nis called a sliding window. When an object is detected with high probability, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1615, "text": "associated window location then deﬁnes the corresponding bounding box.\nOne obvious drawback of this approach is that it can be computationally very\ncostly due to the large number of potential window positions in the image. Further-\nmore, the process may have to be repeated using windows of various scales to allow\nfor different sizes of object within the image. A cost saving can be made by moving\nthe input window in strides across the image, both horizontally and vertically, which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1616, "text": "are larger than one pixel. However, there is a trade-off between precision of location\nusing a small stride and reducing the computational cost by using a larger stride. The\ncomputational cost of a sliding window approach may be reasonable for simple clas-\nsiﬁers, but for deep neural networks potentially containing millions of parameters,\nthe cost of a naive implementation can be prohibitive.\nFortunately, the convolutional structure of the neural network allows for a dra-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1617, "text": "matic improvement in efﬁciency (Sermanet et al., 2013). We note that a convo-\nlutional layer within such a network itself involves sliding a feature detector, with\nshared weights, across the input image in strides. Consequently, when a sliding win-\ndow is used to generate multiple forward passes through a convolutional network\n312 10. CONVOLUTIONAL NETWORKS\nFigure 10.21 Illustration of replicated calculations\nwhen a CNN is used to process data\nfrom a sliding input window, in which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1618, "text": "from a sliding input window, in which\nthe red and blue boxes show two\noverlapping locations for the input\nwindow. The green box represents\none of the locations for the receptive\nﬁeld of a hidden unit in the ﬁrst con-\nvolutional layer, and the evaluation of\nthe corresponding hidden-unit activa-\ntion is shared across the two window\npositions.\nthere is substantial redundancy in the computation, as illustrated in Figure 10.21."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1619, "text": "Because the computational structure of sliding windows mirrors that of convolu-\ntions, it turns out to be remarkably simple to implement sliding windows efﬁciently\nin a convolutional network. Consider the simpliﬁed convolutional network in Fig-\nure 10.22, which consists of a convolutional layer followed by a max-pooling layer\nfollowed by a fully connected layer. For simplicity we have shown only a single\nchannel in each layer, but the extension to multiple channels is straightforward. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1620, "text": "input image to the network has size 6 ×6, the ﬁlters in the convolutional layer have\nsize 3 ×3 with stride 1, and the max-pooling layer has non-overlapping receptive\nﬁelds of size 2 ×2 with stride 1. This is followed by a fully connected layer with\na single output unit. Note that we can also view this ﬁnal layer as another convolu-\ntional layer with a ﬁlter size that is 2 ×2, so that there is only a single position for\nthe ﬁlter and hence a single output."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1621, "text": "the ﬁlter and hence a single output.\nNow suppose this network is trained on centred images of objects and then ap-\nplied to a larger image of size 8 ×8, as shown in Figure 10.23 in which we simply\nenlarge the network by increasing the size of the convolutional and max-pooling\nlayers. The convolution layer now has size 6 ×6 and the pooling layer has size\nFigure 10.22 Example of a simple\nconvolutional network having a sin-\ngle channel at each layer used to il-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1622, "text": "gle channel at each layer used to il-\nlustrate the concept of a sliding win-\ndow for detecting objects in images.\n6 ×6 input image\n3 ×3 convolution\n2 ×2 pooling\nfully connected\n10.4. Object Detection 313\n8 ×8 input image\n3 ×3 convolution\n2 ×2 pooling\nfully connected\nFigure 10.23 Application of the network shown in Figure 10.22 to a larger image in which the additional\ncomputation required corresponds to the blue regions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1623, "text": "3 ×3. There are now four output units each of which has its own softmax function.\nThe weights into this unit are shared across the four units. We see that the calcula-\ntions needed to process the input corresponding to a window position in the top left\ncorner of the input image are the same as those used to process the original 6 ×6\ninputs used in training. For the remaining window positions, only a small amount\nof additional computation is needed, as indicated by the blue squares, leading to a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1624, "text": "signiﬁcant increase in efﬁciency compared to a naive repeated application of the full\nconvolutional network. Note that the fully connected layers themselves now have aExercise 10.12\nconvolutional structure.\n10.4.4 Detection across scales\nAs well as looking for objects in different positions in the image, we also need\nto look for objects at different scales and at different aspect ratios. For example, a\ntight bounding box drawn around a cat will have a different aspect ratio when the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1625, "text": "cat is sitting upright compared to when it is lying down. Instead of using multiple\ndetectors with different sizes and shapes of input window, it is simpler but equivalent\nto use a ﬁxed input window and to make multiple copies of the input image each with\na different pair of horizontal and vertical scaling factors. The input window is then\nscanned over each of the image copies to detect objects, and the associated scaling"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1626, "text": "factors are then used to transform the bounding box coordinates back into the original\nimage space, as illustrated in Figure 10.24.\n314 10. CONVOLUTIONAL NETWORKS\n(a) (b) (c)\nFigure 10.24 Illustration of the detection and localization of objects at multiple scales and aspect ratios using\na ﬁxed input window. The original image (a) is replicated multiple times and each copy is scaled in the horizontal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1627, "text": "and/or vertical directions, as illustrated for a horizontal scaling in (b). A ﬁxed-sized window is then scanned\nover the scaled images. When an object is detected with high probability, as illustrated by the red box in (b),\nthe corresponding window coordinates can be projected back into the original image space to determine the\ncorresponding bounding box as shown in (c).\n10.4.5 Non-max suppression\nBy scanning a trained convolutional network over an image, it is possible to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1628, "text": "detect multiple instances of the same class of object within the image as well as in-\nstances of objects from other classes. However, this also tends to produce multiple\ndetections of the same object at similar locations, as illustrated inFigure 10.25. This\ncan be addressed using non-max suppression, which, for each object class in turn,\nworks as follows. It ﬁrst runs the sliding window over the whole image and evaluates"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1629, "text": "the probability of an object of that class being present at each location. Next it elim-\ninates all the associated bounding boxes whose probability is below some threshold,\nsay 0:7, giving a result of the kind illustrated in Figure 10.25. The box with the\nhighest probability is considered to be a successful detection, and the corresponding\nbounding box is recorded as a prediction. Next, any other boxes whose IoU with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1630, "text": "the successful detection box exceeds some threshold, say 0:5, is discarded. This is\nintended to eliminate multiple nearby detections of the same object. Then of the\nremaining boxes, the one with the highest probability is declared to be another suc-\ncessful detection, and the elimination step is repeated. The process continues until\nall bounding boxes have either been discarded or declared as successful detections.\n10.4.6 Fast region CNNs"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1631, "text": "10.4.6 Fast region CNNs\nAnother way to speed up object detection and localization is to note that a scan-\nning window approach applies the full power of a deep convolutional network to\nall areas of the image, even though some areas may be unlikely to contain an ob-\nject. Instead, we can apply some form of computationally cheaper technique, for\nexample a segmentation algorithm, to identify parts of the image where there is a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1632, "text": "higher probability of ﬁnding an object, and then apply the full network only to these\nareas, leading to techniques such as fast region proposals with CNN or fast R-CNN\n10.5. Image Segmentation 315\nFigure 10.25 Schematic illustration of multiple de-\ntections of the same object at nearby\nlocations, along with their associ-\nated probabilities. The red bounding\nbox corresponds to the highest over-\nall probability. Non-max suppres-\nsion eliminates the other overlapping"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1633, "text": "sion eliminates the other overlapping\ncandidate bounding boxes shown in\nblue, while preserving the detection\nof another instance of the same ob-\nject class shown by the bounding box\nin green.\n0.81\n0.91\n0.75\n0.95\n(Girshick, 2015). It is also possible to use a region proposal convolutional network\nto identify the most promising regions, leading to faster R-CNN (Ren et al., 2015),\nwhich allows end-to-end training of both the region proposal network and the detec-\ntion and localization network."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1634, "text": "tion and localization network.\nA disadvantage of the sliding window approach is that if we want a very precise\nlocalization the objects then we need to consider large numbers of ﬁnely spaced win-\ndow positions, which becomes computationally costly. A more efﬁcient approach is\nto combine sliding windows with the direct bounding box predictions that we dis-\ncussed at the start of this section (Sermanetet al., 2013). In this case, the continuous"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1635, "text": "outputs predict the position of the bounding box relative to the window position and\ntherefore provide some ﬁne-tuning to the predicted position.\n10.5.\nImage Segmentation\nIn an image classiﬁcation problem, an entire image is assigned to a single class la-\nbel. We have seen that more detailed information is provided if multiple objects are\ndetected and their positions recorded using bounding boxes. An even more detailedSection 10.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1636, "text": "analysis is obtained with semantic segmentation in which every pixel of an image is\nassigned to one of a predeﬁned set of classes. This means that the output space will\nhave the same dimensionality as the input image and can therefore be conveniently\nrepresented as an image with the same number of pixels. Although the input image\nwill generally have three channels for R, G, and B, the output array will have C"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1637, "text": "channels, if there are Cclasses, representing the probability for each class. If we as-\nsociate a different (arbitrarily chosen) colour with each class, then the prediction of a\nsegmentation network can be represented as an image in which each pixel is coloured\naccording to the class having the highest probability, as illustrated in Figure 10.26.\n10.5.1 Convolutional segmentation\nA simple way to approach a semantic segmentation problem would be to con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1638, "text": "struct a convolutional network that takes as input a rectangular section of the image\n316 10. CONVOLUTIONAL NETWORKS\nFigure 10.26 Example of an image and its corresponding semantic segmentation in which each pixel is\ncoloured according to its class. For example, blue pixels correspond to the class ‘car’, red pixels to the class\n‘pedestrian’, and orange pixels to the class ‘trafﬁc light’. [Courtesy of Wayve Technologies Ltd.]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1639, "text": "centred on a pixel and that has a single softmax output that classiﬁes that pixel. By\napplying such a network to each pixel in turn, the entire image can be segmented\n(this would require edge padding around the image depending on the size of the\ninput window). However, this approach would be extremely inefﬁcient due to redun-\ndant calculations caused by overlapping patches. As we have seen, we can removeFigure 10.21"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1640, "text": "this inefﬁciency by grouping together the forward-pass calculations for different in-\nput locations into a single network, which results in a model in which the ﬁnal fully\nconnected layers are also convolutional. We could therefore create a CNN in whichFigure 10.23\neach layer has the same dimensionality as the input image, by having stride1 at each\nlayer with same padding and no pooling. Each output unit has a softmax activation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1641, "text": "function with weights that are shared across all outputs. Although this could work,\nsuch a network would still need many layers, with multiple channels in each layer,\nto learn the complex internal representations needed to achieve high accuracy, and\noverall this would be prohibitively costly for images of reasonable resolution.\n10.5.2 Up-sampling\nAs we have already seen, most convolutional networks use several levels of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1642, "text": "down-sampling so that as the number of channels increases, the size of the feature\nmaps decreases, keeping the overall size and cost of the network tractable, while\nallowing the network to extract semantically meaningful high-order features from the\nimage. We can use this concept to create a more efﬁcient architecture for semantic\nsegmentation by taking a standard deep convolutional network and adding additional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1643, "text": "learnable layers that take the low-dimensional internal representation and transform\nit back up to the original image resolution (Long, Shelhamer, and Darrell, 2014; Noh,\nHong, and Han, 2015; Badrinarayanan, Kendall, and Cipolla, 2015), as illustrated in\nFigure 10.27.\nTo do this we need a way to reverse the down-sampling effects of strided convo-\nlutions and pooling operations. Consider ﬁrst the up-sampling analogue of pooling,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1644, "text": "where the output layer has a larger number of units than the input layer, for example\n10.5. Image Segmentation 317\nhigh resolution\nmid resolution\nlow resolution\nFigure 10.27 Illustration of a convolutional neural network used for semantic image segmentation, showing\nthe reduction in the dimensionality of the feature maps through a series of strided convolutions and/or pooling\noperations, followed by a series of transpose convolutions and/or unpooling which increase the dimensionality"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1645, "text": "back up to that of the original image.\nwith each input unit corresponding to a 2 ×2 block of output units. The question is\nthen what values to use for the outputs. To ﬁnd an up-sampling analogue of average\npooling, we can simply copy over each input value into all the corresponding out-\nput units, as shown in Figure 10.28(a). We see that applying average pooling to the\noutput of this operation regenerates the input.\nFor max-pooling, we can consider the operation shown in Figure 10.28(b) in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1646, "text": "which each input value is copied into the ﬁrst unit of the corresponding output block,\nand the remaining values in each block are set to zero. Again we see that apply-\ning a max-pooling operation to the output layer regenerates the input layer. This\nis sometimes called max-unpooling. Assigning the non-zero value to the ﬁrst ele-\nment of the output block seems arbitrary, and so a modiﬁed approach can be used\nthat also preserves more of the spatial information from the down-sampling layers"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1647, "text": "(Badrinarayanan, Kendall, and Cipolla, 2015). This is done by choosing a network\narchitecture in which each max-pooling down-sampling layer has a corresponding\n1 2\n3 4\n1 1 2 2\n1 1 2 2\n3 3 4 4\n3 3 4 4\n(a)\n1 2\n3 4\n1 0 2 0\n0 0 0 0\n3 0 4 0\n0 0 0 0\n(b)\nFigure 10.28 Illustration of unpooling operations showing (a) an analogue of average pooling and (b) an ana-\nlogue of max-pooling.\n318 10. CONVOLUTIONAL NETWORKS\n5 2 4 2\n7 1 0 3\n3 8 9 6\n4 7 8 1\n7 4\n8 9\nintermediate layers 3 7\n9 4\n0 0 7 0\n3 0 0 0"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1648, "text": "8 9\nintermediate layers 3 7\n9 4\n0 0 7 0\n3 0 0 0\n0 9 4 0\n0 0 0 0\nFigure 10.29 Some of the spatial information from a max-pooling layer, shown on the left, can be preserved by\nnoting the location of the maximum value for each 2 ×2 block in the input array, and then in the corresponding\nup-sampling layer, placing the non-zero entry at the corresponding location in the output array.\nup-sampling layer later in the network. Then during down-sampling, a record is kept"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1649, "text": "of which element in each block had the maximum value, and then in the correspond-\ning up-sampling layer, the non-zero element is chosen to have the same location, as\nillustrated for 2 ×2 max-pooling in Figure 10.29.\n10.5.3 Fully convolutional networks\nThe up-sampling methods considered above are ﬁxed functions, much like the\naverage-pooling and max-pooling down-sampling operations. We can also use a\nlearned up-sampling that is analogous to strided convolution for down-sampling. In"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1650, "text": "strided convolution, each unit on the output map is connected via shared learnable\nweights to a small patch on the input map, and as we move one step through the\noutput array, the ﬁlter is moved two or more steps across the input array, and hence\nthe output array has lower dimensionality than the input array. For up-sampling, we\nuse a ﬁlter that connects one pixel in the input array to a patch in the output array,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1651, "text": "and then chose the architecture so that as we move one step across the input array, we\nmove two or more steps across the output array (Dumoulin and Visin, 2016). This is\nillustrated for 3 ×3 ﬁlters, and an output stride of 2, in Figure 10.30. Note that there\nare output cells for which multiple ﬁlter positions overlap, and the corresponding\noutput values can be found either by summing or by averaging the contributions\nfrom the individual ﬁlter positions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1652, "text": "from the individual ﬁlter positions.\nThis up-sampling is called transpose convolution because, if the down-sampling\nconvolution is expressed in matrix form, the corresponding up-sampling is given by\nthe transpose matrix. It is also called ‘fractionally strided convolution’ because theExercise 10.13\nstride of a standard convolution is the ratio of the step size in the output layer to the\nstep size in the input layer. In Figure 10.30, for example, this ratio is 1=2. Note"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1653, "text": "that this is sometimes also referred to as ‘deconvolution’, but it is better to avoid this\nterm since deconvolution is widely used in mathematics to mean the inverse of the\noperation of convolution used in functional analysis, which is a different concept. If\nwe have a network architecture with no pooling layers, so that the down-sampling\nand up-sampling are done purely using convolutions, then the architecture is known"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1654, "text": "as a fully convolutional network (Long, Shelhamer, and Darrell, 2014). It can take\nan arbitrarily sized image and will output a segmentation map of the same size.\n10.5. Image Segmentation 319\nFigure 10.30 Illustration of transpose convolution for a3 ×\n3 ﬁlter with an output stride of 2. This can\nbe thought of as the inverse operation to a\n3 ×3 convolution. The red output patch is\ngiven by multiplying the kernel by the acti-\nvation of the red unit in the input layer, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1655, "text": "vation of the red unit in the input layer, and\nsimilarly for the blue output patch. The ac-\ntivations of cells for which patches overlap\nare calculated by summing or averaging the\ncontributions from the individual patches.\ninput 2 ×2\noutput 5 ×5\n10.5.4 The U-net architecture\nWe have seen that the down-sampling associated with strided convolutions and\npooling allows the number of channels to be increased without the size of the net-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1656, "text": "work becoming prohibitive. This also has the effect of reducing the spatial resolution\nand hence discarding positional information as the signals ﬂow through the network.\nAlthough this is ﬁne for image classiﬁcation, the loss of spatial information is a\nproblem for semantic segmentation as we want to classify each pixel. One approach\nfor addressing this is the U-net architecture (Ronneberger, Fischer, and Brox, 2015)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1657, "text": "illustrated in Figure 10.31, where the name comes from the U-shape of the diagram.\nFigure 10.31 The U-net architecture has a symmetrical arrangement of down-sampling and up-sampling lay-\ners, and the output from each down-sampling layer is concatenated with the corresponding up-sampling layer.\n320 10. CONVOLUTIONAL NETWORKS\nFigure 10.32 An example of neural style transfer showing a photograph of a canal scene (left) that has been"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1658, "text": "rendered in the style of The Wreck of a Transport Ship by J. M. W. Turner (centre) and in the style ofThe Starry\nNight by Vincent van Gogh (right). In each case the image used to provide the style is shown in the inset. [From\nGatys, Ecker, and Bethge (2015) with permission.]\nThe core concept is that for each down-sampling layer there is a corresponding up-\nsampling layer, and the ﬁnal set of channel activations at each down-sampling layer"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1659, "text": "is concatenated with the corresponding ﬁrst set of channels in the up-sampling layer,\nthereby giving those layers access to higher-resolution spatial information. Note that\n1 ×1 convolutions may be used in the ﬁnal layer of a U-net to reduce the number\nof channels down to the number of classes, which is then followed by a softmax\nactivation function.\n10.6.\nStyle Transfer\nAs we have seen, early layers in a deep convolutional network learn to detect simple"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1660, "text": "features such as edges and textures whereas later layers learn to detect more complex\nentities such as objects. We can exploit this property to re-render an image in theSection 10.3\nstyle of a different image using a process called neural style transfer (Gatys, Ecker,\nand Bethge, 2015). This is illustrated in Figure 10.32.\nOur goal is to generate a synthetic image G whose ‘content’ is deﬁned by an\nimage C and whose ‘style’ is taken from some other image S. This is achieved"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1661, "text": "by deﬁning an error function E(G) given by the sum of two terms, one of which\nencourages G to have a similar content to C whereas the other encourages G to\nhave a similar style to S:\nE(G) = Econtent(G;C) + Estyle(G;S): (10.13)\nThe concepts of content and style are deﬁned implicitly by the functional forms of\nthese two terms. We can then ﬁnd G by starting from a randomly initialized image\nand using gradient descent to minimize E(G)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1662, "text": "and using gradient descent to minimize E(G).\nTo deﬁne Econtent(G;C), we can pick a particular convolutional layer in the\nnetwork and measure the activations of the units in that layer when image G is\nused as input and also when image C is used as input. We can then encourage the\n10.6. Style Transfer 321\ncorresponding pre-activations to be similar by using a sum-of-squares error function\nof the form\nEcontent(G;C) =\n∑\ni;j;k\n{aijk(G) −aijk(C)}2 (10.14)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1663, "text": "∑\ni;j;k\n{aijk(G) −aijk(C)}2 (10.14)\nwhere aijk(G) denotes the pre-activation of the unit at position (i;j) in channel k\nof that layer when the input image is G, and similarly for aijk(C). The choice of\nwhich layer to use in deﬁning the pre-activations will inﬂuence the ﬁnal result, with\nearlier layers aiming to match low-level features like edges and later layers matching\nmore complex structures or even entire objects.\nIn deﬁning Estyle(G;C), the intuition is that style is determined by the co-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1664, "text": "occurrence of features from different channels within a convolutional layer. For\nexample, if the style image S is such that vertical edges are generally associated\nwith orange blobs, then we would like the same to be true for the generated image\nG. However, although Econtent(G;C) tries to match features in G at the same lo-\ncations as corresponding features in C, for the style error Estyle(G;S) we want G\nto have characteristics that match those of S but taken from any location, and so"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1665, "text": "we take an average over locations in a feature map. Again, consider a particular\nconvolutional layer. We can measure the extent to which a feature in channel kco-\noccurs with the corresponding feature in channel k′for input image G by forming\nthe cross-correlation matrix\nFkk′(G) =\nI∑\ni=1\nJ∑\nj=1\naijk(G)aijk′(G) (10.15)\nwhere Iand Jare the dimensions of the feature maps in this particular convolutional\nlayer, and the product aijkaijk′will be large if both features are activated. If there"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1666, "text": "are Kchannels in this layer, then Fkk′form the elements of a K×Kmatrix, called\nthe style matrix. We can measure the extent to which the two images G and S have\nthe same style by comparing their style matrices using\nEstyle(G;S) = 1\n(2IJK)2\nK∑\nk=1\nK∑\nk′=1\n{Fkk′(G) −Fkk′(S)}2 : (10.16)\nAlthough we could again make use of a single layer, more pleasing results are ob-\ntained by using contributions from multiple layers in the form\nEstyle(G;S) =\n∑\nl\n\u0015lE(l)\nstyle(G;S) (10.17)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1667, "text": "Estyle(G;S) =\n∑\nl\n\u0015lE(l)\nstyle(G;S) (10.17)\nwhere l denotes the convolutional layer. The coefﬁcients \u0015l determine the relative\nweighting between the different layers and also the weighting relative to the content\nerror term. These weighting coefﬁcients are adjusted empirically using subjective\njudgement.\n322 10. CONVOLUTIONAL NETWORKS\nExercises\n10.1 (?) Consider a ﬁxed weight vectorw and show that the input vectorx that maximizes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1668, "text": "the scalar product wTx, subject to the constraint that ∥x∥2 is constant, is given by\nx = \u000bw for some scalar\u000b. This can most easily be done using a Lagrange multiplier.Appendix C\n10.2 (??) Consider a convolutional network layer with a one-dimensional input array and\na one-dimensional feature map as shown in Figure 10.2, in which the input array\nhas dimensionality 5 and the ﬁlters have width 3 with a stride of 1. Show that this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1669, "text": "can be expressed as a special case of a fully connected layer by writing down the\nweight matrix in which missing connections are replaced by zeros and where shared\nparameters are indicated by using replicated entries. Ignore any bias parameters.\n10.3 (?) Explicitly calculate the output of the following convolution of a4×4 input matrix\nwith a 2 ×2 ﬁlter:\n2 5 −3 0\n0 6 0 −4\n−1 −3 0 2\n5 0 0 3\n*\n−2 0\n4 6 =\n? ? ?\n? ? ?\n? ? ?\n(10.18)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1670, "text": "5 0 0 3\n*\n−2 0\n4 6 =\n? ? ?\n? ? ?\n? ? ?\n(10.18)\n10.4 (??) If an image I has J ×K pixels and a ﬁlter K has L×M elements, write\ndown the limits for the two summations in (10.2). In the mathematics literature, the\noperation (10.2) would be called a cross-correlation, whereas a convolution would\nbe deﬁned by\nC(j;k) =\n∑\nl\n∑\nm\nI(j−l;k −m)K(l;m): (10.19)\nWrite down the limits for the summations in (10.19). Show that (10.19) can be\nwritten in the equivalent ‘ﬂipped’ form\nC(j;k) =\n∑\nl\n∑\nm"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1671, "text": "C(j;k) =\n∑\nl\n∑\nm\nI(j+ l;k + m)K(l;m) (10.20)\nand again write down the limits for the summations.\n10.5 (?) In mathematics, a convolution for a continuous variable xis deﬁned by\nF(x) =\n∫∞\n−∞\nG(y)k(x−y) dy (10.21)\nwhere k(x−y) is the kernel function. By considering a discrete approximation to\nthe integral, explain the relationship to a convolutional layer, deﬁned by (10.19), in\na CNN.\nExercises 323\n10.6 (?) Consider an image of size J ×K that is padded with an additional P pixels on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1672, "text": "all sides and which is then convolved using a kernel of size M ×M where M is an\nodd number. Show that if we chooseP = (M−1)=2, then the resulting feature map\nwill have size J×Kand hence will be the same size as the original image.\n10.7 (?) Show that if a kernel of sizeM×Mis convolved with an image of sizeJ×Kwith\npadding of depth P and strides of length S then the dimensionality of the resulting\nfeature map is given by (10.5)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1673, "text": "feature map is given by (10.5)\n10.8 (??) For each of the 16 layers in the VGG-16 CNN shown in Figure 10.10, evaluate\n(i) the number of weights (i.e., connections) including biases and (ii) the number\nof independently learnable parameters. Conﬁrm that the total number of learnable\nparameters in the network is approximately 138 million.\n10.9 (??) Consider a convolution of the form (10.2) and suppose that the kernel is sepa-\nrable so that\nK(l;m) = F(l)G(m) (10.22)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1674, "text": "rable so that\nK(l;m) = F(l)G(m) (10.22)\nfor some functions F(-)and G(-). Show that instead of performing a single two-\ndimensional convolution it is now possible to compute the same answer using two\none-dimensional convolutions thereby resulting in a signiﬁcant improvement in efﬁ-\nciency.\n10.10 (?) The DeepDream update procedure involves setting the \u000evariables for backprop-\nagation equal to the pre-activations of the nodes in the chosen layer and then running"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1675, "text": "backpropagation to the input layer to get a gradient vector over the pixels of the im-\nage. Show that this can be derived as a gradient optimization with respect to the\npixels of an image I applied to the function (10.12).\n10.11 (??) When designing a neural network to detect objects from C different classes in\nan image, we can use a1-of-(C+1) class label with one variable for each object class\nand one additional variable representing a ‘background’ class, i.e., an input image"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1676, "text": "region that does not contain an object belonging to any of the deﬁned classes. The\nnetwork will then output a vector of probabilities of length (C + 1). Alternatively,\nwe can use a single binary variable to denote the presence or absence of an object\nand then use a separate 1-of-C vector to denote the speciﬁc object class. In this\ncase, the network outputs a single probability representing the presence of an object"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1677, "text": "and a separate set of probabilities over the class label. Write down the relationship\nbetween these two sets of probabilities.\n10.12 (??) Calculate the number of computational steps required to make one forward\npass through the convolutional network shown in Figure 10.22, ignoring biases and\nignoring the evaluation of activation functions. Similarly, calculate the total num-\nber of computational steps for a single forward pass through the expanded network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1678, "text": "shown in Figure 10.23. Finally, evaluate ratio of nine repeated naive applications of\nthe network in Figure 10.22 to an 8 ×8 image compared to a single application of\nthe network in Figure 10.23. This ratio indicates the improvement in efﬁciency from\nusing a convolutional implementation of the sliding window technique.\n324 10. CONVOLUTIONAL NETWORKS\n10.13 (??) In this exercise we use one-dimensional vectors to demonstrate why a con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1679, "text": "volutional up-sampling is sometimes called a transpose convolution. Consider a\none-dimensional strided convolutional layer with an input having four units with ac-\ntivations (x1;x2;x3;x4), which is padded with zeros to give (0;x1;x2;x3;x4;0),\nand a ﬁlter with parameters (w1;w2;w3). Write down the one-dimensional activa-\ntion vector of the output layer assuming a stride of 2. Express this output in the\nform of a matrix A multiplied by the vector (0;x1;x2;x3;x4;0). Now consider"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1680, "text": "an up-sampling convolution in which the input layer has activations (z1;z2) with\na ﬁlter having values (w1;w2;w3) and an output stride of 2. Write down the six-\ndimensional output vector assuming that overlapping ﬁlter values are summed and\nthat the activation function is just the identity. Show that this can be expressed as a\nmatrix multiplication using the transpose matrix AT.\n11\nStructured\nDistributions\nWe have seen that probability forms one of the most important foundational concepts"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1681, "text": "for deep learning. For example, a neural network used for binary classiﬁcation is\ndescribed by a conditional probability distribution of the form\np(t|x;w) = y(x;w)t{1 −y(x;w)}(1−t) (11.1)\nwhere y(x;w) represents a neural network function that takes a vector x as input\nand is governed by a vector w of learnable parameters. The corresponding cross-\nentropy likelihood forms the basis for deﬁning an error function used to train the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1682, "text": "neural network. Although the network function might be extremely complex, the\nconditional distribution in (11.1) has a simple form. However, there are many im-\nportant deep learning models that have a much richer probabilistic structure, such as\nlarge language models, normalizing ﬂows, variational autoencoders, diffusion mod-\nels, and many others. To describe and exploit this structure, we introduce a powerful\n325© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1683, "text": "C. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_11"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1684, "text": "326 11. STRUCTURED DISTRIBUTIONS\nframework calledprobabilistic graphical models, or simplygraphical models, which\nallows structured probability distributions to be expressed in graphical form. When\ncombined with neural networks to deﬁne associated probability distributions, graph-\nical models offer huge ﬂexibility when creating sophisticated models that can be\ntrained end to end using stochastic gradient descent in which gradients are evaluated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1685, "text": "efﬁciently using auto-differentiation. In this chapter, we will focus on the core con-\ncepts of graphical models needed for applications in deep learning, whereas a more\ncomprehensive treatment of graphical models for machine learning can be found in\nBishop (2006).\n11.1.\nGraphical Models\nProbability theory can be expressed in terms of two simple equations known as the\nsum rule and the product rule. All of the probabilistic manipulations discussed inSection 2.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1686, "text": "this book, no matter how complex, amount to repeated application of these two\nequations. In principle, we could therefore formulate and use complex probabilistic\nmodels purely by using algebraic manipulation. However, we will ﬁnd it advan-\ntageous to augment the analysis using diagrammatic representations of probability\ndistributions, as these offer several useful properties:\n1. They provide a simple way to visualize the structure of a probabilistic model"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1687, "text": "and can be used to design and motivate new models.\n2. Insights into the properties of the model, including conditional independence\nproperties, can be obtained by inspecting the graph.\n3. The complex computations required to perform inference and learning in so-\nphisticated models can be expressed in terms of graphical operations, such as\nmessage-passing, in which the underlying mathematical operations are carried\nout implicitly."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1688, "text": "out implicitly.\nAlthough such graphical models have nodes and edges much like neural network\ndiagrams, their interpretation is speciﬁcally probabilistic and carries a richer seman-\ntics. To help avoid confusion, in this book we denote neural network diagrams in\nblue and probabilistic graphical models in red.\n11.1.1 Directed graphs\nA graph comprises nodes, also called vertices, connected by links, also known\nas edges. In a probabilistic graphical model, each node represents a random variable,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1689, "text": "and the links express probabilistic relationships between these variables. The graph\nthen captures the way in which the joint distribution over all the random variables\ncan be decomposed into a product of factors each depending only on a subset of the\nvariables. In this chapter we will focus on graphical models in which the links of the\ngraphs have a particular direction indicated by arrows. These are known as directed\ngraphical models and are also called Bayesian networks or Bayes nets."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1690, "text": "11.1. Graphical Models 327\nFigure 11.1 A directed graphical model representing the joint probability distri-\nbution over three variablesa, b, and c, corresponding to the decom-\nposition on the right-hand side of (11.3).\na b\nc\nThe other major class of graphical models areMarkov random ﬁelds, also known\nas undirected graphical models, in which the links do not carry arrows and have no\ndirectional signiﬁcance. Directed graphs are useful for expressing causal relation-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1691, "text": "ships between random variables, whereas undirected graphs are better suited to ex-\npressing soft constraints between random variables. Both directed and undirected\ngraphs can be viewed as special cases of a representation calledfactor graphs. From\nnow on we focus our attention on directed graphical models. Note, however, that\nundirected graphs, without the probabilistic interpretation, will also arise in our dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1692, "text": "cussion of graph neural networks in which the nodes represent deterministic vari-Chapter 13\nables as in standard neural networks.\n11.1.2 Factorization\nTo motivate the use of directed graphs to describe probability distributions, con-\nsider ﬁrst an arbitrary joint distribution p(a;b;c) over three variables a, b, and c.\nNote that at this stage, we do not need to specify anything further about these vari-\nables, such as whether they are discrete or continuous. Indeed, one of the powerful"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1693, "text": "aspects of graphical models is that a speciﬁc graph can make probabilistic statements\nfor a broad class of distributions. By application of the product rule of probability\n(2.9), we can write the joint distribution in the form\np(a;b;c) = p(c|a;b)p(a;b): (11.2)\nA second application of the product rule, this time to the second term on the right-\nhand side of (11.2), gives\np(a;b;c) = p(c|a;b)p(b|a)p(a): (11.3)\nNote that this decomposition holds for any choice of the joint distribution. We now"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1694, "text": "represent the right-hand side of (11.3) in terms of a simple graphical model as fol-\nlows. First we introduce a node for each of the random variables a, b, and cand as-\nsociate each node with the corresponding conditional distribution on the right-hand\nside of (11.3). Then, for each conditional distribution we add directed links (depicted\nas arrows) from the nodes corresponding to the variables on which the distribution is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1695, "text": "conditioned. Thus, for the factor p(c|a;b), there will be links from nodesaand bto\nnode c, whereas for the factor p(a), there will be no incoming links. The result is the\ngraph shown in Figure 11.1. If there is a link going from node ato node b, then we\nsay that node ais the parent of node b, and we say that node bis the child of node a.\nNote that we will not make any formal distinction between a node and the variable\nto which it corresponds but will simply use the same symbol to refer to both."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1696, "text": "328 11. STRUCTURED DISTRIBUTIONS\nFigure 11.2 Example of a directed graph describing the joint distri-\nbution over variables x1;:::;x 7. The corresponding de-\ncomposition of the joint distribution is given by (11.5).\nx1 x2 x3\nx4 x5\nx6 x7\nAn important point to note about (11.3) is that the left-hand side is symmetrical\nwith respect to the three variables a, b, and c, whereas the right-hand side is not. In\nmaking the decomposition in (11.3), we have implicitly chosen a particular ordering,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1697, "text": "namely a;b;c, and had we chosen a different ordering we would have obtained a\ndifferent decomposition and hence a different graphical representation.\nFor the moment let us extend the example of Figure 11.1 by considering the\njoint distribution over Kvariables given by p(x1;:::;x K). By repeated application\nof the product rule of probability, this joint distribution can be written as a product\nof conditional distributions, one for each of the variables:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1698, "text": "p(x1;:::;x K) = p(xK|x1;:::;x K−1 ) :::p (x2|x1)p(x1): (11.4)\nFor a given choice of K, we can again represent this as a directed graph having K\nnodes, one for each conditional distribution on the right-hand side of (11.4), with\neach node having incoming links from all lower numbered nodes. We say that this\ngraph is fully connected because there is a link between every pair of nodes.\nSo far, we have worked with completely general joint distributions, and so their"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1699, "text": "factorization, and associated representation as fully connected graphs, will be appli-\ncable to any choice of distribution. As we will see shortly, it is the absence of links\nin the graph that conveys interesting information about the properties of the class\nof distributions that the graph represents. Consider the graph shown in Figure 11.2.\nNote that it is not a fully connected graph because, for instance, there is no link from"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1700, "text": "x1 to x2 or from x3 to x7. We take this graph and extract the corresponding repre-\nsentation of the joint probability distribution written in terms of the product of a set\nof conditional distributions, one for each node in the graph. Each such conditional\ndistribution will be conditioned only on the parents of the corresponding node in the\ngraph. For instance, x5 will be conditioned on x1 and x3. The joint distribution of\nall seven variables is therefore given by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1701, "text": "all seven variables is therefore given by\np(x1)p(x2)p(x3)p(x4|x1;x2;x3)p(x5|x1;x3)p(x6|x4)p(x7|x4;x5): (11.5)\nThe reader should take a moment to study carefully the correspondence between\n(11.5) and Figure 11.2.\nWe can now state in general terms the relationship between a given directed\ngraph and the corresponding distribution over the variables. The joint distribution\ndeﬁned by a graph is given by the product, over all of the nodes of the graph, of\n11.1. Graphical Models 329"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1702, "text": "11.1. Graphical Models 329\na conditional distribution for each node conditioned on the variables corresponding\nto the parents of that node in the graph. Thus, for a graph with K nodes, the joint\ndistribution is given by\np(x1;:::;x K) =\nK∏\nk=1\np(xk|pa(k)) (11.6)\nwhere pa(k) denotes the set of parents ofxk. This key equation expresses thefactor-\nization properties of the joint distribution for a directed graphical model. Although"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1703, "text": "we have considered each node to correspond to a single variable, we can equally\nwell associate sets of variables and vector-valued or tensor-valued variables with the\nnodes of a graph. It is easy to show that the representation on the right-hand side of\n(11.6) is always correctly normalized provided the individual conditional distribu-\ntions are normalized.Exercise 11.1\nThe directed graphs that we are considering are subject to an important restric-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1704, "text": "tion, namely that there must be nodirected cycles. In other words, there are no closed\npaths within the graph such that we can move from node to node along links follow-\ning the direction of the arrows and end up back at the starting node. Such graphs are\nalso called directed acyclic graphs, or DAGs. This is equivalent to the statement thatExercise 11.2\nthere exists an ordering of the nodes such that there are no links that go from any\nnode to any lower-numbered node.\n11.1.3 Discrete variables"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1705, "text": "11.1.3 Discrete variables\nWe have discussed the importance of probability distributions that are mem-\nbers of the exponential family, and we have seen that this family includes manySection 3.4\nwell-known distributions as special cases. Although such distributions are relatively\nsimple, they form useful building blocks for constructing more complex probability\ndistributions, and the framework of graphical models is very useful in expressing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1706, "text": "the way in which these building blocks are linked together. There are two particu-\nlar choices for the component distributions that are widely used, corresponding to\ndiscrete variables and to Gaussian variables. We begin by examining the discrete\ncase.\nThe probability distribution p(x|\u0016)for a single discrete variable x having K\npossible states (using the 1-of-Krepresentation) is given by\np(x|\u0016) =\nK∏\nk=1\n\u0016xk\nk (11.7)\nand is governed by the parameters \u0016 = (\u00161;:::;\u0016 K)T. Due to the constraint∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1707, "text": "k\u0016k = 1, only K−1 values for \u0016k need to be speciﬁed to deﬁne the distribution.\nNow suppose that we have two discrete variables, x1 and x2, each of which has\nKstates, and we wish to model their joint distribution. We denote the probability of\nobserving both x1k = 1 and x2l = 1 by the parameter \u0016kl, where x1k denotes the\n330 11. STRUCTURED DISTRIBUTIONS\nFigure 11.3 (a) This fully connected\ngraph describes a general\ndistribution over two K-state\ndiscrete variables having a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1708, "text": "discrete variables having a\ntotal of K2 −1 parameters.\n(b) By dropping the link\nbetween the nodes, the\nnumber of parameters is\nreduced to 2(K−1).\nx1 x2\n\u0016\n(a)\nx1 x2\n\u00161 \u00162\n(b)\nkth component of x1, and similarly for x2l. The joint distribution can be written\np(x1;x2|\u0016) =\nK∏\nk=1\nK∏\nl=1\n\u0016x1kx2l\nkl :\nBecause the parameters \u0016kl are subject to the constraint ∑\nk\n∑\nl\u0016kl = 1, this distri-\nbution is governed by K2 −1 parameters. It is easily seen that the total number of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1709, "text": "parameters that must be speciﬁed for an arbitrary joint distribution overM variables\nis KM −1 and therefore grows exponentially with the number M of variables.\nUsing the product rule, we can factor the joint distributionp(x1;x2) in the form\np(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the\nx1 node to the x2 node as shown in Figure 11.3(a). The marginal distribution p(x1)\nis governed by K−1 parameters, as before. Similarly, the conditional distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1710, "text": "p(x2|x1) requires the speciﬁcation of K−1 parameters for each of the K possible\nvalues of x1. The total number of parameters that must be speciﬁed in the joint\ndistribution is therefore (K−1) + K(K−1) = K2 −1 as before.\nNow suppose that the variables x1 and x2 are independent, corresponding to\nthe graphical model shown in Figure 11.3(b). Each variable is then described by a\nseparate discrete distribution, and the total number of parameters would be2(K−1)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1711, "text": "For a distribution over M independent discrete variables, each having K states, the\ntotal number of parameters would be M(K −1), which therefore grows linearly\nwith the number of variables. From a graphical perspective, we have reduced the\nnumber of parameters by dropping links in the graph, at the expense of having a\nmore restricted class of distributions.\nMore generally, if we have M discrete variables x1;:::; xM, we can model the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1712, "text": "joint distribution using a directed graph with one variable for each node. The condi-\ntional distribution at each node is given by a set of non-negative parameters subject\nto the usual normalization constraint. If the graph is fully connected, then we have a\ncompletely general distribution having KM −1 parameters, whereas if there are no\nlinks in the graph, the joint distribution factorizes into the product of the marginal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1713, "text": "distributions, and the total number of parameters is M(K −1). Graphs having in-\ntermediate levels of connectivity allow for more general distributions than the fully\nfactorized one while requiring fewer parameters than the general joint distribution.\nAs an illustration, consider the chain of nodes shown in Figure 11.4. The marginal\ndistribution p(x1) requires K −1 parameters, whereas each of the M −1 condi-\ntional distributions p(xi|xi−1 ), for i = 2;:::;M , requires K(K −1) parameters."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1714, "text": "11.1. Graphical Models 331\nFigure 11.4 This chain of M discrete nodes, each hav-\ning K states, requires the speciﬁcation of\nK−1 + (M−1)K(K−1) parameters, which\ngrows linearly with the length M of the chain.\nIn contrast, a fully connected graph of M\nnodes would haveKM−1 parameters, which\ngrows exponentially with M.\nx1 x2 xM\n\u00161 \u00162 \u0016M\nThis gives a total parameter count ofK−1 + (M−1)K(K−1), which is quadratic\nin K and which grows linearly (rather than exponentially) with the length M of the\nchain."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1715, "text": "chain.\nAn alternative way to reduce the number of independent parameters in a model\nis by sharing parameters (also known as tying of parameters). For instance, in the\nchain example of Figure 11.4, we can arrange that all the conditional distributions\np(xi|xi−1 ), for i = 2;:::;M , are governed by the same set of K(K −1) param-\neters, giving the model shown in Figure 11.5. Together with the K−1 parameters\ngoverning the distribution of x1, this gives a total ofK2 −1 parameters that must be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1716, "text": "speciﬁed to deﬁne the joint distribution.\nAnother way of controlling the exponential growth of the number of parameters\nin models of discrete variables is to use parameterized representations for the condi-\ntional distributions instead of complete tables of conditional probability values. To\nillustrate this idea, consider the graph inFigure 11.6 in which all the nodes represent\nbinary variables. Each of the parent variables xi is governed by a single parame-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1717, "text": "ter \u0016i representing the probability p(xi = 1), giving M parameters in total for the\nparent nodes. The conditional distribution p(y|x1;:::;x M), however, would require\n2M parameters representing the probability p(y = 1) for each of the 2M possible\nsettings of the parent variables. Thus, in general the number of parameters required\nto specify this conditional distribution will grow exponentially with M. We can ob-\ntain a more parsimonious form for the conditional distribution by using a logistic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1718, "text": "sigmoid function acting on a linear combination of the parent variables, givingSection 3.4\np(y= 1|x1;:::;x M) = \u001b\n(\nw0 +\nM∑\ni=1\nwixi\n)\n= \u001b(wTx) (11.8)\nwhere \u001b(a) = (1+exp(−a))−1 is the logistic sigmoid,x = (x0;x1;:::;x M)T is an\n(M + 1)-dimensional vector of parent states augmented with an additional variable\nx0 whose value is clamped to 1, and w = (w0;w1;:::;w M)T is a vector of M + 1\nparameters. This is a more restricted form of conditional distribution than the general"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1719, "text": "case but is now governed by a number of parameters that grows linearly with M. In\nFigure 11.5 As in Figure 11.4 but with a single set of\nparameters \u0016shared amongst all the condi-\ntional distributions p(xi|xi−1).\nx1 x2 xM\n\u0016\u00161\n332 11. STRUCTURED DISTRIBUTIONS\nFigure 11.6 A graph comprising M parents x1;:::;x M and a single\nchild y, used to illustrate the idea of parameterized con-\nditional distributions for discrete variables.\nx1 x2 xM\ny\n …"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1720, "text": "x1 x2 xM\ny\n …\nthis sense, it is analogous to the choice of a restrictive form of covariance matrix (for\nexample, a diagonal matrix) in a multivariate Gaussian distribution.\n11.1.4 Gaussian variables\nWe now turn to graphical models in which the nodes represent continuous vari-\nables having Gaussian distributions. Each distribution is conditioned on the state\nof its parents in the graph. That dependence could take many forms, and here we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1721, "text": "focus on a speciﬁc choice in which the mean of each Gaussian is some linear func-\ntion of the states of the Gaussian parent variables. This leads to a class of models\ncalled linear-Gaussian models, which include many cases of practical interest such\nas probabilistic principal component analysis, factor analysis, and linear dynamicalSection 16.2\nsystems (Roweis and Ghahramani, 1999).\nConsider an arbitrary directed acyclic graph over D variables in which node i"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1722, "text": "represents a single continuous random variable xi having a Gaussian distribution.\nThe mean of this distribution is taken to be a linear combination of the states of its\nparent nodes pa(i) of node i:\np(xi|pa(i)) = N\n\nxi\n⏐⏐⏐⏐⏐⏐\n∑\nj∈pa(i)\nwijxj + bi;vi\n\n (11.9)\nwhere wij and bi are parameters governing the mean and vi is the variance of the\nconditional distribution for xi. The log of the joint distribution is then the log of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1723, "text": "product of these conditionals over all nodes in the graph and hence takes the form\nln p(x) =\nD∑\ni=1\nln p(xi|pa(i)) (11.10)\n= −\nD∑\ni=1\n1\n2vi\n\nxi −\n∑\nj∈pa(i)\nwijxj −bi\n\n\n2\n+ const (11.11)\nwhere x = (x1;:::;x D)T and ‘const’ denotes terms independent ofx. We see that\nthis is a quadratic function of the components of x, and hence the joint distribution\np(x) is a multivariate Gaussian.\nWe can ﬁnd the mean and covariance of this joint distribution as follows. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1724, "text": "mean of each variable is given by the recursion relationExercise 11.6\nE[xi] =\n∑\nj∈pa(i)\nwijE[xj] + bi: (11.12)\n11.1. Graphical Models 333\nFigure 11.7 A directed graph over three Gaussian variables with one\nmissing link. x1 x2 x3\nand so we can ﬁnd the components of E[x] = ( E[x1];:::; E[xD])T by starting at\nthe lowest numbered node and working recursively through the graph, where we\nassume that the nodes are numbered such that each node has a higher number than"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1725, "text": "its parents. Similarly, the elements of the covariance matrix of the joint distribution\nsatisfy a recursion relation of the formExercise 11.7\ncov[xi;xj] =\n∑\nk∈pa(j)\nwjkcov[xi;xk] + Iijvj (11.13)\nand so the covariance can similarly be evaluated recursively starting from the lowest\nnumbered node.\nWe now consider two extreme cases of possible graph structures. First, suppose\nthat there are no links in the graph, which therefore comprises D isolated nodes."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1726, "text": "In this case, there are no parameters wij and so there are just D parameters bi and\nDparameters vi. From the recursion relations (11.12) and (11.13), we see that the\nmean of p(x) is given by (b1;:::;b D)T and the covariance matrix is diagonal of\nthe form diag(v1;:::;v D). The joint distribution has a total of 2Dparameters and\nrepresents a set of Dindependent univariate Gaussian distributions.\nNow consider a fully connected graph in which each node has all lower num-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1727, "text": "bered nodes as parents. In this case the total number of independent parameters\n{wij}and {vi}in the covariance matrix is D(D+ 1)=2 corresponding to a generalExercise 11.8\nsymmetric covariance.\nGraphs having some intermediate level of complexity correspond to joint Gaus-\nsian distributions with partially constrained covariance matrices. Consider for exam-\nple the graph shown in Figure 11.7, which has a link missing between variables x1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1728, "text": "and x3. Using the recursion relations (11.12) and (11.13), we see that the mean and\ncovariance of the joint distribution are given byExercise 11.9\n\u0016= (b1;b2 + w21b1;b3 + w32b2 + w32w21b1)T (11.14)\n\u0006 =\n\n\nv1 w21v1 w32w21v1\nw21v1 v2 + w2\n21v1 w32(v2 + w2\n21v1)\nw32w21v1 w32(v2 + w2\n21v1) v3 + w2\n32(v2 + w2\n21v1)\n\n: (11.15)\nWe can readily extend the linear-Gaussian graphical model to a situation in\nwhich the nodes of the graph represent multivariate Gaussian variables. In this case,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1729, "text": "we can write the conditional distribution for node iin the form\np(xi|pa(i)) = N\n\nxi\n⏐⏐\n⏐⏐⏐⏐\n∑\nj∈pa(i)\nWijxj + bi;\u0006i\n\n (11.16)\nwhere now Wij is a matrix (which is non-square if xi and xj have different dimen-\nsionality). Again it is easy to verify that the joint distribution over all variables is\nGaussian.Exercise 11.10\n334 11. STRUCTURED DISTRIBUTIONS\nFigure 11.8 Directed graphical model representing the binary classiﬁer\nmodel described by the joint distribution (11.17) showing only"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1730, "text": "the stochastic variables {t1;:::;t N}and w.\nw\nt1 tN …\n11.1.5 Binary classiﬁer\nWe can illustrate the use of directed graphs to describe probability distributions\nusing a two-class classiﬁer model with Gaussian prior over the learnable parameters.Section 2.6.2\nWe can write this in the form\np(t;w|X;\u0015) =p(w|\u0015)\nN∏\nn=1\np(tn|w;xn) (11.17)\nwhere t = ( t1;:::;t N)T is the vector of target values, X is the data matrix with\nrows xT\n1 ;::: xT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1731, "text": "rows xT\n1 ;::: xT\nN, and the distribution p(t|x;w) is given by (11.1). We also assume a\nGaussian prior over the parameter vector w given by\np(w|\u0015) =N(w|0;\u0015I): (11.18)\nThe stochastic variables in this model are {t1;:::;t N}and w. In addition, this\nmodel contains the noise variance \u001b2 and the hyperparameter \u0015, both of which are\nparameters of the model rather than stochastic variables. If we consider for a mo-\nment only the stochastic variables, then the distribution given by (11.17) can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1732, "text": "represented by the graphical model shown in Figure 11.8.\nWhen we start to deal with more complex models, it becomes inconvenient to\nhave to write out multiple nodes of the form t1;:::;t N explicitly as in Figure 11.8.\nWe therefore introduce a graphical notation that allows such multiple nodes to be ex-\npressed more compactly. We draw a single representative nodetn and then surround\nthis with a box, called a plate, labelled with N to indicate that there are N nodes of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1733, "text": "this kind. Rewriting the graph of Figure 11.8 in this way, we obtain the graph shown\nin Figure 11.9.\n11.1.6 Parameters and observations\nWe will sometimes ﬁnd it helpful to make the parameters of a model, as well\nas its stochastic variables, explicit in the graphical representation. To do this, we\nwill adopt the convention that random variables are denoted by open circles and\ndeterministic parameters are denoted by ﬂoating variables. If we take the graph of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1734, "text": "Figure 11.9 An alternative, more compact, representation of the graph shown\nin Figure 11.8 in which we have introduced a plate (the box la-\nbelled N) that represents N nodes of which only a single exam-\nple tn is shown explicitly.\nw tn\nN\n11.1. Graphical Models 335\nFigure 11.10 The same model as in Figure 11.9 but with the determin-\nistic parameters shown explicitly by the ﬂoating variables.\nw\n\u0015\ntn\nxn\n\u001b2\nN\nFigure 11.9 and include the deterministic parameters, we obtain the graph shown in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1735, "text": "Figure 11.10.\nWhen we apply a graphical model to a problem in machine learning, we will\ntypically set some of the random variables to speciﬁc observed values. For example,\nthe stochastic variables {tn}in the linear regression model will be set equal to the\nspeciﬁc values given in the training set. In a graphical model, we denote such ob-\nserved variables by shading the corresponding nodes. Thus, the graph corresponding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1736, "text": "to Figure 11.10 in which the variables {tn}are observed is shown in Figure 11.11.\nNote that the value of w is not observed, and so w is an example of a latent\nvariable, also known as ahidden variable. Such variables play a crucial role in many\nof the models discussed in this book. We therefore have three kinds of variables in a\ndirected graphical model. First, there are unobserved (also called latent, or hidden)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1737, "text": "stochastic variables, which are denoted by open red circles. Second, when stochastic\nvariables are observed, so that that they are set to speciﬁc values, they are denoted\nby red circles shaded with blue. Finally, non-stochastic parameters are denoted by\nﬂoating variables, as seen in Figure 11.11.\nNote that model parameters such as w are generally of little direct interest in\nthemselves, because our ultimate goal is to make predictions for new input values."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1738, "text": "Suppose we are given a new input value ˆx and we wish to ﬁnd the corresponding\nprobability distribution forˆtconditioned on the observed data. The joint distribution\nof all the random variables in this model, conditioned on the deterministic parame-\nters, is given by\np(ˆt;t;w|ˆx;X;\u0015) = p(w|\u0015)p(ˆt|w;ˆx)\nN∏\nn=1\np(tn|w;xn) (11.19)\nand the corresponding graphical model is shown in Figure 11.12.\nFigure 11.11 As in Figure 11.10 but with the nodes {tn}shaded to indi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1739, "text": "cate that the corresponding random variables have been\nset to their observed values given by the training set.\nw\n\u0015\ntn\nxn\n\u001b2\nN\n336 11. STRUCTURED DISTRIBUTIONS\nFigure 11.12 The classiﬁcation model, corresponding to Fig-\nure 11.11, showing a new input value bxtogether with\nthe corresponding model prediction bt.\nw\n\u0015\ntn\nxn\n^t\n^x\nN\nThe required predictive distribution for ˆt is then obtained from the sum rule\nof probability by integrating out the model parameters w. This integration over"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1740, "text": "parameters represents a fully Bayesian treatment, which is rarely used in practice,\nespecially with deep neural networks. Instead, we approximate this integral by ﬁrst\nﬁnding the most probable value wMAP that maximizes the posterior distribution and\nthen using just this single value to make predictions using p(ˆt|wMAP;ˆx).\n11.1.7 Bayes’ theorem\nWhen stochastic variables in a probabilistic model are set equal to observed val-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1741, "text": "ues, the distributions over other unobserved stochastic variables change accordingly.\nThe process of calculating these updated distributions is known asinference. We can\nillustrate this by considering the graphical interpretation of Bayes’ theorem. Suppose\nwe decompose the joint distributionp(x;y) over two variablesxand yinto a product\nof factors in the form p(x;y) = p(x)p(y|x). This can be represented by the directed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1742, "text": "graph shown in Figure 11.13(a). Now suppose we observe the value of y, as indi-\ncated by the shaded node in Figure 11.13(b). We can view the marginal distribution\np(x) as a prior over the latent variable x, and our goal is to infer the corresponding\nposterior. Using the sum and product rules of probability we can evaluate\np(y) =\n∑\nx′\np(y|x′)p(x′); (11.20)\nwhich can then be used in Bayes’ theorem to calculate\np(x|y) = p(y|x)p(x)\np(y) : (11.21)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1743, "text": "p(x|y) = p(y|x)p(x)\np(y) : (11.21)\nThus, the joint distribution is now expressed in terms of p(x|y) and p(y). From\na graphical perspective, the joint distribution p(x;y) is represented by the graph\nshown in Figure 11.13(c), in which the direction of the arrow is reversed. This is the\nsimplest example of an inference problem for a graphical model.\nFor complex graphical models that capture rich probabilistic structure, the pro-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1744, "text": "cess of calculating posterior distributions once some of the stochastic variables are\nobserved can be complex and subtle. Conceptually, it simply involves the systematic\napplication of the sum and product rules of probability, or equivalently Bayes’ theo-\nrem. In practice, however, managing these calculations efﬁciently can beneﬁt greatly\nfrom an exploitation of the graphical structure. These calculations can be expressed\n11.2. Conditional Independence 337"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1745, "text": "11.2. Conditional Independence 337\nFigure 11.13 A graphical representation\nof Bayes’ theorem showing\n(a) a joint distribution over\ntwo variables x and y ex-\npressed in factorized form,\n(b) the case with y set to an\nobserved value, and (c) the\nresulting posterior distribu-\ntion over x, given by Bayes’\ntheorem.\nx\ny\n(a)\nx\ny\n(b)\nx\ny\n(c)\nin terms of elegant calculations on the graph that involve sending local messages be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1746, "text": "tween nodes. Such methods give exact answers for tree-structured graphs and give\napproximate iterative algorithms for graphs with loops. Since we will not discuss\nthese further here, see Bishop (2006) for a more comprehensive discussion in the\ncontext of machine learning.\n11.2.\nConditional Independence\nAn important concept for probability distributions over multiple variables is that of\nconditional independence (Dawid, 1980). Consider three variables a, b, and c, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1747, "text": "suppose that the conditional distribution of agiven band cis such that it does not\ndepend on the value of b, so that\np(a|b;c) = p(a|c): (11.22)\nWe say that ais conditionally independent of bgiven c. This can be expressed in a\nslightly different way if we consider the joint distribution of aand bconditioned on\nc, which we can write in the form\np(a;b|c) = p(a|b;c)p(b|c)\n= p(a|c)p(b|c) (11.23)\nwhere we have used the product rule of probability together with (11.22). We see"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1748, "text": "that, conditioned on c, the joint distribution of aand bfactorizes into the product of\nthe marginal distribution of aand the marginal distribution of b(again both condi-\ntioned on c). This says that the variables aand bare statistically independent, given\nc. Note that our deﬁnition of conditional independence will require that (11.22), or\nequivalently (11.23), must hold for every possible value of c, and not just for some"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1749, "text": "values. We will sometimes use a shorthand notation for conditional independence\n(Dawid, 1979) in which\na⊥ ⊥b|c (11.24)\ndenotes that ais conditionally independent of bgiven c. Conditional independence\nproperties play an important role in probabilistic models for machine learning be-\ncause they simplify both the structure of a model and the computations needed to\nperform inference and learning under that model.\n338 11. STRUCTURED DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1750, "text": "338 11. STRUCTURED DISTRIBUTIONS\nFigure 11.14 The ﬁrst of three examples of graphs over three variables a, b,\nand c used to discuss conditional independence properties of\ndirected graphical models.\na\nc\nb\nIf we are given an expression for the joint distribution over a set of variables\nin terms of a product of conditional distributions (i.e., the mathematical representa-\ntion underlying a directed graph), then we could in principle test whether any po-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1751, "text": "tential conditional independence property holds by repeated application of the sum\nand product rules of probability. In practice, such an approach would be very time-\nconsuming. An important and elegant feature of graphical models is that conditional\nindependence properties of the joint distribution can be read directly from the graph\nwithout having to perform any analytical manipulations. The general framework"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1752, "text": "for achieving this is called d-separation, where the ‘d’ stands for ‘directed’ (Pearl,\n1988). Here we will motivate the concept of d-separation and give a general state-\nment of the d-separation criterion. A formal proof can be found in Lauritzen (1996).\n11.2.1 Three example graphs\nWe begin our discussion of the conditional independence properties of directed\ngraphs by considering three simple examples each involving graphs having just three"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1753, "text": "nodes. Together, these will motivate and illustrate the key concepts of d-separation.\nThe ﬁrst of the three examples is shown in Figure 11.14, and the joint distribution\ncorresponding to this graph is easily written down using the general result (11.6) to\ngive\np(a;b;c) = p(a|c)p(b|c)p(c): (11.25)\nIf none of the variables are observed, then we can investigate whether aand bare\nindependent by marginalizing both sides of (11.25) with respect to cto give\np(a;b) =\n∑\nc\np(a|c)p(b|c)p(c): (11.26)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1754, "text": "p(a;b) =\n∑\nc\np(a|c)p(b|c)p(c): (11.26)\nIn general, this does not factorize into the product p(a)p(b), and so\na̸⊥ ⊥b|∅ (11.27)\nwhere ∅denotes the empty set, and the symbol ̸⊥ ⊥means that the conditional inde-\npendence property does not hold in general. Of course, it may hold for a particular\ndistribution by virtue of the speciﬁc numerical values associated with the various\nconditional probabilities, but it does not follow in general from the structure of the\ngraph."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1755, "text": "graph.\nNow suppose we condition on the variable c, as represented by the graph of\nFigure 11.15. From (11.25), we can easily write down the conditional distribution of\n11.2. Conditional Independence 339\nFigure 11.15 As in Figure 11.14 but where we have conditioned on the value\nof variable c.\na\nc\nb\naand b, given c, in the form\np(a;b|c) = p(a;b;c)\np(c)\n= p(a|c)p(b|c)\nand so we obtain the conditional independence property\na⊥ ⊥b|c:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1756, "text": "a⊥ ⊥b|c:\nWe can provide a simple graphical interpretation of this result by considering\nthe path from node ato node bvia c. The node cis said to be tail-to-tail with respect\nto this path because the node is connected to the tails of the two arrows, and the\npresence of such a path connecting nodes aand bcauses these nodes to be depen-\ndent. However, when we condition on node c, as in Figure 11.15, the conditioned\nnode ‘blocks’ the path from a to b and causes a and b to become (conditionally)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1757, "text": "independent.\nWe can similarly consider the graph shown in Figure 11.16. The joint distribu-\ntion corresponding to this graph is again obtained from our general formula (11.6)\nto give\np(a;b;c) = p(a)p(c|a)p(b|c): (11.28)\nFirst, suppose that none of the variables are observed. Again, we can test to see if a\nand bare independent by marginalizing over cto give\np(a;b) = p(a)\n∑\nc\np(c|a)p(b|c) =p(a)p(b|a)\nwhich in general does not factorize into p(a)p(b), and so\na̸⊥ ⊥b|∅ (11.29)\nas before."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1758, "text": "a̸⊥ ⊥b|∅ (11.29)\nas before.\nNow suppose we condition on node c, as shown in Figure 11.17. Using Bayes’\nFigure 11.16 The second of our three examples of three-node graphs\nused to motivate the conditional independence frame-\nwork for directed graphical models.\na c b\n340 11. STRUCTURED DISTRIBUTIONS\nFigure 11.17 As in Figure 11.16 but now conditioning on node c. a c b\ntheorem together with (11.28), we obtain\np(a;b|c) = p(a;b;c)\np(c)\n= p(a)p(c|a)p(b|c)\np(c)\n= p(a|c)p(b|c)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1759, "text": "p(c)\n= p(a)p(c|a)p(b|c)\np(c)\n= p(a|c)p(b|c)\nand so again we obtain the conditional independence property\na⊥ ⊥b|c:\nAs before, we can interpret these results graphically. The node cis said to be\nhead-to-tail with respect to the path from node ato node b. Such a path connects\nnodes aand band renders them dependent. If we now observe c, as in Figure 11.17,\nthen this observation ‘blocks’ the path fromato band so we obtain the conditional\nindependence property a⊥ ⊥b|c."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1760, "text": "independence property a⊥ ⊥b|c.\nFinally, we consider the third of our three-node examples, shown by the graph in\nFigure 11.18. As we will see, this has a more subtle behaviour than the two previous\ngraphs. The joint distribution can again be written down using our general result\n(11.6) to give\np(a;b;c) = p(a)p(b)p(c|a;b): (11.30)\nConsider ﬁrst the case where none of the variables are observed. Marginalizing both\nsides of (11.30) over cwe obtain\np(a;b) = p(a)p(b)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1761, "text": "p(a;b) = p(a)p(b)\nand so a and b are independent with no variables observed, in contrast to the two\nprevious examples. We can write this result as\na⊥ ⊥b|∅: (11.31)\nNow suppose we condition on c, as indicated in Figure 11.19. The conditional dis-\ntribution of aand bis then given by\np(a;b|c) = p(a;b;c)\np(c)\n= p(a)p(b)p(c|a;b)\np(c) ;\nFigure 11.18 The last of our three examples of three-node graphs used to ex-\nplore conditional independence properties in graphical models."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1762, "text": "This graph has rather different properties from the two previous\nexamples.\na\nc\nb\n11.2. Conditional Independence 341\nFigure 11.19 As in Figure 11.18 but conditioning on the value of node c. In\nthis graph, the act of conditioning induces a dependence be-\ntween aand b.\na\nc\nb\nwhich in general does not factorize into the product p(a|c)p(b|c), and so\na̸⊥ ⊥b|c:\nThus, our third example has the opposite behaviour from the ﬁrst two. Graphically,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1763, "text": "we say that node cis head-to-head with respect to the path from ato bbecause it\nconnects to the heads of the two arrows. The node cis sometimes called a collider\nnode. When node cis unobserved, it ‘blocks’ the path, and the variablesaand bare\nindependent. However, conditioning on c‘unblocks’ the path and renders aand b\ndependent.\nThere is one more subtlety associated with this third example that we need to\nconsider. First we introduce some more terminology. We say that node y is a de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1764, "text": "scendant of node x if there is a path from x to y in which each step of the path\nfollows the directions of the arrows. Then it can be shown that a head-to-head path\nwill become unblocked if either the node, or any of its descendants, is observed.Exercise 11.13\nIn summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked\nunless it is observed, in which case it blocks the path. By contrast, a head-to-head"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1765, "text": "node blocks a path if it is unobserved, but once the node and/or at least one of its\ndescendants is observed the path becomes unblocked.\n11.2.2 Explaining away\nIt is worth spending a moment to understand further the unusual behaviour of the\ngraph in Figure 11.19. Consider a particular instance of such a graph corresponding\nto a problem with three binary random variables relating to the fuel system on a car,\nas shown inFigure 11.20. The variables areB, which represents the state of a battery"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1766, "text": "that is either charged (B = 1) or ﬂat (B = 0), F which represents the state of the\nfuel tank that is either full of fuel (F = 1) or empty (F = 0), and G, which is the\nstate of an electric fuel gauge and which indicates that the fuel tank is either full\n(G= 1) or empty (G = 0). The battery is either charged or ﬂat, and independently,\nB\nG\nF B\nG\nF B\nG\nF\nFigure 11.20 An example of a three-node graph used to illustrate ‘explaining away’. The three nodes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1767, "text": "represent the state of the battery (B ), the state of the fuel tank (F ), and the reading on\nthe electric fuel gauge (G). See the text for details.\n342 11. STRUCTURED DISTRIBUTIONS\nthe fuel tank is either full or empty, with prior probabilities\np(B = 1) = 0:9\np(F = 1) = 0:9:\nGiven the state of the fuel tank and the battery, the fuel gauge reads full with proba-\nbilities given by\np(G= 1|B= 1;F = 1) = 0:8\np(G= 1|B= 1;F = 0) = 0:2\np(G= 1|B= 0;F = 1) = 0:2\np(G= 1|B= 0;F = 0) = 0:1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1768, "text": "p(G= 1|B= 0;F = 1) = 0:2\np(G= 1|B= 0;F = 0) = 0:1\nso this is a rather unreliable fuel gauge! All remaining probabilities are determined\nby the requirement that probabilities sum to one, and so we have a complete speciﬁ-\ncation of the probabilistic model.\nBefore we observe any data, the prior probability of the fuel tank being empty\nis p(F = 0) = 0:1. Now suppose that we observe the fuel gauge and discover that\nit reads empty, i.e., G = 0, corresponding to the middle graph in Figure 11.20. We"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1769, "text": "can use Bayes’ theorem to evaluate the posterior probability of the fuel tank being\nempty. First we evaluate the denominator for Bayes’ theorem:\np(G= 0) =\n∑\nB∈{0;1}\n∑\nF∈{0;1}\np(G= 0|B;F)p(B)p(F) = 0:315 (11.32)\nand similarly we evaluate\np(G= 0|F= 0) =\n∑\nB∈{0;1}\np(G= 0|B;F= 0)p(B) = 0:81 (11.33)\nand using these results, we have\np(F = 0|G= 0) = p(G= 0|F= 0)p(F = 0)\np(G= 0) ≃0:257 (11.34)\nand so p(F = 0|G= 0) > p(F= 0). Thus, observing that the gauge reads empty"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1770, "text": "makes it more likely that the tank is indeed empty, as we would intuitively expect.\nNext suppose that we also check the state of the battery and ﬁnd that it is ﬂat, i.e.,\nB = 0. We have now observed the states of both the fuel gauge and the battery, as\nshown by the right-hand graph in Figure 11.20. The posterior probability that the\nfuel tank is empty given the observations of both the fuel gauge and the battery state\nis then given by\np(F = 0|G= 0;B = 0) = p(G= 0|B= 0;F = 0)p(F = 0)\n∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1771, "text": "∑\nF∈{0;1}p(G= 0|B= 0;F)p(F) ≃0:111 (11.35)\nwhere the prior probability p(B = 0) has cancelled between the numerator and\ndenominator. Thus, the probability that the tank is empty hasdecreased (from 0:257\n11.2. Conditional Independence 343\nFigure 11.21 Illustration of d-separation.\nSee the text for details. a\ne\nf\nb\nc\na\ne\nf\nb\nc\n(a) (b)\nto 0:111) as a result of the observation of the state of the battery. This accords with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1772, "text": "our intuition that ﬁnding that the battery is ﬂatexplains away the observation that the\nfuel gauge reads empty. We see that the state of the fuel tank and that of the battery\nhave indeed become dependent on each other as a result of observing the reading on\nthe fuel gauge. In fact, this would also be the case if, instead of observing the fuel\ngauge directly, we observed the state of some descendant of G, for example a rather"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1773, "text": "unreliable witness who reports seeing that the gauge was reading empty. Note thatExercise 11.14\nthe probability p(F = 0|G= 0;B = 0) ≃0:111 is greater than the prior probability\np(F = 0) = 0:1 because the observation that the fuel gauge reads zero still provides\nsome evidence in favour of an empty fuel tank.\n11.2.3 D-separation\nWe now give a general statement of the d-separation property (Pearl, 1988) for\ndirected graphs. Consider a general directed graph in which A, B, and C are arbi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1774, "text": "trary non-intersecting sets of nodes (whose union may be smaller than the complete\nset of nodes in the graph). We wish to ascertain whether a particular conditional\nindependence statement A⊥ ⊥B|Cis implied by a given directed acyclic graph. To\ndo so, we consider all possible paths from any node inAto any node in B. Any such\npath is said to be blocked if it includes a node such that either\n(a) the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1775, "text": "node is in the set C, or\n(b) the arrows meet head-to-head at the node and neither the node, nor any of its\ndescendants is in the set C.\nIf all paths are blocked, then Ais said to be d-separated from Bby C, and the joint\ndistribution over all the variables in the graph will satisfy A⊥ ⊥B|C.\nD-separation is illustrated in Figure 11.21. In graph (a), the path from ato bis\nnot blocked by nodefbecause it is a tail-to-tail node for this path and is not observed,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1776, "text": "nor is it blocked by node ebecause, although the latter is a head-to-head node, it has\na descendant cin the conditioning set. Thus, the conditional independence statement\na⊥ ⊥b|cdoes not follow from this graph. In graph (b), the path fromato bis blocked\nby node f because this is a tail-to-tail node that is observed, and so the conditional\nindependence property a⊥ ⊥b|f will be satisﬁed by any distribution that factorizes\n344 11. STRUCTURED DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1777, "text": "344 11. STRUCTURED DISTRIBUTIONS\nFigure 11.22 A graphical representation of the naive Bayes model for clas-\nsiﬁcation. Conditioned on the class label Ck, the elements of\nthe observed vector x = (x(1);:::; x(L)) are assumed to be\nindependent.\nx(1)\nCk\nx(L) …\naccording to this graph. Note that this path is also blocked by node ebecause eis\na head-to-head node and neither it nor its descendant are in the conditioning set. In"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1778, "text": "d-separation, parameters such as \u0015in Figure 11.12, which are indicated by ﬂoating\nvariables, behave in the same way as observed nodes. However, there are no marginal\ndistributions associated with such nodes, and consequently parameter nodes never\nthemselves have parents and so all paths through these nodes will always be tail-to-\ntail and hence blocked. Consequently they play no role in d-separation.\nAnother example of conditional independence and d-separation is provided by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1779, "text": "i.i.d. (independent and identically distributed) data. Consider the binary classiﬁca-Section 2.3.2\ntion model shown in Figure 11.12. Here the stochastic nodes correspond to {tn},\nw, and ˆt. We see that the node for w is tail-to-tail with respect to the path from ˆt\nto any one of the nodes tn, and so we have the following conditional independence\nproperty:\nˆt⊥ ⊥tn |w: (11.36)\nThus, conditioned on the network parameters w, the predictive distribution for ˆtis"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1780, "text": "independent of the training data {t1;:::;t N}. We can therefore ﬁrst use the training\ndata to determine the posterior distribution (or some approximation to the posterior\ndistribution) over the coefﬁcientsw and then we can discard the training data and use\nthe posterior distribution for w to make predictions of ˆtfor new input observations\nˆx.\n11.2.4 Naive Bayes\nA related graphical structure arises in an approach to classiﬁcation called the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1781, "text": "naive Bayes model, in which we use conditional independence assumptions to sim-\nplify the model structure. Suppose our data consists of observations of a vector x,\nand we wish to assign values of x to one of K classes. We can deﬁne a class-\nconditional density p(x|Ck) for each of the classes, along with prior class probabili-\nties p(Ck). The key assumption of the naive Bayes model is that, conditioned on the\nclass Ck, the distribution of the input variable factorizes into the product of two or"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1782, "text": "more densities. Suppose we partition x into Lelements x = (x(1);:::; x(L)). Naive\nBayes then takes the form\np(x|Ck) =\nL∏\nl=1\np(x(l)|Ck) (11.37)\nwhere it is assumed that (11.37) holds for each of the classes Ck separately. The\ngraphical representation of this model is shown in Figure 11.22. We see that an\nobservation of Ck would block the path betweenx(i) and x(j) for j ̸=ibecause such\n11.2. Conditional Independence 345\nx1\nx2\np(x|C1)\np(x|C2)\n(a)\nx1\nx2\np(x) (b)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1783, "text": "x1\nx2\np(x|C1)\np(x|C2)\n(a)\nx1\nx2\np(x) (b)\nFigure 11.23 Illustration of a naive Bayes classiﬁer for a two-dimensional data space, showing (a) the condi-\ntional distributions p(x|Ck) for each of the two classes and (b) the marginal distribution p(x) in which we have\nassumed equal class priors p(C1) = p(C2) = 0:5. Note that the conditional distributions factorize with respect to\nx1 and x2, whereas the marginal distribution does not."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1784, "text": "paths are tail-to-tail at the nodeCk, and sox(i) and x(j) are conditionally independent\ngiven Ck. If, however, we marginalize out Ck, the tail-to-tail path from x(i) to x(j) is\nno longer blocked, which tells us that in general the marginal density p(x) will not\nfactorize with respect to the elements x(1);:::; x(L).\nIf we are given a labelled training set, comprising observations {x1;:::; xN}\ntogether with their class labels, then we can ﬁt the naive Bayes model to the training"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1785, "text": "data using maximum likelihood by assuming that the data are drawn independently\nfrom the model. The solution is obtained by ﬁtting the model for each class sepa-Exercise 11.15\nrately using the corresponding labelled data and then setting the class priors p(Ck)\nequal to the fraction of training data points in each class. The probability that a\nvector x belongs to class Ck is then given by Bayes’ theorem in the form\np(Ck|x) =p(x|Ck)p(Ck)\np(x) (11.38)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1786, "text": "p(Ck|x) =p(x|Ck)p(Ck)\np(x) (11.38)\nwhere p(x|Ck) is given by (11.37), and p(x) can be evaluated using\np(x) =\nK∑\nk=1\np(x|Ck)p(Ck): (11.39)\nThe naive Bayes model is illustrated for a two-dimensional data space in Fig-\nure 11.23 in which x = (x1;x2). Here we assume that the conditional densities\n346 11. STRUCTURED DISTRIBUTIONS\np(x|Ck) for each of the two classes are axis-aligned Gaussians, and hence that they\neach factorize with respect to x1 and x2 so that\np(x|Ck) = p(x1|Ck)p(x2|Ck): (11.40)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1787, "text": "p(x|Ck) = p(x1|Ck)p(x2|Ck): (11.40)\nHowever, the marginal density p(x) given by\np(x) =\nK∑\nk=1\np(x|Ck)p(Ck) (11.41)\nis now a mixture of Gaussians and does not factorize with respect to x1 and x2.\nWe have already encountered a simple application of the naive Bayes model in the\ncontext of fusing data from different sources, such as blood tests and skin images for\nmedical diagnosis.Section 5.2.4\nThe naive Bayes assumption is helpful when the dimensionality Dof the input"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1788, "text": "space is high, making density estimation in the full D-dimensional space more chal-\nlenging. It is also useful if the input vector contains both discrete and continuous\nvariables, since each can be represented separately using appropriate models (e.g.,\nBernoulli distributions for binary observations or Gaussians for real-valued vari-\nables). The conditional independence assumption of this model is clearly a strong"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1789, "text": "one that may lead to rather poor representations of the class-conditional densities.\nNevertheless, even if this assumption is not precisely satisﬁed, the model may still\ngive good classiﬁcation performance in practice because the decision boundaries can\nbe insensitive to some of the details in the class-conditional densities, as illustrated\nin Figure 5.8.\n11.2.5 Generative models\nMany applications of machine learning can be viewed as examples of inverse"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1790, "text": "problems in which there is an underlying, often physical, process that generates data,\nand the goal is to learn now to invert this process. For example, an image of an\nobject can be viewed as the output of a generative process in which the type of\nobject is selected from some distribution of possible object classes. The position and\norientation of the object are also chosen from some prior distributions, and then the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1791, "text": "resulting image is created. Given a large data set of images labelled with the type,\nposition, and scale of the objects they contain, the goal is to train a machine learning\nmodel that can take new, unlabelled images and detect the presence of an object\nincluding its location within the image and its size. The machine learning solution\ntherefore represents the inverse of the process that generated the data.\nOne approach would be to train a deep neural network, such as a convolutional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1792, "text": "network, to take an image as input and to generate outputs that describe the object’s\ntype, position, and scale. This approach therefore tries to solve the inverse problem\ndirectly and is an example of a discriminative model. It can achieve high accuracy\nprovided ample examples of labelled images are available. In practice, unlabelled\nimages are often plentiful, and much of the effort in obtaining a training set goes\ninto proving the labels, which may be done by hand. Our simple discriminative"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1793, "text": "model cannot directly make use of unlabelled images during training.\n11.2. Conditional Independence 347\nFigure 11.24 A graphical model representing the process by which\nimages of objects are created. The identity of an object\n(a discrete variable) and the position and orientation\nof that object (continuous variables) have independent\nprior probabilities. The image (an array of pixel intensi-\nties) has a probability distribution that is dependent on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1794, "text": "the identity of the object as well as on its position and\norientation.\nclass position scale\nimage\nAn alternative approach is to model the generative process and then subse-\nquently to invert it computationally. In our image example, if we assume that the\nobject’s class, position, and scale are all chosen independently, then we can represent\nthe generative process using a directed graphical model as shown in Figure 11.24."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1795, "text": "Note that the directions of the arrows correspond to the sequence of generative steps,\nand so the model represents the causal process (Pearl, 1988) by which the observed\ndata is generated. This is an example of agenerative model because once it is trained,\nit can be used to generate synthetic images by ﬁrst selecting values for object’s class,\nposition, and scale from the learned prior distributions and then subsequently sam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1796, "text": "pling an image from the learned conditional distribution. We will later see how diffu-\nsion models and other generative models can synthesize impressive high-resolution\nimages based on a textual description of the desired content and style of the image.Chapter 20\nThe graph in Figure 11.24 assumes that, when no image is observed, the class,\nposition, and scale variables are independent. This follows because every path be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1797, "text": "tween any two of these variables is head-to-head with respect to the image variable,\nwhich is unobserved. However, when we observe an image, those paths become\nunblocked, and the class, position, and scale variables are no longer independent.\nIntuitively this is reasonable because being told the identity of the object within the\nimage provides us with very relevant information to assist us with determining its\nlocation."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1798, "text": "location.\nThe hidden variables in a probabilistic model need not, however, have any ex-\nplicit physical interpretation but may be introduced simply to allow a more complex\njoint distribution to be constructed from simpler components. For example, models\nsuch as normalizing ﬂows, variational autoencoders, and diffusion models all use\ndeep neural networks to create complex distributions in the data space by transform-\ning hidden variables having a simple Gaussian distribution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1799, "text": "11.2.6 Markov blanket\nA conditional independence property that is helpful when discussing more com-\nplex directed graphs is called the Markov blanket or Markov boundary. Consider\na joint distribution p(x1;:::; xD) represented by a directed graph having Dnodes,\nand consider the conditional distribution of a particular node with variables xi con-\nditioned on all the remaining variablesxj̸=i. Using the factorization property (11.6),\n348 11. STRUCTURED DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1800, "text": "348 11. STRUCTURED DISTRIBUTIONS\nFigure 11.25 The Markov blanket of a nodexi comprises the\nset of parents, children, and co-parents of the\nnode. It has the property that the conditional\ndistribution of xi, conditioned on all the remain-\ning variables in the graph, is dependent only on\nthe variables in the Markov blanket.\nxi\nwe can express this conditional distribution in the form\np(xi|x{j ̸=i}) = p(x1;:::; xD)∫\np(x1;:::; xD) dxi\n=\n∏\nk\np(xk|pa(k))\n∫∏\nk\np(xk|pa(k)) dxi"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1801, "text": "=\n∏\nk\np(xk|pa(k))\n∫∏\nk\np(xk|pa(k)) dxi\nin which the integral is replaced by a summation for discrete variables. We now\nobserve that any factor p(xk|pa(k)) that does not have any functional dependence\non xi can be taken outside the integral over xi and will therefore cancel between\nnumerator and denominator. The only factors that remain will be the conditional\ndistribution p(xi|pa(i)) for node xi itself, together with the conditional distributions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1802, "text": "for any nodes xk such that node xi is in the conditioning set ofp(xk|pa(k)), in other\nwords for which xi is a parent of xk. The conditional p(xi|pa(i)) will depend on\nthe parents of node xi, whereas the conditionals p(xk|pa(k)) will depend on the\nchildren of xi as well as on the co-parents, in other words variables corresponding\nto parents of node xk other than node xi. The set of nodes comprising the parents,\nthe children, and the co-parents is called the Markov blanket and is illustrated in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1803, "text": "Figure 11.25.\nWe can think of the Markov blanket of a node xi as being the minimal set of\nnodes that isolates xi from the rest of the graph. Note that it is not sufﬁcient to\ninclude only the parents and children of node xi because explaining away means\nthat observations of the child nodes will not block paths to the co-parents. We must\ntherefore observe the co-parent nodes as well.\n11.2.7 Graphs as ﬁlters\nWe have seen that a particular directed graph represents a speciﬁc decomposi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1804, "text": "tion of a joint probability distribution into a product of conditional probabilities, and\nit also expresses a set of conditional independence statements obtained through the\nd-separation criterion. The d-separation theorem is really an expression of the equiv-\nalence of these two properties. To make this clear, it is helpful to think of a directed\ngraph as a ﬁlter. Suppose we consider a particular joint probability distribution p(x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1805, "text": "over the variables x corresponding to the (unobserved) nodes of the graph. The ﬁl-\nter will allow this distribution to pass through if, and only if, it can be expressed in\n11.3. Sequence Models 349\np(x) DF\nFigure 11.26 We can view a graphical model (in this case a directed graph) as a ﬁlter in which a prob-\nability distribution p(x) is allowed through the ﬁlter if, and only if, it satisﬁes the directed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1806, "text": "factorization property (11.6). The set of all possible probability distributionsp(x) that pass\nthrough the ﬁlter is denoted DF. We can alternatively use the graph to ﬁlter distributions\naccording to whether they respect all the conditional independence properties implied by\nthe d-separation properties of the graph. The d-separation theorem says the same set of\ndistributions DFwill be allowed through this second kind of ﬁlter."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1807, "text": "terms of the factorization (11.6) implied by the graph. If we present to the ﬁlter the\nset of all possible distributions p(x) over the set of variables x, then the subset of\ndistributions that are passed by the ﬁlter is denoted DF, for directed factorization.\nThis is illustrated in Figure 11.26.\nAlternatively, we can use the graph as a different kind of ﬁlter by ﬁrst listing\nall the conditional independence properties obtained by applying the d-separation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1808, "text": "criterion to the graph and then allowing a distribution to pass only if it satisﬁes all of\nthese properties. If we present all possible distributions p(x) to this second kind of\nﬁlter, then the d-separation theorem tells us that the set of distributions that will be\nallowed through is precisely the set DF.\nIt should be emphasized that the conditional independence properties obtained\nfrom d-separation apply to any probabilistic model described by that particular di-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1809, "text": "rected graph. This will be true, for instance, whether the variables are discrete or\ncontinuous or a combination of these. Again, we see that a particular graph describes\na whole family of probability distributions.\nAt one extreme, we have a fully connected graph that exhibits no conditional\nindependence properties at all and which can represent any possible joint probability\ndistribution over the given variables. The set DFwill contain all possible distri-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1810, "text": "butions p(x). At the other extreme, we have a fully disconnected graph, i.e., one\nhaving no links at all. This corresponds to joint distributions that factorize into the\nproduct of the marginal distributions over the variables comprising the nodes of the\ngraph. Note that for any given graph, the set of distributions DFwill include any\ndistributions that have additional independence properties beyond those described by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1811, "text": "the graph. For instance, a fully factorized distribution will always be passed through\nthe ﬁlter implied by any graph over the corresponding set of variables.\n11.3.\nSequence Models\nThere are many important applications of machine learning in which the data consists\nof a sequence of values. For example, text comprises a sequence of words, whereas\na protein comprises a sequence of amino acids. Many sequences are ordered by\n350 11. STRUCTURED DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1812, "text": "350 11. STRUCTURED DISTRIBUTIONS\nFigure 11.27 An illustration of a general autoregressive\nmodel of the form (11.42) with four nodes.\nx1 x2 x3 x4\ntime, such as the audio signals from a microphone or daily rainfall measurements\nat a particular location. Sometimes the terminology of ‘time’ as well as ‘past’ and\n‘future’ are used when referring to other types of sequential data, not just temporal\nsequences. Applications involving sequences include speech recognition, automatic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1813, "text": "translation between languages, detecting genes in DNA, synthesizing music, writ-\ning computer code, holding a conversation with a modern search engine, and many\nothers.\nWe will denote a data sequence by x1;:::; xN where each element xn of the\nsequence comprises a vector of values. Note that we might have several such se-\nquences drawn independently from the same distribution, in which case the joint\ndistribution over all the sequences factorizes into the product of the distributions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1814, "text": "over each sequence individually. From now on, we focus on modelling just one of\nthose sequences.\nWe have already seen in (11.4) that by repeated application of the product rule\nof probability, a general distribution over N variables can be written as the product\nof conditional distributions, and that the form of this decomposition depends on a\nspeciﬁc of ordering for the variables. For vector-valued variables, and if we chose"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1815, "text": "an ordering that corresponds to the order of the variables in the sequence, then we\ncan write\np(x1;:::; xN) =\nN∏\nn=1\np(xn|x1;:::; xn−1 ): (11.42)\nThis corresponds to a directed graph in which each node receives a link from every\nprevious node in the sequence, as illustrated using four variables in Figure 11.27.\nThis is known as an autoregressive model.\nThis representation has complete generality and therefore from a modelling per-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1816, "text": "spective adds no value since it encodes no assumptions. We can constrain the space\nof models by introducing conditional independence properties by removing links\nfrom the graph, or equivalently by removing variables from the conditioning set of\nthe factors on the right-hand-side of (11.42).\nThe strongest assumption would be to remove all conditioning variables, giving\na joint distribution of the form\np(x1;:::; xN) =\nN∏\nn=1\np(xn); (11.43)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1817, "text": "p(x1;:::; xN) =\nN∏\nn=1\np(xn); (11.43)\nwhich treats the variables as independent and therefore completely ignores the order-\ning information. This corresponds to a probabilistic graphical model without links,\nas shown in Figure 11.28.\n11.3. Sequence Models 351\nFigure 11.28 The simplest approach to mod-\nelling a sequence of observations\nis to treat them as independent,\ncorresponding to a probabilistic\ngraphical model without links.\nx1 x2 x3 x4 …"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1818, "text": "graphical model without links.\nx1 x2 x3 x4 …\nInteresting models that capture sequential properties while introducing mod-\nelling assumptions lie between these two extremes. One strong assumption would\nbe to assume that each conditional distribution depends only on the immediately\npreceding variable in the sequence, giving a joint distribution of the form\np(x1;:::; xN) = p(x1)\nN∏\nn=2\np(xn|xn−1 ): (11.44)\nNote that the ﬁrst variable in the sequence is treated slightly differently since it has"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1819, "text": "no conditioning variable. The functional form (11.44) is known as a Markov model,\nor Markov chain, and is represented by a graph consisting of a simple chain of nodes,\nas seen in Figure 11.29. Using d-separation, we see that the conditional distributionSection 11.2.3\nfor observation xn, given all of the observations up to time n, is given by\np(xn|x1;:::; xn−1 ) = p(xn|xn−1 ); (11.45)\nwhich is easily veriﬁed by direct evaluation starting from (11.44) and using the prod-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1820, "text": "uct rule of probability. Thus, if we use such a model to predict the next observationExercise 11.16\nin a sequence, the distribution of predictions will depend only on the value of the im-\nmediately preceding observation and will be independent of all earlier observations.\nMore speciﬁcally, (11.44) is known as a ﬁrst-order Markov model because only\none conditioning variable appears in each conditional distribution. We can extend"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1821, "text": "the model by allowing each conditional distribution to depend on the two preceding\nvariables, giving a second-order Markov model of the form\np(x1;:::; xN) = p(x1)p(x2|x1)\nN∏\nn=3\np(xn|xn−1 ;xn−2 ): (11.46)\nNote that the ﬁrst two variables are treated differently as they have fewer than two\nconditioning variables. This model is shown as a directed graph in Figure 11.30.\nBy using d-separation (or by direct evaluation using the rules of probability),Exercise 11.17"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1822, "text": "we see that in the second-order Markov model, the conditional distribution of xn\ngiven all previous observations x1;:::; xn−1 is independent of the observations\nx1;::: xn−3 . We can similarly consider extensions to anMth order Markov chain in\nFigure 11.29 A ﬁrst-order Markov chain of ob-\nservations in which the distribu-\ntion of a particular observation xn\nis conditioned on the value of the\nprevious observation xn−1.\nx1 x2 x3 x4 …\n352 11. STRUCTURED DISTRIBUTIONS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1823, "text": "x1 x2 x3 x4 …\n352 11. STRUCTURED DISTRIBUTIONS\nFigure 11.30 A second-order Markov chain in\nwhich the conditional distribution\nof a particular observation xn de-\npends on the values of the two\nprevious observations xn−1 and\nxn−2.\nx1 x2 x3 x4 …\nwhich the conditional distribution for a particular variable depends on the previous\nM variables. However, we have paid a price for this increased ﬂexibility because the\nnumber of parameters in the model is now much larger. Suppose the observations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1824, "text": "are discrete variables havingKstates. Then the conditional distribution p(xn|xn−1 )\nin a ﬁrst-order Markov chain will be speciﬁed by a set of K −1 parameters for\neach of the K states of xn−1 giving a total of K(K−1) parameters. Now suppose\nwe extend the model to an Mth order Markov chain, so that the joint distribution is\nbuilt up from conditionals p(xn|xn−M;:::; xn−1 ). If the variables are discrete and\nif the conditional distributions are represented by general conditional probability ta-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1825, "text": "bles, then such a model will have KM−1 (K−1) parameters. Thus, the number of\nparameters grows exponentially with M, which will generally render this approach\nimpractical for larger values of M.\n11.3.1 Hidden variables\nSuppose we wish to build a model for sequences that is not limited by the\nMarkov assumption to any order and yet can be speciﬁed using a limited number of\nfree parameters. We can achieve this by introducing additional latent variables, thus"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1826, "text": "permitting a rich class of models to be constructed out of simple components. For\neach observation xn, we introduce a corresponding latent variablezn (which may be\nof different type or dimensionality to the observed variable). We now assume that it\nis the latent variables that form a Markov chain, giving rise to the graphical structure\nknown as a state-space model, which is shown in Figure 11.31. It satisﬁes the key"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1827, "text": "conditional independence property that zn−1 and zn+1 are independent given zn, so\nthat\nzn+1 ⊥ ⊥zn−1 |zn: (11.47)\nThe joint distribution for this model is given by\np(x1;:::; xN;z1;:::; zN) = p(z1)\n[N∏\nn=2\np(zn|zn−1 )\n]N∏\nn=1\np(xn|zn): (11.48)\nFigure 11.31 A state-space model expresses the joint\nprobability distribution over a sequence\nof observed states x1;:::; xN in terms\nof a Markov chain of hidden states\nz1;:::; zN in the form (11.48).\nz1 z2 zN\nx1 x2 xN\nExercises 353"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1828, "text": "z1 z2 zN\nx1 x2 xN\nExercises 353\nUsing the d-separation criterion, we see that in the state-space model there\nis always a path connecting any two observed variables xn and xm via the la-\ntent variables and that this path is never blocked. Thus, the predictive distribution\np(xn+1|x1;:::; xn) for observation xn+1 given all previous observations does not\nexhibit any conditional independence properties, and so our predictions forxn+1 de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1829, "text": "pend on all previous observations. The observed variables, therefore, do not satisfy\nthe Markov property at any order.\nThere are two important models for sequential data that are described by this\ngraph. If the latent variables are discrete, then we obtain a hidden Markov model\n(Elliott, Aggoun, and Moore, 1995). Note that the observed variables in a hidden\nMarkov model may be discrete or continuous, and a variety of different conditional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1830, "text": "distributions can be used to model them. If both the latent and the observed variables\nare Gaussian (with a linear-Gaussian dependence of the conditional distributions on\ntheir parents), then we obtain a linear dynamical system, also known as a Kalman\nﬁlter (Zarchan and Musoff, 2005). Both hidden Markov models and Kalman ﬁlters\nare discussed at length, along with algorithms for training them, in Bishop (2006).\nSuch models can be made considerably more ﬂexible by replacing the simple discrete"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1831, "text": "probability tables, or linear-Gaussian distributions, used to deﬁne p(xn|zn) with\ndeep neural networks.\nExercises\n11.1 (?) By marginalizing out the variables in order, show that the representation (11.6)\nfor the joint distribution of a directed graph is correctly normalized, provided each\nof the conditional distributions is normalized.\n11.2 (?) Show that the property of there being no directed cycles in a directed graph"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1832, "text": "follows from the statement that there exists an ordered numbering of the nodes such\nthat for each node there are no links going to a lower-numbered node.\n11.3 (??) Consider three binary variables a;b;c ∈ {0;1}having the joint distribution\ngiven in Table 11.1. Show by direct evaluation that this distribution has the property\nthat a and b are marginally dependent, so that p(a;b) ̸=p(a)p(b), but that they\nbecome independent when conditioned on c, so that p(a;b|c) = p(a|c)p(b|c)for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1833, "text": "both c= 0 and c= 1.\nTable 11.1 The joint distribution over three binary variables. a\nb c p(a;b;c)\n0 0 0 0.192\n0 0 1 0.144\n0 1 0 0.048\n0 1 1 0.216\n1 0 0 0.192\n1 0 1 0.064\n1 1 0 0.048\n1 1 1 0.096\n354 11. STRUCTURED DISTRIBUTIONS\n11.4 (??) Evaluate the distributions p(a), p(b|c), andp(c|a)corresponding to the joint\ndistribution given in Table 11.1. Hence, show by direct evaluation that p(a;b;c) =\np(a)p(c|a)p(b|c). Draw the corresponding directed graph."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1834, "text": "11.5 (?) For the model shown in Figure 11.6, we have seen that the number of parameters\nrequired to specify the conditional distribution p(y|x1;:::;x M), where xi ∈{0;1},\ncould be reduced from 2M to M+ 1by making use of the logistic sigmoid represen-\ntation (11.8). An alternative representation (Pearl, 1988) is given by\np(y= 1|x1;:::;x M) = 1 −(1 −\u00160)\nM∏\ni=1\n(1 −\u0016i)xi (11.49)\nwhere the parameters \u0016irepresent the probabilities p(xi = 1) and \u00160 is an additional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1835, "text": "parameter satisfying 0 6 \u00160 6 1. The conditional distribution (11.49) is known as\nthe noisy-OR. Show that this can be interpreted as a ‘soft’ (probabilistic) form of the\nlogical OR function (i.e., the function that gives y = 1 whenever at least one of the\nxi = 1). Discuss the interpretation of \u00160.\n11.6 (??) Starting from the deﬁnition (11.9) for the conditional distributions, derive the\nrecursion relation (11.12) for the mean of the joint distribution for a linear-Gaussian\nmodel."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1836, "text": "model.\n11.7 (??) Starting from the deﬁnition (11.9) for the conditional distributions, derive the\nrecursion relation (11.13) for the covariance matrix of the joint distribution for a\nlinear-Gaussian model.\n11.8 (??) Show that the number of parameters in the covariance matrix of a fully con-\nnected linear-Gaussian graphical model overDvariables deﬁned by (11.9) isD(D+\n1)=2.\n11.9 (??) Using the recursion relations (11.12) and (11.13), show that the mean and co-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1837, "text": "variance of the joint distribution for the graph shown in Figure 11.7 are given by\n(11.14) and (11.15), respectively.\n11.10 (?) Verify that the joint distribution over a set of vector-valued variables deﬁned by a\nlinear-Gaussian model in which each node corresponds to a distribution of the form\n(11.16) is itself a Gaussian.\n11.11 (?) Show that a⊥ ⊥b;c |dimplies a⊥ ⊥b|d.\n11.12 (?) Using the d-separation criterion, show that the conditional distribution for a node"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1838, "text": "x in a directed graph, conditioned on all the nodes in the Markov blanket, is inde-\npendent of the remaining variables in the graph.\n11.13 (?) Consider the directed graph shown inFigure 11.32 in which none of the variables\nis observed. Show that a⊥ ⊥b|∅. Suppose we now observe the variable d. Show\nthat in general a̸⊥ ⊥b|d.\nExercises 355\nFigure 11.32 Example of a graphical model used to explore the conditional\nindependence properties of the head-to-head path a–c–bwhen"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1839, "text": "a descendant of c, namely the node d, is observed.\na\nc\nb\nd\n11.14 (??) Consider the example of the car fuel system shown inFigure 11.20, and suppose\nthat instead of observing the state of the fuel gauge Gdirectly, the gauge is seen by\nthe driver D, who reports to us the reading on the gauge. This report says that the\ngauge shows either that the tank is full D = 1 or that it is empty D = 0. Our driver\nis a bit unreliable, as expressed through the following probabilities:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1840, "text": "p(D= 1|G= 1) = 0:9 (11.50)\np(D= 0|G= 0) = 0:9: (11.51)\nSuppose that the driver tells us that the fuel gauge shows empty, in other words\nthat we observe D = 0. Evaluate the probability that the tank is empty given only\nthis observation. Similarly, evaluate the corresponding probability given also the\nobservation that the battery is ﬂat, and note that this second probability is lower.\nDiscuss the intuition behind this result, and relate the result to Figure 11.32."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1841, "text": "11.15 (??) Suppose we train a naive Bayes model, with the assumption (11.37), using\nmaximum likelihood. Assume that each of the class-conditional densities p(x(l)|Ck)\nis governed by its own independent parameters w(l). Show that the maximum like-\nlihood solution involves ﬁtting each of the class-conditional densities using the cor-\nresponding observed data vectors x(l)\n1 ;:::; x(l)\nN by maximizing the likelihood with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1842, "text": "1 ;:::; x(l)\nN by maximizing the likelihood with\nrespect to the corresponding class label data, and then setting the class priors p(Ck)\nto the fraction of training data points in each class.\n11.16 (??) Consider the joint probability distribution (11.44) corresponding to the directed\ngraph of Figure 11.29. Using the sum and product rules of probability, verify that\nthis joint distribution satisﬁes the conditional independence property (11.45) forn="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1843, "text": "2;:::;N . Similarly, show that the second-order Markov model described by the joint\ndistribution (11.46) satisﬁes the conditional independence property\np(xn|x1;:::; xn−1 ) = p(xn|xn−1 ;xn−2 ) (11.52)\nfor n= 3;:::;N .\n11.17 (?) Use d-separation, as discussed in Section 11.2, to verify that the Markov model\nshown in Figure 11.29 having Nnodes in total satisﬁes the conditional independence\nproperties (11.45) for n= 2;:::;N . Similarly, show that a model described by the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1844, "text": "graph in Figure 11.30 in which there are N nodes in total satisﬁes the conditional\nindependence properties (11.52) for n= 3;:::;N .\n356 11. STRUCTURED DISTRIBUTIONS\n11.18 (?) Consider a second-order Markov process described by the graph inFigure 11.30.\nBy combining adjacent pairs of variables, show that this can be expressed as a ﬁrst-\norder Markov process over the new variables.\n11.19 (?) By using d-separation, show that the distribution p(x1;:::; xN) of the observed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1845, "text": "data for the state-space model represented by the directed graph inFigure 11.31 does\nnot satisfy any conditional independence properties and hence does not exhibit the\nMarkov property at any ﬁnite order.\n12\nTransformers\nTransformers represent one of the most important developments in deep learning.\nThey are based on a processing concept called attention, which allows a network to\ngive different weights to different inputs, with weighting coefﬁcients that themselves"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1846, "text": "depend on the input values, thereby capturing powerful inductive biases related to\nsequential and other forms of data.\nThese models are known as transformers because they transform a set of vec-\ntors in some representation space into a corresponding set of vectors, having the\nsame dimensionality, in some new space. The goal of the transformation is that the\nnew space will have a richer internal representation that is better suited to solving"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1847, "text": "downstream tasks. Inputs to a transformer can take the form of unstructured sets\nof vectors, ordered sequences, or more general representations, giving transformers\nbroad applicability.\nTransformers were originally introduced in the context of natural language pro-\n357© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_12"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1848, "text": "358 12. TRANSFORMERS\ncessing, or NLP (where a ‘natural’ language is one such as English or Mandarin) and\nhave greatly surpassed the previous state-of-the-art approaches based on recurrent\nneural networks (RNNs). Transformers have subsequently been found to achieve\nexcellent results in many other domains. For example, vision transformers often\noutperform CNNs in image processing tasks, whereas multimodal transformers that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1849, "text": "combine multiple types of data, such as text, images, audio, and video, are amongst\nthe most powerful deep learning models.\nOne major advantage of transformers is that transfer learning is very effective, so\nthat a transformer model can be trained on a large body of data and then the trained\nmodel can be applied to many downstream tasks using some form of ﬁne-tuning. A\nlarge-scale model that can subsequently be adapted to solve multiple different tasks"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1850, "text": "is known as a foundation model. Furthermore, transformers can be trained in a self-\nsupervised way using unlabelled data, which is especially effective with language\nmodels since transformers can exploit vast quantities of text available from the inter-\nnet and other sources. The scaling hypothesis asserts that simply by increasing the\nscale of the model, as measured by the number of learnable parameters, and train-\ning on a commensurately large data set, signiﬁcant improvements in performance"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1851, "text": "can be achieved, even with no architectural changes. Moreover, the transformer is\nespecially well suited to massively parallel processing hardware such as graphical\nprocessing units, or GPUs, allowing exceptionally large neural network language\nmodels having of the order of a trillion (10 12) parameters to be trained in reason-\nable time. Such models have extraordinary capabilities and show clear indications\nof emergent properties that have been described as the early signs of artiﬁcial general"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1852, "text": "intelligence (Bubeck et al., 2023).\nThe architecture of a transformer can seem complex, or even daunting, to a\nnewcomer as it involves multiple different components working together, in which\nthe various design choices can seem arbitrary. In this chapter we therefore aim to give\na comprehensive step-by-step introduction to all the key ideas behind transformers\nand to provide clear intuition to motivate the design of the various elements. We ﬁrst"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1853, "text": "describe the transformer architecture and then focus on natural language processing,\nbefore exploring other application domains.\n12.1.\nAttention\nThe fundamental concept that underpins a transformer is attention. This was orig-\ninally developed as an enhancement to RNNs for machine translation (Bahdanau,Section 12.2.5\nCho, and Bengio, 2014). However, Vaswani et al. (2017) later showed that signiﬁ-\ncantly improved performance could be obtained by eliminating the recurrence struc-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1854, "text": "ture and instead focusing exclusively on the attention mechanism. Today, transform-\ners based on attention have completely superseded RNNs in almost all applications.\nWe will motivate the use of attention using natural language as an example,\n12.1. Attention 359\nI swam across the river to get to the other bank\nI swam across the river to get to the other bank\nFigure 12.1 Schematic illustration of attention in which the interpretation of the word ‘bank’ is inﬂuenced by the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1855, "text": "words ‘river’ and ‘swam’, with the thickness of each line being indicative of the strength of its inﬂuence.\nalthough it has much broader applicability. Consider the following two sentences:\nI swam across the river to get to the other bank.\nI walked across the road to get cash from the bank.\nHere the word ‘bank’ has different meanings in the two sentences. However, this\ncan be detected only by looking at the context provided by other words in the se-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1856, "text": "quence. We also see that some words are more important than others in determining\nthe interpretation of ‘bank’. In the ﬁrst sentence, the words ‘swam’ and ‘river’ most\nstrongly indicate that ‘bank’ refers to the side of a river, whereas in the second sen-\ntence, the word ‘cash’ is a strong indicator that ‘bank’ refers to a ﬁnancial institution.\nWe see that to determine the appropriate interpretation of ‘bank’, a neural network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1857, "text": "processing such a sentence should attend to, in other words rely more heavily on,\nspeciﬁc words from the rest of the sequence. This concept of attention is illustrated\nin Figure 12.1.\nMoreover, we also see that the particular locations that should receive more\nattention depend on the input sequence itself: in the ﬁrst sentence it is the second and\nﬁfth words that are important whereas in the second sentence it is the eighth word."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1858, "text": "In a standard neural network, different inputs will inﬂuence the output to different\nextents according to the values of the weights that multiply those inputs. Once the\nnetwork is trained, however, those weights, and their associated inputs, are ﬁxed.\nBy contrast, attention uses weighting factors whose values depend on the speciﬁc\ninput data. Figure 12.2 shows the attention weights from a section of a transformer\nnetwork trained on natural language."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1859, "text": "network trained on natural language.\nWhen we discuss natural language processing, we will see how word embed-\nding can be used to map words into vectors in an embedding space. These vectors\ncan then be used as inputs for subsequent neural network processing. These embed-\ndings capture elementary semantic properties, for example by mapping words with\nsimilar meanings to nearby locations in the embedding space. One characteristic of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1860, "text": "such embeddings is that a given word always maps to the same embedding vector.\n360 12. TRANSFORMERS\nFigure 12.2 An example of learned attention weights. [From Vaswani et al. (2017) with permission.]\nA transformer can be viewed as a richer form of embedding in which a given vector\nis mapped to a location that depends on the other vectors in the sequence. Thus,\nthe vector representing ‘bank’ in our example above could map to different places"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1861, "text": "in a new embedding space for the two different sentences. For example, in the ﬁrst\nsentence the transformed representation might put ‘bank’ close to ‘water’ in the em-\nbedding space, whereas in the second sentence the transformed representation might\nput it close to ‘money’.\nAs an example of attention, consider the modelling of proteins. We can view\na protein as a one-dimensional sequence of molecular units called amino acids. A"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1862, "text": "protein can comprise potentially hundreds or thousands of such units, each of which\nis given by one of 22 possibilities. In a living cell, a protein folds up into a three-\ndimensional structure in which amino acids that are widely separated in the one-\ndimensional sequence can become physically close in three-dimensional space and\nthereby interact. Transformer models allows these distant amino acids to ‘attend’ toFigure 1.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1863, "text": "each other thereby greatly improving the accuracy with which their 3-dimensional\nstructure can be modelled (Vig et al., 2020).\n12.1.1 Transformer processing\nThe input data to a transformer is a set of vectors {xn}of dimensionality D,\nwhere n= 1;:::;N . We refer to these data vectors as tokens, where a token might,\nfor example, correspond to a word within a sentence, a patch within an image, or\nan amino acid within a protein. The elements xni of the tokens are called features."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1864, "text": "Later we will see how to construct these token vectors for natural language data and\nfor images. A powerful property of transformers is that we do not have to design a\nnew neural network architecture to handle a mix of different data types but instead\ncan simply combine the data variables into a joint set of tokens.\nBefore we can gain a clear understanding of the operation of a transformer, it\n12.1. Attention 361\nFigure 12.3 The structure of the data matrix X, of di-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1865, "text": "mension N ×D, in which row n repre-\nsents the transposed data vector xT\nn.\nX\nN (tokens)\nD(features)\nxT\nn\nis important to be precise about notation. We will follow the standard convention\nand combine the data vectors into a matrix X of dimensions N ×D in which the\nnth row comprises the token vector xT\nn, and where n = 1;:::;N labels the rows,\nas illustrated in Figure 12.3. Note that this matrix represents one set of input tokens,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1866, "text": "and that for most applications, we will require a data set containing many sets of\ntokens, such as independent passages of text where each word is represented as one\ntoken. The fundamental building block of a transformer is a function that takes a\ndata matrix as input and creates a transformed matrix ˜X of the same dimensionality\nas the output. We can write this function in the form\n˜X = TransformerLayer [X]: (12.1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1867, "text": "˜X = TransformerLayer [X]: (12.1)\nWe can then apply multiple transformer layers in succession to construct deep net-\nworks capable of learning rich internal representations. Each transformer layer con-\ntains its own weights and biases, which can be learned using gradient descent using\nan appropriate cost function, as we will discuss in detail later in the chapter.Section 12.3\nA single transformer layer itself comprises two stages. The ﬁrst stage, which im-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1868, "text": "plements the attention mechanism, mixes together the corresponding features from\ndifferent token vectors across the columns of the data matrix, whereas the second\nstage then acts on each row independently and transforms the features within each\ntoken vector. We start by looking at the attention mechanism.\n12.1.2 Attention coefﬁcients\nSuppose that we have a set of input tokens x1;:::; xN in an embedding space\nand we want to map this to another sety1;:::; yN having the same number of tokens"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1869, "text": "but in a new embedding space that captures a richer semantic structure. Consider a\nparticular output vector yn. The value of yn should depend not just on the corre-\nsponding input vector xn but on all the vectorsx1;:::; xN in the set. With attention,\nthis dependence should be stronger for those inputs xm that are particularly impor-\ntant for determining the modiﬁed representation of yn. A simple way to achieve this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1870, "text": "is to deﬁne each output vector yn to be a linear combination of the input vectors\n362 12. TRANSFORMERS\nx1;:::; xN with weighting coefﬁcients anm:\nyn =\nN∑\nm=1\nanmxm (12.2)\nwhere anm are called attention weights. The coefﬁcients should be close to zero for\ninput tokens that have little inﬂuence on the output yn and largest for inputs that\nhave most inﬂuence. We therefore constrain the coefﬁcients to be non-negative to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1871, "text": "avoid situations in which one coefﬁcient can become large and positive while another\ncoefﬁcient compensates by becoming large and negative. We also want to ensure that\nif an output pays more attention to a particular input, this will be at the expense of\npaying less attention to the other inputs, and so we constrain the coefﬁcients to sum\nto unity. Thus, the weighting coefﬁcients must satisfy the following two constraints:\nanm > 0 (12.3)\nN∑\nm=1\nanm = 1: (12.4)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1872, "text": "anm > 0 (12.3)\nN∑\nm=1\nanm = 1: (12.4)\nTogether these imply that each coefﬁcient lies in the range 0 6 anm 6 1 and so theExercise 12.1\ncoefﬁcients deﬁne a ‘partition of unity’. For the special caseamm = 1, it follows that\nanm = 0 for n̸=m, and therefore ym = xm so that the input vector is unchanged\nby the transformation. More generally, the output ym is a blend of the input vectors\nwith some inputs given more weight than others."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1873, "text": "with some inputs given more weight than others.\nNote that we have a different set of coefﬁcients for each output vector yn;and\nthe constraints (12.3) and (12.4) apply separately for each value of n. These co-\nefﬁcients anm depend on the input data, and we will shortly see how to calculate\nthem.\n12.1.3 Self-attention\nThe next question is how to determine the coefﬁcients anm. Before we discuss\nthis in detail, it is useful to ﬁrst introduce some terminology taken from the ﬁeld of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1874, "text": "information retrieval. Consider the problem of choosing which movie to watch in\nan online movie streaming service. One approach would be to associate each movie\nwith a list of attributes describing things such as the genre (comedy, action, etc.), the\nnames of the leading actors, the length of the movie, and so on. The user could then\nsearch through a catalogue to ﬁnd a movie that matches their preferences. We could"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1875, "text": "automate this by encoding the attributes of each movie in a vector called the key.\nThe corresponding movie ﬁle itself is called a value. Similarly, the user could then\nprovide their own personal vector of values for the desired attributes, which we call\nthe query. The movie service could then compare the query vector with all the key\nvectors to ﬁnd the best match and send the corresponding movie to the user in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1876, "text": "form of the value ﬁle. We can think of the user ‘attending’ to the particular movie\nwhose key most closely matches their query. This would be considered a form of\nhard attention in which a single value vector is returned. For the transformer, we\ngeneralize this to soft attention in which we use continuous variables to measure\n12.1. Attention 363\nthe degree of match between queries and keys and we then use these variables to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1877, "text": "weight the inﬂuence of the value vectors on the outputs. This will also ensure that\nthe transformer function is differentiable and can therefore be trained by gradient\ndescent.\nFollowing the analogy with information retrieval, we can view each of the input\nvectors xnas a value vector that will be used to create the output tokens. We also use\nthe vector xn directly as the key vector for input token n. That would be analogous"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1878, "text": "to using the movie itself to summarize the characteristics of the movie. Finally, we\ncan use xm as the query vector for output ym, which can then be compared to each\nof the key vectors. To see how much the token represented by xn should attend to\nthe token represented by xm, we need to work out how similar these vectors are.\nOne simple measure of similarity is to take their dot product xT\nnxm. To impose the\nconstraints (12.3) and (12.4), we can deﬁne the weighting coefﬁcients anm by using"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1879, "text": "the softmax function to transform the dot products:Section 5.3\nanm = exp(xT\nnxm)\n∑N\nm′=1 exp(xT\nnxm′)\n: (12.5)\nNote that in this case there is no probabilistic interpretation of the softmax function\nand it is simply being used to normalize the attention weights appropriately.\nSo in summary, each input vector xn is transformed to a corresponding output\nvector yn by taking a linear combination of input vectors of the form (12.2) in which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1880, "text": "the weight anm applied to input vector xm is given by the softmax function (12.5)\ndeﬁned in terms of the dot product xT\nnxm between the query xn for input nand the\nkey xm associated with input m. Note that, if all the input vectors are orthogonal,\nthen each output vector is simply equal to the corresponding input vector so that\nym = xm for m= 1;:::;N .Exercise 12.3\nWe can write (12.2) in matrix notation by using the data matrix X, along with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1881, "text": "the analogous N ×Doutput matrix Y, whose rows are given by ym, so that\nY = Softmax\n[\nXXT]\nX (12.6)\nwhere Softmax[L] is an operator that takes the exponential of every element of a\nmatrix L and then normalizes each row independently to sum to one. From now on,\nwe will focus on matrix notation for clarity.\nThis process is called self-attention because we are using the same sequence to\ndetermine the queries, keys, and values. We will encounter variants of this attention"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1882, "text": "mechanism later in this chapter. Also, because the measure of similarity between\nquery and key vectors is given by a dot product, this is known as dot-product self-\nattention.\n12.1.4 Network parameters\nAs it stands, the transformation from input vectors {xn}to output vectors {yn}\nis ﬁxed and has no capacity to learn from data because it has no adjustable parame-\nters. Furthermore, each of the feature values within a token vectorxn plays an equal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1883, "text": "role in determining the attention coefﬁcients, whereas we would like the network to\n364 12. TRANSFORMERS\nhave the ﬂexibility to focus more on some features than others when determining\ntoken similarity. We can address both issues if we deﬁne modiﬁed feature vectors\ngiven by a linear transformation of the original vectors in the form\n˜X = XU (12.7)\nwhere U is a D×Dmatrix of learnable weight parameters, analogous to a ‘layer’"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1884, "text": "in a standard neural network. This gives a modiﬁed transformation of the form\nY = Softmax\n[\nXUUTXT]\nXU: (12.8)\nAlthough this has much more ﬂexibility, it has the property that the matrix\nXUUTXT (12.9)\nis symmetric, whereas we would like the attention mechanism to support signiﬁcant\nasymmetry. For example, we might expect that ‘chisel’ should be strongly associ-\nated with ‘tool’ since every chisel is a tool, whereas ‘tool’ should only be weakly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1885, "text": "associated with ‘chisel’ because there are many other kinds of tools besides chis-\nels. Although the softmax function means the resulting matrix of attention weights\nis not itself symmetric, we can create a much more ﬂexible model by allowing the\nqueries and the keys to have independent parameters. Furthermore, the form (12.8)\nuses the same parameter matrix U to deﬁne both the value vectors and the attention\ncoefﬁcients, which again seems like an undesirable restriction."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1886, "text": "We can overcome these limitations by deﬁning separate query, key, and value\nmatrices each having their own independent linear transformations:\nQ = XW(q) (12.10)\nK = XW(k) (12.11)\nV = XW(v) (12.12)\nwhere the weight matrices W(q), W(k), and W(v) represent parameters that will\nbe learned during the training of the ﬁnal transformer architecture. Here the matrix\nW(k) has dimensionality D×Dk where Dk is the length of the key vector. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1887, "text": "matrix W(q) must have the same dimensionality D×Dk as W(k) so that we can\nform dot products between the query and key vectors. A typical choice is Dk = D.\nSimilarly, W(v) is a matrix of size D×Dv, where Dv governs the dimensionality of\nthe output vectors. If we set Dv = D, so that the output representation has the same\ndimensionality as the input, this will facilitate the inclusion of residual connections,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1888, "text": "which we discuss later. Also, multiple transformer layers can be stacked on top ofSection 12.1.7\neach other if each layer has the same dimensionality. We can then generalize (12.6)\nto give\nY = Softmax\n[\nQKT]\nV (12.13)\nwhere QKT has dimension N ×N, and the matrix Y has dimension N ×Dv. The\ncalculation of the matrix QKT is illustrated in Figure 12.4, whereas the evaluation\nof the matrix Y is illustrated in Figure 12.5.\n12.1. Attention 365\n× W(q)\nD×D\n= Q\nN ×D\nW(k)\nD×D\n= K\nN ×D\n×\nQKT\nN ×N\nX\nN ×D"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1889, "text": "D×D\n= Q\nN ×D\nW(k)\nD×D\n= K\nN ×D\n×\nQKT\nN ×N\nX\nN ×D\nFigure 12.4 Illustration of the evaluation of the matrix QKT, which determines the attention coefﬁ-\ncients in a transformer. The input X is separately transformed using (12.10) and (12.11)\nto give the query matrix Q and key matrix K, respectively, which are then multiplied to-\ngether.\nIn practice we can also include bias parameters in these linear transformations.\nHowever, the bias parameters can be absorbed into the weight matrices, as we did"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1890, "text": "with standard neural networks, by augmenting the data matrix X with an additionalSection 6.2.1\ncolumn of 1’s and by augmenting the weight matrices with an additional row of\nparameters to represent the biases. From now on we will treat the bias parameters as\nimplicit to avoid cluttering the notation.\nCompared to a conventional neural network, the signal paths have multiplicative\nrelations between activation values. Whereas standard networks multiply activations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1891, "text": "by ﬁxed weights, here the activations are multiplied by the data-dependent attention\ncoefﬁcients. This means, for example, that if one of the attention coefﬁcients is\nclose to zero for a particular choice of input vector, the resulting signal path will\nignore the corresponding incoming signal, which will therefore have no inﬂuence\nFigure 12.5 Illustration of the evaluation\nof the output from an attention layer given\nthe query, key, and value matrices Q,\nK, and V, respectively. The entry at"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1892, "text": "K, and V, respectively. The entry at\nthe position highlighted in the output ma-\ntrix Y is obtained from the dot prod-\nuct of the highlighted row and column\nof the Softmax\n\u0002\nQKT\u0003\nand V matrices,\nrespectively.\nY\nN ×Dv\n= Softmax QKT\nN ×N\n×\nN ×Dv\nV\n366 12. TRANSFORMERS\nFigure 12.6 Information ﬂow in a scaled dot-\nproduct self-attention neural network\nlayer. Here ‘mat mul’ denotes matrix\nmultiplication, and ‘scale’ refers to the\nnormalization of the argument to the\nsoftmax using √\nDk. This structure"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1893, "text": "softmax using √\nDk. This structure\nconstitutes a single attention ‘head’.\nX\nW(k)W(q) W(v)\nKQ V\nmat mul\nscale\nsoftmax\nmat mul\nY\non the network outputs. By contrast, if a standard neural network learns to ignore a\nparticular input or hidden-unit variable, it does so for all input vectors.\n12.1.5 Scaled self-attention\nThere is one ﬁnal reﬁnement we can make to the self-attention layer. Recall that\nthe gradients of the softmax function become exponentially small for inputs of high"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1894, "text": "magnitude, just as happens with tanh or logistic-sigmoid activation functions. To\nhelp prevent this from happening, we can re-scale the product of the query and key\nvectors before applying the softmax function. To derive a suitable scaling, note that\nif the elements of the query and key vectors were all independent random numbers\nwith zero mean and unit variance, then the variance of the dot product would beDk.Exercise 12.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1895, "text": "We therefore normalize the argument to the softmax using the standard deviation\ngiven by the square root of Dk, so that the output of the attention layer takes the\nform\nY = Attention(Q;K;V) ≡Softmax\n[QKT\n√Dk\n]\nV: (12.14)\nThis is called scaled dot-product self-attention , and is the ﬁnal form of our self-\nattention neural network layer. The structure of this layer is summarized in Fig-\nure 12.6 and in Algorithm 12.1.\n12.1.6 Multi-head attention"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1896, "text": "12.1.6 Multi-head attention\nThe attention layer described so far allows the output vectors to attend to data-\ndependent patterns of input vectors and is called an attention head. However, there\n12.1. Attention 367\nAlgorithm 12.1: Scaled dot-product self-attention\nInput: Set of tokens X ∈RN×D : {x1;:::; xN}\nWeight matrices {W(q);W(k)}∈RD×Dk and W(v) ∈RD×Dv\nOutput: Attention(Q;K;V) ∈RN×Dv : {y1;:::; yN}\nQ = XW(q) // compute queries Q ∈RN×Dk\nK = XW(k) // compute keys K ∈RN×Dk"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1897, "text": "K = XW(k) // compute keys K ∈RN×Dk\nV = XW(v) // compute values V ∈RN×D\nreturn Attention(Q;K;V) = Softmax\n[QKT\n√Dk\n]\nV\nmight be multiple patterns of attention that are relevant at the same time. In natu-\nral language, for example, some patterns might be relevant to tense whereas others\nmight be associated with vocabulary. Using a single attention head can lead to av-\neraging over these effects. Instead we can use multiple attention heads in parallel."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1898, "text": "These consist of identically structured copies of the single head, with independent\nlearnable parameters that govern the calculation of the query, key, and value matri-\nces. This is analogous to using multiple different ﬁlters in each layer of a convolu-\ntional network.\nSuppose we have H heads indexed by h= 1;:::;H of the form\nHh = Attention(Qh;Kh;Vh) (12.15)\nwhere Attention(-;-;-)is given by (12.14), and we have deﬁned separate query, key,\nand value matrices for each head using\nQh = XW(q)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1899, "text": "and value matrices for each head using\nQh = XW(q)\nh (12.16)\nKh = XW(k)\nh (12.17)\nVh = XW(v)\nh : (12.18)\nThe heads are ﬁrst concatenated into a single matrix, and the result is then linearly\ntransformed using a matrix W(o) to give a combined output in the form\nY(X) = Concat [H1;:::; HH] W(o): (12.19)\nThis is illustrated in Figure 12.7.\nEach matrix Hh has dimension N ×Dv, and so the concatenated matrix has\ndimension N ×HDv. This is transformed by the linear matrix W(o) of dimension"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1900, "text": "HDv ×Dto give the ﬁnal output matrix Y of dimension N×D, which is the same\nas the original input matrix X. The elements of the matrix W(o) are learned during\nthe training phase along with the query, key, and value matrices. Typically Dv is\n368 12. TRANSFORMERS\nFigure 12.7 Network architecture for multi-\nhead attention. Each head com-\nprises the structure shown in Fig-\nure 12.6, and has its own key,\nquery, and value parameters. The\noutputs of the heads are con-\ncatenated and then linearly pro-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1901, "text": "catenated and then linearly pro-\njected back to the input data\ndimensionality.\nN ×HDv\nH1 H2 … HH × W(o)\nHDv ×D\n= Y\nN ×D\nchosen to be equal to D=H so that the resulting concatenated matrix has dimension\nN×D. Multi-head attention is summarized in Algorithm 12.2, and the information\nﬂow in a multi-head attention layer is illustrated in Figure 12.8.\nNote that the formulation of multi-head attention given above, which follows"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1902, "text": "that used in the research literature, includes some redundancy in the successive mul-\ntiplication of the W(v) matrix for each head and the output matrix W(o). Removing\nthis redundancy allows a multi-head self-attention layer to be written as a sum over\ncontributions from each of the heads separately.Exercise 12.5\n12.1.7 Transformer layers\nMulti-head self-attention forms the core architectural element in a transformer"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1903, "text": "network. We know that neural networks beneﬁt greatly from depth, and so we would\nlike to stack multiple self-attention layers on top of each other. To improve training\nAlgorithm 12.2: Multi-head attention\nInput: Set of tokens X ∈RN×D : {x1;:::; xN}\nQuery weight matrices {W(q)\n1 ;:::; W(q)\nH }∈RD×D\nKey weight matrices {W(k)\n1 ;:::; W(k)\nH }∈RD×D\nValue weight matrices {W(v)\n1 ;:::; W(v)\nH }∈RD×Dv\nOutput weight matrix W(o) ∈RHDv×D\nOutput: Y ∈RN×D : {y1;:::; xN}"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1904, "text": "Output: Y ∈RN×D : {y1;:::; xN}\n// compute self-attention for each head (Algorithm 12.1)\nfor h= 1;:::;H do\nQh = XW(q)\nh ; Kh = XW(k)\nh ; Vh = XW(v)\nh\nHh = Attention (Qh;Kh;Vh) // Hh ∈RN×Dv\nend for\nH = Concat [H1;:::; HN] // concatenate heads\nreturn Y(X) = HW(o)\n12.1. Attention 369\nX\nself-attentionself-attention … self-attention\nconcat\nlinear\nY\nFigure 12.8 Information ﬂow in a multi-head attention layer. The associated computation, given by\nAlgorithm 12.2, is illustrated in Figure 12.7."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1905, "text": "Algorithm 12.2, is illustrated in Figure 12.7.\nefﬁciency, we can introduce residual connections that bypass the multi-head struc-Section 9.5\nture. To do this we require that the output dimensionality is the same as the input\ndimensionality, namely N ×D. This is then followed by layer normalization (Ba,Section 7.4.3\nKiros, and Hinton, 2016), which improves training efﬁciency. The resulting trans-\nformation can be written as\nZ = LayerNorm [Y(X) + X] (12.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1906, "text": "Z = LayerNorm [Y(X) + X] (12.20)\nwhere Y is deﬁned by (12.19). Sometimes the layer normalization is replaced by\npre-norm in which the normalization layer is applied before the multi-head self-\nattention instead of after, as this can result in more effective optimization, in which\ncase we have\nZ = Y(X′) + X; where X′= LayerNorm [X] : (12.21)\nIn each case, Z again has the same dimensionality N ×Das the input matrix X.\nWe have seen that the attention mechanism creates linear combinations of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1907, "text": "value vectors, which are then linearly combined to produce the output vectors. Also,\nthe values are linear functions of the input vectors, and so we see that the outputs\nof an attention layer are constrained to be linear combinations of the inputs. Non-\nlinearity does enter through the attention weights, and so the outputs will depend\nnonlinearly on the inputs via the softmax function, but the output vectors are still"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1908, "text": "constrained to lie in the subspace spanned by the input vectors and this limits the\nexpressive capabilities of the attention layer. We can enhance the ﬂexibility of the\ntransformer by post-processing the output of each layer using a standard nonlinear\nneural network with D inputs and D outputs, denoted MLP[-]for ‘multilayer per-\nceptron’. For example, this might consist of a two-layer fully connected network\nwith ReLU hidden units. This needs to be done in a way that preserves the ability"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1909, "text": "370 12. TRANSFORMERS\nFigure 12.9 One layer of the transformer architecture that\nimplements the transformation (12.1). Here\n‘MLP’ stands for multilayer perceptron, while\n‘add and norm’ denotes a residual connection\nfollowed by layer normalization.\nX\nmulti-head\nself-attention\nadd & norm\nMLP\nadd & norm\neX\nZ\nof the transformer to process sequences of variable length. To achieve this, the same\nshared network is applied to each of the output vectors, corresponding to the rows of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1910, "text": "Z. Again, this neural network layer can be improved by using a residual connection.\nIt also includes layer normalization so that the ﬁnal output from the transformer layer\nhas the form\n˜X = LayerNorm [MLP [Z] + Z] : (12.22)\nThis leads to an overall architecture for a transformer layer shown inFigure 12.9 and\nsummarized in Algorithm 12.3. Again, we can use a pre-norm instead, in which case\nthe ﬁnal output is given by\n˜X = MLP(Z′) + Z; where Z′= LayerNorm [Z] : (12.23)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1911, "text": "In a typical transformer there are multiple such layers stacked on top of each other.\nThe layers generally have identical structures, although there is no sharing of weights\nand biases between different layers.\n12.1.8 Computational complexity\nThe attention layer discussed so far takes a set of N vectors each of length\nD and maps them into another set of N vectors having the same dimensionality.\nThus, the inputs and outputs each have overall dimensionalityND. If we had used a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1912, "text": "standard fully connected neural network to map the input values to the output values,\nit would have O(N2D2) independent parameters. Likewise the computational cost\nof evaluating one forward pass through such a network would also be O(N2D2).\nIn the attention layer, the matrices W(q), W(k), and W(v) are shared across in-\nput tokens, and therefore the number of independent parameters isO(D2), assuming\nDk ≃Dv ≃D. Since there are N input tokens, the number of computational steps\n12.1. Attention 371"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1913, "text": "12.1. Attention 371\nAlgorithm 12.3: Transformer layer\nInput: Set of tokens X ∈RN×D : {x1;:::; xN}\nMulti-head self-attention layer parameters\nFeed-forward network parameters\nOutput: ˜X ∈RN×D : {˜x1;:::; ˜xN}\nZ = LayerNorm [Y(X) + X] // Y(X) from Algorithm 12.2\n˜X = LayerNorm [MLP [Z] +Z] // shared neural network\nreturn ˜X\nin evaluating the dot products in a self-attention layer is O(N2D). We can think\nof a self-attention layer as a sparse matrix in which parameters are shared between"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1914, "text": "speciﬁc blocks of the matrix. The subsequent neural network layer, which has DExercise 12.6\ninputs and D outputs, has a cost that is O(D2). Since it is shared across tokens, it\nhas a complexity that is linear in N, and therefore overall this layer has a cost that is\nO(ND2). Depending on the relative sizes of N and D, either the transformer layer\nor the MLP layer may dominate the computational cost. Compared to a fully con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1915, "text": "nected network, a transformer layer is computationally more efﬁcient. Many vari-\nants of the transformer architecture have been proposed (Lin et al., 2021; Phuong\nand Hutter, 2022) including modiﬁcations aimed at improving efﬁciency (Tay et al.,\n2020).\n12.1.9 Positional encoding\nIn the transformer architecture, the matrices W(q)\nh , W(k)\nh , and W(v)\nh are shared\nacross the input tokens, as is the subsequent neural network. As a consequence, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1916, "text": "transformer has the property that permuting the order of the input tokens, i.e., the\nrows of X, results in the same permutation of the rows of the output matrix ˜X. InExercise 12.7\nother words a transformer is equivariant with respect to input permutations. TheSection 10.2\nsharing of parameters in the network architecture facilitates the massively parallel\nprocessing of the transformer, and also allows the network to learn long-range de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1917, "text": "pendencies just as effectively as short-range dependencies. However, the lack of\ndependence on token order becomes a major limitation when we consider sequential\ndata, such as the words in a natural language, because the representation learned by\na transformer will be independent of the input token ordering. The two sentences\n‘The food was bad, not good at all.’ and ‘The food was good, not bad at all.’ con-\ntain the same tokens but they have very different meanings because of the different"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1918, "text": "token ordering. Clearly token order is crucial for most sequential processing tasks\nincluding natural language processing, and so we need to ﬁnd a way to inject token\norder information into the network.\nSince we wish to retain the powerful properties of the attention layers that we\nhave carefully constructed, we aim to encode the token order in the data itself in-\n372 12. TRANSFORMERS\nstead of having to be represented in the network architecture. We will therefore"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1919, "text": "construct a position encoding vector rn associated with each input position n and\nthen combine this with the associated input token embedding xn. One obvious way\nto combine these vectors would be to concatenate them, but this would increase the\ndimensionality of the input space and hence of all subsequent attention spaces, cre-\nating a signiﬁcant increase in computational cost. Instead, we can simply add the\nposition vectors onto the token vectors to give\n˜xn = xn + rn: (12.24)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1920, "text": "˜xn = xn + rn: (12.24)\nThis requires that the positional encoding vectors have the same dimensionality as\nthe token-embedding vectors.\nAt ﬁrst it might seem that adding position information onto the token vector\nwould corrupt the input vectors and make the task of the network much more difﬁ-\ncult. However, some intuition as to why this can work well comes from noting that\ntwo randomly chosen uncorrelated vectors tend to be nearly orthogonal in spaces of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1921, "text": "high dimensionality, indicating that the network is able to process the token identityExercise 12.8\ninformation and the position information relatively separately. Note also that, be-\ncause of the residual connections across every layer, the position information does\nnot get lost in going from one transformer layer to the next. Moreover, due to the\nlinear processing layers in the transformer, a concatenated representation has similar\nproperties to an additive one.Exercise 12.9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1922, "text": "properties to an additive one.Exercise 12.9\nThe next task is to construct the embedding vectors {rn}. A simple approach\nwould be to associate an integer 1;2;3;::: with each position. However, this has the\nproblem that the magnitude of the value increases without bound and therefore may\nstart to corrupt the embedding vector signiﬁcantly. Also it may not generalize well\nto new input sequences that are longer than those used in training, since these will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1923, "text": "involve coding values that lie outside the range of those used in training. Alterna-\ntively we could assign a number in the range (0;1) to each token in the sequence,\nwhich keeps the representation bounded. However, this representation is not unique\nfor a given position as it depends on the overall sequence length.\nAn ideal positional encoding should provide a unique representation for each\nposition, it should be bounded, it should generalize to longer sequences, and it should"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1924, "text": "have a consistent way to express the number of steps between any two input vectors\nirrespective of their absolute position because the relative position of tokens is often\nmore important than the absolute position.\nThere are many approaches to positional encoding (Dufter, Schmitt, and Sch¨utze,\n2021). Here we describe a technique based on sinusoidal functions introduced by\nVaswani et al. (2017). For a given position nthe associated position-encoding vec-\ntor has components rni given by\nrni =\n"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1925, "text": "tor has components rni given by\nrni =\n\n\n\nsin\n( n\nLi=D\n)\n; if iis even;\ncos\n( n\nL(i−1)=D\n)\n; if iis odd.\n(12.25)\nWe see that the elements of the embedding vectorrnare given by a series of sine and\ncosine functions of steadily increasing wavelength, as illustrated in Figure 12.10(a).\n12.1. Attention 373\nr6 r5 r4 r3 r2 r1\nembedding dimension\nn\nm\nposition\n(a)\nembedding dimension\nposition\n−1\n0\n1\n (b)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1926, "text": "(a)\nembedding dimension\nposition\n−1\n0\n1\n (b)\nFigure 12.10 Illustrations of the functions deﬁned by (12.25) and used to construct position-encoding vectors.\n(a) A plot in which the horizontal axis shows the different components of the embedding vector r whereas the\nvertical axis shows the position in the sequence. The values of the vector elements for two positionsnand mare\nshown by the intersections of the sine and cosine curves with the horizontal grey lines. (b) A heat map illustration"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1927, "text": "of the position-encoding vectors deﬁned by (12.25) for dimension D = 100 with L = 30 for the ﬁrst N = 200\npositions.\n374 12. TRANSFORMERS\nThis encoding has the property that the elements of the vector rn all lie in the\nrange (−1;1). It is reminiscent of the way binary numbers are represented, with the\nlowest order bit alternating with high frequency, and subsequent bits alternating with\nsteadily decreasing frequencies:\n1 : 0 0 0 1\n2 : 0 0 1 0\n3 : 0 0 1 1\n4 : 0 1 0 0\n5 : 0 1 0 1\n6 : 0 1 1 0"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1928, "text": "3 : 0 0 1 1\n4 : 0 1 0 0\n5 : 0 1 0 1\n6 : 0 1 1 0\n7 : 0 1 1 1\n8 : 1 0 0 0\n9 : 1 0 0 1\nFor the encoding given by (12.25), however, the vector elements are continuous\nvariables rather than binary. A plot of the position-encoding vectors is shown in\nFigure 12.10(b).\nOne nice property of the sinusoidal representation given by (12.25) is that, for\nany ﬁxed offset k, the encoding at position n+ k can be represented as a linear"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1929, "text": "combination of the encoding at position n, in which the coefﬁcients do not dependExercise 12.10\non the absolute position but only on the value of k. The network should therefore be\nable to learn to attend to relative positions. Note that this property requires that the\nencoding makes use of both sine and cosine functions.\nAnother popular approach to positional representation is to use learned position\nencodings. This is done by having a vector of weights at each token position that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1930, "text": "can be learned jointly with the rest of the model parameters during training, and\navoids using hand-crafted representations. Because the parameters are not shared\nbetween the token positions, the tokens are no longer invariant under a permutation,\nwhich is the purpose of a positional encoding. However, this approach does not\nmeet the criteria we mentioned earlier of generalizing to longer input sequences,\nas the encoding will be untrained for positional encodings not seen during training."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1931, "text": "Therefore, this approach is generally most suitable when the input length is relatively\nconstant during both training and inference.\n12.2.\nNatural Language\nNow that we have studied the architecture of the transformer, we will explore how\nthis can be used to process language data consisting of words, sentences, and para-\ngraphs. Although this is the modality that transformers were originally developed to\noperate on, they have proved to be a very general class of models and have become"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1932, "text": "the state-of-the-art for most input data types. Later in this chapter we will look at\ntheir use in other domains.Section 12.4\nMany languages, including English, comprise a series of words separated by\nwhite space, along with punctuation symbols, and therefore represent an example of\n12.2. Natural Language 375\nsequential data. For the moment we will focus on the words, and we will return toSection 11.3\npunctuation later.Section 12.2.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1933, "text": "punctuation later.Section 12.2.2\nThe ﬁrst challenge is to convert the words into a numerical representation that\nis suitable for use as the input to a deep neural network. One simple approach is to\ndeﬁne a ﬁxed dictionary of words and then introduce vectors of length equal to the\nsize of the dictionary along with a ‘one hot’ representation for each word, in which\nthe kth word in the dictionary is encoded with a vector having a 1 in position kand"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1934, "text": "0 in all other positions. For example if ‘aardwolf’ is the third word in our dictionary\nthen its vector representation would be (0;0;1;0;:::; 0).\nAn obvious problem with a one-hot representation is that a realistic dictionary\nmight have several hundred thousand entries leading to vectors of very high dimen-\nsionality. Also, it does not capture any similarities or relationships that might exist\nbetween words. Both issues can be addressed by mapping the words into a lower-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1935, "text": "dimensional space through a process called word embedding in which each word is\nrepresented as a dense vector in a space of typically a few hundred dimensions.\n12.2.1 Word embedding\nThe embedding process can be deﬁned by a matrix E of size D ×K where\nDis the dimensionality of the embedding space and K is the dimensionality of the\ndictionary. For each one-hot encoded input vector xn we can then calculate the\ncorresponding embedding vector using\nvn = Exn: (12.26)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1936, "text": "vn = Exn: (12.26)\nBecause xnhas a one-hot encoding, the vectorvnis simply given by the correspond-\ning column of the matrix E.\nWe can learn the matrix E from a corpus (i.e., a large data set) of text, and\nthere are many approaches to doing this. Here we look at a popular technique called\nword2vec (Mikolov et al., 2013), which can be viewed as a simple two-layer neural\nnetwork. A training set is constructed in which each sample is obtained by consid-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1937, "text": "ering a ‘window’ of M adjacent words in the text, where a typical value might be\nM = 5. The samples are considered to be independent, and the error function is de-\nﬁned as the sum of the error functions for each sample. There are two variants of this\napproach. In continuous bag of words, the target variable for network training is the\nmiddle word, and the remaining context words form the inputs, so that the network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1938, "text": "is being trained to ‘ﬁll in the blank’. A closely related approach, called skip-grams,\nreverses the inputs and outputs, so that the centre word is presented as the input and\nthe target values are the context words. These models are illustrated inFigure 12.11.\nThis training procedure can be viewed as a form ofself-supervised learning since\nthe data consists simply of a large corpus of unlabelled text from which many small"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1939, "text": "windows of word sequences are drawn at random. Labels are obtained from the text\nitself by ‘masking’ out those words whose values the network is trying to predict.\nOnce the model is trained, the embedding matrix E is given by the transpose\nof the second-layer weight matrix for the continuous bag-of-words approach and\nby the ﬁrst-layer weight matrix for skip-grams. Words that are semantically related\nare mapped to nearby positions in the embedding space. This is to be expected"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1940, "text": "376 12. TRANSFORMERS\nxn−2\nxn−1\nxn+1\nxn+2\nv\nxn\n(a)\nxn−2\nxn−1\nxn+1\nxn+2\nv\nxn\n(b)\nFigure 12.11 Two-layer neural networks used to learn word embeddings, where (a) shows the continuous\nbag-of-words approach, and (b) shows the skip-grams approach.\nsince related words are more likely to occur with similar context words compared\nto unrelated words. For example, the words ‘city’ and ‘capital’ might occur with\nhigher frequency as context for target words such as ‘Paris’ or ‘London’ and less"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1941, "text": "frequently as context for ‘orange’ or ‘polynomial’. The network can more easily\npredict the probability of the missing words if ‘Paris’ and ‘London’ are mapped to\nnearby embedding vectors.\nIt turns out that the learned embedding space often has an even richer semantic\nstructure than just the proximity of related words, and that this allows for simple\nvector arithmetic. For example, the concept that ‘Paris is to France as Rome is to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1942, "text": "Italy’ can be expressed through operations on the embedding vectors. If we use\nv(word) to denote the embedding vector for ‘word’, then we ﬁnd\nv(Paris) −v(France) +v(Italy) ≃v(Rome): (12.27)\nWord embeddings were originally developed as natural language processing\ntools in their own right. Today, they are more likely to be used as pre-processing\nsteps for deep neural networks. In this regard they can be viewed as the ﬁrst layer"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1943, "text": "in a deep neural network. They can be ﬁxed using some standard pre-trained em-\nbedding matrix, or they can be treated as an adaptive layer that is learned as part of\nthe overall end-to-end training of the system. In the latter case the embedding layer\ncan be initialized either using random weight values or using a standard embedding\nmatrix.\n12.2. Natural Language 377\nFigure 12.12 An illustration of the process of\ntokenizing natural language by analogy with byte"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1944, "text": "tokenizing natural language by analogy with byte\npair encoding. In this example, the most fre-\nquently occurring pair of characters is ‘pe’, which\noccurs four times, and so these form a new to-\nken that replaces all the occurrences of ‘pe’.\nNote that ‘Pe’ is not included in this since upper-\ncase ‘P’ and lower-case ‘p’ are distinct charac-\nters. Next the pair ‘ck’ is added since this occurs\nthree times. This is followed by tokens such as\n‘pi’, ‘ed’, and ‘per’, all of which occur twice, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1945, "text": "so on.\nPeter Piper picked a peck of pickled peppers\nPeter Piper picked a peck of pickled peppers\nPeter Piper picked a peck of pickled peppers\nPeter Piper picked a peck of pickled peppers\nPeter Piper picked a peck of pickled peppers\nPeter Piper picked a peck of pickled peppers\n12.2.2 Tokenization\nOne problem with using a ﬁxed dictionary of words is that it cannot cope with\nwords not in the dictionary or which are misspelled. It also does not take account"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1946, "text": "of punctuation symbols or other character sequences such as computer code. An\nalternative approach that addresses these problems would be to work at the level of\ncharacters instead of using words, so that our dictionary comprises upper-case and\nlower-case letters, numbers, punctuation, and white-space symbols such as spaces\nand tabs. A disadvantage of this approach, however, is that it discards the semanti-\ncally important word structure of language, and the subsequent neural network would"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1947, "text": "have to learn to reassemble words from elementary characters. It would also require\na much larger number of sequential steps for a given body of text, thereby increasing\nthe computational cost of processing the sequence.\nWe can combine the beneﬁts of character-level and word-level representations\nby using a pre-processing step that converts a string of words and punctuation sym-\nbols into a string oftokens, which are generally small groups of characters and might"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1948, "text": "include common words in their entirety, along with fragments of longer words as\nwell as individual characters that can be assembled into less common words (Schus-\nter and Nakajima, 2012). This tokenization also allows the system to process other\nkinds of sequences such as computer code or even other modalities such as images.Section 12.4.1\nIt also means that variations of the same word can have related representations. For"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1949, "text": "example, ‘cook’, ‘cooks’, ‘cooked’, ‘cooking’, and ‘cooker’ are all related and share\nthe common element ‘cook’, which itself could be represented as one of the tokens.\nThere are many approaches to tokenization. As an example, a technique called\nbyte pair encoding that is used for data compression, can be adapted to text tokeniza-\ntion by merging characters instead of bytes (Sennrich, Haddow, and Birch, 2015).\nThe process starts with the individual characters and iteratively merges them into"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1950, "text": "longer strings. The list of tokens is ﬁrst initialized with the list of individual char-\nacters. Then a body of text is searched for the most frequently occurring adjacent\npairs of tokens and these are replaced with a new token. To ensure that words are not\nmerged, a new token is not formed from two tokens if the second token starts with a\nwhite space. The process is repeated iteratively as illustrated in Figure 12.12."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1951, "text": "Initially the number of tokens is equal to the number of characters, which is\nrelatively small. As tokens are formed, the total number of tokens increases, and\n378 12. TRANSFORMERS\nif this is continued long enough, the tokens will eventually correspond to the set of\nwords in the text. The total number of tokens is generally ﬁxed in advance, as a\ncompromise between character-level and word-level representations. The algorithm\nis stopped when this number of tokens is reached."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1952, "text": "is stopped when this number of tokens is reached.\nIn practical applications of deep learning to natural language, the input text is\ntypically ﬁrst mapped into a tokenized representation. However, for the remainder of\nthis chapter, we will use word-level representations as this makes it easier to illustrate\nand motivate key concepts.\n12.2.3 Bag of words\nWe now turn to the task of modelling the joint distribution p(x1;:::; xN) of an"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1953, "text": "ordered sequence of vectors, such as words (or tokens) in a natural language. The\nsimplest approach is to assume that the words are drawn independently from the\nsame distribution and hence that the joint distribution is fully factorized in the form\np(x1;:::; xN) =\nN∏\nn=1\np(xn): (12.28)\nThis can be expressed as a probabilistic graphical model in which the nodes are\nisolated with no interconnecting links.Figure 11.28"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1954, "text": "The distribution p(x) is shared across the variables and can be represented, with-\nout loss of generality, as a simple table listing the probabilities of each of the possi-\nble states of x (corresponding to the dictionary of words or tokens). The maximum\nlikelihood solution for this model is obtained simply by setting each of these proba-\nbilities to the fraction of times that the word occurs in the training set. This is knownExercise 12.11"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1955, "text": "as a bag-of-words model because it completely ignores the ordering of the words.\nWe can use the bag-of-words approach to construct a simple text classiﬁer. This\ncould be used for example in sentiment analysis in which a passage of text represent-\ning a restaurant review is to be classiﬁed as positive or negative. The naive Bayes\nclassiﬁer assumes that the words are independent within each class Ck, but with a\ndifferent distribution for each class, so that\np(x1;:::; xN|Ck) =\nN∏\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1956, "text": "p(x1;:::; xN|Ck) =\nN∏\nn=1\np(xn|Ck): (12.29)\nGiven prior class probabilities p(Ck), the posterior class probabilities for a new se-\nquence are given by:\np(Ck|x1;:::; xN) ∝p(Ck)\nN∏\nn=1\np(xn|Ck): (12.30)\nBoth the class-conditional densities p(x|Ck) and the prior probabilities p(Ck) can\nbe estimated using frequencies from the training data set. For a new sequence, the\ntable entries are multiplied together to get the desired posterior probabilities. Note"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1957, "text": "that if a word occurs in the test set that was not present in the training set then the\n12.2. Natural Language 379\ncorresponding probability estimate will be zero, and so these estimates are typically\n‘smoothed’ after training by reassigning a small level of probability uniformly across\nall entries to avoid zero values.\n12.2.4 Autoregressive models\nOne obvious major limitation of the bag-of-words model is that it completely"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1958, "text": "ignores word order. To address this we can take an autoregressive approach. Without\nloss of generality we can decompose the distribution over the sequence of words into\na product of conditional distributions in the form\np(x1;:::; xN) =\nN∏\nn=1\np(xn|x1;:::; xn−1 ): (12.31)\nThis can be represented as a probabilistic graphical model in which each node in the\nsequence receives a link from every previous node. We could represent each termFigure 11.27"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1959, "text": "on the right-hand side of (12.31) by a table whose entries are once again estimated\nusing simple frequency counts from the training set. However, the size of these\ntables grows exponentially with the length of the sequence, and so this approachExercise 12.12\nwould become prohibitively expensive.\nWe can simplify the model dramatically by assuming that each of the condi-\ntional distributions on the right-hand side of (12.31) is independent of all previous"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1960, "text": "observations except the Lmost recent words. For example, if L = 2 then the joint\ndistribution for a sequence of N observations under this model is given by\np(x1;:::; xN) = p(x1)p(x2|x1)\nN∏\nn=3\np(xn|xn−1 ;xn−2 ): (12.32)\nIn the corresponding graphical model each node has links from the two previous\nnodes. Here we assume that the conditional distributions p(xn|xn−1 ) are sharedFigure 11.30\nacross all variables. Again each of the distributions on the right-hand side of (12.32)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1961, "text": "can be represented as tables whose values are estimated from the statistics of triplets\nof successive words drawn from a training corpus.\nThe case with L = 1 is known as a bi-gram model because it depends on pairs\nof adjacent words. Similarly L = 2 , which involves triplets of adjacent words, is\ncalled a tri-gram model, and in general these are called n-gram models.\nAll the models discussed so far in this section can be rungeneratively to synthe-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1962, "text": "size novel text. For example, if we provide the ﬁrst and second words in a sequence,\nthen we can sample from the tri-gram statistics p(xn|xn−1 ;xn−2 ) to generate the\nthird word, and then we can use the second and third words to sample the fourth\nword, and so on. The resulting text, however, will be incoherent because each word\nis predicted only on the basis of the two previous words. High-quality text models\nmust take account of the long-range dependencies in language. On the other hand,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1963, "text": "we cannot simply increase the value of Lbecause the size of the probability tables\ngrows exponentially in L so that it is prohibitively expensive to go much beyond\ntri-gram models. However, the autoregressive representation will play a central role\n380 12. TRANSFORMERS\nFigure 12.13 A general RNN with parame-\nters w. It takes a sequence x1;:::; xN as input\nand generates a sequence y1;:::; yN as out-\nput. Each of the boxes corresponds to a multi-\nlayer network with nonlinear hidden units."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1964, "text": "layer network with nonlinear hidden units.\nz0 w w w\ny1 y2 y3\nx1 x2 x3\nz1 z2\nwhen we consider modern language models based not on probability tables but on\ndeep neural networks conﬁgured as transformers.\nOne way to allow longer-range dependencies, while avoiding the exponential\ngrowth in the number of parameters of an n-gram model, is to use a hidden Markov\nmodel whose graphical structure is shown in Figure 11.31. The number of learn-Section 11.3.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1965, "text": "able parameters is governed by the dimensionality of the latent variables whereas\nthe distribution over a given observation xn depends, in principle, on all previous\nobservations. However, the inﬂuence of more distant observations is still very lim-\nited since their effect must be carried through the chain of latent states which are\nthemselves being updated by more recent observations.\n12.2.5 Recurrent neural networks\nTechniques such as n-grams have very poor scaling with sequence length be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1966, "text": "cause they store completely general tables of conditional distributions. We can\nachieve much better scaling by using parameterized models based on neural net-\nworks. Suppose we simply apply a standard feed-forward neural network to se-\nquences of words in natural language. One problem that arises is that the network\nhas a ﬁxed number of inputs and outputs, whereas we need to be able to handle se-\nquences in the training and test sets that have variable length. Furthermore, if a word,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1967, "text": "or group of words, at a particular location in a sequence represents some concept then\nthe same word, or group of words, at a different location is likely to represent the\nsame concept at that new location. This is reminiscent of the equivariance property\nwe encountered in processing image data. If we can construct a network architectureChapter 10\nthat is able to share parameters across the sequence then not only can we capture this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1968, "text": "equivariance property but we can greatly reduce the number of free parameters in\nthe model as well as handle sequences having different lengths.\nTo address this we can borrow inspiration from the hidden Markov model and\nintroduce an explicit hidden variable zn associated with each step nin the sequence.\nThe neural network takes as input both the current word xn and the current hidden\nstate zn−1 and produces an output word yn as well as the next statezn of the hidden"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1969, "text": "variable. We can then chain together copies of this network, in which the weight\nvalues are shared across the copies. The resulting architecture is called a recurrent\nneural network (RNN) and is illustrated in Figure 12.13. Here the initial value of\nthe hidden state may be initialized for example to some default value such as z0 =\n12.2. Natural Language 381\nz0 w w w w w w w\nIk ben gelukkig ⟨stop⟩\nI am happy ⟨start⟩ Ik ben gelukigg\nz∗\nencoder decoder"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1970, "text": "z∗\nencoder decoder\nFigure 12.14 An example of a recurrent neural network used for language translation. See the text for details.\n(0;0;:::; 0)T.\nAs an example of how an RNN might be used in practice, consider the spe-\nciﬁc task of translating sentences from English into Dutch. The sentences can have\nvariable length, and each output sentence might have a different length from the cor-\nresponding input sentence. Furthermore, the network may need to see the whole of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1971, "text": "the input sentence before it can even start to generate the output sentence. We can\naddress this using an RNN by feeding in the complete English sentence followed by\na special input token, which we denote by⟨start⟩, to trigger the start of translation.\nDuring training the network learns to associate ⟨start⟩with the beginning of the\noutput sentence. We also take each successively generated word and feed it into the\ninput at the next time step, as shown in Figure 12.14. The network can be trained"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1972, "text": "to generate a speciﬁc ⟨stop⟩token to signify the completion of the translation. The\nﬁrst few stages of the network are used to absorb the input sequence, and the associ-\nated output vectors are simply ignored. This part of the network can be viewed as an\n‘encoder’ in which the entire input sentence has been compressed into the statez? of\nthe hidden variable. The remaining network stages function as the ‘decoder’, which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1973, "text": "generates the translated sentence as output one word at a time. Notice that each out-\nput word is fed as input to the next stage of the network, and so this approach has an\nautoregressive structure analogous to (12.31).\n12.2.6 Backpropagation through time\nRNNs can be trained by stochastic gradient descent using gradients calculated\nby backpropagation and evaluated through automatic differentiation, just as with reg-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1974, "text": "ular neural networks. The error function consists of a sum over all output units of\nthe error for each unit, in which each output unit has a softmax activation function\nalong with an associated cross-entropy error function. During forward propagation,Section 5.4.4\nthe activation values are propagated all the way from the ﬁrst input in the sequence\nthrough to all the output nodes in the sequence, and error signals are then backprop-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1975, "text": "agated along the same paths. This process is called backpropagation through time\n382 12. TRANSFORMERS\nand in principle is straightforward. However, in practice, for very long sequences,\ntraining can be difﬁcult due to the problems of vanishing gradients or exploding\ngradients that arise with very deep network architectures.Section 7.4.2\nAnother problem with standard RNNs is that they deal poorly with long-range\ndependencies. This is especially problematic for natural language where such depen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1976, "text": "dencies are widespread. In a long passage of text, a concept might be introduced that\nplays an important role in predicting words occurring much later in the text. In the\narchitecture shown in Figure 12.14, the entire concept of the English sentence must\nbe captured in the single hidden vector z? of ﬁxed length, and this becomes increas-\ningly problematic with longer sequences. This is known as the bottleneck problem"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1977, "text": "because a sequence of arbitrary length has to be summarized in a single hidden vec-\ntor of activations and the network can start to generate the output translation only\nonce the full input sequence has been processed.\nOne approach for addressing both the vanishing and exploding gradients prob-\nlems and the limited long-range dependencies is to modify the architecture of the\nneural network to allow additional signal paths that bypass many of the processing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1978, "text": "steps within each stage of the network and hence allow information to be remem-\nbered over a larger number of time steps. Long short-term memory (LSTM) models\n(Hochreiter and Schmidhuber, 1997) andgated recurrent unit(GRU) models (Choet\nal., 2014) are the most widely known examples. Although they improve performance\ncompared to standard RNNs, they still have a limited ability to model long-range de-\npendencies. Also, the additional complexity of each cell means that LSTMs are even"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1979, "text": "slower to train than standard RNNs. Furthermore, all recurrent networks have signal\npaths that grow linearly with the number of steps in the sequence. Moreover, they do\nnot support parallel computation within a single training example due to the sequen-\ntial nature of the processing. In particular, this means that RNNs struggle to make\nefﬁcient use of modern highly parallel hardware based on GPUs. These problems\nare addressed by replacing RNNs with transformers.\n12.3."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1980, "text": "12.3.\nTransformer Language Models\nThe transformer processing layer is a highly ﬂexible component for building pow-\nerful neural network models with broad applicability. In this section we explore the\napplication of transformers to natural language. This has given rise to the develop-\nment of massive neural networks known as large language models (LLMs), which\nhave proven to be exceptionally capable (Zhao et al., 2023).\nTransformers can be applied to many different kinds of language processing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1981, "text": "task, and can be grouped into three categories according to the form of the input\nand output data. In a problem such as sentiment analysis, we take a sequence of\nwords as input and provide a single variable representing the sentiment of the text,\nfor example happy or sad, as output. Here a transformer is acting as an ‘encoder’\nof the sequence. Other problems might take a single vector as input and generate\na word sequence as output, for example if we wish to generate a text caption given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1982, "text": "an input image. In such cases the transformer functions as a ‘decoder’, generating\n12.3. Transformer Language Models 383\na sequence as output. Finally, in sequence-to-sequence processing tasks, both the\ninput and the output comprise a sequence of words, for example if our goal is to\ntranslate from one language to another. In this case, transformers are used in both\nencoder and decoder roles. We discuss each of these classes of language model in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1983, "text": "turn, using illustrative examples of model architectures.\n12.3.1 Decoder transformers\nWe start by considering decoder-only transformer models. These can be used as\ngenerative models that create output sequences of tokens. As an illustrative example,\nwe will focus on a class of models called GPT which stands for generative pre-\ntrained transformer (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023). The\ngoal is to use the transformer architecture to construct an autoregressive model of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1984, "text": "form deﬁned by (12.31) in which the conditional distributions p(xn|x1;:::; xn−1 )\nare expressed using a transformer neural network that is learned from data.\nThe model takes as input a sequence consisting of the ﬁrst n−1 tokens, and\nits corresponding output represents the conditional distribution for token n. If we\ndraw a sample from this distribution then we have extended the sequence tontokens\nand this new sequence can be fed back through the model to give a distribution over"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1985, "text": "token n+ 1, and so on. The process can be repeated to generate sequences up to\na maximum length determined by the number of inputs to the transformer. We will\nshortly discuss strategies for sampling from the conditional distributions, but for theSection 12.3.2\nmoment we focus on how to construct and train the network.\nThe architecture of a GPT model consists of a stack of transformer layers that\ntake a sequence x1;:::; xN of tokens, each of dimensionality D, as input and pro-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1986, "text": "duce a sequence ˜x1;:::; ˜xN of tokens, again of dimensionality D, as output. Each\noutput needs to represent a probability distribution over the dictionary of tokens at\nthat time step, and this dictionary has dimensionality K whereas the tokens have a\ndimensionality of D. We therefore make a linear transformation of each output to-\nken using a matrix W(p) of dimensionality D×Kfollowed by a softmax activation\nfunction in the form\nY = Softmax\n(\n˜XW(p)\n)\n(12.33)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1987, "text": "Y = Softmax\n(\n˜XW(p)\n)\n(12.33)\nwhere Y is a matrix whose nth row is yT\nn, and ˜X is a matrix whose nth row is\n˜xT\nn. Each softmax output unit has an associated cross-entropy error function. TheSection 5.4.4\narchitecture of the model is shown in Figure 12.15.\nThe model can be trained using a large corpus of unlabelled natural language\nby taking a self-supervised approach. Each training sample consists of a sequence"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1988, "text": "of tokens x1;:::; xn, which form the input to the network, along with an associated\ntarget value xn+1 consisting of the next token in the sequence. The sequences are\nconsidered to be independent and identically distributed so that the error function\nused for training is the sum of the cross-entropy error values summed over the train-\ning set, grouped into appropriate mini-batches. Naively we could process each such\ntraining sample independently using a forward pass through the model. However,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1989, "text": "we can achieve much greater efﬁciency by processing an entire sequence at once so\nthat each token acts both as a target value for the sequence of previous tokens and as\n384 12. TRANSFORMERS\n⟨start⟩ x1 … … xN\nembedding embedding embedding … …\n+ + +\npositional\nencoding\nmasked transformer layer\n … … … … …\nmasked transformer layer\nLSM LSM LSM\ny1 y2 yN+1 … …\n … …\nLlayers\nFigure 12.15 Architecture of a GPT decoder transformer network. Here ‘LSM’ stands for linear-softmax and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1990, "text": "denotes a linear transformation whose learnable parameters are shared across the token positions, followed by\na softmax activation function. Masking is explained in the text.\nan input value for subsequent tokens. For example, consider the word sequence\nI swam across the river to get to the other bank.\nWe can use ‘I swam across’ as an input sequence with an associated target of ‘the’,\nand also use ‘I swam across the’ as an input sequence with an associated target of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1991, "text": "‘river’, and so on. However, to process these in parallel we have to ensure that the\nnetwork is not able to ‘cheat’ by looking ahead in the sequence, otherwise it will\nsimply learn to copy the next input directly to the output. If it did this, it would\nthen be unable to generate new sequences since the subsequent token by deﬁnition is\nnot available at test time. To address this problem we do two things. First, we shift"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1992, "text": "the input sequence to the right by one step, so that input xn corresponds to output\nyn+1, with target xn+1, and an additional special token denoted ⟨start⟩is pre-\npended in the ﬁrst position of the input sequence. Second, note that the tokens in a\ntransformer are processed independently, except when they are used to compute the\nattention weights, when they interact in pairs through the dot product. We therefore\nintroduce masked attention, sometimes called causal attention, into each of the at-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1993, "text": "12.3. Transformer Language Models 385\nFigure 12.16 An illustration of the mask matrix\nfor masked self-attention. Atten-\ntion weights corresponding to the\nred elements are set to zero. Thus,\nin predicting the token ‘across’,\nthe output can depend only on\nthe input tokens ‘⟨start⟩’ ‘I’ and\n‘swam’.\nI\nswam\nacross\nthe\nriver\n⟨start⟩\nI\nswam\nacross\nthe\ninputs\noutputs\ntention layers, in which we set to zero all of the attention weights that correspond to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1994, "text": "a token attending to any later token in the sequence. This simply involves setting to\nzero all the corresponding elements of the attention matrix Attention(Q;K;V) de-\nﬁned by (12.14) and then normalizing the remaining elements so that each row once\nagain sums to one. In practice, this can be achieved by setting the corresponding\npre-activation values to −∞ so that the softmax evaluates to zero for the associated\noutputs and also takes care of the normalization across the non-zero outputs. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1995, "text": "structure of the masked attention matrix is illustrated in Figure 12.16.\nIn practice, we wish to make efﬁcient use of the massive parallelism of GPUs,\nand hence multiple sequences may be stacked together into an input tensor for par-\nallel processing in a single batch. However, this requires the sequences to be of the\nsame length, whereas text sequences naturally have variable length. This can be ad-\ndressed by introducing a speciﬁc token, which we denote by ⟨pad⟩, that is used to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1996, "text": "ﬁll unused positions to bring all sequences up to the same length so that they can\nbe combined into a single tensor. An additional mask is then used in the attention\nweights to ensure that the output vectors do not pay attention to any inputs occupied\nby the ⟨pad⟩token. Note that the form of this mask depends on the particular input\nsequence.\nThe output of the trained model is a probability distribution over the space of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1997, "text": "tokens, given by the softmax output activation function, which represents the prob-\nability of the next token given the current token sequence. Once this next word is\nchosen, the token sequence with the new token included can then be fed through the\nmodel again to generate the subsequent token in the sequence, and this process can\nbe repeated indeﬁnitely or until an end-of-sequence token is generated. This may ap-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1998, "text": "pear to be quite inefﬁcient since data must be fed through the whole model for each\nnew generated token. However, note that due to the masked attention, the embedding\nlearned for a particular token depends only on that token itself and on earlier tokens\n386 12. TRANSFORMERS\nand hence does not change when a new, later token is generated. Consequently, much\nof the computation can be recycled when processing a new token.\n12.3.2 Sampling strategies"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 1999, "text": "12.3.2 Sampling strategies\nWe have seen that the output of a decoder transformer is a probability distribu-\ntion over values for the next token in the sequence, from which a particular value\nfor that token must be chosen to extend the sequence. There are several options for\nselecting the value of the token based on the computed probabilities (Holtzman et\nal., 2019). One obvious approach, called greedy search, is simply to select the token"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2000, "text": "with the highest probability. This has the effect of making the model deterministic,\nin that a given input sequence always generates the same output sequence. Note that\nsimply choosing the highest probability token at each stage is not the same as select-\ning the highest probability sequence of tokens. To ﬁnd the most probable sequence,Exercise 12.15\nwe would need to maximize the joint distribution over all tokens, which is given by\np(y1;:::; yN) =\nN∏\nn=1\np(yn|y1;:::; yn−1 ): (12.34)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2001, "text": "N∏\nn=1\np(yn|y1;:::; yn−1 ): (12.34)\nIf there are N steps in the sequence and the number of token values in the dictionary\nis Kthen the total number of sequences is O(KN), which grows exponentially with\nthe length of the sequence, and hence ﬁnding the single most probable sequence is\ninfeasible. By comparison, greedy search has cost O(KN), which is linear in the\nsequence length.\nOne technique that has the potential to generate higher probability sequences"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2002, "text": "than greedy search is calledbeam search. Instead of choosing the single most proba-\nble token value at each step, we maintain a set ofBhypotheses, where Bis called the\nbeam width, each consisting of a sequence of token values up to stepn. We then feed\nall these sequences through the network, and for each sequence we ﬁnd the Bmost\nprobable token values, thereby creating B2 possible hypotheses for the extended\nsequence. This list is then pruned by selecting the most probable B hypotheses ac-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2003, "text": "cording to the total probability of the extended sequence. Thus, the beam search\nalgorithm maintains B alternative sequences and keeps track of their probabilities,\nﬁnally selecting the most probable sequence amongst those considered. Because the\nprobability of a sequence is obtained by multiplying the probabilities at each step of\nthe sequence and since these probability are always less than or equal to one, a long"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2004, "text": "sequence will generally have a lower probability than a short one, biasing the results\ntowards short sequences. For this reason the sequence probabilities are generally\nnormalized by the corresponding lengths of the sequence before making compar-\nisons. Beam search has cost O(BKN), which is again linear in the sequence length.\nHowever, the cost of generating a sequence is increased by a factor of B, and so for\nvery large language models, where the cost of inference can become signiﬁcant, this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2005, "text": "makes beam search much less attractive.\nOne problem with approaches such as greedy search and beam search is that they\nlimit the diversity of potential outputs and can even cause the generation process to\nbecome stuck in a loop, where the same sub-sequence of words is repeated over and\n12.3. Transformer Language Models 387\nFigure 12.17 A comparison of the token probabilities from beam search and human text for a given trained"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2006, "text": "transformer language model and a given initial input sequence, showing how the human sequence has much\nlower token probabilities. [From Holtzman et al. (2019) with permission.]\nover. As can be seen in Figure 12.17, human-generated text may have lower proba-\nbility and hence be more surprising with respect to a given model than automatically\ngenerated text.\nInstead of trying to ﬁnd a sequence with the highest probability, we can instead"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2007, "text": "generate successive tokens simply by sampling from the softmax distribution at each\nstep. However, this can lead to sequences that are nonsensical. This arises from\nthe typically very large size of the token dictionary, in which there is a long tail of\nmany token states each of which has a very small probability but which in aggregate\naccount for a signiﬁcant fraction of the total probability mass. This leads to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2008, "text": "problem in which there is a signiﬁcant chance that the system will make a bad choice\nfor the next token.\nAs a balance between these extremes, we can consider only the states having the\ntop Kprobabilities, for some choice of K, and then sample from these according to\ntheir renormalized probabilities. A variant of this approach, called top-p sampling\nor nucleus sampling, calculates the cumulative probability of the top outputs until a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2009, "text": "threshold is reached and then samples from this restricted set of token states.\nA ‘softer’ version of top-Ksampling is to introduce a parameter T called tem-\nperature into the deﬁnition of the softmax function (Hinton, Vinyals, and Dean,\n2015) so that\nyi = exp(ai=T)\n∑\njexp(aj=T) (12.35)\nand then sample the next token from this modiﬁed distribution. When T = 0, the\nprobability mass is concentrated on the most probable state, with all other states"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2010, "text": "having zero probability, and hence this becomes greedy selection. For T = 1, we\n388 12. TRANSFORMERS\nrecover the unmodiﬁed softmax distribution, and as T →∞, the distribution be-\ncomes uniform across all states. By choosing a value in the range 0 < T <1, the\nprobability is concentrated towards the higher values.\nOne challenge with sequence generation is that during the learning phase, the\nmodel is trained on a human-generated input sequence, whereas when it is running"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2011, "text": "generatively, the input sequence is itself generated from the model. This means that\nthe model can drift away from the distribution of sequences seen during training.\n12.3.3 Encoder transformers\nWe next consider transformer language models based on encoders, which are\nmodels that take sequences as input and produce ﬁxed-length vectors, such as class\nlabels, as output. An example of such a model is BERT, which stands for bidirec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2012, "text": "tional encoder representations from transformers (Devlin et al., 2018). The goal is\nto pre-train a language model using a large corpus of text and then to ﬁne-tune the\nmodel using transfer learning for a broad range of downstream tasks each of which\nrequires a smaller application-speciﬁc training data set. The architecture of an en-\ncoder transformer is illustrated in Figure 12.18. This approach is a straightforward\napplication of the transformer layers discussed previously.Section 12.1.7"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2013, "text": "The ﬁrst token of every input string is given by a special token⟨class⟩, and the\ncorresponding output of the model is ignored during pre-training. Its role will be-\ncome apparent when we discuss ﬁne-tuning. The model is pre-trained by presenting\ntoken sequences at the input. A randomly chosen subset of the tokens, say 15%, are\nreplaced with a special token denoted ⟨mask⟩. The model is trained to predict the\nmissing tokens at the corresponding output nodes. This is analogous to the masking"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2014, "text": "used in word2vec to learn word embeddings. For example, an input sequence mightSection 12.2.1\nbe\nI ⟨mask⟩across the river to get to the ⟨mask⟩bank.\nand the network should predict ‘swam’ at output node 2 and ‘other’ at output node\n10. In this case only two of the outputs contribute to the error function and the other\noutputs are ignored.\nThe term ‘bidirectional’ refers to the fact that the network sees words both be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2015, "text": "fore and after the masked word and can use both sources of information to make a\nprediction. As a consequence, unlike decoder models, there is no need to shift the\ninputs to the right by one place, and there is no need to mask the outputs of each layer\nfrom seeing input tokens occurring later in the sequence. Compared to the decoder\nmodel, an encoder is less efﬁcient since only a fraction of the sequence tokens are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2016, "text": "used as training labels. Moreover, an encoder model is unable to generate sequences.\nThe procedure of replacing randomly selected tokens with ⟨mask⟩means the\ntraining set has a mismatch compared to subsequent ﬁne-tuning sets in that the lat-\nter will not contain any ⟨mask⟩tokens. To mitigate any problems this might cause,\nDevlin et al. (2018) modiﬁed the procedure slightly, so that of the 15% of randomly\nselected tokens, 80% are replaced with ⟨mask⟩, 10% are replaced with a word se-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2017, "text": "lected at random from the vocabulary, and in 10% of the cases, the original words\nare retained at the input, but they still have to be correctly predicted at the output.\n12.3. Transformer Language Models 389\n⟨class⟩ x1 … … xN\nembedding embedding embedding … …\n+ + +positional\nencoding\ntransformer layer\n … … … … …\ntransformer layer\nLSM LSM LSM\nc y1 yN … …\n … …\nLlayers\nFigure 12.18 Architecture of an encoder transformer model. The boxes labelled ‘LSM’ denote a linear trans-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2018, "text": "formation whose learnable parameters are shared across the token positions, followed by a softmax activation\nfunction. The main differences compared to the decoder model are that the input sequence is not shifted to the\nright, and the ‘look ahead’ masking matrix is omitted and therefore, within each self-attention layer, every output\ntoken can attend to any of the input tokens.\nOnce the encoder model is trained it can then be ﬁne-tuned for a variety of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2019, "text": "different tasks. To do this a new output layer is constructed whose form is speciﬁc\nto the task being solved. For a text classiﬁcation task, only the ﬁrst output position\nis used, which corresponds to the ⟨class⟩token that always appears in the ﬁrst\nposition of the input sequence. If this output has dimension D then a matrix of\nparameters of dimension D×K, where K is the number of classes, is appended to\nthe ﬁrst output node and this in turn feeds into a K-dimensional softmax function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2020, "text": "or a vector of dimension D ×1 followed by a logistic sigmoid for K = 2. The\nlinear output transformation could alternatively be replaced with a more complex\ndifferentiable model such as an MLP. If the goal is to classify each token of the\ninput string, for example to assign each token to a category (such as person, place,\ncolour, etc) then the ﬁrst output is ignored and the subsequent outputs have a shared\nlinear-plus-softmax layer. During ﬁne-tuning all model parameters including the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2021, "text": "new output matrix are learned by stochastic gradient descent using the log probability\n390 12. TRANSFORMERS\nof the correct label. Alternatively the output of a pre-trained model might feed into a\nsophisticated generative deep learning model for applications such as text-to-imageChapter 20\nsynthesis.\n12.3.4 Sequence-to-sequence transformers\nFor completeness, we discuss brieﬂy the third category of transformer model,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2022, "text": "which combines an encoder with a decoder, as discussed in the original transformer\npaper of Vaswani et al. (2017). Consider the task of translating an English sentence\ninto a Dutch sentence. We can use a decoder model to generate the token sequence\ncorresponding to the Dutch output, token by token, as discussed previously. TheSection 12.3.1\nmain difference is that this output needs to be conditioned on the entire input se-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2023, "text": "quence corresponding to the English sentence. An encoder transformer can be used\nto map the input token sequence into a suitable internal representation, which we\ndenote by Z. To incorporate Z into the generative process for the output sequence,\nwe use a modiﬁed form of the attention mechanism calledcross attention. This is the\nsame as self-attention except that although the query vectors come from the sequence\nbeing generated, in this case the Dutch output sequence, the key and value vectors"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2024, "text": "come from the sequence represented by Z, as illustrated in Figure 12.19. Returning\nto our analogy with a video streaming service, this would be like the user sending\ntheir query vector to a different streaming company who then compares it with their\nown set of key vectors to ﬁnd the best match and then returns the associated value\nvector in the form of a movie.\nWhen we combine the encoder and decoder modules, we obtain the architecture"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2025, "text": "of the model shown in Figure 12.20. The model can be trained using paired input\nand output sentences.\n12.3.5 Large language models\nThe most important recent development in the ﬁeld of machine learning has\nbeen the creation of very large transformer-based neural networks for natural lan-\nguage processing, known as large language models or LLMs. Here ‘large’ refers to\nthe number of weight and bias parameters in the network, which can number up to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2026, "text": "around one trillion (1012) at the time of writing. Such models are expensive to train,\nand the motivation for building them comes from their extraordinary capabilities.\nIn addition to the availability of large data sets, the training of ever larger mod-\nels has been facilitated by the advent of massively parallel training hardware based\non GPUs (graphics processing units) and similar processors tightly coupled in large"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2027, "text": "clusters equipped fast interconnect and lots of onboard memory. The transformer\narchitecture has played a key role in the development of these models because it is\nable to make very efﬁcient use of such hardware. Very often, increasing the size of\nthe training data set, along with a commensurate increase in the number of model pa-\nrameters, leads to improvements in performance that outpace architectural improve-\nments or other ways to incorporate more domain knowledge (Sutton, 2019; Kaplan"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2028, "text": "et al., 2020). For example, the impressive increase in performance of the GPT se-\nries of models (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) through\nsuccessive generations has come primarily from an increase in scale. These kinds\n12.3. Transformer Language Models 391\nFigure 12.19 Schematic illustration of one cross-\nattention layer as used in the decoder\nsection of a sequence-to-sequence trans-\nformer. Here Z denotes the output from\nthe encoder section. Z determines the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2029, "text": "the encoder section. Z determines the\nkey and value vectors for the cross-\nattention layer, whereas the query vec-\ntors are determined within the decoder\nsection.\nX\nmasked\nmulti-head\nself-attention\nadd & norm\nmulti-head\ncross-attention\nadd & norm\nMLP\nadd & norm\neX\nZ\nK V Q\nof performance improvements have driven a new kind of Moore’s law in which the\nnumber of compute operations required to train a state-of-the-art machine learning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2030, "text": "model has grown exponentially since about 2012 with a doubling time of around 3.4\nmonths.Figure 1.16\nEarly language models were trained using supervised learning. For example, to\nbuild a translation system, the training set would consist of matched pairs of sen-\ntences in two languages. A major limitation of supervised learning, however, is\nthat the data typically has to be human-curated to provide labelled examples, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2031, "text": "this severely limits the quantity of data available, thereby requiring heavy use of\ninductive biases such as feature engineering and architecture constraints to achieve\nreasonable performance.\nLarge language models are trained instead by self-supervised learning on very\nlarge data sets of text, along with potentially other token sequences such as computer\ncode. We have seen how a decoder transformer can be trained on token sequencesSection 12.3.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2032, "text": "in which each token acts as a labelled target example, with the preceding sequence\nas input, to learn a conditional probability distribution. This ‘self-labelling’ hugely\nexpands the quantity of training data available and therefore allows exploitation of\ndeep neural networks having large numbers of parameters.\n392 12. TRANSFORMERS\nX\nembedding\n+\nself-attention transformer layer\n …\nself-attention transformer layer\ncross-attention transformer layer\n …\ncross-attention transformer layer\nLSM\nYN\n+"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2033, "text": "…\ncross-attention transformer layer\nLSM\nYN\n+\nembedding\n{⟨start⟩;Y1:N−1}\nZ\npositional\nencoding\nencoder decoder\nFigure 12.20 Schematic illustration of a sequence-to-sequence transformer. To keep the diagram uncluttered\nthe input tokens are collectively shown as a single box, and likewise for the output tokens. Positional-encoding\nvectors are added to the input tokens for both the encoder and decoder sections. Each layer in the encoder"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2034, "text": "corresponds to the structure shown in Figure 12.9, and each cross-attention layer is of the form shown in Fig-\nure 12.19.\nThis use of self-supervised learning led to a paradigm shift in which a large\nmodel is ﬁrst pre-trained using unlabelled data and then subsequently ﬁne-tuned\nusing supervised learning based on a much smaller set of labelled data. This is\neffectively a form of transfer learning, and the same pre-trained model can be used"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2035, "text": "for multiple ‘downstream’ applications. A model with broad capabilities that can be\nsubsequently ﬁne-tuned for speciﬁc tasks is called a foundation model (Bommasani\net al., 2021).\nThe ﬁne-tuning can be done by adding extra layers to the outputs of the network\nor by replacing the last few layers with fresh parameters and then using the labelled\ndata to train these ﬁnal layers. During the ﬁne-tuning stage, the weights and biases"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2036, "text": "in the main model can either be left unchanged or be allowed to undergo small levels\nof adaptation. Typically the cost of the ﬁne-tuning is small compared to that of pre-\ntraining.\nOne very efﬁcient approach to ﬁne-tuning is calledlow-rank adaptationor LoRA\n(Hu et al., 2021). This approach is inspired by results which show that a trained over-\nparameterized model has a low intrinsic dimensionality with respect to ﬁne-tuning,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2037, "text": "meaning that changes in the model parameters during ﬁne-tuning lie on a manifold\n12.3. Transformer Language Models 393\n× W0\nD×D\nA\nD×R\n× B\nR×D\n×\n+\nXW0\n+\nXAB\nN ×D\nX\nN ×D\nFigure 12.21 Schematic illustration low-rank adaptation showing a weight matrix W0 from one of the\nattention layers in a pre-trained transformer. Additional weights given by matrices A and\nB are adapted during ﬁne-tuning and their productAB is then added to the original matrix\nfor subsequent inference."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2038, "text": "for subsequent inference.\nwhose dimensionality is much smaller than the total number of learnable parameters\nin the model (Aghajanyan, Zettlemoyer, and Gupta, 2020). LoRa exploits this by\nfreezing the weights of the original model and adding additional learnable weight\nmatrices into each layer of the transformer in the form of low-rank products. Typi-\ncally only attention-layer weights are modiﬁed, whereas MLP-layer weights are kept"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2039, "text": "ﬁxed. Consider a weight matrix W0 having dimension D×D, which might rep-\nresent a query, key, or value matrix in which the matrices from multiple attention\nheads are treated together as a single matrix. We introduce a parallel set of weights\ndeﬁned by the product of two matricesA and B with dimensions D×Rand R×D,\nrespectively, as shown schematically in Figure 12.21. This layer then generates an\noutput given by XW0 + XAB. The number of parameters in the additional weight"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2040, "text": "matrix AB is 2RD compared to the D2 parameters in the original weight matrix\nW0, and so if R≪ Dthen the number of parameters that need to be adapted during\nﬁne-tuning is much smaller than the number in the original transformer. In prac-\ntice, this can reduce the number of parameters that need to be trained by a factor of\n10,000. Once the ﬁne-tuning is complete, the additional weights can be added to the\noriginal weight matrices to give a new weight matrix\nˆW = W0 + AB (12.36)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2041, "text": "ˆW = W0 + AB (12.36)\nso that during inference there is no additional computational overhead compared to\nrunning the original model since the updated model has the same size as the original.\nAs language models have become larger and more powerful, the need for ﬁne-\ntuning has diminished, with generative language models now able to solve a broad\nrange of tasks simply through text-based interaction. For example, if a text string\nEnglish: the cat sat on the mat. French:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2042, "text": "English: the cat sat on the mat. French:\nis given as the input sequence, an autoregressive language model can continue to gen-\nerate subsequent tokens until a ⟨stop⟩token is generated, in which the newly gen-\n394 12. TRANSFORMERS\nerated tokens represent the French translation. Note that the model was not trained\nspeciﬁcally to do translation but has learned to do so as a result of being trained on a\nvast corpus of data that includes multiple languages."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2043, "text": "A user can interact with such models using a natural language dialogue, mak-\ning them very accessible to broad audiences. To improve the user experience and\nthe quality of the generated outputs, techniques have been developed for ﬁne-tuning\nlarge language models through human evaluation of generated output, using methods\nsuch as reinforcement learning through human feedbackor RLHF (Christiano et al.,\n2017). Such techniques have helped to create large language models with impres-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2044, "text": "sively easy-to-use conversational interfaces, most notably the system from OpenAI\ncalled ChatGPT.\nThe sequence of input tokens given by the user is called aprompt. For example,\nit might consist of the opening words of a story, which the model is required to com-\nplete. Or it might comprise a question, and the model should provide the answer. By\nusing different prompts, the same trained neural network may be capable of solving a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2045, "text": "broad range of tasks such as generating computer code from a simple text request or\nwriting rhyming poetry on demand. The performance of the model now depends on\nthe form of the prompt, leading to a new ﬁeld called prompt engineering (Liu et al.,\n2021), which aims to design a good form for a prompt that results in high-quality\noutput for the downstream task. The behaviour of the model can also be modiﬁed by\nadapting the user’s prompt before feeding it into the language model by pre-pending"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2046, "text": "an additional token sequence called a preﬁx prompt to the user prompt to modify\nthe form of the output. For example, the pre-prompt might consist of instructions,\nexpressed in standard English, to tell the network not to include offensive language\nin its output.\nThis allows the model to solve new tasks simply by providing some examples\nwithin the prompt, without needing to adapt the parameters of the model. This is an\nexample of few-shot learning."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2047, "text": "example of few-shot learning.\nCurrent state-of-the-art models such as GPT-4 have become so powerful that\nthey are exhibiting remarkable properties which have been described as the ﬁrst in-\ndications of artiﬁcial general intelligence (Bubeck et al., 2023) and are driving a\nnew wave of technological innovation. Moreover, the capabilities of these models\ncontinue to improve at an impressive pace.\n12.4.\nMultimodal Transformers"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2048, "text": "12.4.\nMultimodal Transformers\nAlthough transformers were initially developed as an alternative to recurrent net-\nworks for processing sequential language data, they have become prevalent in nearly\nall areas of deep learning. They have proved to be general-purpose models, as they\nmake very few assumptions about the input data, in contrast, for example, to convo-\nlutional networks, which make strong assumptions about equivariances and locality.Chapter 10"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2049, "text": "Due to their generality, transformers have become the state-of-the-art for many dif-\nferent modalities, including text, image, video, point cloud, and audio data, and have\nbeen used for both discriminative and generative applications within each of these\n12.4. Multimodal Transformers 395\ndomains. The core architecture of the transformer layer has remained relatively con-\nstant, both over time and across applications. Therefore, the key innovations that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2050, "text": "enabled the use of transformers in areas other than natural language have largely\nfocused on the representation and encoding of the inputs and outputs.\nOne big advantage of a single architecture that is capable of processing many\ndifferent kinds of data is that it makes multimodal computation relatively straight-\nforward. In this context, multimodal refers to applications that combine two or more\ndifferent types of of data, either in the inputs or outputs or both. For example, we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2051, "text": "may wish to generate an image from a text prompt or design a robot that can com-\nbine information from multiple sensors such as cameras, radar, and microphones.\nThe important thing to note is that if we can tokenize the inputs and decode the\noutput tokens, then it is likely that we can use a transformer.\n12.4.1 Vision transformers\nTransformers have been applied with great success to computer vision and have\nachieved state-of-the-art performance on many tasks. The most common choice for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2052, "text": "discriminative tasks is a standard transformer encoder, and this approach in the vi-\nsion domain is known as a vision transformer, or ViT (Dosovitskiy et al., 2020).\nWhen using a transformer, we need to decide how to convert an input image into\ntokens, and the simplest choice is to use each pixel as a token, following a linear\nprojection. However, the memory required by a standard transformer implementa-\ntion grows quadratically with the number of input tokens, and so this approach is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2053, "text": "generally infeasible. Instead, the most common approach to tokenization is to split\nthe image into a set of patches of the same size. Suppose the images have dimension\nx ∈RH×W×C where H and W are the height and width of the image in pixels\nand C is the number of channels (where typically C = 3 for R, G, and B colours).\nEach image is split into non-overlapping patches of size P ×P (where P = 16 is\na common choice) and then ‘ﬂattened’ into a one-dimensional vector, which gives a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2054, "text": "representation xp ∈RN×(P2C) where N = HW=P2 is the total number of patches\nfor one image. The ViT architecture is shown in Figure 12.22.\nAnother approach to tokenization is to feed the image through a small convolu-\ntional neural network (CNN). This can down-sample the image to give a manageableChapter 10\nnumber of tokens each represented by one of the network outputs. For example a typ-\nical ResNet18 encoder architecture down-samples an image by a factor of 8 in both"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2055, "text": "the height and width dimensions, giving 64 times fewer tokens than pixels.\nWe also need a way to encode positional information in the tokens. It is pos-\nsible to construct explicit positional embeddings that encode the two-dimensional\npositional information of the image patches, but in practice this does not generally\nimprove performance, and so it is most common to just use learned positional em-\nbeddings. In contrast to the transformers used for natural language, vision trans-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2056, "text": "formers generally take a ﬁxed number of tokens as input, which avoids the problem\nof learned positional encodings not generalizing to inputs of a different size.\nA vision transformer has a very different architectural design compared to a\nCNN. Although strong inductive biases are baked into a CNN model, the only two-\ndimensional inductive bias in a vision transformer is due to the patches used to tok-\n396 12. TRANSFORMERS\n⟨class⟩ ﬂatten … … ﬂatten\nembedding embedding embedding … …\n+ + +\nlearned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2057, "text": "embedding embedding embedding … …\n+ + +\nlearned\npositional\nencoding\ntransformer encoder\nLSM\nc\nFigure 12.22 Illustration of the vision transformer architecture for a classiﬁcation task. Here a learnable⟨class⟩\ntoken is included as an additional input, and the associated output is transformed by a linear layer with a softmax\nactivation, denoted by LSM, to give the ﬁnal class-vector output c.\nenize the input. A transformer therefore generally requires more training data than a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2058, "text": "comparable CNN as it has to learn the geometrical properties of images from scratch.\nHowever, because there are no strong assumptions about the structure of the inputs,\ntransformers are often able to converge to a higher accuracy. This provides another\nillustration of the trade-off between inductive bias and the scale of the training data\n(Sutton, 2019).\n12.4.2 Generative image transformers\nIn the language domain, the most impressive results have come when trans-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2059, "text": "formers are used as an autoregressive generative model for synthesizing text. It is\ntherefore natural to ask whether we can also use transformers to synthesize realistic\nimages. Since natural language is inherently sequential, it ﬁts neatly into the au-\ntoregressive framework, whereas images have no natural ordering of their pixels so\nthat it is not as intuitive that decoding them autoregressively would be useful. How-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2060, "text": "ever, any distribution can be decomposed into a product of conditionals, provided weSection 11.1.2\nﬁrst deﬁne some ordering of the variables. Thus, the joint distribution over ordered\n12.4. Multimodal Transformers 397\nFigure 12.23 Illustration of a raster scan that deﬁnes a speciﬁc linear\nordering of the pixels in a two-dimensional image. x1 x2 x3 x4\nx5 x6 x7 x8\nx9 x10 x11 x12\nx13 x14 x15 x16\nvariables x1;:::; xN can be written\np(x1;:::; xN) =\nN∏\nn=1\np(xn|x1;:::; xn−1): (12.37)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2061, "text": "N∏\nn=1\np(xn|x1;:::; xn−1): (12.37)\nThis factorization is completely general and makes no restrictions on the form of the\nindividual conditional distributions p(xn|x1;:::; xn−1).\nFor an image we can choosexn to represent the nth pixel as a three-dimensional\nvector of the RGB values. We now need to decide on an ordering for the pixels, and\none widely used choice is called a raster scan as illustrated in Figure 12.23 . A"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2062, "text": "schematic illustration of an image being generated using an autoregressive model,\nbased on a raster-scan ordering, is shown in Figure 12.24.\nNote that the use of autoregressive generative models of images predates the\nintroduction of transformers. For example, PixelCNN (Oord et al., 2016) and Pixel-\nRNN (Oord, Kalchbrenner, and Kavukcuoglu, 2016) used bespoke masked convolu-\ntion layers that preserve the conditional independence deﬁned for each pixel by the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2063, "text": "corresponding term on the right-hand side of 12.37.\nRepresentations of an image using continuous values can work well in discrim-\ninative tasks. However, much better results are obtained for image generation by\nusing discrete representations. Continuous conditional distributions learned by max-\nimum likelihood, such as Gaussians for which the negative log likelihood function\nis a sum-of-squares error function, tend to learn averages of the training data, lead-Section 4.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2064, "text": "ing to blurry images. Conversely, discrete distributions can handle multimodality\nwith ease. For example, one of the conditional distributions p(xn|x1;:::; xn−1) in\nFigure 12.24 An illustration of how an image can be sampled from an autoregressive model. The ﬁrst pixel is\nsampled from the marginal distribution p(x11), the second pixel from the conditional distribution p(x12|x11), and\nso on in raster scan order until we have a complete image.\n398 12. TRANSFORMERS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2065, "text": "398 12. TRANSFORMERS\n(12.37) might learn that a pixel could be either black or white, whereas a regression\nmodel might learn that the pixel should be grey.\nHowever, working with discrete spaces also brings its challenges. The R, G, and\nB values of image pixels are typically represented with at least 8 bits of precision,\nso that each pixel has 224 ≃16M possible values. Learning a conditional softmax\ndistribution over a such a high-dimensional space is infeasible."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2066, "text": "One way to address the problem of the high dimensionality is to use the tech-\nnique of vector quantization, which can be viewed as a form of data compression.Section 15.1.1\nSuppose we have a set of data vectors x1;:::; xN each of dimensionality D, which\nmight, for example, represent image pixels, and we then introduce a set of K code-\nbook vectors C= c1;:::; cK also of dimensionality D, where typically K ≪ D.\nWe now approximate each data vector by its nearest codebook vector according to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2067, "text": "some similarity metric, usually Euclidean distance, so that\nxn → arg min\nck∈C\n||xn −ck||2: (12.38)\nSince there are Kcodebook vectors, we can represent eachxn by a one-hot encoded\nK-dimensional vector, and since we can choose the value of K, we can control the\ntrade-off between more accurate representation of the data, by using a larger value\nof K, or greater compression, by using a smaller value of K.\nWe can therefore take the original image pixels and map them into the lower-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2068, "text": "dimensional codebook space. An autoregressive transformer can then be trained to\ngenerate a sequence of codebook vectors, and this sequence can be mapped back into\nthe original image space by replacing each codebook indexkwith the corresponding\nD-dimensional codebook vector ck.\nAutoregressive transformers were ﬁrst applied to images in ImageGPT (Chen,\nRadford, et al., 2020). Here each pixel is treated as one of a discrete set of three-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2069, "text": "dimensional colour codebook vectors, each corresponding to a cluster in aK-means\nclustering of the colour space. A one-hot encoding therefore gives discrete tokens,Section 15.1\nanalogous to language tokens, and allows the transformer to be trained in the same\nway as language models, with a next-token classiﬁcation objective. This is a power-\nful objective for representation learning for subsequent ﬁne-tuning, again in a similarSection 6.3.3\nway to language modelling."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2070, "text": "way to language modelling.\nUsing the individual pixels as tokens directly, however, can lead to high com-\nputational cost since a forward pass is required per pixel, which means that both\ntraining and inference scale poorly with image resolution. Also, using individual\npixels as inputs means that low-resolution images have to be used to give a reason-\nable context length when decoding the pixels later in the raster scan. As we saw with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2071, "text": "the ViT model, it is preferable to use patches of the image as tokens instead of pixels,\nas this can result in dramatically fewer tokens and therefore facilitates working with\nhigher-resolution images. As before, we need to work with a discrete space of token\nvalues due to the potential multimodality of the conditional distributions. Again, this\nraises the challenge of dimensionality, which is now much more severe with patches"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2072, "text": "than with individual pixels since the dimensionality is exponential with respect to\nthe number of pixels in the patch. For example, even with just two possible pixel\n12.4. Multimodal Transformers 399\ntime\nfrequency\nFigure 12.25 An example mel spectrogram of a humpback whale song. [Source data copyright ©2013–\n2023, librosa development team.]\ntokens, representing black and white, and patches of size 16 ×16, we would have a\ndictionary of patch tokens of size 2256 ≃1077."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2073, "text": "dictionary of patch tokens of size 2256 ≃1077.\nOnce again we turn to vector quantization to address the challenge of dimension-\nality. The codebook vectors can be learned from a data set of image patches using\nsimple clustering algorithms such as K-means or with more sophisticated meth-\nods such as fully convolutional networks (Oord, Vinyals, and Kavukcuoglu, 2017;\nEsser, Rombach, and Ommer, 2020) or even vision transformers (Yu et al., 2021)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2074, "text": "One problem with learning to map each patch to a discrete set of codes and back\nagain, is that vector quantization is a non-differentiable operation. Fortunately we\ncan use a technique called straight-through gradient estimation (Bengio, L ´eonard,\nand Courville, 2013), which is a simple approximation that just copies the gradients\nthrough the non-differentiable function during backpropagation.\nThe use of autoregressive transformers to generate images can be extended to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2075, "text": "videos by treating a video as one long sequence of these vector-quantized tokens\n(Rakhimov et al., 2020; Yan et al., 2021; Hu et al., 2023).\n12.4.3 Audio data\nWe next look at the application of transformers to audio data. Sound is gener-\nally stored as a waveform obtained by measuring the amplitude of the air pressure\nat regular time intervals. Although this waveform could be used directly as input to\na deep learning model, in practice it is more effective to pre-process it into a mel"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2076, "text": "spectrogram. This is a matrix whose columns represent time steps and whose rows\ncorrespond to frequencies. The frequency bands follow a standard convention that\nwas chosen through subjective assessment to give equal perceptual differences be-\ntween successive frequencies (the word ‘mel’ comes from melody). An example of\na mel spectrogram is shown in Figure 12.25.\nOne application for transformers in the audio domain is classiﬁcation in which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2077, "text": "segments of audio are assigned to one of a number of predeﬁned categories. For\nexample, the AudioSet data set (Gemmeke et al., 2017) is a widely used benchmark.\n400 12. TRANSFORMERS\nIt contains classes such as ‘car’, ‘animal’, and ‘laughter’. Until the development of\nthe transformer, the state-of-the-art approach for audio classiﬁcation was based on\nmel spectrograms treated as images and used as the input to a convolutional neural"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2078, "text": "network (CNN). However, although a CNN is good at understanding local relation-Chapter 10\nships, one drawback is that it struggles with longer-range dependencies, which can\nbe important in processing audio.\nJust as transformers replaced RNNs as the state-of-the-art in natural language\nprocessing, they have also come to replace CNNs for tasks such as audio classiﬁca-\ntion. For example, a transformer encoder model of identical structure to that used"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2079, "text": "for both language and vision, as shown in Figure 12.18, can be used to predict the\nclass of audio inputs (Gong, Chung, and Glass, 2021). Here the mel spectrogram\nis viewed as an image which is then tokenized. This is done by splitting the image\ninto patches in a similar way to vision transformers, possibly with some overlap so\nas not to lose any important neighbourhood relations. Each patch is then ﬂattened,\nmeaning it is converted to a one-dimensional array, in this case of length 256. A"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2080, "text": "unique positional encoding is then added to each token, a speciﬁc ⟨class⟩token is\nappended, and the tokens are then fed through the transformer encoder. The output\ntoken corresponding to the ⟨class⟩input token from the last transformer layer can\nthen be decoded using a linear layer followed by a softmax activation function, and\nthe whole model can be trained end-to-end using a cross-entropy loss.\n12.4.4 Text-to-speech"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2081, "text": "12.4.4 Text-to-speech\nClassiﬁcation is not the only task that deep learning, and more speciﬁcally the\ntransformer architecture, has revolutionized in the audio domain. The success of\ntransformers at synthesizing speech that imitates the voice of a given speaker is an-\nother demonstration of their versatility, and their application to this task is an infor-\nmative case study in how to apply transformers in a new context."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2082, "text": "Generating speech corresponding to a given passage of text is known as text-\nto-speech synthesis. A more traditional approach would be to collect recordings\nof speech from a given speaker and train a supervised regression model to predict\nthe speech output, possibly in the form of a mel spectrogram, from corresponding\ntranscribed text. During inference, the text for which we would like to synthesize\nspeech is presented as input and the resulting mel spectrogram output can then be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2083, "text": "decoded back to an audio waveform since this is a ﬁxed mapping.\nThis approach has a few major drawbacks, however. First, if we predict speech\nat a low level, for example using sub-word components known as phonemes, a\nlarger context is needed to make the resulting sentences sound ﬂuid. However, if we\npredict longer segments, then the space of possible inputs grows signiﬁcantly, and an\ninfeasible amount of training data might be required to achieve good generalization."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2084, "text": "Second, this approach does not transfer knowledge across speakers, and so a lot of\ndata will be required for each new speaker. Finally, the problem is really a generative\nmodelling task, as there are multiple correct speech outputs for a given speaker and\ntext pair, so regression may not be suitable since it tends to average over target values.Section 4.2\nIf instead we treat audio data in the same way as natural language and frame"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2085, "text": "text-to-speech as a conditional language modelling task, then we should be able to\n12.4. Multimodal Transformers 401\ntransformer\ntext prompt\ntokens discrete\ntokenizer\nacoustic prompt\naudio decoder\nsynthesized speech\nFigure 12.26 A diagram showing the high-level architecture of Vall-E. The input to the transformer model con-\nsists of standard text tokens, which prompt the model as to what words the synthesized speech should contain,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2086, "text": "together with acoustic prompt tokens that determine the speaker style and tone information. The sampled model\noutput tokens are decoded back to speech with the learned decoder. For simplicity, the positional encodings and\nlinear projections are not shown.\ntrain the model in much the same way as with text-based large language models.\nThere are two main implementation details that need to be addressed. The ﬁrst is\nhow to tokenize the training data and decode the predictions, and the second is how"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2087, "text": "to condition the model on the speaker’s voice.\nOne approach to text-to-speech synthesis that makes use of transformers and lan-\nguage modelling techniques is Vall-E (Wang et al., 2023). New text can be mapped\ninto speech in the voice of a new speaker using only a few seconds of sample speech\nfrom that person. Speech data is converted into a sequence of discrete tokens from\na learned dictionary or codebook obtained using vector quantization, and we canSection 12.4.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2088, "text": "think of these tokens as analogous to the one-hot encoded tokens from the natural\nlanguage domain. The input consists of text tokens from a passage of text whereas\nthe target outputs for training consist of the corresponding speech tokens. Additional\nspeech tokens from a short segment of unrelated speech from the same speaker are\n402 12. TRANSFORMERS\nappended to the input text tokens, as illustrated in Figure 12.26. By including exam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2089, "text": "ples from many different speakers, the system can learn to read out a passage of text\nwhile imitating the voice represented by the additional speech input tokens. Once\ntrained the system can be presented with new text, along with audio tokens from a\nbrief segment of speech captured from a new speaker, and the resulting output tokens\ncan be decoded, using the same codebook used during training, to create a speech\nwaveform. This allows the system to synthesize speech corresponding to the input"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2090, "text": "text in the voice of the new speaker.\n12.4.5 Vision and language transformers\nWe have seen how to generate discrete tokens for text, audio, and images, and so\nit is a natural next step to ask if we can train a model with input tokens of one modal-\nity and output tokens of another, or whether we can have a combination of different\nmodalities for either inputs or outputs or both. We will focus on the combination\nof text and vision data as this is the most widely studied example, but in principle"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2091, "text": "the approaches discussed here could be applied to other combinations of input and\noutput modalities.\nThe ﬁrst requirement is that we have a large data set for training. The LAION-\n400M data set (Schuhmann et al., 2021) has greatly accelerated research in text-to-\nimage generation and image-to-text captioning in much the same way that ImageNet\nwas critical in the development of deep image classiﬁcation models. Text-to-image"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2092, "text": "generation is actually much like the unconditional image generation we have looked\nat so far, except that we also allow the model to take as input the text information to\ncondition the generation process. This is straightforward when using transformers as\nwe can simply provide the text tokens as additional input when decoding each image\ntoken.\nThis approach can also be viewed as treating the text-to-image problem as a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2093, "text": "sequence-to-sequence language modelling problem, such as machine translation, ex-\ncept that the target tokens are discrete image tokens rather than language tokens. It\ntherefore makes sense to choose a full encoder-decoder transformer model, as shown\nin Figure 12.20, in which X corresponds to the input text tokens and Y corresponds\nto the output image tokens. This is the approach taken in a model called Parti (Yu et"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2094, "text": "al., 2022) in which the transformer is scaled to 20 billion parameters while showing\nconsistent performance improvements with increasing model size.\nA lot of research has also been done on using pre-trained language models,\nand modifying or ﬁne-tuning them so that they can also accept visual data as in-\nput (Alayrac et al., 2022; Li et al., 2022). These approaches largely use bespoke\narchitectures, along with continuous-valued image tokens, and therefore are not nat-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2095, "text": "ural ﬁts for also generating visual data. Moreover, they cannot be used directly if we\nwish to include new modalities such as audio tokens. Although this is a step towards\nmultimodality, we would ideally like to use both text and image tokens as both input\nand output. The simplest approach is to treat everything as a sequence of tokens as\nif this were natural language but with a dictionary that is the concatenation of a lan-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2096, "text": "guage token dictionary and the image token codebook. We can then treat any stream\nof audio and visual data as simply a sequence of tokens.\nExercises 403\nFigure 12.27 Examples of the CM3Leon model performing a variety of different tasks in the joint space of text\nand images. [From (Yu et al., 2023) with permission.]\nIn CM3 (Aghajanyan et al., 2022) and CM3Leon (Yuet al., 2023), a variation of\nlanguage modelling is used to train on HTML documents containing both image and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2097, "text": "text data taken from online sources. When this large quantity of training data was\ncombined with a scalable architecture, the models became very powerful. Moreover,\nthe multimodal nature of the training means that the models are very ﬂexible. Such\nmodels are capable of completing many tasks that otherwise might require task-\nspeciﬁc model architectures and training regimes, such as text-to-image generation,\nimage-to-text captioning, image editing, text completion, and many more, including"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2098, "text": "anything a regular language model is capable of. Examples of the CM3Leon model\ncompleting instances of a few different tasks are shown in Figure 12.27.\nExercises\n12.1 (??) Consider a set of coefﬁcients anm, for m= 1;:::;N , with the properties that\nanm > 0 (12.39)\n∑\nm\nanm = 1: (12.40)\n404 12. TRANSFORMERS\nBy using a Lagrange multiplier show that the coefﬁcients must also satisfyAppendix C\nanm 6 1 for n= 1;:::;N: (12.41)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2099, "text": "anm 6 1 for n= 1;:::;N: (12.41)\n12.2 (?) Verify that the softmax function (12.5) satisﬁes the constraints (12.3) and (12.4)\nfor any values of the vectors x1;:::; xN.\n12.3 (?) Consider the input vectors xn in the simple transformation deﬁned by (12.2), in\nwhich the weighting coefﬁcientsanmare deﬁned by (12.5). Show that if all the input\nvectors are orthogonal, so that xT\nnxm = 0 for n ̸=m, then the output vectors will\nsimply be equal to the input vectors so that yn = xn for n= 1;:::;N ."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2100, "text": "12.4 (?) Consider two independent random vectors a and b each of dimension D and\neach being drawn from a Gaussian distribution with zero mean and unit variance\nN(-|0;I). Show that the expected value of (aTb)2 is given by D.\n12.5 (???) Show that multi-head attention deﬁned by (12.19) can be rewritten in the form\nY =\nH∑\nh=1\nHhXW(h) (12.42)\nwhere Hh is given by (12.15) and we have deﬁned\nW(h) = W(v)\nh W(o)\nh : (12.43)\nHere we have partitioned the matrix W(o) horizontally into sub-matrices denoted"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2101, "text": "W(o)\nh each of dimension Dv ×D, corresponding to the vertical segments of the\nconcatenated attention matrix. Since Dv is typically smaller than D, for exampleFigure 12.7\nDv = D=H is a common choice, this combined matrix is rank deﬁcient. Therefore,\nusing a fully ﬂexible matrix to replace W(v)\nh W(o)\nh would not be equivalent to the\noriginal formulation given in the text.\n12.6 (??) Express the self-attention function (12.14) as a fully connected network in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2102, "text": "form of a matrix that maps the full input sequence of concatenated word vectors\ninto an output vector of the same dimension. Note that such a matrix would have\nO(N2D2) parameters. Show that the self-attention network corresponds to a sparse\nversion of this matrix with parameter sharing. Draw a sketch showing the structure\nof this matrix, indicating which blocks of parameters are shared and which blocks\nhave all elements equal to zero."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2103, "text": "have all elements equal to zero.\n12.7 (?) Show that if we omit the positional encoding of input vectors then the outputs\nof a multi-head attention layer deﬁned by (12.19) are equivariant with respect to a\nreordering of the input sequence.\n12.8 (???) Consider two D-dimensional unit vectors a and b, satisfying ∥a∥= 1 and\n∥b∥= 1, drawn from a random distribution. Assume that the distribution is sym-\nmetrical around the origin, i.e., it depends only on the distance from the origin and\nExercises 405"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2104, "text": "Exercises 405\nnot the direction. Show that for large values of Dthe magnitude of the cosine of the\nangle between these vectors is close to zero and hence that these random vectors are\nnearly orthogonal in a high-dimensional space. To do this, consider an orthonormal\nbasis set {ui}where uT\ni uj = \u000eij and express a and b as expansions in this basis.\n12.9 (??) Consider a position encoding in which the input token vectorx is concatenated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2105, "text": "with a position-encoding vector e. Show that when this concatenated vector under-\ngoes a general linear transformation by multiplication using a matrix, the result can\nbe expressed as the sum of a linearly transformed input and a linearly transformed\nposition vector.\n12.10 (??) Show that the positional encoding deﬁned by (12.25) has the property that,\nfor a ﬁxed offset k, the encoding at position n+ k can be represented as a linear"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2106, "text": "combination of the encoding at position nwith coefﬁcients that depend only on k\nand not on n. To do this make use of the following trigonometric identities:\ncos(A+ B) = cos Acos B−sin Asin B (12.44)\nsin(A+ B) = cos Asin B+ sinAcos B: (12.45)\nShow that if the encoding is based purely on sine functions, without cosine functions,\nthen this property no longer holds.\n12.11 (?) Consider the bag-of-words model (12.28) in which each of the component distri-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2107, "text": "butions p(xn) is given by a general probability table that is shared across all words.\nShow that the maximum likelihood solution, given a training set of vectors, is given\nby a table whose entries are the fractions of times each word occurs in the training\nset.\n12.12 (?) Consider the autoregressive language model given by (12.31) and suppose that\nthe terms p(xn|x1;:::; xn−1 ) on the right-hand side are represented by general"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2108, "text": "probability tables. Show that the number of entries in these tables grows exponen-\ntially with the value of n.\n12.13 (?) When using n-grams it is usual to train the n-gram and (n−1)-gram models at\nthe same time and then compute the conditional probability using the product rule of\nprobability in the form\np(xn|xn−L+1 ;:::; xn−1 ) = pL(xn−L+1 ;:::; xn)\npL−1 (xn−L+1 ;:::; xn−1 ): (12.46)\nExplain why this is more convenient than storing the left-hand-side directly, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2109, "text": "show that to obtain the correct probabilities the ﬁnal token from each sequence must\nbe omitted when evaluating pL−1 (---).\n12.14 (??) Write down pseudo-code for the inference process in a trained RNN with an\narchitecture of the form depicted in Figure 12.13.\n12.15 (??) Consider a sequence of two tokens y1 and y2 each of which can take the states\nAor B. The table below shows the joint probability distribution p(y1;y2):\n406 12. TRANSFORMERS\ny1 = A y 1 = B\ny2 = A 0.0 0.4\ny2 = B 0.1 0.25"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2110, "text": "y1 = A y 1 = B\ny2 = A 0.0 0.4\ny2 = B 0.1 0.25\nWe see that the most probable sequence is y1 = B, y2 = B and that this has prob-\nability 0:4. Using the sum and product rules of probability, write down the values\nof the marginal distribution p(y1) and the conditional distribution p(y2|y1). Show\nthat if we ﬁrst maximize p(y1) to give a value y?\n1 and then subsequently maximize\np(y2|y?\n1 ) then we obtain a sequence that is different from the overall most probable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2111, "text": "sequence. Find the probability of the sequence.\n12.16 (?) The BERT-Large model (Devlinet al., 2018) has a maximum input length of 512\ntokens, each of dimensionality D= 1;024 and taken from a vocabulary of 30,000. It\nhas 24 transformer layers each with 16 self-attention heads with Dq = Dk = Dv =\n64, and the MLP position-wise networks have two layers with 4,096 hidden nodes.\nShow that the total number of parameters in the BERT encoder transformer language\nmodel is approximately 340 million.\n13"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2112, "text": "model is approximately 340 million.\n13\nGraph Neural\nNetworks\nIn previous chapters we have encountered structured data in the form of sequences\nand images, corresponding to one-dimensional and two-dimensional arrays of vari-\nables respectively. More generally, there are many types of structured data that are\nbest described by a graph as illustrated in Figure 13.1. In general a graph consists of\na set of objects, known as nodes, connected by edges. Both the nodes and the edges"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2113, "text": "can have data associated with them. For example, in a molecule the nodes and edges\nare associated with discrete variables corresponding to the types of atom (carbon, ni-\ntrogen, hydrogen, etc.) and the types of bonds (single bond, double bond, etc.). For a\nrail network, each railway line might be associated with a continuous variable given\nby the average journey time between two cities. Here we are assuming that the edges"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2114, "text": "are symmetrical, for example that the journey time from London to Cambridge is the\nsame as the journey time from Cambridge to London. Such edges are depicted by\nundirected links between the nodes. For the worldwide web the edges are directed\n407© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_13"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2115, "text": "408 13. GRAPH NEURAL NETWORKS\n(a)\n(b)\n(c)\nFigure 13.1 Three examples of graph-structured data: (a) the caffeine molecule consisting of atoms connected\nby chemical bonds, (b) a rail network consisting of cities connected by railway lines, and (c) the worldwide web\nconsisting of pages connected by hyperlinks.\nsince if there is a hyperlink on page A that points to page B there is not necessarily\na hyperlink on page B pointing back to page A."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2116, "text": "a hyperlink on page B pointing back to page A.\nOther examples of graph-structured data include a protein interaction network,\nin which the nodes are proteins and the edges express how strongly pairs of pro-\nteins interact, an electrical circuit where the nodes are components and the edges\nare conductors, or a social network where the nodes are people and the edges are\n‘friendships’. More complex graphical structures are also possible, for example the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2117, "text": "knowledge graph inside a company comprises multiple different kinds of nodes such\nas people, documents, and meetings, along with multiple kinds of edges capturing\ndifferent properties such as a person being present at a meeting or a document refer-\nencing another document.\nIn this chapter we explore how to apply deep learning to graph-structured data.\nWe have already encountered an example of structured data when we discussed im-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2118, "text": "ages, in which the individual elements of an image data vectorx correspond to pixels\non a regular grid. An image is therefore a special instance of graph-structured data\nin which the nodes are the pixels and the edges describe which pixels are adjacent.\nConvolutional neural networks (CNNs) take this structure into account, incorporat-Chapter 10\ning prior knowledge of the relative positions of the pixels, together with the equiv-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2119, "text": "ariance of properties such as segmentation and the invariance of properties such as\nclassiﬁcation. We will use CNNs for images as a source of inspiration to construct\nmore general approaches to deep learning for graphical data known as graph neu-\nral networks (Zhou et al., 2018; Wu et al., 2019; Hamilton, 2020; Veliˇckovi´c, 2023).\nWe will see that a key consideration when applying deep learning to graph-structured"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2120, "text": "data is to ensure either equivariance or invariance with respect to a reordering of the\nnodes in the graph.\n13.1. Machine Learning on Graphs 409\n13.1. Machine Learning on Graphs\nThere are many kinds of applications that we might wish to address using graph-\nstructured data, and we can group these broadly according to whether the goal is\nto predict properties of nodes, of edges, or of the whole graph. An example of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2121, "text": "node prediction would be to classify documents according to their topic based on the\nhyperlinks and citations between the documents.\nRegarding edges we might, for example, know some of the interactions in a pro-\ntein network and would like to predict the presence of any additional ones. Such\ntasks are called edge prediction or graph completion tasks. There are also tasks\nwhere the edges are known in advance and the goal is to discover clusters or ‘com-\nmunities’ within the graph."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2122, "text": "munities’ within the graph.\nFinally, we may wish to predict properties that relate to the graph as a whole. For\nexample, we might wish to predict whether a particular molecule is soluble in water.\nHere instead of being given a single graph we will have a data set of different graphs,\nwhich we can view as being drawn from some common distribution, in other words\nwe assume that the graphs themselves are independent and identically distributed."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2123, "text": "Such tasks can be considered as graph regression or graph classiﬁcation tasks.\nFor the molecule solubility classiﬁcation example, we might be given a labelled\ntraining set of molecules, along with a test set of new molecules whose solubility\nneeds to be predicted. This is a standard example of an inductive task of the kind\nwe have seen many times in previous chapters. However, some graph prediction\nexamples are transductive in which we are given the structure of the entire graph"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2124, "text": "along with labels for some of the nodes and the goal is to predict the labels of the\nremaining nodes. An example would be a large social network in which our goal is to\nclassify each node as either a real person or an automated bot. Here a small number\nof nodes might be manually labelled, but it would be prohibitive to investigate every\nnode individually in a large and ever-changing social network. During training, we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2125, "text": "therefore have access to the whole graph along with labels for a subset of the nodes,\nand we wish to predict the labels for the remaining nodes. This can be viewed as a\nform of semi-supervised learning.\nAs well as solving prediction tasks directly, we can also use deep learning on\ngraphs to discover useful internal representations that can subsequently facilitate a\nrange of downstream tasks. This is known as graph representation learning. For"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2126, "text": "example we could seek to build afoundation model for molecules by training a deep\nlearning system on a large corpus of molecular structures. The goal is that once\ntrained, such a foundation model can be ﬁne-tuned to speciﬁc tasks by using a small,\nlabelled data set.\nGraph neural networks deﬁne anembedding vector for each of the nodes, usually\ninitialized with the observed node properties, which are then transformed through a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2127, "text": "series of learnable layers to create a learned representation. This is analogous to\nthe way word embeddings, or tokens, are processed through a series of layers in the\ntransformer to give a representation that better captures the meaning of the wordsChapter 12\nin the context of the rest of the text. Graph neural networks can also use learned\nembeddings associated with the edges and with the graph as a whole.\n410 13. GRAPH NEURAL NETWORKS\nB\nA\nC\nD\nE\n(a)\nA B C D E\nA\nB\nC\nD\nE\n(b)\nC E A D B\nC\nE\nA\nD\nB"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2128, "text": "E\n(a)\nA B C D E\nA\nB\nC\nD\nE\n(b)\nC E A D B\nC\nE\nA\nD\nB\n(c)\nFigure 13.2 An example of an adjacency matrix showing (a) an example of a graph with ﬁve nodes, (b) the\nassociated adjacency matrix for a particular choice of node order, and (c) the adjacency matrix corresponding to\na different choice for the node order.\n13.1.1 Graph properties\nIn this chapter we will focus on simple graphs where there is at most one edge\nbetween any pair of nodes, where the edges are undirected, and where there are no"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2129, "text": "self-edges that connect a node to itself. This sufﬁces to introduce the key concepts\nof graph neural networks, and it also encompasses a wide range of practical applica-\ntions. These concepts can then be applied to more complex graphical structures.\nWe begin by introducing some notation associated with graphs and by deﬁning\nsome important properties. A graph G= (V;E) consists of a set ofnodes or vertices,\ndenoted by V, along with a set of edges or links, denoted by E. We index the nodes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2130, "text": "by n = 1;:::N , and we write the edge from node nto node mas (n;m). If two\nnodes are linked by an edge they are calledneighbours, and the set of all neighbours\nof node nis denoted by N(n).\nIn addition to the graph structure, we usually also have observed data associated\nwith the nodes. For each node nwe can represent the corresponding node variables\nas a D-dimensional column vector xn and we can group these into a data matrix X\nof dimensionality N ×D in which row nis given by xT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2131, "text": "n. There may also be data\nvariables associated with the edges in the graph, although to start with we will focusSection 13.3.2\njust on node variables.\n13.1.2 Adjacency matrix\nA convenient way to specify the edges in a graph is to use an adjacency matrix\ndenoted by A. To deﬁne the adjacency matrix we ﬁrst have to choose an ordering for\nthe nodes. If there are N nodes in the graph, we can index them usingn= 1;:::;N .\nThe adjacency matrix has dimensionsN×N and contains a 1 in every locationn;m"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2132, "text": "for which there is an edge going from node nto node m, with all other entries being\n0. For graphs with undirected edges, the adjacency matrix will be symmetric since\nthe presence of an edge from node nto node mimplies that there is also an edge\nfrom node mto node n, and therefore Amn = Anm for all nand m. An example of\nan adjacency matrix is shown in Figure 13.2.\nSince the adjacency matrix deﬁnes the structure of a graph, we could consider\n13.1. Machine Learning on Graphs 411"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2133, "text": "13.1. Machine Learning on Graphs 411\nusing it directly as the input to a neural network. To do this we could ‘ﬂatten’ the ma-\ntrix, for example by concatenating the columns into one long column vector. How-\never, a major problem with this approach is that the adjacency matrix depends on the\narbitrary choice of node ordering, as seen in Figure 13.2. Suppose for instance that\nwe want to predict the solubility of a molecule. This clearly should not depend on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2134, "text": "the ordering assigned to the nodes when writing down an adjacency matrix. Because\nthe number of permutations increases factorially with the number of nodes, it is im-\npractical to try to learn permutation invariance by using large data sets or by data\naugmentation. Instead, we should treat this invariance property as an inductive bias\nwhen constructing a network architecture.\n13.1.3 Permutation equivariance\nWe can express node label permutation mathematically by introducing the con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2135, "text": "cept of a permutation matrix P, which has the same size as the adjacency matrix\nand which speciﬁes a particular permutation of a node ordering. It contains a single\n1 in each row and a single 1 in each column, with 0 in all the other elements, such\nthat a 1 in position n;m indicates that node n will be relabelled as node m after\nthe permutation. Consider, for example, the permutation from (A;B;C;D;E ) →\n(C;E;A;D;B ) corresponding to the two choices of node ordering in Figure 13.2."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2136, "text": "The corresponding permutation matrix takes the formExercise 13.1\nP =\n\n\n0 0 1 0 0\n0 0 0 0 1\n1 0 0 0 0\n0 0 0 1 0\n0 1 0 0 0\n\n: (13.1)\nWe can deﬁne the permutation matrix more formally as follows. First we in-\ntroduce the standard unit vector un, for n = 1;:::;N . This is a column vector\nin which all elements are 0 except element n, which equals 1. In this notation the\nidentity matrix is given by\nI =\n\n\nuT\n1\nuT\n2\n---\nuT\nN\n\n: (13.2)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2137, "text": "I =\n\n\nuT\n1\nuT\n2\n---\nuT\nN\n\n: (13.2)\nWe can now introduce a permutation function \u0019(-)that maps nto m = \u0019(n). The\nassociated permutation matrix is given by\nP =\n\n\nuT\n\u0019(1)\nuT\n\u0019(2)\n:::\nuT\n\u0019(N)\n\n: (13.3)\nWhen we reorder the labelling on the nodes of a graph, the effect on the corre-\nsponding node data matrix X is to permute the rows according to\u0019(-), which can be\nachieved by pre-multiplication by P to giveExercise 13.4\n412 13. GRAPH NEURAL NETWORKS\n˜X = PX: (13.4)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2138, "text": "412 13. GRAPH NEURAL NETWORKS\n˜X = PX: (13.4)\nFor the adjacency matrix, both the rows and the columns become permuted. Again\nthe rows can be permuted using pre-multiplication by P whereas the columns are\npermuted using post-multiplication by PT, giving a new adjacency matrix:Exercise 13.5\n˜A = PAPT: (13.5)\nWhen applying deep learning to graph-structured data, we will need to repre-\nsent the graph structure in numerical form so that it can be fed into a neural network,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2139, "text": "which requires that we assign an ordering to the nodes. However, the speciﬁc or-\ndering we choose is arbitrary and so it will be important to ensure that any global\nproperty of the graph does not depend on this ordering. In other words, the network\npredictions must be invariant to node label reordering, so that\ny( ˜X;˜A) = y(X;A) Invariance (13.6)\nwhere y(-;-)is the output of the network.\nWe may also want to make predictions that relate to individual nodes. In this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2140, "text": "case, if we reorder the node labelling then the corresponding predictions should show\nthe same reordering so that a given prediction is always associated with the same\nnode irrespective of the choice of order. In other words, node predictions should be\nequivariant with respect to node label reordering. This can be expressed as\ny( ˜X;˜A) = Py(X;A) Equivariance (13.7)\nwhere y(-;-)is a vector of network outputs, with one element per node.\n13.2.\nNeural Message-Passing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2141, "text": "13.2.\nNeural Message-Passing\nEnsuring invariance or equivariance under node label permutations is a key design\nconsideration when we apply deep neural networks to graph-structured data. An-\nother consideration is that we want to exploit the representational capabilities of deep\nneural networks and so we retain the concept of a ‘layer’ as a computational trans-\nformation that can be applied repeatedly. If each layer of the network is equivariant"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2142, "text": "under node reordering then multiple layers applied in succession will also exhibit\nequivariance, while allowing each layer of the network to be informed by the graph\nstructure.\nFor networks whose outputs represent node-level predictions, the whole network\nwill be equivariant as required. If the network is being used to predict a graph-\nlevel property then a ﬁnal layer can be included that is invariant to permutations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2143, "text": "of its inputs. We also want to ensure that each layer is a highly ﬂexible nonlinear\nfunction and is differentiable with respect to its parameters so that it can be trained\nby stochastic gradient descent using gradients obtained by automatic differentiation.\nGraphs come in various sizes. For example different molecules can have differ-\nent numbers of atoms, so a ﬁxed-length representation as used for standard neural\n13.2. Neural Message-Passing 413\ni\ni\nl l+ 1\n(a)\ni\n(b)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2144, "text": "i\ni\nl l+ 1\n(a)\ni\n(b)\nFigure 13.3 A convolutional ﬁlter for images can be represented as a graph-structured computation. (a) A ﬁlter\ncomputed by node iin layer l+ 1 of a deep convolutional network is a function of the activation values in layer\nl over a local patch of pixels. (b) The same computation structure expressed as a graph showing ‘messages’\nﬂowing into node ifrom its neighbours.\nnetworks is unsuitable. A further requirement is therefore that the network should be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2145, "text": "able to handle variable-length inputs, as we saw with transformer networks. SomeChapter 12\ngraphs can be very large, for example a social network with many millions of par-\nticipants, and so we also want to construct models that scale well. Not surpris-\ningly, parameter sharing will play an important role, both to allow the invariance and\nequivariance properties to be built into the network architecture but also to facilitate\nscaling to large graphs.\n13.2.1 Convolutional ﬁlters"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2146, "text": "13.2.1 Convolutional ﬁlters\nTo develop a framework that meets all of these requirements, we can seek inspi-\nration from image processing using convolutional neural networks. First note thatChapter 10\nan image can be viewed as a speciﬁc instance of graph-structured data, in which the\nnodes are the pixels and the edges represent pairs of pixels that are adjacent in the\nimage, where adjacency includes nodes that are diagonally adjacent as well as those\nthat are horizontally or vertically adjacent."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2147, "text": "that are horizontally or vertically adjacent.\nIn a convolutional network, we make successive transformations of the image\ndomain such that a pixel at a particular layer computes a function of states of pixels\nin the previous layer through a local function called aﬁlter. Consider a convolutionalSection 10.2\nlayer using 3 ×3 ﬁlters, as illustrated in Figure 13.3(a). The computation performed\n414 13. GRAPH NEURAL NETWORKS\nby a single ﬁlter at a single pixel in layer l+ 1 can be expressed as\nz(l+1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2148, "text": "z(l+1)\ni = f\n\n∑\nj\nwjz(l)\nj + b\n\n (13.8)\nwhere f(-)is a differentiable nonlinear activation function such as ReLU, and the\nsum over jis taken over all nine pixels in a small patch in layerl. The same function\nis applied across multiple patches in the image, so that the weights wj and bias bare\nshared across the patches (and therefore do not carry the index i).\nAs it stands, (13.8) is not equivariant under reordering of the nodes in layer l"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2149, "text": "because the weight vector, with elements wj, is not invariant under permutation of\nits elements. However, we can achieve equivariance with some simple modiﬁcations\nas follows. We ﬁrst view the ﬁlter as a graph, as shown in Figure 13.3(b), and\nseparate out the contribution from node i. The other eight 8 nodes are its neighbours\nN(i). We then assume that a single weight parameter wneigh is shared across the\nneighbours so that\nz(l+1)\ni = f\n\nwneigh\n∑\nj∈N(i)\nz(l)\nj + wselfz(l)\ni + b\n\n (13.9)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2150, "text": "∑\nj∈N(i)\nz(l)\nj + wselfz(l)\ni + b\n\n (13.9)\nwhere node ihas its own weight parameter wself.\nWe can interpret (13.9) as updating a local representationzi at node iby gather-\ning information from the neighbouring nodes by passing messages from the neigh-\nbouring nodes into node i. In this case the messages are simply the activations of\nthe other nodes. These messages are then combined with information from node i,\nand the result is transformed using a nonlinear function. The information from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2151, "text": "neighbouring nodes is aggregated through a simple summation in (13.9), and this is\nclearly invariant to any permutation of the labels associated with those nodes. Fur-\nthermore, the operation (13.9) is applied synchronously to every node in a graph, and\nso if the nodes are permuted then the resulting computations will be unchanged but\ntheir ordering will be likewise permuted, and hence, this calculation is equivariant"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2152, "text": "under node reordering. Note that this depends on the parameters wneigh, wself, and b\nbeing shared across all nodes.\n13.2.2 Graph convolutional networks\nWe now use the convolution example as a template to construct deep neural net-\nworks for graph-structured data. Our goal is to deﬁne a ﬂexible, nonlinear transfor-\nmation of the node embeddings that is differentiable with respect to a set of weight\nand bias parameters and which maps the variables in layerlinto corresponding vari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2153, "text": "ables in layer l+ 1. For each node nin the graph and for each layer l in the net-\nwork, we introduce a D-dimensional column vector h(l)\nn of node-embedding vari-\nables, where n= 1;:::;N and l= 1;:::;L .\nWe see that the transformation given by (13.9) ﬁrst gathers and combines in-\nformation from neighbouring nodes and then updates the node as a function of the\n13.2. Neural Message-Passing 415\nAlgorithm 13.1: Simple message-passing neural network\nInput: Undirected graph G= (V;E)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2154, "text": "Input: Undirected graph G= (V;E)\nInitial node embeddings {h(0)\nn = xn}\nAggregate(-)function\nUpdate(-;-)function\nOutput: Final node embeddings {h(L)\nn }\n// Iterative message-passing\nfor l∈{0;:::;L −1}do\nz(l)\nn ← Aggregate\n({\nh(l)\nm : m∈N(n)\n})\nh(l+1)\nn ← Update\n(\nh(l)\nn ;z(l)\nn\n)\nend for\nreturn {h(L)\nn }\ncurrent embedding of the node and the incoming messages. We can therefore view\neach layer of processing as having two successive stages. The ﬁrst is theaggregation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2155, "text": "stage in which, for each node n, messages are passed to that node from its neigh-\nbours and combined to form a new vector z(l)\nn in a way that is permutation invariant.\nThis is followed by an update step in which the aggregated information from neigh-\nbouring nodes is combined with local information from the node itself and used to\ncalculate a revised embedding vector for that node.\nConsider a speciﬁc node n in the graph. We ﬁrst aggregate the node vectors\nfrom all the neighbours of node n:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2156, "text": "from all the neighbours of node n:\nz(l)\nn = Aggregate\n({\nh(l)\nm : m∈N(n)\n})\n: (13.10)\nThe form of this aggregation function is very ﬂexible if it is well deﬁned for a vari-\nable number of neighbouring nodes and does not depend on the ordering of those\nnodes. It can potentially contain learnable parameters as long as it is a differentiable\nfunction with respect to those parameters to facilitate gradient descent training.\nWe then use another operation to update the embedding vector at node n:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2157, "text": "h(l+1)\nn = Update\n(\nh(l)\nn ;z(l)\nn\n)\n: (13.11)\nAgain, this can be a differentiable function of a set of learnable parameters. Appli-\ncation of the Aggregate operation followed by the Update operation in parallel for\nevery node in the graph represents one layer of the network. The node embeddings\nare typically initialized using observed node data so that h(0)\nn = xn. Note that each\nlayer generally has its own independent parameters, although the parameters can also"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2158, "text": "be shared across layers. This framework is called a message-passing neural network\n(Gilmer et al., 2017) and is summarized in Algorithm 13.1.\n416 13. GRAPH NEURAL NETWORKS\n13.2.3 Aggregation operators\nThere are many possible forms for the Aggregate function, but it must depend\nonly on the set of inputs and not on their ordering. It must also be a differentiable\nfunction of any learnable parameters. The simplest such aggregation function, fol-\nlowing from (13.9), is summation:\nAggregate\n({\nh(l)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2159, "text": "Aggregate\n({\nh(l)\nm : m∈N(n)\n})\n=\n∑\nm∈N(n)\nh(l)\nm: (13.12)\nA simple summation is clearly independent of the ordering of the neighbouring nodes\nand is also well deﬁned no matter how many nodes are in the neighbourhood set.\nNote that this has no learnable parameters.\nA summation gives a stronger inﬂuence over nodes that have many neighbours\ncompared to those with few neighbours, and this can lead to numerical issues, par-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2160, "text": "ticularly in applications such as social networks where the size of the neighbourhood\nset can vary by several orders of magnitude. A variation of this approach is to deﬁne\nthe Aggregation operation to be the average of the neighbouring embedding vectors\nso that\nAggregate\n({\nh(l)\nm : m∈N(n)\n})\n= 1\n|N(n)|\n∑\nm∈N(n)\nh(l)\nm (13.13)\nwhere |N(n)|denotes the number of nodes in the neighbourhood set N(n). How-\never, this normalization also discards information about the network structure and is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2161, "text": "provably less powerful than a simple summation (Hamilton, 2020), and so the choice\nof whether to use it depends on the relative importance of node features compared to\ngraph structure.\nAnother variation of this approach (Kipf and Welling, 2016) takes account of\nthe number of neighbours for each of the neighbouring nodes:\nAggregate\n({\nh(l)\nm : m∈N(n)\n})\n=\n∑\nm∈N(n)\nh(l)\nm\n√\n|N(n)||N(m)|\n: (13.14)\nYet another possibility is to take the element-wise maximum (or minimum) of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2162, "text": "neighbouring embedding vectors, which also satisﬁes the desired properties of being\nwell deﬁned for a variable number of neighbours and of being independent of their\norder.\nSince each node in a given layer of the network is updated by aggregating infor-\nmation from its neighbours in the previous layer, this deﬁnes a receptive ﬁeld anal-\nogous to the receptive ﬁelds of ﬁlters used in CNNs. As information is processedChapter 10"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2163, "text": "through successive layers, the updates to a given node depend on a steadily increas-\ning fraction of other nodes in earlier layers until the effective receptive ﬁeld poten-\ntially spans the whole graph as illustrated in Figure 13.4. However, large, sparse\ngraphs may require an excessive number of layers before each output is inﬂuenced\nby every input. Some architectures therefore introduce an additional ‘super-node’\n13.2. Neural Message-Passing 417\nFigure 13.4 Schematic illustration of infor-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2164, "text": "Figure 13.4 Schematic illustration of infor-\nmation ﬂow through successive layers of a\ngraph neural network. In the third layer a sin-\ngle node is highlighted in red. It receives in-\nformation from its two neighbours in the previ-\nous layer and those in turn receive informa-\ntion from their neighbours in the ﬁrst layer.\nAs with convolutional neural networks for im-\nages, we see that the effective receptive ﬁeld,\ncorresponding to the number of nodes shown"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2165, "text": "corresponding to the number of nodes shown\nin red, grows with the number of processing\nlayers.\nthat connects directly to every node in the original graph to ensure fast propagation\nof information.\nThe aggregation operators discussed so far have no learnable parameters. We\ncan introduce such parameters if we ﬁrst transform each of the embedding vectors\nfrom neighbouring nodes using a multilayer neural network, denoted by MLP\u001e,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2166, "text": "before combining their outputs, where MLP denotes ‘multilayer perceptron’ and\u001e\nrepresents the parameters of the network. So long as the network has a structure and\nparameter values that are shared across nodes then this aggregation operator again\nbe permutation invariant. We can also transform the combined vector with another\nneural network MLP\u0012, with parameters \u0012, to give an overall aggregation operator:\nAggregate\n({\nh(l)\nm : m∈N(n)\n})\n= MLP\u0012\n\n ∑\nm∈N(n)\nMLP\u001e(h(l)\nm)\n\n (13.15)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2167, "text": "})\n= MLP\u0012\n\n ∑\nm∈N(n)\nMLP\u001e(h(l)\nm)\n\n (13.15)\nin which MLP\u001e and MLP\u0012 are shared across layer l. Due to the ﬂexibility of MLPs,\nthe transformation deﬁned by (13.15) represents a universal approximator for any\npermutation-invariant function that maps a set of embeddings to a single embedding\n(Zaheer et al., 2017). Note that the summation can be replaced by other invariant\nfunctions such as averages or an element-wise maximum or minimum."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2168, "text": "A special case of graph neural networks arises if we consider a graph having no\nedges, which corresponds simply to an unstructured set of nodes. In this case if we\nuse (13.15) for each vector h(l)\nn in the set, in which the summation is taken over all\nother vectors except h(l)\nn , then we have a general framework for learning functions\nover unstructured sets of variables known as deep sets.\n418 13. GRAPH NEURAL NETWORKS\n13.2.4 Update operators"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2169, "text": "13.2.4 Update operators\nHaving chosen a suitable Aggregate operator, we similarly need to decide on the\nform of the Update operator. By analogy with (13.9) for the CNN, a simple form for\nthis operator would be\nUpdate\n(\nh(l)\nn ;z(l)\nn\n)\n= f\n(\nWselfh(l)\nn + Wneighz(l)\nn + b\n)\n(13.16)\nwhere f(-)is a nonlinear activation function such as ReLU applied element-wise to\nits vector argument, and where Wself, Wneigh, and b are the learnable weights and\nbiases and z(l)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2170, "text": "biases and z(l)\nn is deﬁned by the Aggregate operator (13.10).\nIf we choose a simple summation (13.12) as the aggregation function and if\nwe also share the same weight matrix between nodes and their neighbours so that\nWself = Wneigh, we obtain a particularly simple form of Update operator given by\nh(l+1)\nn = Update\n(\nh(l)\nn ;z(l)\nn\n)\n= f\n\nWneigh\n∑\nm∈N(n);n\nh(l)\nm + b\n\n: (13.17)\nThe message-passing algorithm is typically initialized by setting h(0)\nn = xn."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2171, "text": "n = xn.\nSometimes, however, we may want to have an internal representation vector for each\nnode that has a higher, or lower, dimensionality than that of xn. Such a represen-\ntation can be initialized by padding the node vectors xn with additional zeros (to\nachieve a higher dimensionality) or simply by transforming the node vectors using a\nlearnable linear transformation to a space of the desired number of dimensions. An"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2172, "text": "alternative form of initialization, particularly when there are no data variables asso-\nciated with the nodes, is to use a one-hot vector that labels the degree of each node\n(i.e., the number of neighbours).\nOverall, we can represent a graph neural network as a sequence of layers that\nsuccessively transform the node embeddings. If we group these embeddings into a\nmatrix H whose nth row is the vector hT\nn, which is initialized to the data matrix X,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2173, "text": "n, which is initialized to the data matrix X,\nthen we can write the successive transformations in the form\nH(1) = F\n(\nX;A;W(1))\nH(2) = F\n(\nH(1);A;W(2))\n..\n. = .\n.\n.\nH(L) = F\n(\nH(L−1) ;A;W(L))\n(13.18)\nwhere A is the adjacency matrix, and W(l) represents the complete set of weight\nand biases in layer lof the network. Under a node reordering deﬁned by a permu-\ntation matrix P, the transformation of the node embeddings computed by layer lis\nequivariant:\nPH(l) = F\n(\nPH(l−1) ;PAPT;W(l))\n: (13.19)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2174, "text": "PH(l) = F\n(\nPH(l−1) ;PAPT;W(l))\n: (13.19)\nAs a consequence, the complete network computes an equivariant transformation.Exercise 13.7\n13.2. Neural Message-Passing 419\n13.2.5 Node classiﬁcation\nA graph neural network can be viewed as a series of layers each of which trans-\nforms a set of node-embedding vectors {h(l)\nn }into a new set {h(l+1)\nn }of the same\nsize and dimensionality. After the ﬁnal convolutional layer of the network, we need"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2175, "text": "to obtain predictions so that we can deﬁne a cost function for training and also for\nmaking predictions on new data using the trained network.\nConsider ﬁrst the task of classifying the nodes in a graph, which is one of the\nmost common uses for graph neural networks. We can deﬁne an output layer, some-\ntimes called a readout layer, which calculates a softmax function for each node cor-\nresponding to a classiﬁcation over Cclasses, of the form\nyni = exp(wT\ni h(L)\nn )\n∑\njexp(wT\nj h(L)\nn )\n(13.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2176, "text": "i h(L)\nn )\n∑\njexp(wT\nj h(L)\nn )\n(13.20)\nwhere {wi}is a set of learnable weight vectors and i = 1;:::;C . We can then\ndeﬁne a loss function as the sum of the cross-entropy loss across all nodes and all\nclasses:\nL= −\n∑\nn∈Vtrain\nC∑\ni=1\nytni\nni (13.21)\nwhere {tni}are target values with a one-hot encoding for each value of n. Because\nthe weight vectors {wi}are shared across the output nodes, the outputs yni are\nequivariant to permutation of the node ordering, and hence the loss function (13.21)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2177, "text": "is invariant. If the goal is to predict continuous values at the outputs then a sim-\nple linear transformation can be combined with a sum-of-squares error to deﬁne a\nsuitable loss function.\nThe sum overnin (13.21) is taken over the subset of the nodes denoted byVtrain\nand used for training. We can distinguish between three types of nodes as follows:\n1. The nodes Vtrain are labelled and included in the message-passing operations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2178, "text": "of the graph neural network and are also used to compute the loss function\nused for training.\n2. There is potentially also a transductive subset of nodes denoted by Vtrans,\nwhich are unlabelled and which do not contribute to the evaluation of the\nloss function used for training. However, they still participate in the message-\npassing operations during both training and inference, and their labels may be\npredicted as part of the inference process."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2179, "text": "predicted as part of the inference process.\n3. The remaining nodes, denoted Vinduct, are a set of inductive nodes that are not\nused to compute the loss function, and neither these nodes nor their associated\nedges participate in message-passing during the training phase. However, they\ndo participate in message-passing during the inference phase and their labels\nare predicted as the outcome of inference.\n420 13. GRAPH NEURAL NETWORKS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2180, "text": "420 13. GRAPH NEURAL NETWORKS\nIf there are no transductive nodes, and hence the test nodes (and their associated\nedges) are not available during the training phase, then the training is generally\nreferred to as inductive learning, which can be considered to be a form of super-\nvised learning. However, if there are transductive nodes then it is calledtransductive\nlearning, which may be viewed as a form of semi-supervised learning.\n13.2.6 Edge classiﬁcation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2181, "text": "13.2.6 Edge classiﬁcation\nIn some applications we wish to make predictions about the edges of the graph\nrather than the nodes. A common form of edge classiﬁcation task is edge completion\nin which the goal is to determine whether an edge should be present between two\nnodes. Given a set of node embeddings, the dot product between pairs of embeddings\ncan be used to deﬁne a probabilityp(n;m) for the presence of an edge between nodes\nnand mby using the logistic sigmoid function:\np(n;m) = \u001b\n(\nhT\nnhm"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2182, "text": "p(n;m) = \u001b\n(\nhT\nnhm\n)\n: (13.22)\nAn example application would be predicting whether two people in a social network\nhave shared interests and therefore might wish to connect.\n13.2.7 Graph classiﬁcation\nIn some applications of graph neural networks, the goal is to predict the proper-\nties of new graphs given a training set of labelled graphs G1;:::; GN. This requires\nthat we combine the ﬁnal-layer embedding vectors in a way that does not depend"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2183, "text": "on the arbitrary node ordering, thereby ensuring that the output predictions will be\ninvariant to that ordering. The goal is somewhat like that of the Aggregate function\nexcept that all nodes in the graph are included, not just the neighbourhood sets of the\nindividual nodes. The simplest approach is to take the sum of the node-embedding\nvectors:\ny = f\n(∑\nn∈V\nh(L)\nn\n)\n(13.23)\nwhere the function f may contain learnable parameters such as a linear transforma-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2184, "text": "tion or a neural network. Other invariant aggregation functions can be used such as\naverages or element-wise minimum or maximum.\nA cross-entropy loss is typically used for classiﬁcation problems, such as la-\nbelling a candidate drug molecule as toxic or safe, and a squared-error loss for re-\ngression problems, such as predicting the solubility of a candidate drug molecule.\nGraph-level predictions correspond to an inductive task since there must be separate"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2185, "text": "sets of graphs for training and for inference.\n13.3.\nGeneral Graph Networks\nThere are many variations and extensions of the graph networks considered so far.\nHere we outline a few of the key concepts along with some practical considerations.\n13.3. General Graph Networks 421\n13.3.1 Graph attention networks\nThe attention mechanism is very powerful when used as the basis of a trans-Section 12.1\nformer architecture. It can be used in the context of graph neural networks to con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2186, "text": "struct an aggregation function that combines messages from neighbouring nodes.\nThe incoming messages are weighted by attention coefﬁcients Anm to give\nz(l)\nn = Aggregate\n({\nh(l)\nm : m∈N(n)\n})\n=\n∑\nm∈N(n)\nAnmh(l)\nm (13.24)\nwhere the attention coefﬁcients satisfy\nAnm > 0 (13.25)\n∑\nm∈N(n)\nAnm = 1: (13.26)\nThis is known as a graph attention network (Veliˇckovi´c et al., 2017) and can capture\nan inductive bias that says some neighbouring nodes will be more important than"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2187, "text": "others in determining the best update in a way that depends on the data itself.\nThere are multiple ways to construct the attention coefﬁcients, and these gener-\nally employ a softmax function. For example, we can use a bilinear form:\nAnm = exp\n(\nhT\nnWhm\n)\n∑\nm′∈N(n) exp (hT\nnWhm′) (13.27)\nwhere W is a D×D matrix of learnable parameters. A more general option is to\nuse a neural network to combine the embedding vectors from the nodes at each end\nof the edge:\nAnm = exp {MLP (hn;hm)}∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2188, "text": "of the edge:\nAnm = exp {MLP (hn;hm)}∑\nm′∈N(n) exp {MLP (hn;hm′)} (13.28)\nwhere the MLP has a single continuous output variable whose value is invariant if\nthe input vectors are exchanged. Provided the MLP is shared across all the nodes in\nthe network, this aggregation function will be equivariant under node reordering.Exercise 13.8\nA graph attention network can be extended by introducing multiple attention\nheads in which H distinct sets of attention weights A(h)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2189, "text": "nm are deﬁned, for h =Section 12.1.6\n1;:::;H , in which each head is evaluated using one of the mechanisms described\nabove and with its own independent parameters. These are then combined in the\naggregation step using concatenation and linear projection. Note that, for a fully-\nconnected network, a multi-head graph attention network becomes a standard trans-\nformer encoder.Exercise 13.9\n13.3.2 Edge embeddings\nThe graph neural networks discussed above use embedding vectors that are as-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2190, "text": "sociated with the nodes. We have seen that some networks also have data associated\nwith the edges. Even when there are no observable values associated with the edges,\n422 13. GRAPH NEURAL NETWORKS\nwe can still maintain and update edge-based hidden variables and these can con-\ntribute to the internal representations learned by the graph neural network.\nIn addition to the node embeddings given by h(l)\nn , we therefore introduce edge\nembeddings e(l)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2191, "text": "n , we therefore introduce edge\nembeddings e(l)\nnm. We can then deﬁne general message-passing equations in the form\ne(l+1)\nnm = Updateedge\n(\ne(l)\nnm;h(l)\nn ;h(l)\nm\n)\n(13.29)\nz(l+1)\nn = Aggregatenode\n({\ne(l+1)\nnm : m∈N(n)\n})\n(13.30)\nh(l+1)\nn = Updatenode\n(\nh(l)\nn ;z(l+1)\nn\n)\n: (13.31)\nThe learned edge embeddings e(L)\nnm from the ﬁnal layer can be used directly to make\npredictions associated with the edges.\n13.3.3 Graph embeddings"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2192, "text": "13.3.3 Graph embeddings\nIn addition to node and edge embeddings we can also maintain and update an\nembedding vector g(l) that relates to the graph as a whole. Bringing all these aspects\ntogether allows us to deﬁne a more general set of message-passing functions, and a\nricher set of learned representations, for graph-structured applications. Speciﬁcally,\nwe can deﬁne general message-passing equations (Battaglia et al., 2018):\ne(l+1)\nnm = Updateedge\n(\ne(l)\nnm;h(l)\nn ;h(l)\nm;g(l))\n(13.32)\nz(l+1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2193, "text": "(\ne(l)\nnm;h(l)\nn ;h(l)\nm;g(l))\n(13.32)\nz(l+1)\nn = Aggregatenode\n({\ne(l+1)\nnm : m∈N(n)\n})\n(13.33)\nh(l+1)\nn = Updatenode\n(\nh(l)\nn ;z(l+1)\nn ;g(l))\n(13.34)\ng(l+1) = Updategraph\n(\ng(l);{h(l+1)\nn : n∈V};{e(l+1)\nnm : ( n;m) ∈E}\n)\n: (13.35)\nThese update equations start in (13.32) by updating the edge embedding vectors\ne(l+1)\nnm based on the previous states of those vectors, on the node embeddings for the\nnodes connected by each edge, and on a graph-level embedding vector g(l). These"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2194, "text": "updated edge embeddings are then aggregated across every edge connected to each\nnode using (13.33) to give a set of aggregated vectors. These in turn then contribute\nto the update of the node-embedding vector {h(l+1)\nn }based on the current node-\nembedding vectors and on the graph-level embedding vector using (13.34). Finally,\nthe graph-level embedding vector is updated using (13.35) based on information\nfrom all the nodes and all the edges in the graph along with the graph-level em-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2195, "text": "bedding from the previous layer. These message-passing updates are illustrated in\nFigure 13.5 and are summarized in Algorithm 13.2.\n13.3.4 Over-smoothing\nOne signiﬁcant problem that can arise with some graph neural networks is called\nover-smoothing in which the node-embedding vectors tend to become very similar to\neach other after a number of iterations of message-passing, which effectively limits\nthe depth of the network. One way to help alleviate this issue is to introduce residual"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2196, "text": "connections. For example, we can modify the update operator (13.34):Section 9.5\nh(l+1)\nn = Updatenode\n(\nh(l)\nn ;z(l+1)\nn ;g(l))\n+ h(l)\nn : (13.36)\n13.3. General Graph Networks 423\nn\nhn\nm\nhm\nG g\nenm\n(a)\nn\nhn\nm\nG g\ne\ne\ne\nenm\n(b)\nh\nh\nh\nh\nh\nh\nG g\nee\nee\ne\ne\ne\n(c)\nFigure 13.5 Illustration of the general graph message-passing updates deﬁned by (13.32) to (13.35), showing\n(a) edge updates, (b) node updates, and (c) global graph updates. In each case the variable being updated is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2197, "text": "shown in red and the variables that contribute to that update are those shown in red and blue.\nAnother approach for mitigating the effects of over-smoothing is to allow the\noutput layer to take information from all previous layers of the network and not just\nthe ﬁnal convolutional layer. This can be done for example by concatenating the\nrepresentations from previous layers:\nyn = f\n(\nh(1)\nn ⊕h(2)\nn ⊕---⊕h(L)\nn\n)\n(13.37)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2198, "text": "yn = f\n(\nh(1)\nn ⊕h(2)\nn ⊕---⊕h(L)\nn\n)\n(13.37)\nwhere a⊕b denotes the concatenation of vectorsa and b. A variant of this would be\nto combine the vectors using max pooling instead of concatenation. In this case each\nelement of the output vector is given by the max of all the corresponding elements\nof the embedding vectors from the previous layers.\n13.3.5 Regularization\nStandard techniques for regularization can be used with graph neural networks,Chapter 9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2199, "text": "including the addition of penalty terms, such as the sum-of-squares of the parameter\nvalues, to the loss function. In addition, some regularization methods have been\ndeveloped speciﬁcally for graph neural networks.\nGraph neural networks already employ weight sharing to achieve permutation\nequivariance and invariance, but typically they have independent parameters in each\nlayer. However, weights and biases can also be shared across layers to reduce the\nnumber of independent parameters."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2200, "text": "number of independent parameters.\nDropout in the context of graph neural networks involves omitting random sub-\nsets of the graph nodes during training, with a fresh random subset chosen for each\nforward pass. This can likewise be applied to the edges in the graph in which ran-\ndomly selected subsets of entries in the adjacency matrix are removed, or masked,\nduring training.\n424 13. GRAPH NEURAL NETWORKS\nAlgorithm 13.2: Graph neural network with node, edge, and graph embeddings"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2201, "text": "Input: Undirected graph G= (V;E)\nInitial node embeddings {h(0)\nn }\nInitial edge embeddings {e(0)\nnm}\nInitial graph embedding g(0)\nOutput: Final node embeddings {h(L)\nn }\nFinal edge embeddings {e(L)\nnm}\nFinal graph embedding g(L)\n// Iterative message-passing\nfor l∈{0;:::;L −1}do\ne(l+1)\nnm ← Updateedge\n(\ne(l)\nnm;h(l)\nn ;h(l)\nm;g(l)\n)\nz(l+1)\nn ← Aggregatenode\n({\ne(l+1)\nnm : m∈N(n)\n})\nh(l+1)\nn ← Updatenode\n(\nh(l)\nn ;z(l+1)\nn ;g(l)\n)\ng(l+1) ← Updategraph\n(\ng(l);{h(l+1)\nn };{e(l+1)\nnm }\n)\nend for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2202, "text": "(\ng(l);{h(l+1)\nn };{e(l+1)\nnm }\n)\nend for\nreturn {h(L)\nn },{e(L)\nnm},g(L)\n13.3.6 Geometric deep learning\nWe have seen how permutation symmetry is a key consideration when design-\ning deep learning models for graph-structured data. It acts as a form of inductive\nbias, dramatically reducing the data requirements while improving predictive perfor-\nmance. In applications of graph neural networks associated with spatial properties,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2203, "text": "such as graphics meshes, ﬂuid ﬂow simulations, or molecular structures, there are\nadditional equivariance and invariance properties that can be built into the network\narchitecture.\nConsider the task of predicting the properties of a molecule, for example when\nexploring the space of candidate drugs. The molecule can be represented as a list\nof atoms of given types (carbon, hydrogen, nitrogen, etc.) along with the spatial"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2204, "text": "coordinates of each atom expressed as a three-dimensional column vector. We can\nintroduce an associated embedding vector for each atom nat each layer l, denoted\nby r(l)\nn , and these vectors can be initialized with the known atom coordinates. How-\never, the values for the elements of these vectors depends on the arbitrary choice of\ncoordinate system, whereas the properties of the molecule do not. For example, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2205, "text": "solubility of the molecule is unchanged if it is rotated in space or translated to a new\nposition relative to the origin of the coordinate system, or if the coordinate system\nitself is reﬂected to give the mirror image version of the molecule. The molecular\nExercises 425\nproperties should therefore be invariant under such transformations.\nBy making careful choices of the functional forms for the update and aggre-\ngation operations (Satorras, Hoogeboom, and Welling, 2021), the new embeddings"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2206, "text": "r(l)\nn can be incorporated into the graph neural network update equations (13.29) to\n(13.31) to achieve the required symmetry properties:\ne(l+1)\nnm = Updateedge\n(\ne(l)\nnm;h(l)\nn ;h(l)\nm;∥r(l)\nn −r(l)\nm∥2)\n(13.38)\nr(l+1)\nn = r(l)\nn + C\n∑\n(n;m)∈E\n(\nr(l)\nn −r(l)\nm\n)\n\u001e\n(\ne(l+1)\nnm\n)\n(13.39)\nz(l+1)\nn = Aggregatenode\n(\n{e(l+1)\nnm : m∈N(n)}\n)\n(13.40)\nh(l+1)\nn = Updatenode\n(\nh(l)\nn ;z(l+1)\nn\n)\n(13.41)\nNote that the quantity ∥r(l)\nn −r(l)\nm∥2 represents the squared distance between the\ncoordinates r(l)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2207, "text": "coordinates r(l)\nn and r(l)\nm, and this does not depend on translations, rotations, or re-\nﬂections. Also, the coordinates r(l)\nn are updated through a linear combination of the\nrelative differences\n(\nr(l)\nn −r(l)\nm\n)\n. Here \u001e\n(\ne(l+1)\nnm\n)\nis a general scalar function of the\nedge embeddings and is represented by a neural network, and the coefﬁcient C is\ntypically set equal to the reciprocal of the number of terms in the sum. It follows"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2208, "text": "that under such transformations, the messages in (13.38), (13.40), and (13.41) are\ninvariant and the coordinate embeddings given by (13.39) are equivariant.Exercise 13.10\nWe have seen many examples of symmetries in structured data, from transla-\ntions of objects within images and the permutation of node orderings on graphs, to\nrotations and translations of molecules in three-dimensional space. Capturing these\nsymmetries in the structure of a deep neural network is a powerful form of inductive"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2209, "text": "bias and forms the basis of a rich ﬁeld of research known asgeometric deep learning\n(Bronstein et al., 2017; Bronstein et al., 2021).\nExercises\n13.1 (?) Show that the permutation(A;B;C;D;E ) → (C;E;A;D;B ) corresponding to\nthe two choices of node ordering in Figure 13.2 can be expressed in the form (13.5)\nwith a permutation matrix given by (13.1).\n13.2 (??) Show that the number of edges connected to each node of a graph is given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2210, "text": "by the corresponding diagonal element of the matrix A2 where A is the adjacency\nmatrix.\n13.3 (?) Draw the graph whose adjacency matrix is given by\nA =\n\n\n0 1 1 0 1\n1 0 1 1 1\n1 1 0 1 0\n0 1 1 0 0\n1 1 0 0 0\n\n: (13.42)\n426 13. GRAPH NEURAL NETWORKS\n13.4 (??) Show that the effect of pre-multiplying a data matrix X using a permutation\nmatrix P deﬁned by (13.3) is to create a new data matrix ˜X given by (13.4) whose\nrows are permuted according to the permutation function \u0019(-)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2211, "text": "13.5 (??) Show that the transformed adjacency matrix ˜A deﬁned by (13.5), where P is\ndeﬁned by (13.3), is such that both the rows and the columns are permuted according\nto the permutation function \u0019(-)relative to the original adjacency matrix A.\n13.6 (??) In this exercise we write the update equations (13.16) as graph-level equations\nusing matrices. To keep the notation uncluttered, we omit the layer index l. First,\ngather the node-embedding vectors {hn}into an N ×Dmatrix H in which row n"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2212, "text": "is given by hT\nn. Then show that the neighbourhood-aggregated vectors zn given by\nzn =\n∑\nm∈N(n)\nhm (13.43)\ncan be written in matrix form as Z = AH where Z is the N ×Dmatrix in which\nrow nis given byzT\nn, and A is the adjacency matrix. Finally, show that the argument\nto the nonlinear activation function in (13.16) can be written in matrix form as\nAHWneigh + HWself + 1DbT (13.44)\nwhere 1D is the D-dimensional column vector in which all elements are 1."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2213, "text": "13.7 (??) By making use of the equivariance property (13.19) for layer lof a deep graph\nconvolutional network along with the permutation property (13.4) for the node vari-\nables, show that a complete deep graph convolutional network deﬁned by (13.18) is\nalso equivariant.\n13.8 (??) Explain why the aggregation function deﬁned by (13.24), in which the attention\nweights are given by (13.28), is equivariant under a reordering of the nodes in the\ngraph."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2214, "text": "graph.\n13.9 (?) Show that a graph attention network in which the graph is fully connected, so that\nthere is an edge between every pair of nodes, is equivalent to a standard transformer\narchitecture.\n13.10 (??) When a coordinate system is translated, the location of an object deﬁned by\nthat coordinate system is transformed using\n˜r = r + c (13.45)\nwhere c is a ﬁxed vector describing the translation. Similarly, if the coordinate sys-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2215, "text": "tem is rotated and/or mirror reﬂected, the location vector of an object is transformed\nusing\n˜r = Rr (13.46)\nwhere R is an orthogonal matrix whose inverse is given by its transpose so that\nRRT = RTR = I: (13.47)\nExercises 427\nUsing these properties, show that under translations, rotations, and reﬂections, the\nmessages in (13.38), (13.40), and (13.41) are invariant, and that the coordinate em-\nbeddings given by (13.39) are equivariant.\n14\nSampling"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2216, "text": "14\nSampling\nThere are many situations in deep learning where we need to create synthetic exam-\nples of a variable z from a probability distribution p(z). Here z might be a scalar\nand the distribution might be a univariate Gaussian, or z might be a high-resolution\nimage and p(z) might be a generative model deﬁned by a deep neural network. The\nprocess of creating such examples is known assampling, also known asMonte Carlo"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2217, "text": "sampling. For many simple distributions there are numerical techniques that gener-\nate suitable samples directly, whereas for more complex distributions, including ones\nthat are deﬁned implicitly, we may need more sophisticated approaches. We adopt\nthe convention of referring to each instantiated value as a sample, in contrast to the\nconvention used in classical statistics whereby ‘sample’ refers to a set of values.\nIn this chapter we focus on aspects of sampling that are most relevant to deep"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2218, "text": "learning. Further information on Monte Carlo methods more generally can be found\nin Gilks, Richardson, and Spiegelhalter (1996) and Robert and Casella (1999).\n429© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_14"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2219, "text": "430 14. SAMPLING\n14.1. Basic Sampling Algorithms\nIn this section, we explore a variety of relatively simple strategies for generating\nrandom samples from a given distribution. Because the samples will be generated\nby a computer algorithm, they will in fact be pseudo-random, that is, they will be\ncalculated using a deterministic algorithm but must nevertheless pass appropriate\ntests for randomness. Here we will assume that an algorithm has been provided"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2220, "text": "that generates pseudo-random numbers distributed uniformly over(0;1), and indeed\nmost software environments have such a facility built in.\n14.1.1 Expectations\nAlthough for some applications the samples themselves may be of direct inter-\nest, in other situations the goal is to evaluate expectations with respect to the distri-\nbution. Suppose we wish to ﬁnd the expectation of a function f(z) with respect to a\nprobability distribution p(z). Here, the components of z might comprise discrete or"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2221, "text": "continuous variables or some combination of the two. For continuous variables the\nexpectation is deﬁned by\nE[f] =\n∫\nf(z)p(z) dz (14.1)\nwhere the integral is replaced by summation for discrete variables. This is illus-\ntrated schematically for a single continuous variable inFigure 14.1. We will suppose\nthat such expectations are too complex to be evaluated exactly using analytical tech-\nniques.\nThe general idea behind sampling methods is to obtain a set of samples z(l)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2222, "text": "(where l = 1;:::;L ) drawn independently from the distribution p(z). This allows\nthe expectation (14.1) to be approximated by a ﬁnite sum:\nf = 1\nL\nL∑\nl=1\nf(z(l)): (14.2)\nIf the samples z(l) are drawn from the distribution p(z), then E[f] = E[f(z)] and so\nthe estimator f has the correct mean. We can also write this in the formExercise 14.1\nFigure 14.1 Schematic illustration of a function f(z)\nwhose expectation is to be evaluated\nwith respect to a distribution p(z).\np(z) f(z)\nz"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2223, "text": "with respect to a distribution p(z).\np(z) f(z)\nz\n14.1. Basic Sampling Algorithms 431\nE[f(z)] ≃ 1\nL\nL∑\nl=1\nf(z(l)) (14.3)\nwhere the symbol ≃denotes that the right-hand side is an unbiased estimator of the\nleft-hand side, that is the two sides are equal when averaged over the noise distribu-\ntion.\nThe variance of the estimator (14.2) is given byExercise 14.2\nvar[f] = 1\nLE\n[\n(f −E[f])2]\n; (14.4)\nwhich is the variance of the function f(z) under the distribution p(z). Note that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2224, "text": "linear decrease of this variance with increasingLdoes not depend on the dimension-\nality of z, and that, in principle, high accuracy may be achievable with a relatively\nsmall number of samples {z(l)}. The problem, however, is that the samples {z(l)}\nmight not be independent, and so the effective sample size might be much smaller\nthan the apparent sample size. Also, referring back to Figure 14.1, note that if f(z)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2225, "text": "is small in regions where p(z) is large and vice versa, then the expectation may be\ndominated by regions of small probability, implying that relatively large sample sizes\nwill be required to achieve sufﬁcient accuracy.\n14.1.2 Standard distributions\nWe now consider how to generate random numbers from simple nonuniform dis-\ntributions, assuming that we already have available a source of uniformly distributed\nrandom numbers. Suppose that z is uniformly distributed over the interval (0;1),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2226, "text": "and that we transform the values ofzusing some function g(-)so that y= g(z). The\ndistribution of ywill be governed bySection 2.4\np(y) = p(z)\n⏐⏐\n⏐⏐\ndz\ndy\n⏐⏐⏐⏐ (14.5)\nwhere, in this case, p(z) = 1. Our goal is to choose the function g(z) such that the\nresulting values of yhave some speciﬁc desired distribution p(y). Integrating (14.5)\nwe obtain\nz=\n∫y\n−∞\np(ˆy) ≡h(y) dˆy (14.6)\nwhich is the indeﬁnite integral of p(y). Thus, y = h−1 (z), and so we have toExercise 14.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2227, "text": "transform the uniformly distributed random numbers using a function that is the\ninverse of the indeﬁnite integral of the desired distribution. This is illustrated in\nFigure 14.2.\nConsider for example the exponential distribution\np(y) = \u0015exp(−\u0015y) (14.7)\nwhere 0 6 y <∞. In this case the lower limit of the integral in (14.6) is 0, and so\nh(y) = 1 −exp(−\u0015y). Thus, if we transform our uniformly distributed variable z\nusing y= −\u0015−1 ln(1 −z), then ywill have an exponential distribution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2228, "text": "432 14. SAMPLING\nFigure 14.2 Geometrical interpretation of the\ntransformation method for generating\nnon-uniformly distributed random\nnumbers. h(y) is the indeﬁnite integral\nof the desired distribution p(y). If a\nuniformly distributed random variable\nz is transformed using y = h−1(z),\nthen y will be distributed according to\np(y).\np(y)\nh(y)\ny0\n1\nAnother example of a distribution to which the transformation method can be\napplied is given by the Cauchy distribution\np(y) = 1\n\u0019\n1\n1 + y2 : (14.8)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2229, "text": "p(y) = 1\n\u0019\n1\n1 + y2 : (14.8)\nIn this case, the inverse of the indeﬁnite integral can be expressed in terms of thetan\nfunction.Exercise 14.4\nThe generalization to multiple variables involves the Jacobian of the change of\nvariables, so thatSection 2.4\np(y1;:::;y M) = p(z1;:::;z M)\n⏐⏐\n⏐⏐\n@(z1;:::;z M)\n@(y1;:::;y M)\n⏐⏐⏐⏐: (14.9)\nAs a ﬁnal example of the transformation technique, we consider the Box–Muller\nmethod for generating samples from a Gaussian distribution. First, suppose we gen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2230, "text": "erate pairs of uniformly distributed random numbers z1;z2 ∈(−1;1), which we can\ndo by transforming a variable distributed uniformly over (0;1) using z → 2z−1.\nNext we discard each pair unless it satisﬁes z2\n1 + z2\n2 6 1. This leads to a uniform\ndistribution of points inside the unit circle with p(z1;z2) = 1 =\u0019, as illustrated in\nFigure 14.3. Then, for each pair z1;z2 we evaluate the quantities\ny1 = z1\n(−2 lnr2\nr2\n)1=2\n(14.10)\ny2 = z2\n(−2 lnr2\nr2\n)1=2\n(14.11)\nwhere r2 = z2\n1 + z2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2231, "text": "(−2 lnr2\nr2\n)1=2\n(14.11)\nwhere r2 = z2\n1 + z2\n2. Then the joint distribution of y1 and y2 is given byExercise 14.5\np(y1;y2) = p(z1;z2)\n⏐⏐⏐⏐\n@(z1;z2)\n@(y1;y2)\n⏐\n⏐⏐⏐\n=\n[1\n√\n2\u0019exp(−y2\n1=2)\n][1√\n2\u0019exp(−y2\n2=2)\n]\n(14.12)\n14.1. Basic Sampling Algorithms 433\nFigure 14.3 The Box–Muller method for generating Gaussian-\ndistributed random numbers starts by generating samples\nfrom a uniform distribution inside the unit circle.\n−1−1\n1\n1z1\nz2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2232, "text": "−1−1\n1\n1z1\nz2\nand so y1 and y2 are independent and each has a Gaussian distribution with zero\nmean and unit variance.\nIf yhas a Gaussian distribution with zero mean and unit variance, then \u001by+ \u0016\nwill have a Gaussian distribution with mean \u0016and variance \u001b2. To generate vector-\nvalued variables having a multivariate Gaussian distribution with mean \u0016and co-\nvariance \u0006, we can make use of the Cholesky decomposition, which takes the form"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2233, "text": "\u0006 = LLT (Deisenroth, Faisal, and Ong, 2020). Then, if z is a random vector whose\ncomponents are independent and Gaussian distributed with zero mean and unit vari-\nance, then y = \u0016+ Lz will be Gaussian with mean \u0016and covariance \u0006.Exercise 14.6\nClearly, the transformation technique depends for its success on the ability to\ncalculate and then invert the indeﬁnite integral of the required distribution. Such\noperations are feasible only for a limited number of simple distributions, and so we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2234, "text": "must turn to alternative approaches in search of a more general strategy. Here we\nconsider two techniques called rejection sampling and importance sampling. Al-\nthough mainly limited to univariate distributions and thus not directly applicable to\ncomplex problems in many dimensions, they do form important components in more\ngeneral strategies.\n14.1.3 Rejection sampling\nThe rejection sampling framework allows us to sample from relatively complex"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2235, "text": "distributions, subject to certain constraints. We begin by considering univariate dis-\ntributions and subsequently discuss the extension to multiple dimensions.\nSuppose we wish to sample from a distributionp(z) that is not one of the simple,\nstandard distributions considered so far and that sampling directly from p(z) is dif-\nﬁcult. Furthermore suppose, as is often the case, that we are easily able to evaluate\np(z) for any given value of z, up to some normalizing constant Z, so that\np(z) = 1\nZp"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2236, "text": "p(z) = 1\nZp\n˜p(z) (14.13)\nwhere ˜p(z) can readily be evaluated, but Zp is unknown.\nTo apply rejection sampling, we need some simpler distributionq(z), sometimes\ncalled a proposal distribution, from which we can readily draw samples. We next\nintroduce a constant k whose value is chosen such that kq(z) > ˜p(z) for all val-\nues of z. The function kq(z) is called the comparison function and is illustrated\n434 14. SAMPLING\nFigure 14.4 In the rejection sampling\nmethod, samples are drawn"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2237, "text": "method, samples are drawn\nfrom a simple distribution\nq(z) and rejected if they fall\nin the grey area between the\nunnormalized distribution ep(z)\nand the scaled distribution\nkq(z). The resulting samples\nare distributed according to\np(z), which is the normalized\nversion of ep(z).\nz0 z\nu0\nkq(z0) kq(z)\nep(z)\nfor a univariate distribution in Figure 14.4. Each step of the rejection sampler in-\nvolves generating two random numbers. First, we generate a number z0 from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2238, "text": "distribution q(z). Next, we generate a number u0 from the uniform distribution over\n[0;kq(z0)]. This pair of random numbers has uniform distribution under the curve\nof the function kq(z). Finally, if u0 >˜p(z0) then the sample is rejected, otherwise\nu0 is retained. Thus, the pair is rejected if it lies in the grey shaded region in Fig-\nure 14.4. The remaining pairs then have uniform distribution under the curve of˜p(z),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2239, "text": "and hence the corresponding zvalues are distributed according to p(z), as desired.Exercise 14.7\nThe original values of zare generated from the distributionq(z), and these sam-\nples are then accepted with probability ˜p(z)=kq(z), and so the probability that a\nsample will be accepted is given by\np(accept) =\n∫\n{˜p(z)=kq(z)}q(z) dz\n= 1\nk\n∫\n˜p(z) dz: (14.14)\nThus, the fraction of points that are rejected by this method depends on the ratio of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2240, "text": "the area under the unnormalized distribution ˜p(z) to the area under the curve kq(z).\nWe therefore see that the constant k should be as small as possible subject to the\nlimitation that kq(z) must be nowhere less than ˜p(z).\nAs an illustration of the use of rejection sampling, consider the task of sampling\nfrom the gamma distribution\nGam(z|a;b) =baza−1 exp(−bz)\nΓ(a) ; (14.15)\nwhich, for a > 1, has a bell-shaped form, as shown in Figure 14.5. A suitable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2241, "text": "proposal distribution is therefore the Cauchy (14.8) because this too is bell-shaped\nand because we can use the transformation method, discussed earlier, to sample from\nit. We need to generalize the Cauchy slightly to ensure that it nowhere has a smaller\nvalue than the gamma distribution. This can be achieved by transforming a uniform\nrandom variable y using z = btan y+ c, which gives random numbers distributed\naccording toExercise 14.8\nq(z) = k\n1 + (z−c)2=b2 : (14.16)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2242, "text": "q(z) = k\n1 + (z−c)2=b2 : (14.16)\n14.1. Basic Sampling Algorithms 435\nFigure 14.5 Plot showing the gamma dis-\ntribution given by (14.15) as\nthe green curve, with a scaled\nCauchy proposal distribution\nshown by the red curve. Sam-\nples from the gamma distribu-\ntion can be obtained by sam-\npling from the Cauchy and\nthen applying the rejection\nsampling criterion.\nz\np(z)\n0 10 20 30\n0\n0.05\n0.1\n0.15\nThe minimum reject rate is obtained by settingc= a−1, and b2 = 2a−1 and choos-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2243, "text": "ing the constant k to be as small as possible while still satisfying the requirement\nkq(z) > ˜p(z). The resulting comparison function is also illustrated in Figure 14.5.\n14.1.4 Adaptive rejection sampling\nIn many instances where we might wish to apply rejection sampling, it can be\ndifﬁcult to determine a suitable analytic form for the envelope distribution q(z). An\nalternative approach is to construct the envelope function on the ﬂy based on mea-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2244, "text": "sured values of the distribution p(z) (Gilks and Wild, 1992). Constructing an enve-\nlope function is particularly straightforward whenp(z) is log concave, in other words\nwhen ln p(z) has derivatives that are non-increasing functions ofz. The construction\nof a suitable envelope function is illustrated graphically in Figure 14.6.\nThe function ln p(z) and its gradient are evaluated at some initial set of grid\npoints, and the intersections of the resulting tangent lines are used to construct the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2245, "text": "envelope function. Next a sample value is drawn from the envelope distribution.\nThis is straightforward because the log of the envelope distribution is a successionExercise 14.10\nof linear functions, and hence the envelope distribution itself comprises a piecewise\nexponential distribution of the form\nq(z) = ki\u0015iexp {−\u0015i(z−zi−1 )}; z i−1 <z 6 zi: (14.17)\nOnce a sample has been drawn, the usual rejection criterion can be applied. If the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2246, "text": "sample is accepted, then it will be a draw from the desired distribution. If, however,\nthe sample is rejected, then it is incorporated into the set of grid points, a new tangent\nline is computed, and the envelope function is thereby reﬁned. As the number of\ngrid points increases, so the envelope function becomes a better approximation of\nthe desired distribution p(z) and the probability of rejection decreases.\nThere is a variant of the algorithm exists that avoids the evaluation of derivatives"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2247, "text": "(Gilks, 1992). The adaptive rejection sampling framework can also be extended to\ndistributions that are not log concave, simply by following each rejection sampling\n436 14. SAMPLING\nFigure 14.6 In rejection sampling, if a dis-\ntribution is log concave then an\nenvelope function can be con-\nstructed using the tangent lines\ncomputed at a set of grid points.\nIf a sample point is rejected, it\nis added to the set of grid points\nand used to reﬁne the envelope\ndistribution.\nz1 z2 z3 z\nln p(z)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2248, "text": "distribution.\nz1 z2 z3 z\nln p(z)\nstep with a Metropolis–Hastings step (to be discussed in Section 14.2.3), giving rise\nto adaptive rejection Metropolis sampling (Gilks, Best, and Tan, 1995).\nFor rejection sampling to be of practical value, we require that the comparison\nfunction is close to the required distribution so that the rate of rejection is kept to a\nminimum. Now let us examine what happens when we try to use rejection sampling"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2249, "text": "in spaces of high dimensionality. Consider, for illustration, a somewhat artiﬁcial\nproblem in which we wish to sample from a zero-mean multivariate Gaussian distri-\nbution with covariance \u001b2\npI, where I is the unit matrix, by rejection sampling from a\nproposal distribution that is itself a zero-mean Gaussian distribution having covari-\nance \u001b2\nqI. Clearly, we must have \u001b2\nq > \u001b2\np to ensure that there exists a k such that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2250, "text": "p to ensure that there exists a k such that\nkq(z) > p(z). In D-dimensions, the optimum value of kis given by k= (\u001bq=\u001bp)D,\nas illustrated for D = 1 in Figure 14.7. The acceptance rate will be the ratio of\nvolumes under p(z) and kq(z), which, because both distributions are normalized, is\njust 1=k. Thus, the acceptance rate diminishes exponentially with dimensionality.\nEven if \u001bq exceeds \u001bp by just 1%, for D = 1;000 the acceptance ratio will be ap-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2251, "text": "proximately 1=20;000. In this illustrative example, the comparison function is close\nto the required distribution. For more practical examples, where the desired distri-\nbution may be multimodal and sharply peaked, it will be extremely difﬁcult to ﬁnd\nFigure 14.7 Illustrative example used to\nhighlight a limitation of rejec-\ntion sampling. Samples are\ndrawn from a Gaussian dis-\ntribution p(z) shown by the\ngreen curve, by using rejec-\ntion sampling from a proposal\ndistribution q(z) that is also"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2252, "text": "distribution q(z) that is also\nGaussian and whose scaled\nversion kq(z) is shown by the\nred curve.\nz\np(z)\n−5 0 5\n0\n0.25\n0.5\n14.1. Basic Sampling Algorithms 437\na good proposal distribution and comparison function. Furthermore, the exponential\ndecrease of the acceptance rate with dimensionality is a generic feature of rejection\nsampling. Although rejection can be a useful technique in one or two dimensions, it\nis unsuited to problems of high dimensionality. It can, however, play a role as a sub-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2253, "text": "routine in more sophisticated algorithms for sampling in high-dimensional spaces.\n14.1.5 Importance sampling\nOne reason for wishing to sample from complicated probability distributions is\nto evaluate expectations of the form (14.1). The technique of importance sampling\nprovides a framework for approximating expectations directly but does not itself\nprovide a mechanism for drawing samples from a distribution p(z).\nThe ﬁnite sum approximation to the expectation, given by (14.2), depends on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2254, "text": "being able to draw samples from the distribution p(z). Suppose, however, that it is\nimpractical to sample directly from p(z) but that we can evaluatep(z) easily for any\ngiven value of z. One simplistic strategy for evaluating expectations would be to\ndiscretize z-space into a uniform grid and to evaluate the integrand as a sum of the\nform\nE[f] ≃\nL∑\nl=1\np(z(l))f(z(l)): (14.18)\nAn obvious problem with this approach is that the number of terms in the summation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2255, "text": "grows exponentially with the dimensionality of z. Furthermore, as we have already\nnoted, the kinds of probability distributions of interest will often have much of their\nmass conﬁned to relatively small regions ofz-space and so uniform sampling will be\nvery inefﬁcient because in high-dimensional problems, only a very small proportion\nof the samples will make a signiﬁcant contribution to the sum. We would really\nlike to choose sample points from regions where p(z) is large or ideally where the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2256, "text": "product p(z)f(z) is large.\nAs with rejection sampling, importance sampling is based a proposal distribution\nq(z) from which it is easy to draw samples, as illustrated inFigure 14.8. We can then\nexpress the expectation in the form of a ﬁnite sum over samples {z(l)}drawn from\nq(z):\nE[f] =\n∫\nf(z)p(z) dz\n=\n∫\nf(z)p(z)\nq(z)q(z) dz\n≃ 1\nL\nL∑\nl=1\np(z(l))\nq(z(l))f(z(l)): (14.19)\nThe quantities rl = p(z(l))=q(z(l)) are known as importance weights, and they cor-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2257, "text": "rect the bias introduced by sampling from the wrong distribution. Note that, unlike\nrejection sampling, all the samples generated are retained.\nOften the distribution p(z) can be evaluated only up to a normalization constant,\nso that p(z) = ˜p(z)=Zp where ˜p(z) can be evaluated easily, whereasZp is unknown.\n438 14. SAMPLING\nFigure 14.8 Importance sampling addresses the\nproblem of evaluating the expectation of\na function f(z) with respect to a distri-\nbution p(z) from which it is difﬁcult to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2258, "text": "bution p(z) from which it is difﬁcult to\ndraw samples directly. Instead, samples\n{z(l)}are drawn from a simpler distribu-\ntion q(z), and the corresponding terms\nin the summation are weighted by the\nratios p(z(l))=q(z(l)).\np(z) f(z)\nz\nq(z)\nSimilarly, we may wish to use an importance sampling distributionq(z) = ˜q(z)=Zq,\nwhich has the same property. We then have\nE[f] =\n∫\nf(z)p(z) dz\n= Zq\nZp\n∫\nf(z)˜p(z)\n˜q(z)q(z) dz\n≃ Zq\nZp\n1\nL\nL∑\nl=1\n˜rlf(z(l)) (14.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2259, "text": "≃ Zq\nZp\n1\nL\nL∑\nl=1\n˜rlf(z(l)) (14.20)\nwhere ˜rl = ˜p(z(l))=˜q(z(l)). We can use the same sample set to evaluate the ratio\nZp=Zq with the result\nZp\nZq\n= 1\nZq\n∫\n˜p(z) dz =\n∫ ˜p(z)\n˜q(z)q(z) dz\n≃ 1\nL\nL∑\nl=1\n˜rl (14.21)\nand hence the expectation in (14.20) is given by a weighted sum:\nE[f] ≃\nL∑\nl=1\nwlf(z(l)) (14.22)\nwhere we have deﬁned\nwl = ˜rl∑\nm˜rm\n= ˜p(z(l))=q(z(l))∑\nm˜p(z(m))=q(z(m)): (14.23)\nNote that {wl}are non-negative numbers that sum to one."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2260, "text": "As with rejection sampling, the success of importance sampling depends cru-\ncially on how well the sampling distribution q(z) matches the desired distribution\np(z). If, as is often the case, p(z)f(z) is strongly varying and has a signiﬁcant pro-\nportion of its mass concentrated over relatively small regions of z-space, then the\n14.1. Basic Sampling Algorithms 439\nset of importance weights {rl}may be dominated by a few weights having large"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2261, "text": "values, with the remaining weights being relatively insigniﬁcant. Thus, the effective\nsample size can be much smaller than the apparent sample size L. The problem is\neven more severe if none of the samples falls in the regions wherep(z)f(z) is large.\nIn that case, the apparent variances of rl and rlf(z(l)) may be small even though\nthe estimate of the expectation may be severely wrong. Hence, a major drawback of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2262, "text": "importance sampling is its potential to produce results that are arbitrarily in error and\nwith no diagnostic indication. This also highlights a key requirement for the sam-\npling distribution q(z), namely that it should not be small or zero in regions where\np(z) may be signiﬁcant.\n14.1.6 Sampling-importance-resampling\nThe rejection sampling method discussed in Section 14.1.3 depends in part for\nits success on the determination of a suitable value for the constant k. For many"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2263, "text": "pairs of distributions p(z) and q(z), it will be impractical to determine a suitable\nvalue for kas any value that is sufﬁciently large to guarantee a bound on the desired\ndistribution will lead to impractically small acceptance rates.\nAs with rejection sampling, the sampling-importance-resampling approach also\nmakes use of a sampling distributionq(z) but avoids having to determine the constant\nk. There are two stages to the scheme. In the ﬁrst stage, Lsamples z(1);:::; z(L) are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2264, "text": "drawn from q(z). Then in the second stage, weights w1;:::;w L are constructed us-\ning (14.23). Finally, a second set ofLsamples is drawn from the discrete distribution\n(z(1);:::; z(L)) with probabilities given by the weights (w1;:::;w L).\nThe resulting Lsamples are only approximately distributed according to p(z),\nbut the distribution becomes correct in the limit L →∞. To see this, consider the\nunivariate case, and note that the cumulative distribution of the resampled values is\ngiven by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2265, "text": "given by\np(z6 a) =\n∑\nl:z(l)6a\nwl\n=\n∑\nlI(z(l) 6 a)˜p(z(l))=q(z(l))\n∑\nl˜p(z(l))=q(z(l)) (14.24)\nwhere I(:) is the indicator function (which equals 1 if its argument is true and 0\notherwise). Taking the limit L →∞ and assuming suitable regularity of the dis-\ntributions, we can replace the sums by integrals weighted according to the original\n440 14. SAMPLING\nsampling distribution q(z):\np(z6 a) =\n∫\nI(z6 a) {˜p(z)=q(z)}q(z) dz\n∫\n{˜p(z)=q(z)}q(z) dz\n=\n∫\nI(z6 a)˜p(z) dz\n∫\n˜p(z) dz\n=\n∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2266, "text": "=\n∫\nI(z6 a)˜p(z) dz\n∫\n˜p(z) dz\n=\n∫\nI(z6 a)p(z) dz; (14.25)\nwhich is the cumulative distribution function of p(z). Again, we see that normaliza-\ntion of p(z) is not required.\nFor a ﬁnite value of Land a given initial sample set, the resampled values will\nonly approximately be drawn from the desired distribution. As with rejection sam-\npling, the approximation improves as the sampling distribution q(z) gets closer to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2267, "text": "the desired distribution p(z). When q(z) = p(z), the initial samples (z(1);:::; z(L))\nhave the desired distribution and the weightswn = 1=L, so that the resampled values\nalso have the desired distribution.\nIf moments with respect to the distribution p(z) are required, then they can be\nevaluated directly using the original samples together with the weights, because\nE[f(z)] =\n∫\nf(z)p(z) dz\n=\n∫\nf(z)[˜p(z)=q(z)]q(z) dz\n∫\n[˜p(z)=q(z)]q(z) dz\n≃\nL∑\nl=1\nwlf(zl): (14.26)\n14.2. Markov Chain Monte Carlo"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2268, "text": "wlf(zl): (14.26)\n14.2. Markov Chain Monte Carlo\nIn the previous section, we discussed the rejection sampling and importance sam-\npling strategies for evaluating expectations of functions, and we saw that they suffer\nfrom severe limitations particularly in spaces of high dimensionality. We therefore\nturn in this section to a very general and powerful framework called Markov chain\nMonte Carlo, which allows sampling from a large class of distributions and which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2269, "text": "scales well with the dimensionality of the sample space. Markov chain Monte Carlo\nmethods have their origins in physics (Metropolis and Ulam, 1949), and it was only\n14.2. Markov Chain Monte Carlo 441\ntowards the end of the 1980s that they started to have a signiﬁcant impact in the ﬁeld\nof statistics.\nAs with rejection and importance sampling, we again sample from a proposal\ndistribution. This time, however, we maintain a record of the current state z(\u001c),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2270, "text": "and the proposal distribution q(z|z(\u001c)) is conditioned on this current state, and so\nthe sequence of samples z(1);z(2);::: forms a Markov chain. Again, if we writeSection 14.2.2\np(z) = ˜p(z)=Zp, we will assume that ˜p(z) can readily be evaluated for any given\nvalue of z, although the value of Zp may be unknown. The proposal distribution\nis chosen to be sufﬁciently simple that it is straightforward to draw samples from it"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2271, "text": "directly. At each cycle of the algorithm, we generate a candidate sample z? from\nthe proposal distribution and then accept the sample according to an appropriate\ncriterion.\n14.2.1 The Metropolis algorithm\nIn the basic Metropolis algorithm (Metropolis et al., 1953), we assume that the\nproposal distribution is symmetric, that is q(zA|zB) = q(zB|zA) for all values of\nzA and zB. The candidate sample is then accepted with probability\nA(z?;z(\u001c)) = min\n(\n1; ˜p(z?)\n˜p(z(\u001c))\n)\n: (14.27)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2272, "text": "A(z?;z(\u001c)) = min\n(\n1; ˜p(z?)\n˜p(z(\u001c))\n)\n: (14.27)\nThis can be achieved by choosing a random numberuwith uniform distribution over\nthe unit interval (0;1) and then accepting the sample if A(z?;z(\u001c)) > u. Note that\nif the step from z(\u001c) to z? causes an increase in the value of p(z), then the candidate\npoint is certain to be kept.\nIf the candidate sample is accepted, then z(\u001c+1) = z?, otherwise the candidate\npoint z? is discarded, z(\u001c+1) is set to z(\u001c), and another candidate sample is drawn"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2273, "text": "from the distribution q(z|z(\u001c+1)). This is in contrast to rejection sampling, where re-\njected samples are simply discarded. In the Metropolis algorithm, when a candidate\npoint is rejected, the previous sample is included in the ﬁnal list of samples, leading\nto multiple copies of samples. Of course, in a practical implementation, only a single\ncopy of each retained sample would be kept, along with an integer weighting factor"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2274, "text": "recording how many times that state appears. As we will see, ifq(zA|zB) is positive\nfor any values of zA and zB (this is a sufﬁcient but not necessary condition), the\ndistribution of z(\u001c) tends to p(z) as \u001c →∞. It should be emphasized, however, that\nthe sequence z(1);z(2);::: is not a set of independent samples from p(z) because\nsuccessive samples are highly correlated. If we wish to obtain independent samples,\nthen we can discard most of the sequence and just retain every Mth sample. For"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2275, "text": "M sufﬁciently large, the retained samples will for all practical purposes be inde-\npendent. The Metropolis algorithm in summarized in Algorithm 14.1. Figure 14.9\nshows a simple illustrative example of sampling from a two-dimensional Gaussian\ndistribution using the Metropolis algorithm in which the proposal distribution is an\nisotropic Gaussian.\nFurther insight into the nature of Markov chain Monte Carlo algorithms can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2276, "text": "gleaned by looking at the properties of a speciﬁc example, namely a simple random\n442 14. SAMPLING\nAlgorithm 14.1: Metropolis sampling\nInput: Unnormalized distribution ˜p(z)\nProposal distribution q(z|ˆz)\nInitial state z(0)\nNumber of iterations T\nOutput: z ∼˜p(z)\nzprev ← z(0)\n// Iterative message-passing\nfor \u001c ∈{1;:::;T }do\nz? ∼q(z|zprev) // Sample from proposal distribution\nu∼U(0;1) // Sample from uniform\nif ˜p(z?) =˜p(zprev) >u then\nzprev ← z? // z(\u001c) = z?\nelse\nzprev ← zprev // z(\u001c) = z(\u001c−1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2277, "text": "else\nzprev ← zprev // z(\u001c) = z(\u001c−1)\nend if\nend for\nreturn zprev // z(T)\nwalk. Consider a state space zconsisting of the integers, with probabilities\np(z(\u001c+1) = z(\u001c)) = 0 :5 (14.28)\np(z(\u001c+1) = z(\u001c) + 1) = 0 :25 (14.29)\np(z(\u001c+1) = z(\u001c) −1) = 0 :25 (14.30)\nwhere z(\u001c) denotes the state at step\u001c. If the initial state isz(0) = 0, then by symmetry\nthe expected state at time \u001c will also be zero E[z(\u001c)] = 0, and similarly it is easily"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2278, "text": "seen that E[(z(\u001c))2] = \u001c=2. Thus, after \u001c steps, the random walk has travelledExercise 14.11\nonly a distance that on average is proportional to the square root of \u001c. This square\nroot dependence is typical of random walk behaviour and shows that random walks\nare very inefﬁcient in exploring the state space. As we will see, a central goal in\ndesigning Markov chain Monte Carlo methods is to avoid random walk behaviour.\n14.2.2 Markov chains"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2279, "text": "14.2.2 Markov chains\nBefore discussing Markov chain Monte Carlo methods in more detail, it is useful\nto study some general properties of Markov chains. In particular, we ask under what\ncircumstances will a Markov chain converge to the desired distribution. A ﬁrst-order\n14.2. Markov Chain Monte Carlo 443\nFigure 14.9 A simple illustration using the\nMetropolis algorithm to sam-\nple from a Gaussian distri-\nbution whose one standard-\ndeviation contour is shown by\nthe ellipse. The proposal dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2280, "text": "the ellipse. The proposal dis-\ntribution is an isotropic Gaus-\nsian distribution whose stan-\ndard deviation is 0.2. Steps\nthat are accepted are shown\nas green lines, and rejected\nsteps are shown in red. A\ntotal of 150 candidate sam-\nples are generated, of which\n43 are rejected.\n0 0.5 1 1.5 2 2.5 3\n0\n0.5\n1\n1.5\n2\n2.5\n3\nMarkov chain is deﬁned to be a series of random variables z(1);:::; z(M) such that\nthe following conditional independence property holds for m∈{1;:::;M −1}:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2281, "text": "p(z(m+1)|z(1);:::; z(m)) = p(z(m+1)|z(m)); (14.31)\nwhich can be represented as a directed graphical model in the form of a chain. WeFigure 11.29\ncan then specify the Markov chain by giving the probability distribution for the ini-\ntial variable p(z(0)) together with the conditional distributions for subsequent vari-\nables in the form of transition probabilities Tm(z(m);z(m+1)) ≡p(z(m+1)|z(m)). A\nMarkov chain is called homogeneous if the transition probabilities are the same for\nall m."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2282, "text": "all m.\nThe marginal probability for a particular variable can be expressed in terms of\nthe marginal probability for the previous variable in the chain:\np(z(m+1)) =\n∫\np(z(m+1)|z(m))p(z(m)) dz(m) (14.32)\nwhere the integral is replaced by a summation for discrete variables. A distribution\nis said to be invariant, or stationary, with respect to a Markov chain if each step in\nthe chain leaves that distribution invariant. Thus, for a homogeneous Markov chain"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2283, "text": "with transition probabilities T(z′;z), the distribution p?(z) is invariant if\np?(z) =\n∫\nT(z′;z)p?(z′) dz′: (14.33)\nNote that a given Markov chain may have more than one invariant distribution. For\ninstance, if the transition probabilities are given by the identity transformation, then\nany distribution will be invariant.\n444 14. SAMPLING\nA sufﬁcient (but not necessary) condition for ensuring that the required distribu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2284, "text": "tion p(z) is invariant is to choose the transition probabilities to satisfy the property\nof detailed balance, deﬁned by\np?(z)T(z;z′) = p?(z′)T(z′;z) (14.34)\nfor the particular distribution p?(z). It is easily seen that a transition probability\nthat satisﬁes detailed balance with respect to a particular distribution will leave that\ndistribution invariant, because\n∫\np?(z′)T(z′;z) dz′=\n∫\np?(z)T(z;z′) dz′ (14.35)\n= p?(z)\n∫\np(z′|z) dz′ (14.36)\n= p?(z): (14.37)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2285, "text": "= p?(z)\n∫\np(z′|z) dz′ (14.36)\n= p?(z): (14.37)\nA Markov chain that respects detailed balance is said to be reversible.\nOur goal is to use Markov chains to sample from a given distribution. We can\nachieve this if we set up a Markov chain such that the desired distribution is invariant.\nHowever, we must also require that for m→∞, the distribution p(z(m)) converges\nto the required invariant distribution p?(z), irrespective of the choice of initial dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2286, "text": "tribution p(z(0)). This property is called ergodicity, and the invariant distribution\nis then called the equilibrium distribution. Clearly, an ergodic Markov chain can\nhave only one equilibrium distribution. It can be shown that a homogeneous Markov\nchain will be ergodic, subject only to weak restrictions on the invariant distribution\nand the transition probabilities (Neal, 1993).\nIn practice we often construct the transition probabilities from a set of ‘base’"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2287, "text": "transitions B1;:::;B K. This can be achieved through a mixture distribution of the\nform\nT(z′;z) =\nK∑\nk=1\n\u000bkBk(z′;z) (14.38)\nfor some set of mixing coefﬁcients \u000b1;:::;\u000b K satisfying \u000bk > 0 and ∑\nk\u000bk = 1.\nAlternatively, the base transitions may be combined through successive application,\nso that\nT(z′;z) =\n∑\nz1\n:::\n∑\nzn−1\nB1(z′;z1) :::B K−1 (zK−2 ;zK−1 )BK(zK−1 ;z): (14.39)\nIf a distribution is invariant with respect to each of the base transitions, then clearly it"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2288, "text": "will also be invariant with respect to either of theT(z′;z) given by (14.38) or (14.39).\nFor the mixture (14.38), if each of the base transitions satisﬁes detailed balance, then\nthe mixture transition T will also satisfy detailed balance. This does not hold for the\ntransition probability constructed using (14.39), although by symmetrizing the order\nof application of the base transitions, namely B1;B2;:::;B K;BK;:::;B 2;B1, de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2289, "text": "tailed balance can be restored. A common example of the use of composite transition\nprobabilities is where each base transition changes only a subset of the variables.\n14.2. Markov Chain Monte Carlo 445\n14.2.3 The Metropolis–Hastings algorithm\nEarlier we introduced the basic Metropolis algorithm without actually demon-\nstrating that it samples from the required distribution. Before giving a proof, we ﬁrst\ndiscuss a generalization, known as the Metropolis–Hastings algorithm (Hastings,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2290, "text": "1970), which applies when the proposal distribution is no longer a symmetric func-\ntion of its arguments. In particular at step \u001c of the algorithm, in which the current\nstate is z(\u001c), we draw a sample z? from the distribution qk(z|z(\u001c)) and then accept it\nwith probability Ak(z?;z(\u001c)) where\nAk(z?;z(\u001c)) = min\n(\n1; ˜p(z?)qk(z(\u001c)|z?)\n˜p(z(\u001c))qk(z?|z(\u001c))\n)\n: (14.40)\nHere klabels the members of the set of possible transitions being considered. Again,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2291, "text": "evaluating the acceptance criterion does not require knowledge of the normalizing\nconstant Zp in the probability distribution p(z) = ˜p(z)=Zp. For a symmetric pro-\nposal distribution, the Metropolis–Hastings criterion (14.40) reduces to the standard\nMetropolis criterion given by (14.27). Metropolis–Hastings sampling is summarized\nin Algorithm 14.2.\nWe can show that p(z) is an invariant distribution of the Markov chain deﬁned"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2292, "text": "by the Metropolis–Hastings algorithm by showing that detailed balance, deﬁned by\n(14.34), is satisﬁed. Using (14.40) we have\np(z)qk(z′|z)Ak(z′;z) = min (p(z)q k(z′|z);p(z′)qk(z|z′))\n= min ( p(z′)qk(z|z′);p(z)qk(z′|z))\n= p(z′)qk(z|z′)Ak(z;z′) (14.41)\nas required.\nThe speciﬁc choice of proposal distribution can have a marked effect on the per-\nformance of the algorithm. For continuous state spaces, a common choice is a Gaus-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2293, "text": "sian centred on the current state, leading to an important trade-off in determining the\nvariance parameter of this distribution. If the variance is small, then the proportion of\naccepted transitions will be high, but progress through the state space takes the form\nof a slow random walk leading to long correlation times. However, if the variance\nparameter is large, then the rejection rate will be high because, in the kind of com-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2294, "text": "plex problems we are considering, many of the proposed steps will be to states for\nwhich the probability p(z) is low. Consider a multivariate distribution p(z) having\nstrong correlations between the components of z, as illustrated in Figure 14.10. The\nscale \u001aof the proposal distribution should be as large as possible without incurring\nhigh rejection rates. This suggests that \u001ashould be of the same order as the smallest"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2295, "text": "length scale \u001bmin. The system then explores the distribution along the more extended\ndirection by means of a random walk, and so the number of steps to arrive at a state\nthat is more or less independent of the original state is of order(\u001bmax=\u001bmin)2. In fact\nin two dimensions, the increase in rejection rate as \u001aincreases is offset by the larger\nstep sizes of those transitions that are accepted, and more generally for a multivari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2296, "text": "ate Gaussian, the number of steps required to obtain independent samples scales like\n446 14. SAMPLING\nAlgorithm 14.2: Metropolis-Hastings sampling\nInput: Unnormalized distribution ˜p(z)\nProposal distributions {qk(z|ˆz) : k∈1;:::;K }\nMapping from iteration index to distribution index M(-)\nInitial state z(0)\nNumber of iterations T\nOutput: z ∼˜p(z)\nzprev ← z(0)\n// Iterative message-passing\nfor \u001c ∈{1;:::;T }do\nk← M(\u001c) // get distribution index for this iteration"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2297, "text": "z? ∼qk(z|zprev) // sample from proposal distribution\nu∼U(0;1) // sample from uniform\nif ˜p(z?)q(zprev|z?) =˜p(zprev)q(z?|zprev) >u then\nzprev ← z? // z(\u001c) = z?\nelse\nzprev ← zprev // z(\u001c) = z(\u001c−1)\nend if\nend for\nreturn zprev // z(T)\n(\u001bmax=\u001b2)2 where \u001b2 is the second-smallest standard deviation (Neal, 1993). These\ndetails aside, if the length scales over which the distributions vary are very different\nin different directions, then the Metropolis Hastings algorithm can have very slow\nconvergence."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2298, "text": "convergence.\n14.2.4 Gibbs sampling\nGibbs sampling (Geman and Geman, 1984) is a simple and widely applica-\nble Markov chain Monte Carlo algorithm and can be seen as a special case of the\nMetropolis–Hastings algorithm. Consider the distribution p(z) = p(z1;:::;z M)\nfrom which we wish to sample, and suppose that we have chosen some initial state\nfor the Markov chain. Each step of the Gibbs sampling procedure involves replacing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2299, "text": "the value of one of the variables by a value drawn from the distribution of that vari-\nable conditioned on the values of the remaining variables. Thus, we replace zi by\na value drawn from the distribution p(zi|z\\i), where zi denotes the ith component\nof z, and z\\idenotes {z1;:::;z M}but with zi omitted. This procedure is repeated\neither by cycling through the variables in some particular order or by choosing the\n14.2. Markov Chain Monte Carlo 447"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2300, "text": "14.2. Markov Chain Monte Carlo 447\nFigure 14.10 Schematic illustration of using an isotropic\nGaussian proposal distribution (blue circle) to\nsample from a correlated multivariate Gaus-\nsian distribution (red ellipse) having very differ-\nent standard deviations in different directions,\nusing the Metropolis–Hastings algorithm. To\nkeep the rejection rate low, the scale \u001a of\nthe proposal distribution should be of the or-\nder of the smallest standard deviation \u001bmin,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2301, "text": "der of the smallest standard deviation \u001bmin,\nwhich leads to random walk behaviour in which\nthe number of steps separating states that\nare approximately independent is of order\n(\u001bmax=\u001bmin)2 where \u001bmax is the largest stan-\ndard deviation.\n\u001bmax\n\u001bmin\n\u001a\nvariable to be updated at each step at random from some distribution.\nFor example, suppose we have a distribution p(z1;z2;z3) over three variables,\nand at step \u001c of the algorithm we have selected values z(\u001c)\n1 ;z(\u001c)\n2 , and z(\u001c)\n3 . We ﬁrst"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2302, "text": "1 ;z(\u001c)\n2 , and z(\u001c)\n3 . We ﬁrst\nreplace z(\u001c)\n1 by a new value z(\u001c+1)\n1 obtained by sampling from the conditional distri-\nbution\np(z1|z(\u001c)\n2 ;z(\u001c)\n3 ): (14.42)\nNext we replace z(\u001c)\n2 by a value z(\u001c+1)\n2 obtained by sampling from the conditional\ndistribution\np(z2|z(\u001c+1)\n1 ;z(\u001c)\n3 ) (14.43)\nso that the new value forz1 is used straight away in subsequent sampling steps. Then\nwe update z3 with a sample z(\u001c+1)\n3 drawn from\np(z3|z(\u001c+1)\n1 ;z(\u001c+1)\n2 ) (14.44)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2303, "text": "3 drawn from\np(z3|z(\u001c+1)\n1 ;z(\u001c+1)\n2 ) (14.44)\nand so on, cycling through the three variables in turn. Gibbs sampling is summarized\nin Algorithm 14.3.\nTo show that this procedure samples from the required distribution, we ﬁrst note\nthat the distributionp(z) is an invariant of each of the Gibbs sampling steps individu-\nally and hence of the whole Markov chain. This follows since when we sample from\np(zi|z\\i), the marginal distribution p(z\\i) is clearly invariant because the value of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2304, "text": "z\\iis unchanged. Also, each step by deﬁnition samples from the correct conditional\ndistribution p(zi|z\\i). Because these conditional and marginal distributions together\nspecify the joint distribution, we see that the joint distribution is itself invariant.\nThe second requirement to be satisﬁed to ensure that the Gibbs sampling proce-\ndure samples from the correct distribution is that it is ergodic. A sufﬁcient condition"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2305, "text": "for ergodicity is that none of the conditional distributions are anywhere zero. If this\nis the case, then any point in z-space can be reached from any other point in a ﬁnite\nnumber of steps involving one update of each of the component variables. If this\nrequirement is not satisﬁed, so that some of the conditional distributions have zeros,\nthen ergodicity, if it applies, must be proven explicitly.\n448 14. SAMPLING\nAlgorithm 14.3: Gibbs sampling\nInput: Initial values {zi : i∈1;:::;M }"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2306, "text": "Input: Initial values {zi : i∈1;:::;M }\nConditional distributions {p(zi|{zj̸=i}) :i∈1;:::;M }\nNumber of iterations T\nOutput: Final values {zi : i∈1;:::;M }\nfor \u001c ∈{1;:::;T }do\nfor i∈{1;:::;M }do\nzi ∼p(zi|{zj̸=i})\nend for\nend for\nreturn {zi : i∈1;:::;M }\nThe distribution of initial states must also be speciﬁed to complete the algorithm,\nalthough samples drawn after many iterations will effectively become independent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2307, "text": "of this distribution. Of course, successive samples from the Markov chain will be\nhighly correlated, and so to obtain samples that are nearly independent it will be\nnecessary to sub-sample the sequence.\nWe can obtain the Gibbs sampling procedure as a particular instance of the\nMetropolis–Hastings algorithm as follows. Consider a Metropolis–Hastings sam-\npling step involving the variable zk in which the remaining variables z\\k remain"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2308, "text": "ﬁxed, and for which the transition probability from z to z? is given by qk(z?|z) =\np(z?\nk|z\\k). Note that z?\n\\k = z\\k because these components are unchanged by the\nsampling step. Also, p(z) = p(zk|z\\k)p(z\\k). Thus, the factor that determines the\nacceptance probability in the Metropolis–Hastings (14.40) is given by\nA(z?;z) = p(z?)qk(z|z?)\np(z)qk(z?|z) =\np(z?\nk|z?\n\\k)p(z?\n\\k)p(zk|z?\n\\k)\np(zk|z\\k)p(z\\k)p(z?\nk|z\\k) = 1 (14.45)\nwhere we have used z?"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2309, "text": "k|z\\k) = 1 (14.45)\nwhere we have used z?\n\\k = z\\k. Thus, the Metropolis–Hastings steps are always\naccepted.\nAs with the Metropolis algorithm, we can gain some insight into the behaviour of\nGibbs sampling by investigating its application to a Gaussian distribution. Consider\na correlated Gaussian in two variables, as illustrated in Figure 14.11, having con-\nditional distributions of width land marginal distributions of width L. The typical"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2310, "text": "step size is governed by the conditional distributions and will be of order l. Because\nthe state evolves according to a random walk, the number of steps needed to obtain\nindependent samples from the distribution will be of order (L=l)2. Of course if the\nGaussian distribution were uncorrelated, then the Gibbs sampling procedure would\nbe optimally efﬁcient. For this simple problem, we could rotate the coordinate sys-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2311, "text": "tem such that the new variables are uncorrelated. However, in practical applications\n14.2. Markov Chain Monte Carlo 449\nFigure 14.11 Illustration of Gibbs sampling by al-\nternate updates of two variables\nwhose distribution is a correlated\nGaussian. The step size is governed\nby the standard deviation of the con-\nditional distribution (green curve),\nand is O(l), leading to slow progress\nin the direction of elongation of the\njoint distribution (red ellipse). The\nnumber of steps needed to obtain an"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2312, "text": "number of steps needed to obtain an\nindependent sample from the distri-\nbution is O((L=l)2).\nz1\nz2\nL\nl\nit will generally be infeasible to ﬁnd such transformations.\nOne approach to reducing the random walk behaviour in Gibbs sampling is\ncalled over-relaxation (Adler, 1981). In its original form, it applies to problems for\nwhich the conditional distributions are Gaussian, which represents a more general\nclass of distributions than the multivariate Gaussian because, for example, the non-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2313, "text": "Gaussian distribution p(z;y) ∝exp(−z2y2) has Gaussian conditional distributions.\nAt each step of the Gibbs sampling algorithm, the conditional distribution for a par-\nticular component zi has some mean \u0016i and some variance \u001b2\ni. In the over-relaxation\nframework, the value of zi is replaced with\nz′\ni = \u0016i + \u000bi(zi −\u0016i) + \u001bi(1 −\u000b2\ni)1=2\u0017 (14.46)\nwhere \u0017 is a Gaussian random variable with zero mean and unit variance, and \u000b\nis a parameter such that −1 < \u000b <1. For \u000b = 0, the method is equivalent to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2314, "text": "standard Gibbs sampling, and for \u000b< 0 the step is biased to the opposite side of the\nmean. This step leaves the desired distribution invariant because if zi has mean \u0016i\nand variance \u001b2\ni, then so too does z′\ni. The effect of over-relaxation is to encourageExercise 14.14\ndirected motion through state space when the variables are highly correlated. The\nframework of ordered over-relaxation(Neal, 1999) generalizes this approach to non-\nGaussian distributions."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2315, "text": "Gaussian distributions.\nThe practical applicability of Gibbs sampling depends on the ease with which\nsamples can be drawn from the conditional distributions p(zk|z\\k). For probability\ndistributions speciﬁed using directed graphical models, the conditional distributions\nfor individual nodes depend only on the variables in the corresponding Markov blan-\nket, as illustrated in Figure 14.12. For directed graphs, a wide choice of conditional"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2316, "text": "distributions for the individual nodes conditioned on their parents will lead to condi-\ntional distributions for Gibbs sampling that are log concave. The adaptive rejection\nsampling methods discussed in Section 14.1.4 therefore provide a framework for\nMonte Carlo sampling from directed graphs with broad applicability.\n450 14. SAMPLING\nFigure 14.12 The Gibbs sampling method requires samples\nto be drawn from the conditional distribution\nof a variable z conditioned on the remaining"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2317, "text": "of a variable z conditioned on the remaining\nvariables. For directed graphical models, this\nconditional distribution is a function of only the\nstates of the nodes in the Markov blanket,\nshaded in blue, which comprises the parents,\nthe children, and the co-parents.\nz\nBecause the basic Gibbs sampling technique considers one variable at a time,\nthere are strong dependencies between successive samples. At the opposite extreme,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2318, "text": "if we could draw samples directly from the joint distribution (an operation that we\nare supposing is intractable), then successive samples would be independent. We\ncan hope to improve on the simple Gibbs sampler by adopting an intermediate strat-\negy in which we sample successively from groups of variables rather than individual\nvariables. This is achieved in the blocking Gibbs sampling algorithm by choosing\nblocks of variables, not necessarily disjoint, and then sampling jointly from the vari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2319, "text": "ables in each block in turn, conditioned on the remaining variables (Jensen, Kong,\nand Kjaerulff, 1995).\n14.2.5 Ancestral sampling\nFor many models, the joint distribution p(z) is conveniently speciﬁed in terms\nof a graphical model. For a directed graph with no observed variables, it is straight-\nforward to sample from the joint distribution using the followingancestral sampling\napproach. The joint distribution is speciﬁed by\np(z) =\nM∏\ni=1\np(zi|pa(i)) (14.47)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2320, "text": "p(z) =\nM∏\ni=1\np(zi|pa(i)) (14.47)\nwhere zi are the set of variables associated with node i, and pa(i) denotes the set\nof variables associated with the parents of node i. To obtain a sample from the joint\ndistribution, we make one pass through the set of variables in the order z1;:::; zM\nsampling from the conditional distributions p(zi|pa(i)). This is always possible\nbecause at each step, all the parent values will have been instantiated. After one pass"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2321, "text": "through the graph, we will have obtained a sample from the joint distribution. This\nassumes that it is possible to sample from the individual conditional distributions at\neach node.\nNow consider a directed graph in which some of the nodes, which comprise the\nevidence set E, are instantiated with observed values. We can in principle extend\nthe above procedure, at least for nodes representing discrete variables, to give the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2322, "text": "following logic sampling approach (Henrion, 1988), which can be seen as a special\ncase of importance sampling. At each step, when a sampled value is obtained for aSection 14.1.5\nvariable zi whose value is observed, the sampled value is compared to the observed\nvalue, and if they agree then the sample value is retained and the algorithm proceeds\nto the next variable in turn. However, if the sampled value and the observed value"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2323, "text": "disagree, then the whole sample so far is discarded and the algorithm starts again\n14.3. Langevin Sampling 451\nwith the ﬁrst node in the graph. This algorithm samples correctly from the posterior\ndistribution because it corresponds simply to drawing samples from the joint distri-\nbution of hidden variables and data variables and then discarding those samples that\ndisagree with the observed data (with the slight saving of not continuing with the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2324, "text": "sampling from the joint distribution as soon as one contradictory value is observed).\nHowever, the overall probability of accepting a sample from the posterior decreases\nrapidly as the number of observed variables increases and as the number of states that\nthose variables can take increases, and so this approach is rarely used in practice.\nAn improvement on this approach is called likelihood weighted sampling (Fung"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2325, "text": "and Chang, 1990; Shachter and Peot, 1990). It is based on ancestral sampling com-\nbined with importance sampling. For each variable in turn, if that variable is in the\nevidence set, then it is just set to its instantiated value. If it is not in the evidence set,\nthen it is sampled from the conditional distribution p(zi|pa(i)) in which the condi-\ntioning variables are set to their currently sampled values. The weighting associated\nwith the resulting sample z is then given byExercise 14.15\nr(z) ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2326, "text": "r(z) =\n∏\nzi̸∈e\np(zi|pa(i))\np(zi|pa(i))\n∏\nzi∈e\np(zi|pa(i))\n1 =\n∏\nzi∈e\np(zi|pa(i)): (14.48)\nThis method can be further extended using self-importance sampling (Shachter and\nPeot, 1990) in which the importance sampling distribution is continually updated to\nreﬂect the current estimated posterior distribution.\n14.3. Langevin Sampling\nThe Metropolis–Hastings algorithm draws samples from a probability distribution\nby creating a Markov chain of candidate samples using a proposal distribution and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2327, "text": "then accepting or rejecting them using the criterion (14.40). This can be relatively\ninefﬁcient since the proposal distribution is often a simple, ﬁxed distribution that can\ngenerate updates in any direction in the data space, leading to a random walk.\nWe have seen that when training neural networks, it is hugely advantageous to\nmake use of the gradient of the log likelihood with respect to the learnable param-\neters of the model in order to maximize the likelihood function. By analogy, we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2328, "text": "can introduce Markov chain sampling algorithms that make use of the gradient of\nthe probability density with respect to the data vector so as to take steps that pref-\nerentially move towards regions of higher probability. One such technique is called\nHamiltonian Monte Carlo, also known as hybrid Monte Carlo. This again makes\nuse of a Metropolis acceptance test (Duaneet al., 1987; Bishop, 2006). Here we will\nfocus on a different approach that is widely used in deep learning, called Langevin"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2329, "text": "sampling. Although it avoids the use of an acceptance test, the algorithm has to be\ndesigned carefully to ensure that the resulting samples are unbiased. An important\napplication of Langevin sampling arises in the context of machine learning models\ndeﬁned in terms of energy functions.\n452 14. SAMPLING\n14.3.1 Energy-based models\nMany generative models can be expressed as conditional probability distribu-\ntions p(x|w) where x is the data vector and w represents a vector of learnable pa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2330, "text": "rameters. Such models can be trained by maximizing the corresponding likelihood\nfunction deﬁned with respect to a training data set. However, to represent a valid\nprobability distribution, the model must satisfy\n∫\np(x|w)p(x) dx = 1: (14.49)\nEnsuring that this requirement is met can signiﬁcantly limit the allowable forms\nfor the model. If we put aside the normalization constraint then we can consider\na much broader class of models called energy-based models (LeCun et al., 2006)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2331, "text": "Suppose we have a function E(x;w), called the energy function, which is a real-\nvalued function of its arguments but which has no other constraints. The exponential\nexp{−E(x;w)}is a non-negative quantity and can therefore be viewed as an un-\nnormalized probability distribution over x. Here the introduction of the minus sign\nin the exponent is simply a convention, and it means that higher values of energy cor-\nrespond to lower values of probability. We can then deﬁne a normalized distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2332, "text": "using\np(x|w) = 1\nZ(w) exp {−E(x;w)} (14.50)\nwhere the normalizing constant Z(w), known as the partition function, is deﬁned byExercise 14.16\nZ(w) =\n∫\nexp {−E(x;w)}dx: (14.51)\nThe energy function is often modelled using a deep neural network with input vector\nx and a scalar output E(x;w), where w represents the weights and biases in the\nnetwork.\nNote that the partition function depends on w, which creates problems for train-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2333, "text": "ing. For example, the log likelihood function for a data set D= (x1;:::; xN) of\ni.i.d. data has the formExercise 14.17\nln p(D|w) = −\nN∑\nn=1\nE(xn;w) −Nln Z(w): (14.52)\nTo compute the gradient of ln p(D|w) with respect to w, we need to know the form\nof Z(w). However, for many choices of the energy function E(x;w), it will be\nimpractical to evaluate the partition function in (14.51) because this involves inte-\ngrating (or summing for discrete variables) over all the whole of x-space. The term"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2334, "text": "‘energy-based model’ is generally used for models where this integral is intractable.\nNote, however, that probabilistic models can be seen as special cases of energy-based\nmodels, and therefore many of the models discussed in this book can be viewed as\nenergy-based models. The big advantage of energy-based models, therefore, is their\nﬂexibility in that they bypass the requirement for normalization. A corresponding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2335, "text": "disadvantage, however, is that since the normalizing constant is unknown, they can\nbe more difﬁcult to train.\n14.3. Langevin Sampling 453\n14.3.2 Maximizing the likelihood\nVarious approximation methods have been developed to train energy-based mod-\nels without having to evaluate the partition function (Song and Kingma, 2021). Here\nwe look at techniques based on Markov chain Monte Carlo. An alternative approach,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2336, "text": "called score matching, will be discussed in the context of diffusion models.Chapter 20\nWe have seen that for energy-based models, the likelihood function cannot be\nevaluated explicitly due to the unknown partition function Z(w). However, we can\nmake use of Monte Carlo sampling methods to approximate the gradient of the log\nlikelihood with respect to the model parameters. Once an energy-based model has\nbeen trained, by whatever means, we also need a way to draw samples from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2337, "text": "model, and again we can make use of Monte Carlo methods.\nUsing (14.50), the gradient, with respect to the model parameters, of the log\nlikelihood function for an energy-based model can be written in the form\n∇w ln p(x|w) = −∇wE(x;w) −∇w ln Z(w): (14.53)\nThis is the likelihood function for a single data point x, but in practice we want to\nmaximize the likelihood deﬁned over a training set of data points drawn from some"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2338, "text": "unknown distribution pD(x). If we assume the data points are i.i.d., then we can\nconsider the gradient of the expectation of the log likelihood with respect to pD(x),\nwhich is then given by\nEx∼pD[∇w ln p(x|w)] = −Ex∼pD[∇wE(x;w)] −∇w ln Z(w) (14.54)\nwhere we have made use of the fact that the ﬁnal term−∇w ln Z(w) does not depend\non x and can therefore be taken outside the expectation. The partition functionZ(w)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2339, "text": "is assumed to be unknown, but we can make use of (14.51) and rearrange to obtainExercise 14.18\n−∇w ln Z(w) =\n∫\n{∇wE(x;w)}p(x|w) dx: (14.55)\nThe right-hand side of (14.55) corresponds to an expectation over the model distri-\nbution p(x|w) given by\n∫\n{∇wE(x;w)}p(x|w) dx = Ex∼M[∇wE(x;w)] : (14.56)\nCombining (14.54), (14.55), and (14.56) we obtainExercise 14.18\n∇wEx∼pD[ln p(x|w)] = −Ex∼pD[∇wE(x;w)]\n+ Ex∼pM(x) [∇wE(x;w)] : (14.57)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2340, "text": "+ Ex∼pM(x) [∇wE(x;w)] : (14.57)\nThis result is illustrated inFigure 14.13, and has a nice interpretation, as follows. Our\ngoal is to ﬁnd values for the parametersw that maximize the likelihood function, and\ntherefore consider a small change tow in the direction of the gradient∇w ln p(x|w).\nFrom (14.57) we see that expected value of this gradient can be expressed as two\nterms, having opposite signs. The ﬁrst term on the right-hand side of (14.57) acts\n454 14. SAMPLING\nx\npD(x)\npM(x)E(x, w)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2341, "text": "454 14. SAMPLING\nx\npD(x)\npM(x)E(x, w)\nFigure 14.13 Illustration of the training of an energy-based model by maximizing the likelihood, show-\ning the energy function E(x;w) in green along with the associated model distribution\npM(x) and the true data distribution pD(x). Increasing the expected log likelihood by us-\ning (14.57) pushes the energy function up at points corresponding to samples from the\nmodel (shown as blue dots) and pushes it down at points corresponding to samples from"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2342, "text": "the data set (shown as red dots).\nto decrease E(x;w), and therefore to increase the probability density deﬁned by\nthe model, for points x drawn from pD(x). The second term on the right-hand\nside of (14.57) acts to increase the value of E(x;w), and therefore to decrease the\nprobability density deﬁned by the model, for data points drawn from the model itself.\nIn regions where the model density exceeds the training data density, the net effect"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2343, "text": "will be to increase the energy and therefore reduce the probability. Conversely, in\nregions where training data density exceeds the model density, the net effect will be\nto reduce the energy and therefore increase the probability density. Together these\ntwo terms move probability mass away from regions where there is a low density of\ntraining data and towards regions of high data density, as desired. The two terms will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2344, "text": "be equal in magnitude when the model distribution matches the data distribution, at\nwhich point the gradient on the left-hand-side of (14.57) will equal zero.\n14.3.3 Langevin dynamics\nWhen applying (14.57) as a practical training method, we need to approximate\nthe two terms on the right-hand side. For any given value of x, we can evaluate\n∇wE(x;w) using automatic differentiation. For the ﬁrst term in (14.57), we can\nuse the training data set to estimate the expectation over x:"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2345, "text": "Ex∼pD[∇wE(x;w)] ≃ 1\nN\nN∑\nn=1\n∇wE(xn;w): (14.58)\n14.3. Langevin Sampling 455\nThe second term is more challenging because we need to draw samples from the\nmodel distribution deﬁned by an energy function whose corresponding partition\nfunction is intractable. This can be done using Markov chain Monte Carlo methods.\nOne popular approach is called stochastic gradient Langevin dynamics or simply\nLangevin sampling (Parisi, 1981; Welling and Teh, 2011). This term depends on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2346, "text": "the distribution p(x|w) only through the score function, which is deﬁned to be the\ngradient of the log likelihood with respect to the data vector x, and is given by\ns(x;w) = ∇x ln p(x|w): (14.59)\nIt is worth emphasising that this gradient is taken with respect to the data pointx and\nis therefore not the usual gradient with respect to the learnable parameters w. If we\nsubstitute (14.50) into (14.59) we obtain\ns(x;w) = −∇xE(x;w) (14.60)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2347, "text": "s(x;w) = −∇xE(x;w) (14.60)\nwhere we see that the partition function no longer appears at it is independent of x.\nWe start by drawing an initial value x(0) from a prior distribution, and then we\niterate the following Markov chain steps:\nx(\u001c+1) = x(\u001c) + \u0011∇x ln p(x(\u001c);w) +\n√\n2\u0011\u000f(\u001c); \u001c ∈1;:::; T (14.61)\nwhere \u000f(\u001c) ∼N (0;I) are independent samples from a zero-mean, unit-covariance\nGaussian distribution, and the parameter\u0011controls the step size. Each iteration of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2348, "text": "Langevin equation takes a step in the direction of the gradient of the log likelihood,\nand then adds Gaussian noise. It can be show that, in the limits of \u0011 → 0 and\nT → ∞, the value of z(T) is an independent sample from the distribution p(x).\nLangevin sampling is summarized in Algorithm 14.4.\nWe can repeat the process to generate a set of samples {x1;:::; xM}and then\napproximate the second term in (14.57) using\nEx∼pM(x) [∇wE(x;w)] ≃ 1\nM\nM∑\nm=1\n∇wE(xm;w): (14.62)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2349, "text": "M\nM∑\nm=1\n∇wE(xm;w): (14.62)\nRunning long Markov chains to generate independent samples can be compu-\ntationally expensive, and so we need to consider practical approximations. One ap-\nproach is called contrastive divergence (Hinton, 2002). Here the samples used to\nevaluate (14.62) are obtained by running a Monte Carlo chain starting with one of\nthe training data points xn. If the chain is run for a large number of steps, then the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2350, "text": "resulting value will be essentially an unbiased sample from the model distribution.\nInstead Hinton (2002) proposes running for only a few steps of Monte Carlo, perhaps\neven as few as one step, which is computationally much less costly. The resulting\nsample will be far from unbiased and will lie close to the data manifold. As a result,\nthe effect of using gradient descent will be to shape the energy surface, and hence"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2351, "text": "the probability density, only in the neighbourhood of the data manifold. This can\nprove effective for tasks such as discrimination but is expected to be less effective in\nlearning a generative model.\n456 14. SAMPLING\nAlgorithm 14.4: Langevin sampling\nInput: Initial value x(0)\nProbability density p(x;w)\nLearning rate parameter \u0011\nNumber of iterations T\nOutput: Final value x(T)\nx ← x0\nfor \u001c ∈{1;:::;T }do\n\u000f∼N(\u000f|0;I)\nx ← x + \u0011∇x ln p(x;w) + √2\u0011\u000f\nend for\nreturn x // Final value x(T)\nExercises"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2352, "text": "end for\nreturn x // Final value x(T)\nExercises\n14.1 (?) Show that f deﬁned by (14.2) is an unbiased estimator, in other words that the\nexpectation of the right-hand side is equal to E[f(z)].\n14.2 (?) Show that f deﬁned by (14.2) has variance given by (14.4).\n14.3 (?) Suppose that zis a random variable with uniform distribution over(0;1) and that\nwe transform z using y = h−1 (z) where h(y) is given by (14.6). Show that y has\nthe distribution p(y)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2353, "text": "the distribution p(y).\n14.4 (??) Given a random variablezthat is uniformly distributed over(0;1), ﬁnd a trans-\nformation y= f(z) such that yhas a Cauchy distribution given by (14.8).\n14.5 (??) Suppose that z1 and z2 are uniformly distributed over the unit circle, as shown in\nFigure 14.3, and that we make the change of variables given by (14.10) and (14.11).\nShow that (y1;y2) will be distributed according to (14.12)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2354, "text": "14.6 (??) Let z be a D-dimensional random variable having a Gaussian distribution with\nzero mean and unit covariance matrix, and suppose that the positive deﬁnite sym-\nmetric matrix \u0006 has the Cholesky decomposition \u0006 = LLT, where L is a lower-\ntriangular matrix (i.e., one with zeros above the leading diagonal). Show that the\nvariable y = \u0016+ Lz has a Gaussian distribution with mean \u0016and covariance \u0006.\nThis provides a technique for generating samples from a general multivariate Gaus-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2355, "text": "sian using samples from a univariate Gaussian having zero mean and unit variance.\n14.7 (??) In this exercise, we show more carefully that rejection sampling does indeed\ndraw samples from the desired distribution p(z). Suppose the proposal distribution\nis q(z). Show that the probability of a sample value z being accepted is given by\nExercises 457\n˜p(z)=kq(z) where ˜p is any unnormalized distribution that is proportional to p(z),"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2356, "text": "and the constant k is set to the smallest value that ensures kq(z) > ˜p(z) for all\nvalues of z. Note that the probability of drawing a valuez is given by the probability\nof drawing that value from q(z) times the probability of accepting that value given\nthat it has been drawn. Make use of this, along with the sum and product rules of\nprobability, to write down the normalized form for the distribution overz, and show\nthat it equals p(z)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2357, "text": "that it equals p(z).\n14.8 (?) Suppose that z has a uniform distribution over the interval [0;1]. Show that the\nvariable y= btan z+ chas a Cauchy distribution given by (14.16).\n14.9 (??) Determine expressions for the coefﬁcientskiin the envelope distribution (14.17)\nfor adaptive rejection sampling using the requirements of continuity and normaliza-\ntion.\n14.10 (??) By making use of the technique discussed in Section 14.1.2 for sampling from a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2358, "text": "single exponential distribution, devise an algorithm for sampling from the piecewise\nexponential distribution deﬁned by (14.17).\n14.11 (?) Show that the simple random walk over the integers deﬁned by (14.28), (14.29),\nand (14.30) has the property that E[(z(\u001c))2] = E[(z(\u001c−1) )2] + 1=2 and hence by\ninduction that E[(z(\u001c))2] = \u001c=2.\n14.12 (??) Show that the Gibbs sampling algorithm, discussed in Section 14.2.4, satisﬁes\ndetailed balance as deﬁned by (14.34)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2359, "text": "detailed balance as deﬁned by (14.34).\n14.13 (?) Consider the distribution shown in Figure 14.14. Discuss whether the standard\nGibbs sampling procedure for this distribution is ergodic and therefore whether it\nwould sample correctly from this distribution\nFigure 14.14 A probability distribution over two vari-\nables z1 and z2 that is uniform over\nthe shaded regions and zero everywhere\nelse.\nz1\nz2\n14.14 (?) Verify that the over-relaxation update (14.46), in whichzi has mean \u0016i and vari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2360, "text": "ance \u001bi and where \u0017 has zero mean and unit variance gives a value z′\ni with mean \u0016i\nand variance \u001b2\ni.\n458 14. SAMPLING\n14.15 (?) Show that in likelihood weighted sampling from a directed graph the importance\nsampling weights are given by (14.48).\n14.16 (?) Show that the distribution (14.50) is normalized with respect tox provided Z(w)\nsatisﬁes (14.51).\n14.17 (??) By making use of (14.50) show that the gradient of the log likelihood function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2361, "text": "for an energy-based model can be written in the form (14.52).\n14.18 (??) By making use of (14.54), (14.55), and (14.56), show that the gradient of the\nlog likelihood function for an energy-based model can be written in the form (14.57).\n15\nDiscrete\nLatent Variables\nWe have seen how complex distributions can be constructed by combining multi-\nple simple distributions and how the resulting models can be described by directed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2362, "text": "graphs. In addition to the observed variables, which form part of the data set, suchChapter 11\nmodels often introduce additional hidden, or latent, variables. These might corre-\nspond to speciﬁc quantities involved in the data generation process, such as the un-\nknown orientation of an object in three-dimensional space in the case of images, or\nthey may be introduced simply as modelling constructs to allow much richer models"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2363, "text": "to be created. If we deﬁne a joint distribution over observed and latent variables, the\ncorresponding distribution of the observed variables alone is obtained by marginal-\nization. This allows relatively complex marginal distributions over observed vari-\nables to be expressed in terms of more tractable joint distributions over the expanded\nspace of observed and latent variables.\nIn this chapter, we will see that marginalizing over discrete latent variables gives"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2364, "text": "459© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_15"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2365, "text": "460 15. DISCRETE LATENT V ARIABLES\nrise to mixture distributions. Our focus will be on mixtures of Gaussians that pro-\nvide a good illustration of mixture distributions and that are also widely used in\nmachine learning. One simple application for mixture models is to discover clusters\nin data, and we begin our discussion by considering a technique for clustering called\nthe K-means algorithm, which corresponds to a particular non-probabilistic limit of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2366, "text": "Gaussian mixtures. Then we introduce the latent-variable view of mixture distribu-\ntions in which the discrete latent variables can be interpreted as deﬁning assignments\nof data points to speciﬁc components of the mixture.\nA general technique for ﬁnding maximum likelihood estimators in latent-variable\nmodels is the expectation–maximization (EM) algorithm. We ﬁrst use the Gaussian\nmixture distribution to motivate the EM algorithm in an informal way, and then we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2367, "text": "give a more careful treatment based on the latent-variable viewpoint. Finally we pro-\nvide a general perspective by introducing the evidence lower bound (ELBO), which\nwill play an important role in generative models such as variational autoencoders\nand diffusion models.\n15.1.\nK-means Clustering\nWe begin by considering the problem of identifying groups, or clusters, of data points\nin a multi-dimensional space. Suppose we have a data set{x1;:::; xN}consisting of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2368, "text": "Nobservations of aD-dimensional Euclidean variablex. Our goal is to partition the\ndata set into some number Kof clusters, where we will suppose for the moment that\nthe value ofKis given. Intuitively, we might think of a cluster as comprising a group\nof data points whose inter-point distances are small compared with the distances to\npoints outside the cluster. We can formalize this notion by ﬁrst introducing a set\nof D-dimensional vectors \u0016k, where k = 1;:::;K , in which \u0016k is a ‘prototype’"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2369, "text": "associated with the kth cluster. As we will see shortly, we can think of the \u0016k as\nrepresenting the centres of the clusters. Our goal is then to ﬁnd a set of cluster\nvectors {\u0016k}, along with an assignment of data points to clusters, such that the sum\nof the squares of the distances of each data point to its closest cluster vector \u0016k is a\nminimum.\nIt is convenient at this point to deﬁne some notation to describe the assignment"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2370, "text": "of data points to clusters. For each data point xn, we introduce a corresponding set\nof binary indicator variables rnk ∈{0;1}, where k = 1;:::;K . These indicators\ndescribe which of the Kclusters the data point xn is assigned to, so that if data point\nxn is assigned to cluster kthen rnk = 1, and rnj = 0 for j ̸=k. This is an example\nof the 1-of-Kcoding scheme. We can then deﬁne an error function:\nJ =\nN∑\nn=1\nK∑\nk=1\nrnk∥xn −\u0016k∥2; (15.1)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2371, "text": "J =\nN∑\nn=1\nK∑\nk=1\nrnk∥xn −\u0016k∥2; (15.1)\nwhich represents the sum of the squares of the distances of each data point to its\nassigned vector \u0016k. Our goal is to ﬁnd values for the {rnk}and the {\u0016k}so as to\nminimize J. We can do this through an iterative procedure in which each iteration\n15.1. K-means Clustering 461\ninvolves two successive steps corresponding to successive optimizations with respect\nto the {rnk}and the {\u0016k}. First we choose some initial values for the {\u0016k}. Then"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2372, "text": "in the ﬁrst step, we minimize J with respect to the {rnk}, keeping the {\u0016k}ﬁxed.\nIn the second step, we minimize J with respect to the {\u0016k}, keeping {rnk}ﬁxed.\nThis two-step optimization is then repeated until convergence. We will see that these\ntwo stages of updating {rnk}and updating {\u0016k}correspond, respectively, to the\nE (expectation) and M (maximization) steps of the EM algorithm, and to emphasizeSection 15.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2373, "text": "this, we will use the terms E step and M step in the context of theK-means algorithm.\nConsider ﬁrst the determination of the {rnk}with the {\u0016k}held ﬁxed (the E\nstep). Because J in (15.1) is a linear function of the {rnk}, this optimization can be\nperformed easily to give a closed-form solution. The terms involving different nare\nindependent, and so we can optimize for each nseparately by choosing rnk to be 1\nfor whichever value of kgives the minimum value of ∥xn −\u0016k∥2. In other words,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2374, "text": "we simply assign the nth data point to the closest cluster centre. More formally, this\ncan be expressed as\nrnk =\n{\n1; if k= arg minj∥xn −\u0016j∥2;\n0; otherwise: (15.2)\nNow consider the optimization of the {\u0016k}with the {rnk}held ﬁxed (the M\nstep). The objective functionJis a quadratic function of\u0016k, and it can be minimized\nby setting its derivative with respect to \u0016k to zero giving\n2\nN∑\nn=1\nrnk(xn −\u0016k) = 0; (15.3)\nwhich we can easily solve for \u0016k to give\n\u0016k =\n∑\nnrnkxn\n∑\nnrnk\n: (15.4)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2375, "text": "\u0016k =\n∑\nnrnkxn\n∑\nnrnk\n: (15.4)\nThe denominator in this expression is equal to the number of points assigned to\ncluster k, and so this result has a simple interpretation, namely that \u0016k is equal\nto the mean of all the data points xn assigned to cluster k. For this reason, the\nprocedure is known as the K-means algorithm (Lloyd, 1982). It is summarized in\nAlgorithm 15.1. Because the assignments {rnk}are discrete and each iteration will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2376, "text": "not lead to an increase in the error function, the K-means algorithm is guaranteed to\nconverge in a ﬁnite number of steps.Exercise 15.1\nThe two phases of reassigning data points to clusters and recomputing the cluster\nmeans are repeated in turn until there is no further change in the assignments (or until\nsome maximum number of iterations is exceeded). However, this approach may\nconverge to a local rather than a global minimum of J. The convergence properties"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2377, "text": "of the K-means algorithm were studied by MacQueen (1967).\nThe K-means algorithm is illustrated in Figure 15.1 using data derived from\neruptions of the Old Faithful geyser in Yellowstone National Park. The data setSection 3.2.9\nconsists of 272 data points, each of which gives the duration of an eruption on the\n462 15. DISCRETE LATENT V ARIABLES\nAlgorithm 15.1: K-means algorithm\nInput: Initial prototype vectors \u00161;:::; \u0016K\nData set x1;:::; xN\nOutput: Final prototype vectors \u00161;:::; \u0016K"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2378, "text": "Output: Final prototype vectors \u00161;:::; \u0016K\n{rnk ← 0}// Initially set all assignments to zero\nrepeat\n{r(old)\nnk }←{r nk}\n// Update assignments\nfor N ∈{1;:::;N }do\nk← arg minj∥xn −\u0016j∥2\nrnk ← 1\nrnj ← 0; j ∈{1;:::;K }; j̸=k\nend for\n// Update prototype vectors\nfor k∈{1;:::;K }do\n\u0016k ← ∑\nnrnkxn=∑\nnrnk\nend for\nuntil {rnk}= {r(old)\nnk }// Assignments unchanged\nreturn \u00161;:::; \u0016K;{rnk}\nhorizontal axis and the time to the next eruption on the vertical axis. Here we have"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2379, "text": "made a linear re-scaling of the data, known as standardizing, such that each of the\nvariables has zero mean and unit standard deviation.\nFor this example, we have chosen K = 2 and so the assignment of each data\npoint to the nearest cluster centre is equivalent to a classiﬁcation of the data points\naccording to which side they lie of the perpendicular bisector of the two cluster\ncentres. A plot of the cost function J given by (15.1) for the Old Faithful example is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2380, "text": "shown in Figure 15.2. Note that we have deliberately chosen poor initial values for\nthe cluster centres so that the algorithm takes several steps before convergence. In\npractice, a better initialization procedure would be to choose the cluster centres\u0016k to\nbe equal to a random subset ofKdata points. Also note that the K-means algorithm\nis often used to initialize the parameters in a Gaussian mixture model before applying\nthe EM algorithm.Section 15.2.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2381, "text": "the EM algorithm.Section 15.2.2\nSo far, we have considered a batch version ofK-means in which the whole data\nset is used together to update the prototype vectors. We can also derive a sequential\nupdate in which, for each data point xn in turn, we update the nearest prototype \u0016k\nusingExercise 15.2\n15.1. K-means Clustering 463\n(a)\n−2 0 2\n−2\n0\n2\n(b)\n−2 0 2\n−2\n0\n2\n(c)\n−2 0 2\n−2\n0\n2\n(d)\n−2 0 2\n−2\n0\n2\n(e)\n−2 0 2\n−2\n0\n2\n(f)\n−2 0 2\n−2\n0\n2\n(g)\n−2 0 2\n−2\n0\n2\n(h)\n−2 0 2\n−2\n0\n2\n(i)\n−2 0 2\n−2\n0\n2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2382, "text": "−2 0 2\n−2\n0\n2\n(h)\n−2 0 2\n−2\n0\n2\n(i)\n−2 0 2\n−2\n0\n2\nFigure 15.1 Illustration of the K-means algorithm using the re-scaled Old Faithful data set. (a) Green points\ndenote the data set in a two-dimensional Euclidean space. The initial choices for centres \u00161 and \u00162 are shown\nby the red and blue crosses, respectively. (b) In the initial E step, each data point is assigned either to the red"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2383, "text": "cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the\npoints according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta\nline, they lie. (c) In the subsequent M step, each cluster centre is recomputed to be the mean of the points\nassigned to the corresponding cluster. (d)–(i) show successive E and M steps through to ﬁnal convergence of\nthe algorithm.\n464 15. DISCRETE LATENT V ARIABLES"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2384, "text": "the algorithm.\n464 15. DISCRETE LATENT V ARIABLES\nFigure 15.2 Plot of the cost function J\ngiven by (15.1) after each\nE step (blue points) and M\nstep (red points) of the K-\nmeans algorithm for the ex-\nample shown in Figure 15.1.\nThe algorithm has converged\nafter the third M step, and\nthe ﬁnal EM cycle produces\nno changes in either the as-\nsignments or the prototype\nvectors.\nJ\n1 2 3 4\n0\n500\n1000\n\u0016new\nk = \u0016old\nk + 1\nNk\n(xn −\u0016old\nk ) (15.5)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2385, "text": "1000\n\u0016new\nk = \u0016old\nk + 1\nNk\n(xn −\u0016old\nk ) (15.5)\nwhere Nk is the number of data points that have so far been used to update\u0016k. This\nallows each data point to be used once and then discarded before seeing the next data\npoint.\nOne notable feature of the K-means algorithm is that at each iteration, every\ndata point is assigned to one, and only one, of the clusters. Although some data\npoints will be much closer to a particular centre \u0016k than to any other centre, there"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2386, "text": "may be other data points that lie roughly midway between cluster centres. In the\nlatter case, it is not clear that the hard assignment to the nearest cluster is the most\nappropriate. We will see that by adopting a probabilistic approach, we obtain ‘soft’Section 15.2\nassignments of data points to clusters in a way that reﬂects the level of uncertainty\nover the most appropriate assignment. This probabilistic formulation has numerous\nbeneﬁts.\n15.1.1 Image segmentation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2387, "text": "beneﬁts.\n15.1.1 Image segmentation\nAs an illustration of the application of the K-means algorithm, we consider\nthe related problems of image segmentation and image compression. The goal of\nsegmentation is to partition an image into regions such that each region has a rea-\nsonably homogeneous visual appearance or which corresponds to objects or parts\nof objects (Forsyth and Ponce, 2003). Each pixel in an image is a point in a three-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2388, "text": "dimensional space comprising the intensities of the red, blue, and green channels,\nand our segmentation algorithm simply treats each pixel in the image as a sepa-\nrate data point. Note that strictly this space is not Euclidean because the channel\nintensities are bounded by the interval [0;1]. Nevertheless, we can apply the K-\nmeans algorithm without difﬁculty. We illustrate the result of running K-means to\nconvergence, for any particular value of K, by redrawing the image in which we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2389, "text": "replace each pixel vector with the {R;G;B}intensity triplet given by the centre \u0016k\nto which that pixel has been assigned. Results for various values of K are shown in\nFigure 15.3. We see that for a given value of K, the algorithm represents the image\n15.1. K-means Clustering 465\nK = 2\nK = 3\nK = 10\nOriginal image\nFigure 15.3 An example of the application of theK-means clustering algorithm to image segmentation showing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2390, "text": "an initial image together with their K-means segmentations obtained using various values of K. This also illus-\ntrates the use of vector quantization for data compression, in which smaller values ofKgive higher compression\nat the expense of poorer image quality.\nusing a palette of only Kcolours. It should be emphasized that this use of K-means\nis not a particularly sophisticated approach to image segmentation, not least because"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2391, "text": "it takes no account of the spatial proximity of different pixels. Image segmentation\nis in general extremely difﬁcult and remains the subject of active research and is\nintroduced here simply to illustrate the behaviour of the K-means algorithm.\nWe can also use a clustering algorithm to perform data compression. It is im-\nportant to distinguish between lossless data compression , in which the goal is to\nbe able to reconstruct the original data exactly from the compressed representation,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2392, "text": "and lossy data compression, in which we accept some errors in the reconstruction\nin return for higher levels of compression than can be achieved in the lossless case.\nWe can apply the K-means algorithm to the problem of lossy data compression as\nfollows. For each of the N data points, we store only the identity kof the cluster to\nwhich it is assigned. We also store the values of the K cluster centres {\u0016k}, which\ntypically requires signiﬁcantly less data, provided we choose K ≪ N. Each data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2393, "text": "point is then approximated by its nearest centre \u0016k. New data points can similarly\nbe compressed by ﬁrst ﬁnding the nearest \u0016k and then storing the label kinstead of\nthe original data vector. This framework is often called vector quantization, and the\nvectors {\u0016k}are called codebook vectors.\nThe image segmentation problem discussed above also provides an illustration\nof the use of clustering for data compression. Suppose the original image has N"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2394, "text": "pixels comprising {R;G;B }values, each of which is stored with 8 bits of precision.\nDirectly transmitting the whole image would cost 24N bits. Now suppose we ﬁrst\nrun K-means on the image data, and then instead of transmitting the original pixel\nintensity vectors, we transmit the identity of the nearest vector \u0016k. Because there\nare K such vectors, this requires log2 K bits per pixel. We must also transmit the\nK code book vectors {\u0016k}, which requires 24 K bits, and so the total number of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2395, "text": "bits required to transmit the image is 24K + Nlog2 K (rounding up to the nearest\n466 15. DISCRETE LATENT V ARIABLES\ninteger). The original image shown in Figure 15.3 has 240 ×180 = 43;200 pixels\nand so requires 24 ×43;200 = 1;036;800 bits to transmit directly. By comparison,\nthe compressed images require 43;248 bits (K = 2), 86;472 bits (K = 3), and\n173;040 bits (K= 10), respectively, to transmit. These represent compression ratios"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2396, "text": "compared to the original image of 4.2%, 8.3%, and 16.7%, respectively. We see that\nthere is a trade-off between the degree of compression and image quality. Note that\nour aim in this example is to illustrate theK-means algorithm. If we had been aiming\nto produce a good image compressor, then it would be more fruitful to consider small\nblocks of adjacent pixels, for instance5 ×5, and thereby exploit the correlations that\nexist in natural images between nearby pixels.\n15.2.\nMixtures of Gaussians"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2397, "text": "15.2.\nMixtures of Gaussians\nWe have previously motivated the Gaussian mixture model as a simple linear super-\nposition of Gaussian components, aimed at providing a richer class of density mod-\nels than a single Gaussian. We now turn to a formulation of Gaussian mixtures inSection 3.2.9\nterms of discrete latent variables. This will provide us with a deeper insight into this\nimportant distribution and will also serve to motivate the expectation–maximization\nalgorithm."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2398, "text": "algorithm.\nRecall from (3.111) that the Gaussian mixture distribution can be written as a\nlinear superposition of Gaussians in the form\np(x) =\nK∑\nk=1\n\u0019kN(x|\u0016k;\u0006k): (15.6)\nLet us introduce a K-dimensional binary random variable z having a 1-of-K repre-\nsentation in which one of the elements is equal to 1 and all other elements are equal\nto 0. The values of zk therefore satisfy zk ∈{0;1}and ∑\nkzk = 1, and we see that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2399, "text": "kzk = 1, and we see that\nthere are K possible states for the vector z according to which element is non-zero.\nWe will deﬁne the joint distribution p(x;z) in terms of a marginal distribution p(z)\nand a conditional distribution p(x|z). The marginal distribution over z is speciﬁed\nin terms of the mixing coefﬁcients \u0019k, such that\np(zk = 1) = \u0019k\nwhere the parameters {\u0019k}must satisfy\n0 6 \u0019k 6 1 (15.7)\ntogether with\nK∑\nk=1\n\u0019k = 1 (15.8)\n15.2. Mixtures of Gaussians 467"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2400, "text": "k=1\n\u0019k = 1 (15.8)\n15.2. Mixtures of Gaussians 467\nFigure 15.4 Graphical representation of a mixture model, in which the joint distribution is\nexpressed in the form p(x;z) = p(z)p(x|z). z\nx\nif they are to be valid probabilities. Because z uses a 1-of-K representation, we can\nalso write this distribution in the form\np(z) =\nK∏\nk=1\n\u0019zk\nk : (15.9)\nSimilarly, the conditional distribution of x given a particular value for z is a Gaus-\nsian:\np(x|zk = 1) = N(x|\u0016k;\u0006k);"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2401, "text": "sian:\np(x|zk = 1) = N(x|\u0016k;\u0006k);\nwhich can also be written in the form\np(x|z) =\nK∏\nk=1\nN(x|\u0016k;\u0006k)zk : (15.10)\nThe joint distribution is given byp(z)p(x|z)and is described by the graphical model\nin Figure 15.4. The marginal distribution of x is then obtained by summing the joint\ndistribution over all possible states of z to giveExercise 15.3\np(x) =\n∑\nz\np(z)p(x|z) =\nK∑\nk=1\n\u0019kN(x|\u0016k;\u0006k) (15.11)\nwhere we have made use of (15.9) and (15.10). Thus, the marginal distribution ofx is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2402, "text": "a Gaussian mixture of the form (15.6). If we have several observations x1;:::; xN,\nthen, because we have represented the marginal distribution in the form p(x) =∑\nz p(x;z), it follows that for every observed data pointxn there is a corresponding\nlatent variable zn.\nWe have therefore found an equivalent formulation of the Gaussian mixture in-\nvolving explicit latent variables. It might seem that we have not gained much by do-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2403, "text": "ing so. However, we are now able to work with the joint distribution p(x;z) instead\nof the marginal distribution p(x), and this will lead to signiﬁcant simpliﬁcations,\nmost notably through the introduction of the EM algorithm.\nAnother quantity that will play an important role is the conditional probability\nof z given x. We will use \r(zk) to denote p(zk = 1|x), whose value can be found\n468 15. DISCRETE LATENT V ARIABLES\nusing Bayes’ theorem:\n\r(zk) ≡p(zk = 1|x) = p(zk = 1)p(x|zk = 1)\nK∑\nj=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2404, "text": "(zk) ≡p(zk = 1|x) = p(zk = 1)p(x|zk = 1)\nK∑\nj=1\np(zj = 1)p(x|zj = 1)\n= \u0019kN(x|\u0016k;\u0006k)\nK∑\nj=1\n\u0019jN(x|\u0016j;\u0006j)\n: (15.12)\nWe will view \u0019k as the prior probability of zk = 1, and the quantity \r(zk) as the\ncorresponding posterior probability once we have observed x. As we will see later,\n\r(zk) can also be viewed as the responsibility that component ktakes for ‘explain-\ning’ the observationx.\nWe can use ancestral sampling to generate random samples distributed accordingSection 14.2.5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2405, "text": "to the Gaussian mixture model. To do this, we ﬁrst generate a value for z, which we\ndenote ˆz, from the marginal distribution p(z) and then generate a value for x from\nthe conditional distributionp(x|ˆz). We can depict samples from the joint distribution\np(x;z) by plotting points at the corresponding values of x and then colouring them\naccording to the value of z, in other words according to which Gaussian component"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2406, "text": "was responsible for generating them, as shown in Figure 15.5(a). Similarly samples\nfrom the marginal distributionp(x) are obtained by taking the samples from the joint\ndistribution and ignoring the values of z. These are illustrated in Figure 15.5(b) by\nplotting the x values without any coloured labels.\nWe can also use this synthetic data set to illustrate the ‘responsibilities’ by eval-\nuating, for every data point, the posterior probability for each component in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2407, "text": "mixture distribution from which this data set was generated. In particular, we can\nrepresent the value of the responsibilities \r(znk) associated with data point xn by\nplotting the corresponding point using proportions of red, blue, and green ink given\nby \r(znk) for k= 1;2;3, respectively, as shown in Figure 15.5(c). So, for instance,\na data point for which \r(zn1) = 1 will be coloured red, whereas one for which\n\r(zn2) = \r(zn3) = 0 :5 will be coloured with equal proportions of blue and green"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2408, "text": "ink and so will appear cyan. This should be compared with Figure 15.5(a) in which\nthe data points were labelled using the true identity of the component from which\nthey were generated.\n15.2.1 Likelihood function\nSuppose we have a data set of observations{x1;:::; xN}, and we wish to model\nthis data using a mixture of Gaussians. We can represent this data set as an N ×D\nmatrix X in which the nth row is given by xT\nn. From (15.6) the log of the likelihood\nfunction is given by\nln p(X|\u0019;\u0016;\u0006) =\nN∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2409, "text": "function is given by\nln p(X|\u0019;\u0016;\u0006) =\nN∑\nn=1\nln\n{ K∑\nk=1\n\u0019kN(xn|\u0016k;\u0006k)\n}\n: (15.13)\n15.2. Mixtures of Gaussians 469\n(a)\n0 0.5 1\n0\n0.5\n1\n(b)\n0 0.5 1\n0\n0.5\n1\n(c)\n0 0.5 1\n0\n0.5\n1\nFigure 15.5 Example of 500 points drawn from the mixture of three Gaussians shown in Figure 3.8. (a) Sam-\nples from the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2410, "text": "of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal dis-\ntribution p(x), which is obtained by simply ignoring the values of z and just plotting the x values. The data set\nin (a) is said to be complete, whereas that in (b) is incomplete, as discussed further in Section 15.3. (c) The\nsame samples in which the colours represent the value of the responsibilities \r(znk) associated with data point"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2411, "text": "xn, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by\r(znk) for\nk= 1;2;3, respectively.\nMaximizing this log likelihood function (15.13) is a more complex problem than for\na single Gaussian. The difﬁculty arises from the presence of the summation over k\nthat appears inside the logarithm in (15.13), so that the logarithm function no longer\nacts directly on the Gaussian. If we set the derivatives of the log likelihood to zero,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2412, "text": "we will no longer obtain a closed-form solution, as we will see shortly.\nBefore discussing how to maximize this function, it is worth emphasizing that\nthere is a signiﬁcant problem associated with the maximum likelihood framework\nwhen applied to Gaussian mixture models, due to the presence of singularities. For\nsimplicity, consider a Gaussian mixture whose components have covariance matrices\ngiven by \u0006k = \u001b2\nkI, where I is the unit matrix, although the conclusions will hold"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2413, "text": "for general covariance matrices. Suppose that one of the components of the mixture\nmodel, let us say the jth component, has its mean \u0016j exactly equal to one of the data\npoints so that \u0016j = xn for some value of n. This data point will then contribute a\nterm in the likelihood function of the form\nN(xn|xn;\u001b2\njI) = 1\n(2\u0019)1=2\n1\n\u001bj\n: (15.14)\nIf we consider the limit \u001bj → 0, then we see that this term goes to inﬁnity and\nso the log likelihood function will also go to inﬁnity. Thus, the maximization of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2414, "text": "the log likelihood function is not a well posed-problem because such singularities\nwill always be present and will occur whenever one of the Gaussian components\n‘collapses’ onto a speciﬁc data point. Recall that this problem did not arise with\na single Gaussian distribution. To understand the difference, note that if a single\nGaussian collapses onto a data point, it will contribute multiplicative factors to the\n470 15. DISCRETE LATENT V ARIABLES\nFigure 15.6 Illustration of how singularities"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2415, "text": "Figure 15.6 Illustration of how singularities\nin the likelihood function arise\nwith mixtures of Gaussians.\nThis should be compared with\na single Gaussian shown in\nFigure 2.9 for which no singu-\nlarities arise.\nx\np(x)\nlikelihood function arising from the other data points, and these factors will go to\nzero exponentially fast, giving an overall likelihood that goes to zero rather than\ninﬁnity. However, once we have (at least) two components in the mixture, one of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2416, "text": "the components can have a ﬁnite variance and therefore assign ﬁnite probability to\nall the data points while the other component can shrink onto one speciﬁc data point\nand thereby contribute an ever increasing additive value to the log likelihood. This\nis illustrated in Figure 15.6. These singularities provide an example of the over-\nﬁtting that can occur in a maximum likelihood approach. When applying maximum\nlikelihood to Gaussian mixture models, we must take steps to avoid ﬁnding such"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2417, "text": "pathological solutions and instead seek local maxima of the likelihood function that\nare well behaved. We can try to avoid the singularities by using suitable heuristics,\nfor instance by detecting when a Gaussian component is collapsing and resetting\nits mean to a randomly chosen value while also resetting its covariance to some\nlarge value and then continuing with the optimization. The singularities can also be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2418, "text": "avoided by adding a regularization term to the log likelihood corresponding to a prior\ndistribution over the parameters.Section 15.4.3\nA further issue in ﬁnding maximum likelihood solutions arises because for any\ngiven maximum likelihood solution, a K-component mixture will have a total of K!\nequivalent solutions corresponding to theK! ways of assigning Ksets of parameters\nto Kcomponents. In other words, for any given (non-degenerate) point in the space"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2419, "text": "of parameter values, there will be a furtherK!−1 additional points all of which give\nrise to exactly the same distribution. This problem is known asidentiﬁability (Casella\nand Berger, 2002) and is an important issue when we wish to interpret the parameter\nvalues discovered by a model. Identiﬁability will also arise when we discuss models\nhaving continuous latent variables. However, when ﬁnding a good density model, itChapter 16"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2420, "text": "is irrelevant because any of the equivalent solutions is as good as any other.\n15.2.2 Maximum likelihood\nAn elegant and powerful method for ﬁnding maximum likelihood solutions for\nmodels with latent variables is called theexpectation–maximization algorithm or EM\nalgorithm (Dempster, Laird, and Rubin, 1977; McLachlan and Krishnan, 1997). In\nthis chapter we will give three different derivations of the EM algorithm, each more\n15.2. Mixtures of Gaussians 471"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2421, "text": "15.2. Mixtures of Gaussians 471\ngeneral than the previous. We begin here with a relatively informal treatment in\nthe context of a Gaussian mixture model. We emphasize, however, that EM has\nbroad applicability, and the underlying concepts will be encountered in the context\nof several different models in this book.\nWe begin by writing down the conditions that must be satisﬁed at a maximum\nof the likelihood function. Setting the derivatives ofln p(X|\u0019;\u0016;\u0006) in (15.13) with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2422, "text": "respect to the means \u0016k of the Gaussian components to zero, we obtain\n0 =\nN∑\nn=1\n\u0019kN(xn|\u0016k;\u0006k)∑\nj\u0019jN(xn|\u0016j;\u0006j)\n  \n\r(znk)\n\u0006−1\nk (xn −\u0016k) (15.15)\nwhere we have made use of the form (3.26) for the Gaussian distribution. Note\nthat the posterior probabilities, or responsibilities, \r(znk) given by (15.12) appear\nnaturally on the right-hand side. Multiplying by \u0006k (which we assume to be non-\nsingular) and rearranging we obtain\n\u0016k = 1\nNk\nN∑\nn=1\n\r(znk)xn (15.16)\nwhere we have deﬁned\nNk =\nN∑\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2423, "text": "(znk)xn (15.16)\nwhere we have deﬁned\nNk =\nN∑\nn=1\n\r(znk): (15.17)\nWe can interpret Nk as the effective number of points assigned to cluster k. Note\ncarefully the form of this solution. We see that the mean \u0016k for the kth Gaussian\ncomponent is obtained by taking a weighted mean of all the points in the data set,\nin which the weighting factor for data point xn is given by the posterior probability\n\r(znk) that component kwas responsible for generating xn."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2424, "text": "If we set the derivative ofln p(X|\u0019;\u0016;\u0006) with respect to \u0006k to zero and follow\na similar line of reasoning by making use of the result for the maximum likelihood\nsolution for the covariance matrix of a single Gaussian, we obtainSection 3.2.7\n\u0006k = 1\nNk\nN∑\nn=1\n\r(znk)(xn −\u0016k)(xn −\u0016k)T; (15.18)\nwhich has the same form as the corresponding result for a single Gaussian ﬁtted to\nthe data set, but again with each data point weighted by the corresponding poste-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2425, "text": "rior probability and with the denominator given by the effective number of points\nassociated with the corresponding component.\nFinally, we maximize ln p(X|\u0019;\u0016;\u0006) with respect to the mixing coefﬁcients\n\u0019k. Here we must take account of the constraint (15.8), which requires the mixing\ncoefﬁcients to sum to one. This can be achieved using a Lagrange multiplier \u0015andAppendix C\n472 15. DISCRETE LATENT V ARIABLES\nmaximizing the following quantity:\nln p(X|\u0019;\u0016;\u0006) + \u0015\nK∑\nk=1\n\u0019k −1\n)\n; (15.19)\nwhich gives"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2426, "text": "K∑\nk=1\n\u0019k −1\n)\n; (15.19)\nwhich gives\n0 =\nN∑\nn=1\nN(xn|\u0016k;\u0006k)∑\nj\u0019jN(xn|\u0016j;\u0006j) + \u0015 (15.20)\nwhere again we see the appearance of the responsibilities. If we now multiply both\nsides by \u0019k and sum over kmaking use of the constraint (15.8), we ﬁnd \u0015 = −N.\nUsing this to eliminate \u0015and rearranging, we obtain\n\u0019k = Nk\nN (15.21)\nso that the mixing coefﬁcient for the kth component is given by the average respon-\nsibility which that component takes for explaining the data points."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2427, "text": "Note that the results (15.16), (15.18), and (15.21) do not constitute a closed-form\nsolution for the parameters of the mixture model because the responsibilities \r(znk)\ndepend on those parameters in a complex way through (15.12). However, these re-\nsults do suggest a simple iterative scheme for ﬁnding a solution to the maximum\nlikelihood problem, which as we will see turns out to be an instance of the EM algo-\nrithm for the particular case of the Gaussian mixture model. We ﬁrst choose some"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2428, "text": "initial values for the means, covariances, and mixing coefﬁcients. Then we alternate\nbetween the following two updates, which we will call the E step and the M step for\nreasons that will become apparent shortly. In the expectation step, or E step, we use\nthe current values for the parameters to evaluate the posterior probabilities, or re-\nsponsibilities, given by (15.12). We then use these probabilities in the maximization"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2429, "text": "step, or M step, to re-estimate the means, covariances, and mixing coefﬁcients using\nthe results (15.16), (15.18), and (15.21). Note that in so doing, we ﬁrst evaluate the\nnew means using (15.16) and then use these new values to ﬁnd the covariances using\n(15.18), in keeping with the corresponding result for a single Gaussian distribution.\nWe will show that each update to the parameters resulting from an E step followed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2430, "text": "by an M step is guaranteed to increase the log likelihood function. In practice, the al-Section 15.3\ngorithm is deemed to have converged when the change in the log likelihood function,\nor alternatively in the parameters, falls below some threshold.\nWe illustrate the EM algorithm for a mixture of two Gaussians applied to the\nre-scaled Old Faithful data in Figure 15.7. Here a mixture of two Gaussians is used,\nwith centres initialized using the same values as for the K-means algorithm in Fig-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2431, "text": "ure 15.1 and with covariance matrices initialized to be proportional to the unit matrix.\nPlot (a) shows the data points in green, together with the initial conﬁguration of the\nmixture model in which the one standard-deviation contours for the two Gaussian\ncomponents are shown as blue and red circles. Plot (b) shows the result of the initial\nE step, in which each data point is depicted using a proportion of blue ink equal to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2432, "text": "the posterior probability of having been generated from the blue component and a\n15.2. Mixtures of Gaussians 473\n(a)−2 0 2\n−2\n0\n2\n(b)−2 0 2\n−2\n0\n2\n(c)\nL = 1\n−2 0 2\n−2\n0\n2\n(d)\nL = 2\n−2 0 2\n−2\n0\n2\n(e)\nL = 5\n−2 0 2\n−2\n0\n2\n(f)\nL = 20\n−2 0 2\n−2\n0\n2\nFigure 15.7 Application of the EM algorithm to the Old Faithful data set as used for the illustration of the\nK-means algorithm in Figure 15.1. See the text for details.\ncorresponding proportion of red ink given by the posterior probability of having been"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2433, "text": "generated by the red component. Thus, points that have a roughly equal probability\nfor belonging to either cluster appear purple. The situation after the ﬁrst M step is\nshown in plot (c), in which the mean of the blue Gaussian has moved to the mean of\nthe data set, weighted by the probabilities of each data point belonging to the blue\ncluster. In other words it has moved to the centre of mass of the blue ink. Similarly,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2434, "text": "the covariance of the blue Gaussian is set equal to the covariance of the blue ink.\nAnalogous results hold for the red component. Plots (d), (e), and (f) show the results\nafter 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the algorithm is\nclose to convergence.\nNote that the EM algorithm takes many more iterations to reach (approximate)\nconvergence compared with theK-means algorithm and that each cycle requires sig-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2435, "text": "niﬁcantly more computation. It is therefore common to run the K-means algorithm\nto ﬁnd a suitable initialization for a Gaussian mixture model that is subsequently\nadapted using EM. The covariance matrices can conveniently be initialized to the\nsample covariances of the clusters found by the K-means algorithm, and the mix-\ning coefﬁcients can be set to the fractions of data points assigned to the respective\n474 15. DISCRETE LATENT V ARIABLES\nFigure 15.8 A Gaussian mixture model ﬁtted"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2436, "text": "Figure 15.8 A Gaussian mixture model ﬁtted\nto the ‘two-moons’ data set, show-\ning that a large number of mixture\ncomponents may be required to\ngive an accurate representation of\na complex data distribution. Here\nthe ellipses represent the contours\nof constant density for the corre-\nsponding mixture components. As\nwe move to spaces of larger di-\nmensionality, the number of com-\nponents required to model a distri-\nbution accurately can become un-\nacceptably large.\nx2\nx1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2437, "text": "acceptably large.\nx2\nx1\nclusters. Techniques such as parameter regularization must be employed to avoid\nsingularities of the likelihood function in which a Gaussian component collapses\nonto a particular data point. It should be emphasized that there will generally be\nmultiple local maxima of the log likelihood function and that EM is not guaranteed\nto ﬁnd the largest of these maxima. Because the EM algorithm for Gaussian mixtures\nplays such an important role, we summarize it in Algorithm 15.2."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2438, "text": "Mixture models are very ﬂexible and can approximate complicated distributions\nto high accuracy given a sufﬁcient number of components if the model parameters\nare chosen appropriately. In practice, however, the number of components can be ex-\ntremely large, especially in spaces of high dimensionality. This problem is illustrated\nfor the two-moons data set in Figure 15.8. Nevertheless, mixture models are useful\nin many applications. Also, an understanding of mixture models lays the foundations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2439, "text": "for models with continuous latent variables and for generative models based on deepChapter 16\nneural networks, which have much better scaling to spaces of high dimensionality.\n15.3.\nExpectation–Maximization Algorithm\nWe turn now to a more general view of the EM algorithm in which we focus on the\nrole of latent variables. As before we denote the set of all observed data points byX,\nin which thenth row representsxT\nn. Similarly, the corresponding latent variables will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2440, "text": "be denoted by anN×Kmatrix Z with rows zT\nn. If we assume that the data points are\ndrawn independently from the distribution, then we can express the Gaussian mixture\nmodel for this i.i.d. data set using the graphical representation shown inFigure 15.9.\nThe set of all model parameters is denoted by \u0012, and so the log likelihood function\n15.3. Expectation–Maximization Algorithm 475\nAlgorithm 15.2: EM algorithm for a Gaussian mixture model\nInput: Initial model parameters {\u0016k};{\u0006k};{\u0019k}"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2441, "text": "Input: Initial model parameters {\u0016k};{\u0006k};{\u0019k}\nData set {x1;:::; xN}\nOutput: Final model parameters {\u0016k};{\u0006k};{\u0019k}\nrepeat\n// E step\nfor n∈{1;:::;N }do\nfor k∈{1;:::;K }do\n\r(znk) ← \u0019kN(xn|\u0016k;\u0006k)∑K\nj=1 \u0019jN(xn|\u0016j;\u0006j)\nend for\nend for\n// M step\nfor k∈{1;:::;K }do\nNk ←\nN∑\nn=1\n\r(znk)\n\u0016k ← 1\nNk\nN∑\nn=1\n\r(znk)xn\n\u0006k ← 1\nNk\nN∑\nn=1\n\r(znk) (xn −\u0016k) (xn −\u0016k)T\n\u0019k ← Nk\nN\nend for\n// Log likelihood\nL←\nN∑\nn=1\nln\n{ K∑\nk=1\n\u0019kN(xn|\u0016k;\u0006k)\n}\nuntil convergence\nreturn {xk};{\u0006k};{\u0019k}\n476 15. DISCRETE LATENT V ARIABLES"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2442, "text": "476 15. DISCRETE LATENT V ARIABLES\nFigure 15.9 Graphical representation of a Gaussian mixture model\nfor a set of N i.i.d. data points {xn}, with corresponding\nlatent points {zn}, wheren= 1;:::;N . \u0019 zn\nxn\u0016 \u0006\nN\nis given by\nln p(X|\u0012) = ln\n{∑\nZ\np(X;Z|\u0012)\n}\n: (15.22)\nNote that our discussion will apply equally well to continuous latent variables simplyChapter 16\nby replacing the sum over Z with an integral.\nA key observation is that the summation over the latent variables appears inside"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2443, "text": "the logarithm. Even if the joint distribution p(X;Z|\u0012) belongs to the exponential\nfamily, the marginal distribution p(X|\u0012) typically does not as a result of this sum-\nmation. The presence of the sum prevents the logarithm from acting directly on the\njoint distribution, resulting in complicated expressions for the maximum likelihood\nsolution.\nNow suppose that, for each observation in X, we were told the corresponding"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2444, "text": "value of the latent variableZ. We will call {X;Z}the complete data set, and we will\nrefer to the actual observed data X as incomplete, as illustrated in Figure 15.5. The\nlikelihood function for the complete data set simply takes the formln p(X;Z|\u0012), and\nwe will suppose that maximization of this complete-data log likelihood function is\nstraightforward.\nIn practice, however, we are not given the complete data set {X;Z}but only"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2445, "text": "the incomplete data X. Our state of knowledge of the values of the latent variables\nin Z is given only by the posterior distribution p(Z|X;\u0012). Because we cannot use\nthe complete-data log likelihood, we consider instead its expected value under the\nposterior distribution of the latent variables, which corresponds (as we will see) to the\nE step of the EM algorithm. In the subsequent M step, we maximize this expectation."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2446, "text": "If the current estimate for the parameters is denoted by\u0012old, then a pair of successive\nE and M steps gives rise to a revised estimate \u0012new. The algorithm is initialized\nby choosing some starting value for the parameters \u00120. Although this use of the\nexpectation may seem somewhat arbitrary, we will see the motivation for this choice\nwhen we give a deeper treatment of EM in Section 15.4.\nIn the E step, we use the current parameter values \u0012old to ﬁnd the posterior"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2447, "text": "distribution of the latent variables given by p(Z|X;\u0012old). We then use this posterior\ndistribution to ﬁnd the expectation of the complete-data log likelihood evaluated for\nsome general parameter value \u0012. This expectation, denoted by Q(\u0012;\u0012old), is given\nby\nQ(\u0012;\u0012old) =\n∑\nZ\np(Z|X;\u0012old) lnp(X;Z|\u0012): (15.23)\n15.3. Expectation–Maximization Algorithm 477\nAlgorithm 15.3: General EM algorithm\nInput: Joint distribution p(X;Z|\u0012)\nInitial parameters \u0012old\nData set x1;:::; xN\nOutput: Final parameters \u0012\nrepeat"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2448, "text": "Output: Final parameters \u0012\nrepeat\nQ(\u0012;\u0012old) ← ∑\nZ p(Z|X;\u0012old) lnp(X;Z|\u0012) // E step\n\u0012new ← arg max\u0012Q(\u0012;\u0012old) // M step\nL← p(X|\u0012new) // Evaluate log likelihood\n\u0012old ← \u0012new // Update the parameters\nuntil convergence\nreturn \u0012new\nIn the M step, we determine the revised parameter estimate\u0012new by maximizing this\nfunction:\n\u0012new = arg max\n\u0012\nQ(\u0012;\u0012old): (15.24)\nNote that in the deﬁnition of Q(\u0012;\u0012old), the logarithm acts directly on the joint dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2449, "text": "tribution p(X;Z|\u0012), and so the corresponding M-step maximization will, according\nto our assumption, be tractable. The general EM algorithm is summarized in Al-\ngorithm 15.3. It has the property, as we will show later, that each cycle of EM will\nincrease the incomplete-data log likelihood (unless it is already at a local maximum).Section 15.4.1\nThe EM algorithm can also be used to ﬁnd MAP (maximum posterior) solutions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2450, "text": "for models in which a prior p(\u0012) is deﬁned over the parameters. In this case the EExercise 15.5\nstep remains the same as in the maximum likelihood case, whereas in the M step the\nquantity to be maximized is given by Q(\u0012;\u0012old) + lnp(\u0012). Suitable choices for the\nprior will remove the singularities of the kind illustrated in Figure 15.6.\nHere we have considered the use of the EM algorithm to maximize a likelihood\nfunction when there are discrete latent variables. However, it can also be applied"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2451, "text": "when the unobserved variables correspond to missing values in the data set. The\ndistribution of the observed values is obtained by taking the joint distribution of all\nthe variables and then marginalizing over the missing ones. EM can then be used to\nmaximize the corresponding likelihood function. This will be a valid procedure if\nthe data values are missing at random, meaning that the mechanism causing values\nto be missing does not depend on the unobserved values. In many situations this will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2452, "text": "not be the case, for instance if a sensor fails to return a value whenever the quantity\nit is measuring exceeds some threshold.\n478 15. DISCRETE LATENT V ARIABLES\nFigure 15.10 This shows the same graph as inFigure 15.9 except that\nwe now suppose that the discrete variables zn are ob-\nserved, as well as the data variables xn. \u0019 zn\nxn\u0016 \u0006\nN\n15.3.1 Gaussian mixtures\nWe now consider the application of this latent-variable view of EM to the spe-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2453, "text": "ciﬁc case of a Gaussian mixture model. Recall that our goal is to maximize the log\nlikelihood function (15.13), which is computed using the observed data set X, and\nwe saw that this was more difﬁcult than with a single Gaussian distribution due to\nthe summation over k that occurs inside the logarithm. Suppose then that in addi-\ntion to the observed data set X, we were also given the values of the corresponding\ndiscrete variables Z. Recall that Figure 15.5(a) shows a complete data set (i.e., one"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2454, "text": "that includes labels showing which component generated each data point) whereas\nFigure 15.5(b) shows the corresponding incomplete data set. A graphical model for\nthe complete data is shown in Figure 15.10.\nNow consider the problem of maximizing the likelihood for the complete data\nset {X;Z}. From (15.9) and (15.10), this likelihood function takes the form\np(X;Z|\u0016;\u0006;\u0019) =\nN∏\nn=1\nK∏\nk=1\n\u0019znk\nk N(xn|\u0016k;\u0006k)znk (15.25)\nwhere znk denotes the kth component of zn. Taking the logarithm, we obtain"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2455, "text": "ln p(X;Z|\u0016;\u0006;\u0019) =\nN∑\nn=1\nK∑\nk=1\nznk{ln\u0019k + lnN(xn|\u0016k;\u0006k)}: (15.26)\nComparison with the log likelihood function (15.13) for the incomplete data shows\nthat the summation over k and the logarithm have been interchanged. The loga-\nrithm now acts directly on the Gaussian distribution, which itself is a member of\nthe exponential family. Not surprisingly, this leads to a much simpler solution to the\nmaximum likelihood problem, as we now show. Consider ﬁrst the maximization with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2456, "text": "respect to the means and covariances. Becauseznis a K-dimensional vector with all\nelements equal to 0 except for a single element having the value1, the complete-data\nlog likelihood function is simply a sum ofKindependent contributions, one for each\nmixture component. Thus, the maximization with respect to a mean or a covariance\nis exactly as for a single Gaussian, except that it involves only the subset of data\npoints that are ‘assigned’ to that component. For the maximization with respect to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2457, "text": "the mixing coefﬁcients, note that these are coupled for different values ofkby virtue\nof the summation constraint (15.8). Again, this can be enforced using a Lagrange\n15.3. Expectation–Maximization Algorithm 479\nmultiplier as before, which leads to the result\n\u0019k = 1\nN\nN∑\nn=1\nznk (15.27)\nso that the mixing coefﬁcients are equal to the fractions of data points assigned to\nthe corresponding components.\nThus, we see that the complete-data log likelihood function can be maximized"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2458, "text": "trivially in closed form. In practice, however, we do not have values for the latent\nvariables. Therefore, as discussed earlier, we consider the expectation, with respect\nto the posterior distribution of the latent variables, of the complete-data log like-\nlihood. Using (15.9) and (15.10) together with Bayes’ theorem, we see that this\nposterior distribution takes the form\np(Z|X;\u0016;\u0006;\u0019) ∝\nN∏\nn=1\nK∏\nk=1\n[\u0019kN(xn|\u0016k;\u0006k)]znk : (15.28)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2459, "text": "N∏\nn=1\nK∏\nk=1\n[\u0019kN(xn|\u0016k;\u0006k)]znk : (15.28)\nWe see that this factorizes overnso that under the posterior distribution, the{zn}are\nindependent. This is easily veriﬁed by inspecting the directed graph in Figure 15.9Exercise 15.6\nand making use of the d-separation criterion. The expected value of the indicatorSection 11.2\nvariable znk under this posterior distribution is then given by\nE[znk] =\n∑\nzn\nznk\n∏\nk′\n[\u0019k′N(xn|\u0016k′;\u0006k′)]znk′\n∑\nzn\n∏\nj\n[\n\u0019jN(xn|\u0016j;\u0006j)\n]znj\n= \u0019kN(xn|\u0016k;\u0006k)\nK∑\nj=1\n\u0019jN(xn|\u0016j;\u0006j)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2460, "text": "]znj\n= \u0019kN(xn|\u0016k;\u0006k)\nK∑\nj=1\n\u0019jN(xn|\u0016j;\u0006j)\n= \r(znk); (15.29)\nwhich is just the responsibility of componentkfor data pointxn. The expected value\nof the complete-data log likelihood function is therefore given by\nEZ[ln p(X;Z|\u0016;\u0006;\u0019)] =\nN∑\nn=1\nK∑\nk=1\n\r(znk) {ln\u0019k + lnN(xn|\u0016k;\u0006k)}: (15.30)\nWe can now proceed as follows. First we choose some initial values for the param-\neters \u0016old, \u0006old, and \u0019old, and we use these to evaluate the responsibilities (the E"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2461, "text": "step). We then keep the responsibilities ﬁxed and maximize (15.30) with respect to\n\u0016k, \u0006k, and \u0019k (the M step). This leads to closed-form solutions for \u0016new, \u0006new,\nand \u0019new given by (15.16), (15.18), and (15.21) as before. This is precisely theExercise 15.9\nEM algorithm for Gaussian mixtures as derived earlier. We will gain more insight\ninto the role of the expected complete-data log likelihood function when discuss the\nconvergence of the EM algorithm in Section 15.4."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2462, "text": "convergence of the EM algorithm in Section 15.4.\n480 15. DISCRETE LATENT V ARIABLES\nFigure 15.11 The probabilistic graphical model for se-\nquential data corresponding to a hid-\nden Markov model. The discrete latent\nvariables are no longer independent but\nform a Markov chain.\nz1 z2 zN\nx1 x2 xN\nThroughout this chapter we assume that the data observations are i.i.d. For or-\ndered observations that form a sequence, the mixture model can be extended by con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2463, "text": "necting the latent variables in a Markov chain to give ahidden Markov model whose\ngraphical structure is shown in Figure 15.11. The EM algorithm can be extended\nto this more complex model in which the E step involves a sequential calculation in\nwhich messages are passed along the chain of latent variables (Bishop, 2006).\n15.3.2 Relation to K-means\nComparison of the K-means algorithm with the EM algorithm for Gaussian\nmixtures shows that there is a close similarity. Whereas the K-means algorithm"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2464, "text": "performs a hard assignment of data points to clusters in which each data point is\nassociated uniquely with one cluster, the EM algorithm makes a soft assignment\nbased on the posterior probabilities. In fact, we can derive the K-means algorithm\nas a particular limit of EM for Gaussian mixtures as follows.\nConsider a Gaussian mixture model in which the covariance matrices of the\nmixture components are given by \u000fI, where \u000fis a variance parameter that is shared"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2465, "text": "by all the components, and I is the identity matrix, so that\np(x|\u0016k;\u0006k) = 1\n(2\u0019\u000f)D=2 exp\n{\n−1\n2\u000f∥x−\u0016k∥2\n}\n: (15.31)\nWe now consider the EM algorithm for a mixture of K Gaussians of this form in\nwhich we treat \u000fas a ﬁxed constant, instead of a parameter to be re-estimated. From\n(15.12) the posterior probabilities, or responsibilities, for a particular data point xn\nare given by\n\r(znk) = \u0019kexp {−∥xn −\u0016k∥2=2\u000f}∑\nj\u0019jexp\n{\n−∥xn −\u0016j∥2=2\u000f\n}: (15.32)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2466, "text": "j\u0019jexp\n{\n−∥xn −\u0016j∥2=2\u000f\n}: (15.32)\nConsider the limit \u000f→ 0. The denominator consists of a sum of terms indexed by j\neach of which goes to zero. The particular term for which ∥xn −\u0016j∥2 is smallest,\nsay j = l, will go to zero most slowly and will then dominate this sum. Therefore,\nthe responsibilities \r(znk) for the data point xn all go to zero except for term l, for\nwhich the responsibility \r(znl) will go to unity. Note that this holds independently"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2467, "text": "of the values of the\u0019k so long as none of the\u0019k is zero. Thus, in this limit, we obtain\na hard assignment of data points to clusters, just as in theK-means algorithm, so that\n\r(znk) → rnk where rnk is deﬁned by (15.2). Each data point is thereby assigned to\nthe cluster having the closest mean. The EM re-estimation equation for the\u0016k, given\nby (15.16), then reduces to the K-means result (15.4). Note that the re-estimation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2468, "text": "formula for the mixing coefﬁcients (15.21) simply resets the value of \u0019k to be equal\n15.3. Expectation–Maximization Algorithm 481\nto the fraction of data points assigned to cluster k, although these parameters no\nlonger play an active role in the algorithm.\nFinally, in the limit \u000f→ 0, the expected complete-data log likelihood, given by\n(15.30), becomesExercise 15.12\nEZ[ln p(X;Z|\u0016;\u0006;\u0019)] →− 1\n2\nN∑\nn=1\nK∑\nk=1\nrnk∥xn −\u0016k∥2 + const: (15.33)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2469, "text": "2\nN∑\nn=1\nK∑\nk=1\nrnk∥xn −\u0016k∥2 + const: (15.33)\nThus, we see that in this limit, maximizing the expected complete-data log likelihood\nis equivalent to minimizing the error measureJfor the K-means algorithm given by\n(15.1). Note that the K-means algorithm does not estimate the covariances of the\nclusters but only the cluster means.\n15.3.3 Mixtures of Bernoulli distributions\nSo far in this chapter, we have focused on distributions over continuous variables"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2470, "text": "described by mixtures of Gaussians. As a further example of mixture modelling and\nto illustrate the EM algorithm in a different context, we now discuss mixtures of\ndiscrete binary variables described by Bernoulli distributions. This model is also\nknown as latent class analysis (Lazarsfeld and Henry, 1968; McLachlan and Peel,\n2000).\nConsider a set of Dbinary variables xi, where i = 1;:::;D , each of which is\ngoverned by a Bernoulli distribution with parameter \u0016i, so thatSection 3.1.1\np(x|\u0016) =\nD∏"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2471, "text": "p(x|\u0016) =\nD∏\ni=1\n\u0016xi\ni (1 −\u0016i)(1−xi) (15.34)\nwhere x = (x1;:::;x D)T and \u0016 = (\u00161;:::;\u0016 D)T. We see that the individual\nvariables xi are independent, given \u0016. The mean and covariance of this distribution\nare easily seen to beExercise 15.13\nE[x] = \u0016 (15.35)\ncov[x] = diag {\u0016i(1 −\u0016i)}: (15.36)\nNow let us consider a ﬁnite mixture of these distributions given by\np(x|\u0016;\u0019) =\nK∑\nk=1\n\u0019kp(x|\u0016k) (15.37)\nwhere \u0016= {\u00161;:::; \u0016K},\u0019= {\u00191;:::;\u0019 K}, and\np(x|\u0016k) =\nD∏\ni=1\n\u0016xi\nki(1 −\u0016ki)(1−xi): (15.38)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2472, "text": "p(x|\u0016k) =\nD∏\ni=1\n\u0016xi\nki(1 −\u0016ki)(1−xi): (15.38)\nThe mixing coefﬁcients satisfy (15.7) and (15.8). The mean and covariance of this\nmixture distribution are given byExercise 15.14\n482 15. DISCRETE LATENT V ARIABLES\nE[x] =\nK∑\nk=1\n\u0019k\u0016k (15.39)\ncov[x] =\nK∑\nk=1\n\u0019k\n{\n\u0006k + \u0016k\u0016T\nk\n}\n−E[x]E[x]T (15.40)\nwhere \u0006k = diag {\u0016ki(1 −\u0016ki)}. Because the covariance matrix cov[x] is no\nlonger diagonal, the mixture distribution can capture correlations between the vari-\nables, unlike a single Bernoulli distribution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2473, "text": "ables, unlike a single Bernoulli distribution.\nIf we are given a data set X = {x1;:::; xN}then the log likelihood function\nfor this model is given by\nln p(X|\u0016;\u0019) =\nN∑\nn=1\nln\n{ K∑\nk=1\n\u0019kp(xn|\u0016k)\n}\n: (15.41)\nAgain we see the appearance of the summation inside the logarithm, so that the\nmaximum likelihood solution no longer has closed form.\nWe now derive the EM algorithm for maximizing the likelihood function for the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2474, "text": "mixture of Bernoulli distributions. To do this, we ﬁrst introduce an explicit discrete\nlatent variable z associated with each instance of x. As with the Gaussian mixture,\nz has a 1-of-K coding so that z = (z1;:::;z K)T is a binary K-dimensional vector\nhaving a single component equal to 1, with all other components equal to 0. We can\nthen write the conditional distribution of x, given the latent variable, as\np(x|z;\u0016) =\nK∏\nk=1\np(x|\u0016k)zk (15.42)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2475, "text": "p(x|z;\u0016) =\nK∏\nk=1\np(x|\u0016k)zk (15.42)\nwhereas the prior distribution for the latent variables is the same as for the mixture-\nof-Gaussians model, so that\np(z|\u0019) =\nK∏\nk=1\n\u0019zk\nk : (15.43)\nIf we form the product ofp(x|z;\u0016) and p(z|\u0019) and then marginalize overz, then we\nrecover (15.37).Exercise 15.16\nTo derive the EM algorithm, we ﬁrst write down the complete-data log likelihood\nfunction, which is given by\nln p(X;Z|\u0016;\u0019) =\nN∑\nn=1\nK∑\nk=1\nznk\n{\nln \u0019k\n+\nD∑\ni=1\n[xniln \u0016ki + (1 −xni) ln(1−\u0016ki)]\n}\n(15.44)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2476, "text": "D∑\ni=1\n[xniln \u0016ki + (1 −xni) ln(1−\u0016ki)]\n}\n(15.44)\n15.3. Expectation–Maximization Algorithm 483\nwhere X = {xn}and Z = {zn}. Next we take the expectation of the complete-data\nlog likelihood with respect to the posterior distribution of the latent variables to give\nEZ[ln p(X;Z|\u0016;\u0019)] =\nN∑\nn=1\nK∑\nk=1\n\r(znk)\n{\nln \u0019k\n+\nD∑\ni=1\n[xniln \u0016ki + (1 −xni) ln(1−\u0016ki)]\n}\n(15.45)\nwhere \r(znk) = E[znk] is the posterior probability, or responsibility, of component"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2477, "text": "kgiven data pointxn. In the E step, these responsibilities are evaluated using Bayes’\ntheorem, which takes the form\n\r(znk) = E[znk] =\n∑\nzn\nznk\n∏\nk′\n[\u0019k′p(xn|\u0016k′)]znk′\n∑\nzn\n∏\nj\n[\n\u0019jp(xn|\u0016j)\n]znj\n= \u0019kp(xn|\u0016k)\nK∑\nj=1\n\u0019jp(xn|\u0016j)\n: (15.46)\nIf we consider the sum over nin (15.45), we see that the responsibilities enter\nonly through two terms, which can be written as\nNk =\nN∑\nn=1\n\r(znk) (15.47)\nxk = 1\nNk\nN∑\nn=1\n\r(znk)xn (15.48)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2478, "text": "(znk) (15.47)\nxk = 1\nNk\nN∑\nn=1\n\r(znk)xn (15.48)\nwhere Nk is the effective number of data points associated with componentk. In the\nM step, we maximize the expected complete-data log likelihood with respect to the\nparameters \u0016k and \u0019. If we set the derivative of (15.45) with respect to \u0016k equal to\nzero and rearrange the terms, we obtainExercise 15.17\n\u0016k = xk: (15.49)\nWe see that this sets the mean of component k equal to a weighted mean of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2479, "text": "data, with weighting coefﬁcients given by the responsibilities that componentktakes\nfor each of the data points. For the maximization with respect to \u0019k, we need to\nintroduce a Lagrange multiplier to enforce the constraint ∑\nk\u0019k = 1. Following\nanalogous steps to those used for the mixture of Gaussians, we then obtainExercise 15.18\n\u0019k = Nk\nN ; (15.50)\n484 15. DISCRETE LATENT V ARIABLES\nFigure 15.12 Illustration of the Bernoulli mixture model in which the top row shows examples from the digits"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2480, "text": "data set after converting the pixel values from grey scale to binary using a threshold of 0:5. On the bottom row\nthe ﬁrst three images show the parameters \u0016ki for each of the three components in the mixture model. As a\ncomparison, we also ﬁt the same data set using a single multivariate Bernoulli distribution, again using maximum\nlikelihood. This amounts to simply averaging the counts in each pixel and is shown by the right-most image on\nthe bottom row."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2481, "text": "the bottom row.\nwhich represents the intuitively reasonable result that the mixing coefﬁcient for com-\nponent kis given by the effective fraction of points in the data set explained by that\ncomponent.\nNote that in contrast to the mixture of Gaussians, there are no singularities in\nwhich the likelihood function goes to inﬁnity. This can be seen by noting that the\nlikelihood function is bounded above because 0 6 p(xn|\u0016k) 6 1. There exist so-Exercise 15.19"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2482, "text": "lutions for which the likelihood function is zero, but these will not be found by EM\nprovided it is not initialized to a pathological starting point, because the EM algo-\nrithm always increases the value of the likelihood function, until a local maximum\nis found.Section 15.3\nWe illustrate the Bernoulli mixture model in Figure 15.12 by using it to model\nhandwritten digits. Here the digit images have been turned into binary vectors by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2483, "text": "setting all elements whose values exceed0:5 to 1 and setting the remaining elements\nto 0. We now ﬁt a data set of N = 600 such digits, comprising the digits ‘2’, ‘3’,\nand ‘4’, with a mixture ofK = 3 Bernoulli distributions by running 10 iterations of\nthe EM algorithm. The mixing coefﬁcients were initialized to \u0019k = 1=K, and the\nparameters \u0016kj were set to random values chosen uniformly in the range(0:25;0:75)\nand then normalized to satisfy the constraint that ∑\nj\u0016kj = 1. We see that a mix-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2484, "text": "j\u0016kj = 1. We see that a mix-\nture of three Bernoulli distributions is able to ﬁnd the three clusters in the data set\ncorresponding to the different digits. It is straightforward to extend the analysis of\nBernoulli mixtures to the case of multinomial binary variables having M >2 statesExercise 15.20\nby making use of the discrete distribution (3.14).\n15.4. Evidence Lower Bound 485\n15.4. Evidence Lower Bound\nWe now present an even more general perspective on the EM algorithm by deriving"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2485, "text": "a lower bound on the log likelihood function, which is known as the evidence lower\nbound or ELBO. It is sometimes called a variational lower bound. Here the term\nevidence refers to the (log) likelihood function, which is sometimes called the ‘model\nevidence’ in a Bayesian setting as it allows different models to be compared without\nthe use of hold-out data (Bishop, 2006). As an illustration of this bound, we use it"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2486, "text": "to re-derive the EM algorithm for Gaussian mixtures from a third perspective. The\nELBO will play an important role in several of the deep generative models discussed\nin later chapters. It also provides an example of avariational framework in which we\nintroduce a distribution q(Z) over the latent variables and then optimize with respect\nto this distribution using the calculus of variations.Appendix B\nConsider a probabilistic model in which we collectively denote all the observed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2487, "text": "variables by X and all the hidden variables by Z. The joint distribution p(X;Z|\u0012) is\ngoverned by a set of parameters denoted by\u0012. Our goal is to maximize the likelihood\nfunction:\np(X|\u0012) =\n∑\nZ\np(X;Z|\u0012): (15.51)\nHere we are assuming that Z is discrete, although the discussion is identical if Z\ncomprises continuous variables or a combination of discrete and continuous vari-\nables, with summation replaced by integration as appropriate."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2488, "text": "We will suppose that direct optimization ofp(X|\u0012) is difﬁcult, but that optimiza-\ntion of the complete-data likelihood function p(X;Z|\u0012) is signiﬁcantly easier. Next\nwe introduce a distribution q(Z) deﬁned over the latent variables, and we observe\nthat, for any choice of q(Z), the following decomposition holds:\nln p(X|\u0012) = L(q;\u0012) + KL(q∥p) (15.52)\nwhere we have deﬁned\nL(q;\u0012) =\n∑\nZ\nq(Z) ln\n{ p(X;Z|\u0012)\nq(Z)\n}\n(15.53)\nKL(q∥p) = −\n∑\nZ\nq(Z) ln\n{ p(Z|X;\u0012)\nq(Z)\n}\n: (15.54)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2489, "text": "∑\nZ\nq(Z) ln\n{ p(Z|X;\u0012)\nq(Z)\n}\n: (15.54)\nNote that L(q;\u0012) is a functional of the distribution q(Z) and a function of the pa-Appendix B\nrameters \u0012. It is worth studying carefully the forms of the expressions (15.53) and\n(15.54), and in particular noting that they differ in sign and also thatL(q;\u0012) contains\nthe joint distribution of X and Z whereas KL(q∥p)contains the conditional distri-\nbution of Z given X. To verify the decomposition (15.52), we ﬁrst make use of theExercise 15.21"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2490, "text": "product rule of probability to give\nln p(X;Z|\u0012) = ln p(Z|X;\u0012) + lnp(X|\u0012); (15.55)\n486 15. DISCRETE LATENT V ARIABLES\nFigure 15.13 Illustration of the decomposition\ngiven by (15.52), which holds for\nany choice of distribution q(Z).\nBecause the Kullback–Leibler di-\nvergence satisﬁes KL(q∥p) > 0,\nwe see that the quantity L(q;\u0012) is\na lower bound on the log likelihood\nfunction ln p(X|\u0012).\nln p(Xj\u0012)L(q; \u0012)\nKL(qjjp)\nwhich we then substitute into the expression forL(q;\u0012). This gives rise to two terms,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2491, "text": "one of which cancels KL(q∥p)whereas the other gives the required log likelihood\nln p(X|\u0012) after noting that q(Z) is a normalized distribution that sums to 1.\nFrom (15.54), we see that KL(q∥p)is the Kullback–Leibler divergence between\nq(Z) and the posterior distribution p(Z|X;\u0012). Recall that the Kullback–Leibler di-\nvergence satisﬁes KL(q∥p)> 0, with equality if, and only if, q(Z) = p(Z|X;\u0012). ItSection 2.5.7\ntherefore follows from (15.52) thatL(q;\u0012) 6 ln p(X|\u0012), in other words thatL(q;\u0012)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2492, "text": "is a lower bound on ln p(X|\u0012). The decomposition (15.52) is illustrated in Fig-\nure 15.13.\n15.4.1 EM revisited\nWe can use the decomposition (15.52) to derive the EM algorithm and to demon-\nstrate that it does indeed maximize the log likelihood. Suppose that the current value\nof the parameter vector is \u0012old. In the E step, the lower bound L(q;\u0012old) is maxi-\nmized with respect to q(Z) while holding \u0012old ﬁxed. The solution to this maximiza-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2493, "text": "tion problem is easily seen by noting that the value of ln p(X|\u0012old) does not depend\non q(Z) and so the largest value of L(q;\u0012old) will occur when the Kullback–Leibler\ndivergence vanishes, in other words when q(Z) is equal to the posterior distribu-\ntion p(Z|X;\u0012old). In this case, the lower bound will equal the log likelihood, as\nillustrated in Figure 15.14.\nIn the subsequent M step, the distributionq(Z) is held ﬁxed and the lower bound\nFigure 15.14 Illustration of the E step of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2494, "text": "Figure 15.14 Illustration of the E step of the\nEM algorithm. The q distribution is set equal to\nthe posterior distribution for the current parame-\nter values\u0012old, causing the lower bound to move\nup to the same value as the log likelihood func-\ntion, with the KL divergence vanishing.\nln p(Xj\u0012old)L(q; \u0012old)\nKL(qjjp) = 0\n15.4. Evidence Lower Bound 487\nFigure 15.15 Illustration of the M step of the\nEM algorithm. The distribu-\ntion q(Z) is held ﬁxed and the\nlower bound L(q;\u0012) is maxi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2495, "text": "lower bound L(q;\u0012) is maxi-\nmized with respect to the pa-\nrameter vector \u0012 to give a re-\nvised value \u0012new. Because the\nKullback–Leibler divergence is\nnon-negative, this causes the log\nlikelihood ln p(X|\u0012) to increase\nby at least as much as the lower\nbound does.\nln p(Xj\u0012new)L(q; \u0012new)\nKL(qjjp)\nL(q;\u0012) is maximized with respect to \u0012 to give some new value \u0012new. This will\ncause the lower bound Lto increase (unless it is already at a maximum), which will"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2496, "text": "necessarily cause the corresponding log likelihood function to increase. Because the\ndistribution qis determined using the old parameter values rather than the new values\nand is held ﬁxed during the M step, it will not equal the new posterior distribution\np(Z|X;\u0012new), and hence there will be a non-zero Kullback–Leibler divergence. The\nincrease in the log likelihood function is therefore greater than the increase in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2497, "text": "lower bound, as shown in Figure 15.15. If we substitute q(Z) = p(Z|X;\u0012old) into\n(15.53), we see that, after the E step, the lower bound takes the form\nL(q;\u0012) =\n∑\nZ\np(Z|X;\u0012old) lnp(X;Z|\u0012) −\n∑\nZ\np(Z|X;\u0012old) lnp(Z|X;\u0012old)\n= Q(\u0012;\u0012old) + const (15.56)\nwhere the constant is simply the negative entropy of theqdistribution and is therefore\nindependent of \u0012. Here we recognize Q(\u0012;\u0012old) as the expected complete-data log-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2498, "text": "likelihood deﬁned by (15.23), and it is therefore the quantity that is being maximized\nin the M step, as we saw earlier distribution for mixtures of Gaussians. Note thatSection 15.3\nthe variable \u0012over which we are optimizing appears only inside the logarithm. If\nthe joint distribution p(Z;X|\u0012) is a member of the exponential family or a product\nof such members, then we see that the logarithm will cancel the exponential and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2499, "text": "lead to an M step that will be typically much simpler than the maximization of the\ncorresponding incomplete-data log likelihood function p(X|\u0012).\nThe operation of the EM algorithm can also be viewed in the space of param-\neters, as illustrated schematically in Figure 15.16. Here the red curve depicts the\n(incomplete-data) log likelihood function whose value we wish to maximize. We\nstart with some initial parameter value \u0012old, and in the ﬁrst E step we evaluate the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2500, "text": "posterior distribution over latent variables, which gives rise to a lower boundL(q;\u0012)\nwhose value equals the log likelihood at\u0012(old), as shown by the blue curve. Note that\nthe bound makes a tangential contact with the log likelihood at \u0012(old), so that both\n488 15. DISCRETE LATENT V ARIABLES\nFigure 15.16 The EM algorithm involves al-\nternately computing a lower\nbound on the log likelihood\nfor the current parameter val-\nues and then maximizing this\nbound to obtain the new pa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2501, "text": "bound to obtain the new pa-\nrameter values. See the text\nfor a full discussion.\nθ(old) θ(new)\nln p(X|θ)\nL(θ, θ(old))E\nM\ncurves have the same gradient. This bound is a convex function having a uniqueExercise 15.22\nmaximum (for mixture components from the exponential family). In the M step,\nthe bound is maximized giving the value \u0012(new), which gives a larger value of the\nlog likelihood than \u0012(old). The subsequent E step then constructs a bound that is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2502, "text": "tangential at \u0012(new) as shown by the green curve.\nWe have seen that both the E and the M steps of the EM algorithm increase the\nvalue of a well-deﬁned bound on the log likelihood function and that the complete\nEM cycle will change the model parameters in such a way as to cause the log like-\nlihood to increase (unless it is already at a maximum, in which case the parameters\nremain unchanged).\n15.4.2 Independent and identically distributed data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2503, "text": "For the particular case of an i.i.d. data set, X will comprise N data points\n{xn}whereas Z will comprise N corresponding latent variables {zn}, where n =\n1;:::;N . From the independence assumption, we have p(X;Z) = ∏\nnp(xn;zn),\nand by marginalizing over the {zn}we have p(X) = ∏\nnp(xn). Using the sum\nand product rules, we see that the posterior probability that is evaluated in the E step\ntakes the form\np(Z|X;\u0012) = p(X;Z|\u0012)\n∑\nZ\np(X;Z|\u0012)\n=\nN∏\nn=1\np(xn;zn|\u0012)\n∑\nZ\nN∏\nn=1\np(xn;zn|\u0012)\n=\nN∏\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2504, "text": "N∏\nn=1\np(xn;zn|\u0012)\n∑\nZ\nN∏\nn=1\np(xn;zn|\u0012)\n=\nN∏\nn=1\np(zn|xn;\u0012) (15.57)\nand so the posterior distribution also factorizes with respect to n. For a Gaussian\nmixture model, this simply says that the responsibility that each of the mixture com-\nponents takes for a particular data point xn depends only on the value of xn and\n15.4. Evidence Lower Bound 489\non the parameters \u0012of the mixture components, not on the values of the other data\npoints.\n15.4.3 Parameter priors"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2505, "text": "points.\n15.4.3 Parameter priors\nWe can also use the EM algorithm to maximize the posterior distributionp(\u0012|X)\nfor models in which we have introduced a priorp(\u0012) over the parameters. To see this,\nnote that as a function of \u0012, we have p(\u0012|X) =p(\u0012;X)=p(X) and so\nln p(\u0012|X) = lnp(\u0012;X) −ln p(X): (15.58)\nMaking use of the decomposition (15.52), we have\nln p(\u0012|X) = L(q;\u0012) + KL(q∥p) + lnp(\u0012) −ln p(X)\n> L(q;\u0012) + lnp(\u0012) −ln p(X) (15.59)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2506, "text": "> L(q;\u0012) + lnp(\u0012) −ln p(X) (15.59)\nwhere ln p(X) is a constant. We can again optimize the right-hand side alternately\nwith respect to qand \u0012. The optimization with respect to qgives rise to the same E-\nstep equations as for the standard EM algorithm, because qappears only in L(q;\u0012).\nThe M-step equations are modiﬁed through the introduction of the prior termln p(\u0012),\nwhich typically requires only a small modiﬁcation to the standard maximum likeli-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2507, "text": "hood M-step equations. The additional term represents a form of regularization andChapter 9\nhas the effect of removing the singularities of the likelihood function for Gaussian\nmixture models.\n15.4.4 Generalized EM\nThe EM algorithm breaks down the potentially difﬁcult problem of maximizing\nthe likelihood function into two stages, the E step and the M step, each of which will\noften prove simpler to implement. Nevertheless, for complex models it may be the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2508, "text": "case that either the E step or the M step, or indeed both, remain intractable. This\nleads to two possible extensions of the EM algorithm, as follows.\nThe generalized EM, or GEM, algorithm addresses the problem of an intractable\nM step. Instead of aiming to maximize L(q;\u0012) with respect to \u0012, it seeks instead to\nchange the parameters in such a way as to increase its value. Again, becauseL(q;\u0012)\nis a lower bound on the log likelihood function, each complete EM cycle of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2509, "text": "GEM algorithm is guaranteed to increase the value of the log likelihood (unless the\nparameters already correspond to a local maximum). One way to exploit the GEM\napproach would be to use gradient-based iterative optimization algorithms during\nthe M step. Another form of GEM algorithm, known as the expectation conditional\nmaximization algorithm, involves making several constrained optimizations within\neach M step (Meng and Rubin, 1993). For instance, the parameters might be par-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2510, "text": "titioned into groups and the M step broken down into multiple steps each of which\ninvolves optimizing one of the groups with the remainder held ﬁxed.\nWe can similarly generalize the E step of the EM algorithm by performing a\npartial, rather than complete, optimization of L(q;\u0012) with respect to q(Z) (Neal and\nHinton, 1999). As we have seen, for any given value of\u0012there is a unique maximum\nof L(q;\u0012) with respect toq(Z) that corresponds to the posterior distributionq\u0012(Z) ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2511, "text": "490 15. DISCRETE LATENT V ARIABLES\np(Z|X;\u0012) and that for this choice of q(Z), the bound L(q;\u0012) is equal to the log\nlikelihood function ln p(X|\u0012). It follows that any algorithm that converges to the\nglobal maximum of L(q;\u0012) will ﬁnd a value of \u0012 that is also a global maximum\nof the log likelihood ln p(X|\u0012). Provided p(X;Z|\u0012) is a continuous function of \u0012\nthen, by continuity, any local maximum of L(q;\u0012) will also be a local maximum of\nln p(X|\u0012).\n15.4.5 Sequential EM"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2512, "text": "ln p(X|\u0012).\n15.4.5 Sequential EM\nConsider N independent data points x1;:::; xN with corresponding latent vari-\nables z1;:::; zN. The joint distribution p(X;Z|\u0012) factorizes over the data points,\nand this structure can be exploited in an incremental form of EM in which at each\nEM cycle, only one data point is processed at a time. In the E step, instead of recom-\nputing the responsibilities for all the data points, we just re-evaluate the responsibil-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2513, "text": "ities for one data point. It might appear that the subsequent M step would require\na computation involving the responsibilities for all the data points. However, if the\nmixture components are members of the exponential family, then the responsibilities\nenter only through simple sufﬁcient statistics, and these can be updated efﬁciently.\nConsider, for instance, a Gaussian mixture, and suppose we perform an update for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2514, "text": "data point min which the corresponding old and new values of the responsibilities\nare denoted by \rold(zmk) and \rnew(zmk). In the M step, the required sufﬁcient\nstatistics can be updated incrementally. For instance, for the means, the sufﬁcient\nstatistics are deﬁned by (15.16) and (15.17) from which we obtainExercise 15.23\n\u0016new\nk = \u0016old\nk +\n(\rnew(zmk) −\rold(zmk)\nNnew\nk\n)(\nxm −\u0016old\nk\n)\n(15.60)\ntogether with\nNnew\nk = Nold\nk + \rnew(zmk) −\rold(zmk): (15.61)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2515, "text": "Nnew\nk = Nold\nk + \rnew(zmk) −\rold(zmk): (15.61)\nThe corresponding results for the covariances and the mixing coefﬁcients are analo-\ngous.\nThus, both the E step and the M step take a ﬁxed time that is independent of\nthe total number of data points. Because the parameters are revised after each data\npoint, rather than waiting until after the whole data set is processed, this incremental\nversion can converge faster than the batch version. Each E or M step in this incre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2516, "text": "mental algorithm increases the value of L(q;\u0012), and as we have shown above, if the\nalgorithm converges to a local (or global) maximum of L(q;\u0012), this will correspond\nto a local (or global) maximum of the log likelihood function ln p(X|\u0012).\nExercises\n15.1 (?) Consider the K-means algorithm discussed in Section 15.1. Show that as a con-\nsequence of there being a ﬁnite number of possible assignments for the set of discrete"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2517, "text": "indicator variables rnk and that for each such assignment there is a unique optimum\nfor the {\u0016k}, the K-means algorithm must converge after a ﬁnite number of itera-\ntions.\nExercises 491\n15.2 (??) In this exercise we derive the sequential form for the K-means algorithm. At\neach step we consider a new data point xn, and only the prototype vector that is\nclosest to xnis updated. Starting from the expression (15.4) for the prototype vectors"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2518, "text": "in the batch setting, separate out the contribution from the ﬁnal data point xn. By\nrearranging the formula, show that this update takes the form (15.5). Note that, since\nno approximation is made in this derivation, the resulting prototype vectors will have\nthe property that they each equal the mean of all the data vectors that were assigned\nto them.\n15.3 (?) Consider a Gaussian mixture model in which the marginal distribution p(z) for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2519, "text": "the latent variable is given by (15.9) and the conditional distribution p(x|z)for the\nobserved variable is given by (15.10). Show that the marginal distribution p(x),\nobtained by summing p(z)p(x|z)over all possible values ofz, is a Gaussian mixture\nof the form (15.6).\n15.4 (?) Show that the number of equivalent parameter settings due to interchange sym-\nmetries in a mixture model with Kcomponents is K!.\n15.5 (??) Suppose we wish to use the EM algorithm to maximize the posterior distri-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2520, "text": "bution over parameters p(\u0012|X)for a model containing latent variables, where X is\nthe observed data set. Show that the E step remains the same as in the maximum\nlikelihood case, whereas in the M step the quantity to be maximized is given by\nQ(\u0012;\u0012old) + lnp(\u0012) where Q(\u0012;\u0012old) is deﬁned by (15.23).\n15.6 (?) Consider the directed graph for a Gaussian mixture model shown in Figure 15.9.\nBy making use of the d-separation criterion, show that the posterior distribution ofSection 11.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2521, "text": "the latent variables factorizes with respect to the different data points so that\np(Z|X;\u0016;\u0006;\u0019) =\nN∏\nn=1\np(zn|xn;\u0016;\u0006;\u0019): (15.62)\n15.7 (??) Consider a special case of a Gaussian mixture model in which the covariance\nmatrices \u0006k of the components are all constrained to have a common value \u0006. De-\nrive the EM equations for maximizing the likelihood function under such a model.\n15.8 (??) Verify that maximization of the complete-data log likelihood (15.26) for a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2522, "text": "Gaussian mixture model leads to the result that the means and covariances of each\ncomponent are ﬁtted independently to the corresponding group of data points and\nthat the mixing coefﬁcients are given by the fractions of points in each group.\n15.9 (??) Show that if we maximize (15.30) with respect to\u0016k while keeping the respon-\nsibilities \r(znk) ﬁxed, we obtain the closed-form solution given by (15.16).\n15.10 (??) Show that if we maximize (15.30) with respect to\u0006k and \u0019k while keeping the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2523, "text": "responsibilities \r(znk) ﬁxed, we obtain the closed-form solutions given by (15.18)\nand (15.21).\n492 15. DISCRETE LATENT V ARIABLES\n15.11 (??) Consider a density model given by a mixture distribution:\np(x) =\nK∑\nk=1\n\u0019kp(x|k) (15.63)\nand suppose that we partition the vector x into two parts so that x = (x a;xb).\nShow that the conditional density p(xb|xa) is itself a mixture distribution, and ﬁnd\nexpressions for the mixing coefﬁcients and for the component densities."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2524, "text": "15.12 (?) In Section 15.3.2, we obtained a relationship between K means and EM for\nGaussian mixtures by considering a mixture model in which all components have\ncovariance \u000fI. Show that in the limit \u000f→ 0, maximizing the expected complete-data\nlog likelihood for this model, given by (15.30), is equivalent to minimizing the error\nmeasure J for the K-means algorithm given by (15.1).\n15.13 (??) Verify the results (15.35) and (15.36) for the mean and covariance of the Bernoulli\ndistribution."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2525, "text": "distribution.\n15.14 (??) Consider a mixture distribution of the form\np(x) =\nK∑\nk=1\n\u0019kp(x|k) (15.64)\nwhere the elements of x could be discrete or continuous or a combination of these.\nDenote the mean and covariance of p(x|k) by \u0016k and \u0006k, respectively. By making\nuse of the results of Exercise 15.13, show that the mean and covariance of the mixture\ndistribution are given by (15.39) and (15.40).\n15.15 (??) Using the re-estimation equations for the EM algorithm, show that a mixture"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2526, "text": "of Bernoulli distributions, with its parameters set to values corresponding to a maxi-\nmum of the likelihood function, has the property that\nE[x] = 1\nN\nN∑\nn=1\nxn ≡x: (15.65)\nHence, show that if the parameters of this model are initialized such that all compo-\nnents have the same mean \u0016k = ˆ\u0016for k = 1;:::;K , then the EM algorithm will\nconverge after one iteration, for any choice of the initial mixing coefﬁcients, and that\nthis solution has the property \u0016k ="}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2527, "text": "this solution has the property \u0016k =\nx. Note that this represents a degenerate case of\nthe mixture model in which all the components are identical, and in practice we try\nto avoid such solutions by using an appropriate initialization.\n15.16 (?) Consider the joint distribution of latent and observed variables for the Bernoulli\ndistribution obtained by forming the product of p(x|z;\u0016) given by (15.42) and\np(z|\u0019) given by (15.43). Show that if we marginalize this joint distribution with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2528, "text": "respect to z, then we obtain (15.37).\nExercises 493\n15.17 (?) Show that if we maximize the expected complete-data log likelihood function\n(15.45) for a mixture of Bernoulli distributions with respect to \u0016k, we obtain the\nM-step equation (15.49).\n15.18 (?) Show that if we maximize the expected complete-data log likelihood function\n(15.45) for a mixture of Bernoulli distributions with respect to the mixing coefﬁcients"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2529, "text": "\u0019k, and use a Lagrange multiplier to enforce the summation constraint, we obtain the\nM-step equation (15.50).\n15.19 (?) Show that as a consequence of the constraint 0 6 p(xn|\u0016k) 6 1 for the discrete\nvariable xn, the incomplete-data log likelihood function for a mixture of Bernoulli\ndistributions is bounded above and hence that there are no singularities for which the\nlikelihood goes to inﬁnity.\n15.20 (???) Consider a D-dimensional variable x each of whose components iis itself a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2530, "text": "multinomial variable of degree M so that x is a binary vector with components xij\nwhere i= 1;:::;D and j = 1;:::;M , subject to the constraint that ∑\njxij = 1 for\nall i. Suppose that the distribution of these variables is described by a mixture of the\ndiscrete multinomial distributions so thatSection 3.1.3\np(x) =\nK∑\nk=1\n\u0019kp(x|\u0016k) (15.66)\nwhere\np(x|\u0016k) =\nD∏\ni=1\nM∏\nj=1\n\u0016xij\nkij: (15.67)\nThe parameters \u0016kij represent the probabilities p(xij = 1|\u0016k) and must satisfy"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2531, "text": "0 6 \u0016kij 6 1 together with the constraint ∑\nj\u0016kij = 1 for all values of k and i.\nGiven an observed data set{xn}, wheren= 1;:::;N , derive the E-step and M-step\nequations of the EM algorithm for optimizing the mixing coefﬁcients \u0019k and the\ncomponent parameters \u0016kij of this distribution by maximum likelihood.\n15.21 (?) Verify the relation (15.52) in which L(q;\u0012) and KL(q∥p)are deﬁned by (15.53)\nand (15.54), respectively."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2532, "text": "and (15.54), respectively.\n15.22 (?) Show that the lower boundL(q;\u0012) given by (15.53), withq(Z) = p(Z|X;\u0012(old)),\nhas the same gradient with respect to \u0012as the log likelihood function ln p(X|\u0012) at\nthe point \u0012= \u0012(old).\n15.23 (??) Consider the incremental form of the EM algorithm for a mixture of Gaussians,\nin which the responsibilities are recomputed only for a speciﬁc data pointxm. Start-\ning from the M-step formulae (15.16) and (15.17), derive the results (15.60) and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2533, "text": "(15.61) for updating the component means.\n15.24 (??) Derive M-step formulae for updating the covariance matrices and mixing co-\nefﬁcients in a Gaussian mixture model when the responsibilities are updated incre-\nmentally, analogous to the result (15.60) for updating the means.\n16\nContinuous\nLatent Variables\nIn the previous chapter we discussed probabilistic models having discrete latent vari-\nables, such as a mixture of Gaussians. We now explore models in which some, or"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2534, "text": "all, of the latent variables are continuous. An important motivation for such models\nis that many data sets have the property that the data points lie close to a manifoldSection 6.1.3\nof much lower dimensionality than that of the original data space. To see why this\nmight arise, consider an artiﬁcial data set constructed by taking a handwritten digit\nfrom the MNIST data set (LeCun et al., 1998), represented by a 64 ×64 pixel grey-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2535, "text": "level image, and embedding it in a larger image of size 100 ×100 by padding with\npixels having the value zero (corresponding to white pixels) in which the location and\norientation of the digit are varied at random, as illustrated inFigure 16.1. Each of the\nresulting images is represented by a point in the 100 ×100 = 10;000-dimensional\ndata space. However, across a data set of such images, there are only three degrees"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2536, "text": "of freedom of variability, corresponding to vertical and horizontal translations and\n495© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_16"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2537, "text": "496 16. CONTINUOUS LATENT V ARIABLES\nFigure 16.1 A synthetic data set obtained by taking an image of a handwritten digit and creating multiple copies\nin each of which the digit has undergone a random displacement and rotation within some larger image ﬁeld.\nThe resulting images each have 100 ×100 = 10;000 pixels.\nrotations. The data points will therefore live on a subspace of the data space whose\nintrinsic dimensionality is three. Note that the manifold will be nonlinear because,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2538, "text": "for instance, if we translate the digit past a particular pixel, that pixel value will go\nfrom zero (white) to one (black) and back to zero again, which is clearly a nonlinear\nfunction of the digit position. In this example, the translation and rotation parame-\nters are latent variables because we observe only the image vectors and are not told\nwhich values of the translation or rotation variables were used to create them."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2539, "text": "For real data sets of handwritten digits, there will be further degrees of freedom\narising from scaling and other variations due, for example, to the variability in an\nindividual’s writing as well as the differences in writing styles between individuals.\nNevertheless, the number of such degrees of freedom will be small compared to the\ndimensionality of the data set.\nIn practice, the data points will not be conﬁned precisely to a smooth low-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2540, "text": "dimensional manifold, and we can interpret the departures of data points from the\nmanifold as ‘noise’. This leads naturally to a generative view of such models in\nwhich we ﬁrst select a point within the manifold according to some latent-variable\ndistribution and then generate an observed data point by adding noise drawn from\nsome conditional distribution of the data variables given the latent variables.\nThe simplest continuous latent-variable model assumes Gaussian distributions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2541, "text": "for both the latent and observed variables and makes use of a linear-Gaussian de-\npendence of the observed variables on the state of the latent variables. This leadsSection 11.1.4\nto a probabilistic formulation of the well-known technique of principal component\nanalysis (PCA) as well as to a related model called factor analysis. In this chap-\nter we will begin with a standard, non-probabilistic treatment of PCA, and then weSection 16.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2542, "text": "show how PCA arises naturally as the maximum likelihood solution for a linear-\nGaussian latent-variable model. This probabilistic reformulation brings many ad-Section 16.2\nvantages, such as the use of EM for parameter estimation, principled extensions to\nmixtures of PCA models, and Bayesian formulations that allow the number of prin-\ncipal components to be determined automatically from the data (Bishop, 2006). This"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2543, "text": "chapter also lays the foundations for nonlinear models having continuous latent vari-\nables including normalizing ﬂows, variational autoencoders, and diffusion models.\n16.1. Principal Component Analysis 497\nFigure 16.2 Principal component analysis seeks a\nspace of lower dimensionality, known as the\nprincipal subspace and denoted by the ma-\ngenta line, such that the orthogonal projec-\ntion of the data points (red dots) onto this\nsubspace maximizes the variance of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2544, "text": "subspace maximizes the variance of the\nprojected points (green dots). An alterna-\ntive deﬁnition of PCA is based on minimiz-\ning the sum-of-squares of the projection er-\nrors, indicated by the blue lines.\nx2\nx1\nxn\nexn\nu1\n16.1. Principal Component Analysis\nPrincipal component analysis, or PCA, is widely used for applications such as di-\nmensionality reduction, lossy data compression, feature extraction, and data visual-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2545, "text": "ization (Jolliffe, 2002). It is also known as theKosambi–Karhunen–Lo`eve transform.\nConsider the orthogonal projection of a data set onto a lower-dimensional lin-\near space, known as the principal subspace, as shown in Figure 16.2. PCA can be\ndeﬁned as the linear projection that maximizes the variance of the projected data\n(Hotelling, 1933). Equivalently, it can be deﬁned as the linear projection that min-\nimizes the average projection cost, deﬁned as the mean squared distance between"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2546, "text": "the data points and their projections (Pearson, 1901). We consider each of these\ndeﬁnitions in turn.\n16.1.1 Maximum variance formulation\nConsider a data set of observations {xn}where n = 1;:::;N , and xn is a\nEuclidean variable with dimensionality D. Our goal is to project the data onto a\nspace having dimensionalityM <Dwhile maximizing the variance of the projected\ndata. For the moment, we will assume that the value of M is given. Later in this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2547, "text": "chapter, we will consider techniques to determine an appropriate value of M from\nthe data.\nTo begin with, consider the projection onto a one-dimensional space (M = 1).\nWe can deﬁne the direction of this space using a D-dimensional vector u1, which\nfor convenience (and without loss of generality) we will choose to be a unit vector\nso that uT\n1 u1 = 1 (note that we are interested only in the direction deﬁned by u1,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2548, "text": "not in the magnitude of u1 itself). Each data point xn is then projected onto a scalar\nvalue uT\n1 xn. The mean of the projected data is uT\n1\nx where x is the sample set mean\ngiven by\nx = 1\nN\nN∑\nn=1\nxn (16.1)\n498 16. CONTINUOUS LATENT V ARIABLES\nand the variance of the projected data is given by\n1\nN\nN∑\nn=1\n{\nuT\n1 xn −uT\n1\nx\n}2\n= uT\n1 Su1 (16.2)\nwhere S is the data covariance matrix deﬁned by\nS = 1\nN\nN∑\nn=1\n(xn −x)(xn −x)T: (16.3)\nWe now maximize the projected varianceuT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2549, "text": "We now maximize the projected varianceuT\n1 Su1 with respect tou1. Clearly, this has\nto be a constrained maximization to prevent ∥u1∥→∞. The appropriate constraint\ncomes from the normalization condition uT\n1 u1 = 1 . To enforce this constraint,\nwe introduce a Lagrange multiplier that we will denote by \u00151, and then make anAppendix C\nunconstrained maximization of\nuT\n1 Su1 + \u00151\n(\n1 −uT\n1 u1\n)\n: (16.4)\nBy setting the derivative with respect to u1 equal to zero, we see that this quantity"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2550, "text": "will have a stationary point when\nSu1 = \u00151u1; (16.5)\nwhich says that u1 must be an eigenvector of S. If we left-multiply by uT\n1 and make\nuse of uT\n1 u1 = 1, we see that the variance is given by\nuT\n1 Su1 = \u00151 (16.6)\nand so the variance will be a maximum when we set u1 equal to the eigenvector\nhaving the largest eigenvalue \u00151. This eigenvector is known as the ﬁrst principal\ncomponent.\nWe can deﬁne additional principal components in an incremental fashion by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2551, "text": "choosing each new direction to be that which maximizes the projected variance\namongst all possible directions orthogonal to those already considered. If we con-\nsider the general case of an M-dimensional projection space, the optimal linear pro-\njection for which the variance of the projected data is maximized is now deﬁned by\nthe Meigenvectors u1;:::; uM of the data covariance matrixS corresponding to the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2552, "text": "M largest eigenvalues \u00151;:::;\u0015 M. This is easily shown using proof by induction.Exercise 16.1\nTo summarize, PCA involves evaluating the mean\nx and the covariance matrix\nS of a data set and then ﬁnding the M eigenvectors of S corresponding to the M\nlargest eigenvalues. Algorithms for ﬁnding eigenvectors and eigenvalues, as well as\nadditional theorems related to eigenvector decomposition, can be found in Golub and\nVan Loan (1996). Note that the computational cost of computing the full eigenvector"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2553, "text": "decomposition for a matrix of size D ×D is O(D3). If we plan to project our\ndata onto the ﬁrst M principal components, then we only need to ﬁnd the ﬁrst M\neigenvalues and eigenvectors. This can be done with more efﬁcient techniques, such\nas the power method (Golub and Van Loan, 1996), that scale like O(MD2), or\nalternatively we can make use of the EM algorithm.Section 16.3.2\n16.1. Principal Component Analysis 499\n16.1.2 Minimum-error formulation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2554, "text": "16.1.2 Minimum-error formulation\nWe now discuss an alternative formulation of PCA based on projection error\nminimization. To do this, we introduce a complete orthonormal set ofD-dimensionalAppendix A\nbasis vectors {ui}where i= 1;:::;D that satisfy\nuT\ni uj = \u000eij: (16.7)\nBecause this basis is complete, each data point can be represented exactly by a linear\ncombination of the basis vectors\nxn =\nD∑\ni=1\n\u000bniui (16.8)\nwhere the coefﬁcients \u000bni will be different for different data points. This simply"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2555, "text": "corresponds to a rotation of the coordinate system to a new system deﬁned by the\n{ui}, and the originalDcomponents {xn1;:::;x nD}are replaced by an equivalent\nset {\u000bn1;:::;\u000b nD}. Taking the inner product with uj, and making use of the or-\nthonormality property, we obtain \u000bnj = xT\nnuj, and so without loss of generality we\ncan write\nxn =\nD∑\ni=1\n(\nxT\nnui\n)\nui: (16.9)\nOur goal, however, is to approximate this data point using a representation in-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2556, "text": "volving a restricted number M <Dof variables corresponding to a projection onto\na lower-dimensional subspace. The M-dimensional linear subspace can be repre-\nsented, without loss of generality, by the ﬁrst M of the basis vectors, and so we\napproximate each data point xn by\n˜xn =\nM∑\ni=1\nzniui +\nD∑\ni=M+1\nbiui (16.10)\nwhere the {zni}depend on the particular data point, whereas the {bi}are constants\nthat are the same for all data points. We are free to choose the {ui}, the{zni}, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2557, "text": "the {bi}so as to minimize the error introduced by the reduction in dimensionality.\nAs our error measure, we will use the squared distance between the original data\npoint xn and its approximation ˜xn, averaged over the data set, so that our goal is to\nminimize\nJ = 1\nN\nN∑\nn=1\n∥xn −˜xn∥2: (16.11)\nConsider ﬁrst the minimization with respect to the quantities{zni}. Substituting\nfor ˜xn, setting the derivative with respect to znj to zero, and making use of the\northonormality conditions, we obtain"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2558, "text": "orthonormality conditions, we obtain\nznj = xT\nnuj (16.12)\n500 16. CONTINUOUS LATENT V ARIABLES\nwhere j = 1;:::;M . Similarly, setting the derivative of J with respect to bi to zero\nand again making use of the orthonormality relations, gives\nbj = xTuj (16.13)\nwhere j = M+1;:::;D . If we substitute for zni and bi and make use of the general\nexpansion (16.9), we obtain\nxn −˜xn =\nD∑\ni=M+1\n{\n(xn −x)Tui\n}\nui (16.14)\nfrom which we see that the displacement vector from xn to ˜xn lies in the space"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2559, "text": "orthogonal to the principal subspace, because it is a linear combination of {ui}for\ni= M + 1;:::;D , as illustrated in Figure 16.2. This is to be expected because the\nprojected points ˜xn must lie within the principal subspace, but we can move them\nfreely within that subspace, and so the minimum error is given by the orthogonal\nprojection.\nWe therefore obtain an expression for the error measure J as a function purely\nof the {ui}in the form\nJ = 1\nN\nN∑\nn=1\nD∑\ni=M+1\n(\nxT\nnui −xTui\n)2\n=\nD∑\ni=M+1\nuT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2560, "text": "N\nN∑\nn=1\nD∑\ni=M+1\n(\nxT\nnui −xTui\n)2\n=\nD∑\ni=M+1\nuT\ni Sui: (16.15)\nThere remains the task of minimizing J with respect to the {ui}, which must be\na constrained minimization otherwise we will obtain the vacuous result ui = 0. The\nconstraints arise from the orthonormality conditions, and as we will see, the solution\nwill be expressed in terms of the eigenvector expansion of the covariance matrix.\nBefore considering a formal solution, let us try to obtain some intuition about the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2561, "text": "result by considering a two-dimensional data space D = 2 and a one-dimensional\nprincipal subspace M = 1. We have to choose a direction u2 so as to minimize\nJ = uT\n2 Su2, subject to the normalization constraint uT\n2 u2 = 1. Using a Lagrange\nmultiplier \u00152 to enforce the constraint, we consider the minimization of\n˜J = uT\n2 Su2 + \u00152\n(\n1 −uT\n2 u2\n)\n: (16.16)\nSetting the derivative with respect to u2 to zero, we obtain Su2 = \u00152u2 so that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2562, "text": "u2 is an eigenvector of S with eigenvalue \u00152. Thus, any eigenvector will deﬁne\na stationary point of the error measure. To ﬁnd the value of J at the minimum,\nwe back-substitute the solution for u2 into the error measure to give J = \u00152. We\ntherefore obtain the minimum value ofJby choosing u2 to be the eigenvector corre-\nsponding to the smaller of the two eigenvalues. Thus, we should choose the principal\nsubspace to be aligned with the eigenvector having thelarger eigenvalue. This result"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2563, "text": "accords with our intuition that, to minimize the average squared projection distance,\nwe should choose the principal component subspace so that it passes through the\nmean of the data points and is aligned with the directions of maximum variance. If\n16.1. Principal Component Analysis 501\nthe eigenvalues are equal, any choice of principal direction will give rise to the same\nvalue of J.\nThe general solution to the minimization ofJfor arbitrary Dand arbitrary M <Exercise 16.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2564, "text": "Dis obtained by choosing the{ui}to be eigenvectors of the covariance matrix given\nby\nSui = \u0015iui (16.17)\nwhere i = 1;:::;D , and as usual the eigenvectors {ui}are chosen to be orthonor-\nmal. The corresponding value of the error measure is then given by\nJ =\nD∑\ni=M+1\n\u0015i; (16.18)\nwhich is simply the sum of the eigenvalues of those eigenvectors that are orthogonal\nto the principal subspace. We therefore obtain the minimum value of J by selecting"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2565, "text": "these eigenvectors to be those having the D−M smallest eigenvalues, and hence\nthe eigenvectors deﬁning the principal subspace are those corresponding to the M\nlargest eigenvalues.\nAlthough we have considered M < D, the PCA analysis still holds if M =\nD, in which case there is no dimensionality reduction but simply a rotation of the\ncoordinate axes to align with the principal components.\nFinally, note that there is a related linear dimensionality reduction technique"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2566, "text": "called canonical correlation analysis (Hotelling, 1936; Bach and Jordan, 2002).\nWhereas PCA works with a single random variable, canonical correlation analy-\nsis considers two (or more) variables and tries to ﬁnd a corresponding pair of linear\nsubspaces that have high cross-correlation, so that each component within one of the\nsubspaces is correlated with a single component from the other subspace. Its solution\ncan be expressed in terms of a generalized eigenvector problem."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2567, "text": "16.1.3 Data compression\nOne application for PCA is data compression, and we can illustrate this by con-\nsidering a data set of images of handwritten digits. Because each eigenvector of the\ncovariance matrix is a vector in the original D-dimensional space, we can represent\nthe eigenvectors as images of the same size as the data points. The mean image and\nthe ﬁrst four eigenvectors, along with their corresponding eigenvalues, are shown in\nFigure 16.3."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2568, "text": "Figure 16.3.\nA plot of the complete spectrum of eigenvalues, sorted into decreasing order, is\nshown in Figure 16.4(a). The error measure J associated with choosing a particular\nvalue of M is given by the sum of the eigenvalues fromM+1 up to Dand is plotted\nfor different values of M in Figure 16.4(b).\nIf we substitute (16.12) and (16.13) into (16.10), we can write the PCA approx-\n502 16. CONTINUOUS LATENT V ARIABLES\nMean λ 1 = 3.4 - 105 λ 2 = 2.8 - 105 λ 3 = 2.4 - 105 λ 4 = 1.6 - 105"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2569, "text": "Figure 16.3 Illustration of PCA applied to a data set of 6,000 images of size 28 ×28, each comprising a hand-\nwritten image of the numeral ‘3’ , showing the mean vectorx along with the ﬁrst four PCA eigenvectorsu1;:::; u4,\ntogether with their corresponding eigenvalues.\nimation to a data vector xn in the form\n˜xn =\nM∑\ni=1\n(xT\nnui)ui +\nD∑\ni=M+1\n(xTui)ui (16.19)\n= x +\nM∑\ni=1\n(\nxT\nnui −\nxTui\n)\nui (16.20)\nwhere we have made use of the relation\nx =\nD∑\ni=1\n(\nxTui\n)\nui; (16.21)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2570, "text": "x =\nD∑\ni=1\n(\nxTui\n)\nui; (16.21)\nwhich follows from the completeness of the {ui}. This represents a compression\nof the data set, because for each data point we have replaced the D-dimensional\nvector xn with an M-dimensional vector having components\n(\nxT\nnui −xTui\n)\n. The\nsmaller the value of M, the greater the degree of compression. Examples of PCA\nreconstructions of data points for the digits data set are shown in Figure 16.5.\n16.1.4 Data whitening"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2571, "text": "16.1.4 Data whitening\nAnother application of PCA is to data pre-processing. In this case, the goal is\nnot dimensionality reduction but rather the transformation of a data set to standard-\nize certain of its properties. This can be important in allowing subsequent machine\nlearning algorithms to be applied successfully to the data set. Typically, it is done\nwhen the original variables are measured in various different units or have signif-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2572, "text": "icantly different variabilities. For instance in the Old Faithful data set, the time\nbetween eruptions is typically an order of magnitude greater than the duration of an\neruption. When we applied the K-means algorithm to this data set, we ﬁrst made aSection 15.1\nseparate linear re-scaling of the individual variables such that each variable had zero\n16.1. Principal Component Analysis 503"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2573, "text": "i\nλ i\n(a)\n0 200 400 600\n0\n1\n2\n3\nx 10\n5"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2574, "text": "M\nJ\n(b)\n0 200 400 600\n0\n1\n2\n3\nx 10\n6\nFigure 16.4 (a) Plot of the eigenvalue spectrum for the data set of handwritten digits used in Figure 16.3.\n(b) Plot of the sum of the discarded eigenvalues, which represents the sum-of-squares error J introduced by\nprojecting the data onto a principal component subspace of dimensionality M.\nmean and unit variance. This is known as standardizing the data, and the covariance\nmatrix for the standardized data has components\n\u001aij = 1\nN\nN∑\nn=1\n(xni −xi)\n\u001bi"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2575, "text": "\u001aij = 1\nN\nN∑\nn=1\n(xni −xi)\n\u001bi\n(xnj −xj)\n\u001bj\n(16.22)\nwhere \u001bi is the standard deviation of xi. This is known as the correlation matrix of\nthe original data and has the property that if two components xi and xj of the data\nare perfectly correlated, then \u001aij = 1, and if they are uncorrelated, then \u001aij = 0.\nHowever, using PCA we can make a more substantial normalization of the data\nto give it zero mean and unit covariance, so that different variables become decorre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2576, "text": "Original M = 1 M = 10 M = 50 M = 250\nFigure 16.5 An example from the data set of handwritten digits together with its PCA reconstructions obtained\nby retaining M principal components for various values of M. As M increases, the reconstruction becomes\nmore accurate and would become perfect when M = D= 28 ×28 = 784.\n504 16. CONTINUOUS LATENT V ARIABLES\n1 2 3 4 5 6\n50\n70\n90\n−2 0 2\n−2\n0\n2\n−2 0 2\n−2\n0\n2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2577, "text": "1 2 3 4 5 6\n50\n70\n90\n−2 0 2\n−2\n0\n2\n−2 0 2\n−2\n0\n2\nFigure 16.6 Illustration of the effects of linear pre-processing applied to the Old Faithful data set. The plot on\nthe left shows the original data. The centre plot shows the result of standardizing the individual variables to zero\nmean and unit variance. Also shown are the principal axes of this normalized data set, plotted over the range\n±\u00151=2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2578, "text": "±\u00151=2\ni . The plot on the right shows the result of whitening the data to give it zero mean and unit covariance.\nlated. To do this, we ﬁrst write the eigenvector equation (16.17) in the form\nSU = UL (16.23)\nwhere L is a D×Ddiagonal matrix with elements \u0015i, and U is a D×Dorthog-\nonal matrix with columns given by ui. Then we deﬁne, for each data point xn, a\ntransformed value given by\nyn = L−1=2UT(xn −x) (16.24)\nwhere x is the sample mean deﬁned by (16.1). Clearly, the set {yn}has zero mean,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2579, "text": "and its covariance is given by the identity matrix because\n1\nN\nN∑\nn=1\nynyT\nn = 1\nN\nN∑\nn=1\nL−1=2 UT(xn −x)(xn −x)TUL−1=2\n= L−1=2UTSUL−1=2 = L−1=2LL−1=2 = I: (16.25)\nThis operation is known as whitening or sphering the data and is illustrated for the\nOld Faithful data set in Figure 16.6.Section 15.1\n16.1.5 High-dimensional data\nIn some applications of PCA, the number of data points is smaller than the di-\nmensionality of the data space. For example, we might want to apply PCA to a data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2580, "text": "set of a few hundred images, each of which corresponds to a vector in a space of po-\ntentially several million dimensions (corresponding to three colour values for each\nof the pixels in the image). Note that in a D-dimensional space, a set of N points,\nwhere N <D, deﬁnes a linear subspace whose dimensionality is at mostN−1, and\nso there is little point in applying PCA for values of M that are greater than N −1.\nIndeed, if we perform PCA we will ﬁnd that at leastD−N+1 of the eigenvalues are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2581, "text": "16.1. Principal Component Analysis 505\nzero, corresponding to eigenvectors along whose directions the data set has zero vari-\nance. Furthermore, typical algorithms for ﬁnding the eigenvectors of aD×Dmatrix\nhave a computational cost that scales likeO(D3), and so for applications such as the\nimage example, a direct application of PCA will be computationally infeasible.\nWe can resolve this problem as follows. First, let us deﬁneX to be the (N×D)-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2582, "text": "dimensional centred data matrix, whose nth row is given by (xn−x)T. The covari-\nance matrix (16.3) can then be written as S = N−1 XTX, and the corresponding\neigenvector equation becomes\n1\nNXTXui = \u0015iui: (16.26)\nNow pre-multiply both sides by X to give\n1\nNXXT(Xui) = \u0015i(Xui): (16.27)\nIf we now deﬁne vi = Xui, we obtain\n1\nNXXTvi = \u0015ivi; (16.28)\nwhich is an eigenvector equation for the N ×N matrix N−1 XXT. We see that this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2583, "text": "has the same N−1 eigenvalues as the original covariance matrix (which itself has an\nadditional D−N+1 eigenvalues of value zero). Thus, we can solve the eigenvector\nproblem in spaces of lower dimensionality with computational cost O(N3) instead\nof O(D3). To determine the eigenvectors, we multiply both sides of (16.28) by XT\nto give (1\nNXTX\n)\n(XTvi) = \u0015i(XTvi) (16.29)\nfrom which we see that (XTvi) is an eigenvector of S with eigenvalue \u0015i. Note,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2584, "text": "however, that these eigenvectors are not necessarily normalized. To determine the\nappropriate normalization, we re-scaleui ∝XTviby a constant such that∥ui∥= 1,\nwhich, assuming vi has been normalized to unit length, givesExercise 16.3\nui = 1\n(N\u0015i)1=2 XTvi: (16.30)\nIn summary, to apply this approach we ﬁrst evaluate XXT and then ﬁnd its eigen-\nvectors and eigenvalues and then compute the eigenvectors in the original data space\nusing (16.30).\n506 16. CONTINUOUS LATENT V ARIABLES"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2585, "text": "506 16. CONTINUOUS LATENT V ARIABLES\n16.2. Probabilistic Latent Variables\nWe have seen in the previous section that PCA can be deﬁned in terms of a linear\nprojection of the data onto a subspace of lower dimensionality than the original data\nspace. Each data point projects to a unique value of the quantities znj deﬁned by\n(16.12), and we can view these quantities as deterministic latent variables. To intro-\nduce and motivate probabilistic continuous latent variables, we now show that PCA"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2586, "text": "can also be expressed as the maximum likelihood solution of a probabilistic latent-\nvariable model. This reformulation of PCA, known asprobabilistic PCA, has several\nadvantages compared with conventional PCA:\n- A probabilistic PCA model represents a constrained form of a Gaussian dis-\ntribution in which the number of free parameters can be restricted while still\nallowing the model to capture the dominant correlations in a data set."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2587, "text": "- We can derive an EM algorithm for PCA that is computationally efﬁcient in\nsituations where only a few leading eigenvectors are required and that avoids\nhaving to evaluate the data covariance matrix as an intermediate step.Section 16.3.2\n- The combination of a probabilistic model and EM allows us to deal with miss-\ning values in the data set.\n- Mixtures of probabilistic PCA models can be formulated in a principled way\nand trained using the EM algorithm."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2588, "text": "and trained using the EM algorithm.\n- The existence of a likelihood function allows direct comparison with other\nprobabilistic density models. By contrast, conventional PCA will assign a low\nreconstruction cost to data points that are close to the principal subspace even\nif they lie arbitrarily far from the training data.\n- Probabilistic PCA can be used to model class-conditional densities and hence\nbe applied to classiﬁcation problems."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2589, "text": "be applied to classiﬁcation problems.\n- A probabilistic PCA model can be run generatively to provide samples from\nthe distribution.\n- Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which\nthe dimensionality of the principal subspace can be found automatically from\nthe data (Bishop, 2006).\nThis formulation of PCA as a probabilistic model was proposed independently by\nTipping and Bishop 1997; 1999 and by Roweis (1998). As we will see later, it is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2590, "text": "closely related to factor analysis (Basilevsky, 1994).\n16.2.1 Generative model\nProbabilistic PCA is a simple example of the linear-Gaussian framework inSection 11.1.4\nwhich all the marginal and conditional distributions are Gaussian. We can formu-\nlate probabilistic PCA by ﬁrst introducing an explicit M-dimensional latent variable\n16.2. Probabilistic Latent Variables 507\nz corresponding to the principal-component subspace. Next we deﬁne a Gaussian"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2591, "text": "prior distribution p(z) over the latent variable, together with a Gaussian conditional\ndistribution p(x|z)for the D-dimensional observed variable x conditioned on the\nvalue of the latent variable. Speciﬁcally, the prior distribution over z is given by a\nzero-mean unit-covariance Gaussian:\np(z) = N(z|0;I): (16.31)\nSimilarly, the conditional distribution of the observed variablex, conditioned on the\nvalue of the latent variable z, is again Gaussian:\np(x|z) =N(x|Wz+ \u0016;\u001b2I) (16.32)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2592, "text": "p(x|z) =N(x|Wz+ \u0016;\u001b2I) (16.32)\nin which the mean of x is a general linear function of z governed by the D×M\nmatrix W and the D-dimensional vector \u0016. Note that this factorizes with respect to\nthe elements of x. In other words this is an example of a naive Bayes model. AsSection 11.2.3\nwe will see shortly, the columns of W span a linear subspace within the data space\nthat corresponds to the principal subspace. The other parameter in this model is the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2593, "text": "scalar \u001b2 governing the variance of the conditional distribution. Note that there is no\nloss of generality in assuming a zero-mean unit-covariance Gaussian for the latent\ndistribution p(z) because a more general Gaussian distribution would give rise to an\nequivalent probabilistic model.Exercise 16.4\nWe can view the probabilistic PCA model from a generative viewpoint in which\na sampled value of the observed variable is obtained by ﬁrst choosing a value for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2594, "text": "the latent variable and then sampling the observed variable conditioned on this latent\nvalue. Speciﬁcally, the D-dimensional observed variable x is deﬁned by a linear\ntransformation of the M-dimensional latent variable z plus additive Gaussian noise,\nso that\nx = Wz + \u0016+ \u000f (16.33)\nwhere z is an M-dimensional Gaussian latent variable, and \u000fis a D-dimensional\nzero-mean Gaussian-distributed noise variable with covariance \u001b2I. This generative"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2595, "text": "process is illustrated in Figure 16.7. Note that this framework is based on a mapping\nfrom latent space to data space, in contrast to the more conventional view of PCA\ndiscussed above. The reverse mapping, from data space to the latent space, will be\nobtained shortly using Bayes’ theorem.\n16.2.2 Likelihood function\nSuppose we wish to determine the values of the parametersW, \u0016, and \u001b2 using\nmaximum likelihood. To write down the likelihood function, we need an expression"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2596, "text": "for the marginal distribution p(x) of the observed variable. This is expressed, from\nthe sum and product rules of probability, in the form\np(x) =\n∫\np(x|z)p(z) dz: (16.34)\nBecause this corresponds to a linear-Gaussian model, this marginal distribution is\nagain Gaussian, and is given byExercise 16.6\n508 16. CONTINUOUS LATENT V ARIABLES\nz\np(z)\nbz\nx2\nx1\n\u0016\np(xjbz)\ng bzjwj\nw x2\nx1\n\u0016\np(x)\nFigure 16.7 An illustration of the generative view of a probabilistic PCA model for a two-dimensional data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2597, "text": "space and a one-dimensional latent space. An observed data point x is generated by ﬁrst drawing a value bz\nfor the latent variable from its prior distribution p(z) and then drawing a value for x from an isotropic Gaussian\ndistribution (illustrated by the red circles) having mean wbz+ \u0016and covariance \u001b2I. The green ellipses show the\ndensity contours for the marginal distribution p(x).\np(x) = N(x|\u0016;C) (16.35)\nwhere the D×Dcovariance matrix C is deﬁned by\nC = WWT + \u001b2I: (16.36)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2598, "text": "C = WWT + \u001b2I: (16.36)\nThis result can also be derived more directly by noting that the predictive distribution\nwill be Gaussian and then evaluating its mean and covariance using (16.33). This\ngives\nE[x] = E[Wz + \u0016+ \u000f] = \u0016 (16.37)\ncov[x] = E\n[\n(Wz + \u000f)(Wz + \u000f)T]\n= E\n[\nWzzTWT]\n+ E[\u000f\u000fT] (16.38)\n= WWT + \u001b2I (16.39)\nwhere we have used the fact thatz and \u000fare independent random variables and hence\nare uncorrelated.\nIntuitively, we can think of the distribution p(x) as being deﬁned by taking an"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2599, "text": "isotropic Gaussian ‘spray can’ and moving it across the principal subspace spraying\nGaussian ink with density determined by \u001b2 and weighted by the prior distribution.\nThe accumulated ink density gives rise to a ‘pancake’ shaped distribution represent-\ning the marginal density p(x).\nThe predictive distribution p(x) is governed by the parameters \u0016, W, and \u001b2.\nHowever, there is redundancy in this parameterization corresponding to rotations of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2600, "text": "the latent space coordinates. To see this, consider a matrix ˜W = WR where R is\nan orthogonal matrix. Using the orthogonality property RRT = I, we see that the\nquantity ˜W˜WT that appears in the covariance matrix C takes the form\n˜W˜WT = WRRTWT = WWT (16.40)\n16.2. Probabilistic Latent Variables 509\nFigure 16.8 The probabilistic PCA model for a data set of N observa-\ntions of x can be expressed as a directed graph in which\neach observation xn is associated with a value zn of the\nlatent variable."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2601, "text": "latent variable.\n\u001b2 zn\nxn\u0016 W\nN\nand hence is independent of R. Thus, there is a whole family of matrices ˜W all of\nwhich give rise to the same predictive distribution. This invariance can be understood\nin terms of rotations within the latent space. We will return to a discussion of the\nnumber of independent parameters in this model later.\nWhen we evaluate the predictive distribution, we require C−1 , which involves\nthe inversion of aD×Dmatrix. The computation required to do this can be reduced"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2602, "text": "by making use of the matrix inversion identity (A.7) to give\nC−1 = \u001b−2 I −\u001b−2 WM−1 WT (16.41)\nwhere the M ×M matrix M is deﬁned by\nM = WTW + \u001b2I: (16.42)\nBecause we invert M rather than inverting C directly, the cost of evaluating C−1 is\nreduced from O(D3) to O(M3).\nAs well as the predictive distribution p(x), we will also require the posterior\ndistribution p(z|x), which can again be written down directly using the result (3.100)\nfor linear-Gaussian models to giveExercise 16.8\np(z|x) =N\n("}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2603, "text": "p(z|x) =N\n(\nz|M−1 WT(x −\u0016);\u001b2M−1 )\n: (16.43)\nNote that the posterior mean depends on x, whereas the posterior covariance is in-\ndependent of x.\n16.2.3 Maximum likelihood\nWe next consider the determination of the model parameters using maximum\nlikelihood. Given a data set X = {xn}of observed data points, the probabilistic\nPCA model can be expressed as a directed graph, as shown in Figure 16.8. The\ncorresponding log likelihood function is given, from (16.35), by\nln p(X|\u0016;W;\u001b2) =\nN∑\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2604, "text": "ln p(X|\u0016;W;\u001b2) =\nN∑\nn=1\nln p(xn|W;\u0016;\u001b2)\n= −ND\n2 ln(2\u0019) −N\n2 ln |C|−1\n2\nN∑\nn=1\n(xn −\u0016)TC−1 (xn −\u0016): (16.44)\n510 16. CONTINUOUS LATENT V ARIABLES\nSetting the derivative of the log likelihood with respect to \u0016 equal to zero gives\nthe expected result \u0016 = x where x is the data mean deﬁned by (16.1). BecauseExercise 16.9\nthe log likelihood is a quadratic function of \u0016, this solution represents the unique\nmaximum, as can be conﬁrmed by computing second derivatives. Back-substituting,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2605, "text": "we can then write the log likelihood function in the form\nln p(X|W;\u0016;\u001b2) = −N\n2\n{\nDln(2\u0019) + ln|C|+ Tr\n(\nC−1 S\n)}\n(16.45)\nwhere S is the data covariance matrix deﬁned by (16.3).\nMaximization with respect to W and \u001b2 is more complex but nonetheless has\nan exact closed-form solution. It was shown by Tipping and Bishop (1999) that all\nthe stationary points of the log likelihood function can be written as\nWML = UM(LM −\u001b2I)1=2R (16.46)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2606, "text": "WML = UM(LM −\u001b2I)1=2R (16.46)\nwhere UM is a D×M matrix whose columns are given by any subset (of size M)\nof the eigenvectors of the data covariance matrix S. The M ×M diagonal matrix\nLM has elements given by the corresponding eigenvalues \u0015i, and R is an arbitrary\nM ×M orthogonal matrix.\nFurthermore, Tipping and Bishop (1999) showed that the maximum of the like-\nlihood function is obtained when the M eigenvectors are chosen to be those whose"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2607, "text": "eigenvalues are the M largest (all other solutions being saddle points). A similar re-\nsult was conjectured independently by Roweis (1998), although no proof was given.\nAgain, we will assume that the eigenvectors have been arranged in order of decreas-\ning values of the corresponding eigenvalues, so that the M principal eigenvectors\nare u1;:::; uM. In this case, the columns of W deﬁne the principal subspace of\nstandard PCA. The corresponding maximum likelihood solution for \u001b2 is then given\nby"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2608, "text": "by\n\u001b2\nML = 1\nD−M\nD∑\ni=M+1\n\u0015i (16.47)\nso that \u001b2\nML is the average variance associated with the discarded dimensions.\nBecause R is orthogonal, it can be interpreted as a rotation matrix in the M-\ndimensional latent space. If we substitute the solution for W into the expression for\nC and make use of the orthogonality property RRT = I, we see that C is indepen-\ndent of R. This simply says that the predictive density is unchanged by rotations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2609, "text": "in the latent space as discussed earlier. For the particular case R = I, we see that\nthe columns of W are the principal component eigenvectors scaled by the variance\nparameters \u0015i −\u001b2. The interpretation of these scaling factors is clear once we rec-\nognize that for a convolution of independent Gaussian distributions (in this case the\nlatent space distribution and the noise model) the variances are additive. Thus, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2610, "text": "variance \u0015i in the direction of an eigenvector ui is composed of the sum of a contri-\nbution \u0015i −\u001b2 from the projection of the unit-variance latent space distribution into\ndata space through the corresponding column ofW plus an isotropic contribution of\nvariance \u001b2, which is added in all directions by the noise model.\n16.2. Probabilistic Latent Variables 511\nIt is worth taking a moment to study the form of the covariance matrix given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2611, "text": "by (16.36). Consider the variance of the predictive distribution along some direction\nspeciﬁed by the unit vector v, where vTv = 1 , which is given by vTCv. First\nsuppose that v is orthogonal to the principal subspace, in other words it is given by\nsome linear combination of the discarded eigenvectors. Then vTU = 0 and hence\nvTCv = \u001b2. Thus, the model predicts a noise variance orthogonal to the principal\nsubspace, which from (16.47) is just the average of the discarded eigenvalues. Now"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2612, "text": "suppose that v = ui where ui is one of the retained eigenvectors deﬁning the prin-\ncipal subspace. Then vTCv = (\u0015i −\u001b2) + \u001b2 = \u0015i. In other words, this model\ncorrectly captures the variance of the data along the principal axes and approximates\nthe variance in all remaining directions with a single average value \u001b2.\nOne way to construct the maximum likelihood density model would simply be\nto ﬁnd the eigenvectors and eigenvalues of the data covariance matrix and then to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2613, "text": "evaluate W and \u001b2 using the results given above. In this case, we would choose\nR = I for convenience. However, if the maximum likelihood solution is found by\nnumerical optimization of the likelihood function, for instance using an algorithm\nsuch as conjugate gradients (Fletcher, 1987; Nocedal and Wright, 1999) or through\nthe EM algorithm, then the resulting value of R is essentially arbitrary. This impliesSection 16.3.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2614, "text": "that the columns of W need not be orthogonal. If an orthogonal basis is required,\nthe matrix W can be post-processed appropriately (Golub and Van Loan, 1996). Al-\nternatively, the EM algorithm can be modiﬁed in such a way as to yield orthonormal\nprincipal directions, sorted in descending order of the corresponding eigenvalues,\ndirectly (Ahn and Oh, 2003).\nThe rotational invariance in latent space represents a form of statistical non-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2615, "text": "identiﬁability, analogous to that encountered for mixture models for discrete latent\nvariables. Here there is a continuum of parameters, any value of which leads to the\nsame predictive density, in contrast to the discrete non-identiﬁability associated with\ncomponent relabelling in the mixture setting.\nIf we consider M = D, so that there is no reduction of dimensionality, then\nUM = U and LM = L. Making use of the orthogonality properties UUT = I and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2616, "text": "RRT = I, we see that the covariance C of the marginal distribution for x becomes\nC = U(L −\u001b2I)1=2RRT(L −\u001b2I)1=2UT + \u001b2I = ULUT = S (16.48)\nand so we obtain the standard maximum likelihood solution for an unconstrained\nGaussian distribution in which the covariance matrix is given by the sample covari-\nance.\nConventional PCA is generally formulated as a projection of points from theD-\ndimensional data space onto an M-dimensional linear subspace. Probabilistic PCA,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2617, "text": "however, is most naturally expressed as a mapping from the latent space into the data\nspace via (16.33). For applications such as visualization and data compression, we\ncan reverse this mapping using Bayes’ theorem. Any point x in data space can then\nbe summarized by its posterior mean and covariance in latent space. From (16.43)\nthe mean is given by\nE[z|x] =M−1 WT\nML(x −\nx) (16.49)\n512 16. CONTINUOUS LATENT V ARIABLES\nwhere M is given by (16.42). This projects to a point in data space given by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2618, "text": "W E[z|x] +\u0016: (16.50)\nNote that this takes the same form as the equations for regularized linear regressionSection 4.1.6\nand is a consequence of maximizing the likelihood function for a linear-Gaussian\nmodel. Similarly, from (16.43) the posterior covariance is given by \u001b2M−1 and is\nindependent of x.\nIf we take the limit \u001b2 → 0, then the posterior mean reduces to\n(WT\nMLWML)−1 WT\nML(x −x); (16.51)\nwhich represents an orthogonal projection of the data point onto the latent space,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2619, "text": "and so we recover the standard PCA model. The posterior covariance in this limit isExercise 16.11\nzero, however, and the density becomes singular. For \u001b2 > 0, the latent projection\nis shifted towards the origin, relative to the orthogonal projection.Exercise 16.12\nFinally, note that an important role for the probabilistic PCA model is in deﬁn-\ning a multivariate Gaussian distribution in which the number of degrees of freedom,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2620, "text": "in other words the number of independent parameters, can be controlled while still\nallowing the model to capture the dominant correlations in the data. Recall that a\ngeneral Gaussian distribution has D(D + 1)=2 independent parameters in its co-\nvariance matrix (plus another D parameters in its mean). Thus, the number of pa-Section 3.2\nrameters scales quadratically with D and can become excessive in spaces of high"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2621, "text": "dimensionality. If we restrict the covariance matrix to be diagonal, then it has only\nD independent parameters, and so the number of parameters now grows linearly\nwith dimensionality. However, it now treats the variables as if they were indepen-\ndent and hence can no longer express any correlations between them. Probabilistic\nPCA provides an elegant compromise in which the M most signiﬁcant correlations\ncan be captured while still ensuring that the total number of parameters grows only"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2622, "text": "linearly with D. We can see this by evaluating the number of degrees of freedom in\nthe probabilistic PCA model as follows. The covariance matrix C depends on the\nparameters W, which has size D×M, and \u001b2, giving a total parameter count of\nDM + 1. However, we have seen that there is some redundancy in this parameter-\nization associated with rotations of the coordinate system in the latent space. The\northogonal matrix R that expresses these rotations has size M ×M. In the ﬁrst"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2623, "text": "column of this matrix, there are M−1 independent parameters, because the column\nvector must be normalized to unit length. In the second column, there are M −2\nindependent parameters, because the column must be normalized and also must be\northogonal to the previous column, and so on. Summing this arithmetic series, we\nsee that R has a total of M(M−1)=2 independent parameters. Thus, the number of\ndegrees of freedom in the covariance matrix C is given by\nDM + 1 −M(M −1)=2: (16.52)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2624, "text": "DM + 1 −M(M −1)=2: (16.52)\nThe number of independent parameters in this model therefore only grows linearly\nwith D, for ﬁxed M. If we take M = D−1, then we recover the standard result\nfor a full covariance Gaussian. In this case, the variance along D−1 linearly in-Exercise 16.14\n16.2. Probabilistic Latent Variables 513\ndependent directions is controlled by the columns of W, and the variance along the\nremaining direction is given by\u001b2. If M = 0, the model is equivalent to the isotropic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2625, "text": "covariance case.\n16.2.4 Factor analysis\nFactor analysis is a linear-Gaussian latent-variable model that is closely related\nto probabilistic PCA. Its deﬁnition differs from that of probabilistic PCA only in that\nthe conditional distribution of the observed variablex given the latent variable z has\na diagonal rather than an isotropic covariance so that\np(x|z) =N(x|Wz+ \u0016;\t) (16.53)\nwhere \t is a D×Ddiagonal matrix. Note that the factor analysis model, in com-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2626, "text": "mon with probabilistic PCA, assumes that the observed variables x1;:::;x D are\nindependent, given the latent variablez. In essence, a factor analysis model explains\nthe observed covariance structure of the data by representing the independent vari-\nance associated with each coordinate in the matrix \t and capturing the covariance\nbetween variables in the matrix W. In the factor analysis literature, the columns\nof W, which capture the correlations between observed variables, are called factor"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2627, "text": "loadings, and the diagonal elements of \t, which represent the independent noise\nvariances for each of the variables, are called uniquenesses.\nThe origins of factor analysis are as old as those of PCA, and discussions of\nfactor analysis can be found in the books by Everitt (1984), Bartholomew (1987),\nand Basilevsky (1994). Links between factor analysis and PCA were investigated\nby Lawley (1953) and Anderson (1963), who showed that at stationary points of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2628, "text": "the likelihood function, for a factor analysis model with \t = \u001b2I, the columns of\nW are scaled eigenvectors of the sample covariance matrix and \u001b2 is the average\nof the discarded eigenvalues. Later, Tipping and Bishop (1999) showed that the\nmaximum of the log likelihood function occurs when the eigenvectors comprising\nW are chosen to be the principal eigenvectors.\nMaking use of (16.34), we see that the marginal distribution for the observed\nvariable is given by p(x) = N(x|\u0016;C) where now"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2629, "text": "variable is given by p(x) = N(x|\u0016;C) where now\nC = WWT + \t: (16.54)\nAs with probabilistic PCA, this model is invariant to rotations in the latent space.Exercise 16.16\nHistorically, factor analysis has been the subject of controversy when attempts\nhave been made to place an interpretation on the individual factors (the coordinates\nin z-space), which has proven problematic due to the non-identiﬁability of factor\nanalysis associated with rotations in this space. From our perspective, however, we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2630, "text": "shall view factor analysis as a form of latent-variable density model, in which the\nform of the latent space is of interest but not the particular choice of coordinates\nused to describe it. If we wish to remove the degeneracy associated with latent-\nspace rotations, we must consider non-Gaussian latent-variable distributions, giving\nrise to independent component analysis models.Section 16.2.5\nAnother difference between probabilistic PCA and factor analysis is their be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2631, "text": "haviour under transformations of the data set. For PCA and probabilistic PCA, if weExercise 16.17\n514 16. CONTINUOUS LATENT V ARIABLES\nrotate the coordinate system in data space, then we obtain exactly the same ﬁt to the\ndata but with the W matrix transformed by the corresponding rotation matrix. How-\never, for factor analysis, the analogous property is that if we make a component-wise\nre-scaling of the data vectors, then this is absorbed into a corresponding re-scaling\nof the elements of \t."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2632, "text": "of the elements of \t.\n16.2.5 Independent component analysis\nOne generalization of the linear-Gaussian latent-variable model is to consider\nmodels in which the observed variables are related linearly to the latent variables,\nbut for which the latent distribution is non-Gaussian. An important class of such\nmodels, known asindependent component analysis, or ICA, arises when we consider\na distribution over the latent variables that factorizes, so that\np(z) =\nM∏\nj=1\np(zj): (16.55)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2633, "text": "p(z) =\nM∏\nj=1\np(zj): (16.55)\nTo understand the role of such models, consider a situation in which two people\nare talking at the same time, and we record their voices using two microphones.\nIf we ignore effects such as time delay and echoes, then the signals received by\nthe microphones at any point in time will be given by linear combinations of the\namplitudes of the two voices. The coefﬁcients of this linear combination will be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2634, "text": "constant, and if we can infer their values from sample data, then we can invert the\nmixing process (assuming it is non-singular) and thereby obtain two clean signals\neach of which contains the voice of just one person. This is an example of a problem\ncalled blind source separation in which ‘blind’ refers to the fact that we are given\nonly the mixed data, and neither the original sources nor the mixing coefﬁcients are\nobserved (Cardoso, 1998)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2635, "text": "observed (Cardoso, 1998).\nThis type of problem is sometimes addressed using the following approach\n(MacKay, 2003) in which we ignore the temporal nature of the signals and treat the\nsuccessive samples as i.i.d. We consider a generative model in which there are two\nlatent variables corresponding to the unobserved speech signal amplitudes, and there\nare two observed variables given by the signal values at the microphones. The latent"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2636, "text": "variables have a joint distribution that factorizes as above, and the observed variables\nare given by a linear combination of the latent variables. There is no need to include\na noise distribution because the number of latent variables equals the number of ob-\nserved variables, and therefore the marginal distribution of the observed variables\nwill not in general be singular, so the observed variables are simply deterministic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2637, "text": "linear combinations of the latent variables. Given a data set of observations, the\nlikelihood function for this model is a function of the coefﬁcients in the linear com-\nbination. The log likelihood can be maximized using gradient-based optimization\ngiving rise to a particular version of ICA.\nThe success of this approach requires that the latent variables have non-Gaussian\ndistributions. To see this, recall that in probabilistic PCA (and in factor analysis) the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2638, "text": "latent-space distribution is given by a zero-mean isotropic Gaussian. The model\ntherefore cannot distinguish between two different choices for the latent variables\n16.2. Probabilistic Latent Variables 515\nif these differ simply by a rotation in latent space. This can be veriﬁed directly\nby noting that the marginal density (16.35), and hence the likelihood function, is\nunchanged if we make the transformation W → WR where R is an orthogonal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2639, "text": "matrix satisfying RRT = I, because the matrixC given by (16.36) is itself invariant.\nExtending the model to allow more general Gaussian latent distributions does not\nchange this conclusion because, as we have seen, such a model is equivalent to the\nzero-mean isotropic Gaussian latent-variable model.\nAnother way to see why a Gaussian latent-variable distribution in a linear model\nis insufﬁcient to ﬁnd independent components is to note that the principal compo-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2640, "text": "nents represent a rotation of the coordinate system in data space so as to diagonalize\nthe covariance matrix. The data distribution in the new coordinates is then uncorre-\nlated. Although zero correlation is a necessary condition for independence it is not,\nhowever, sufﬁcient. In practice, a common choice for the latent-variable distributionExercise 2.39\nis given by\np(zj) = 1\n\u0019cosh(zj) = 2\n\u0019(ezj + e−zj ); (16.56)\nwhich has heavy tails compared to a Gaussian, reﬂecting the observation that many"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2641, "text": "real-world distributions also exhibit this property.\nThe original ICA model (Bell and Sejnowski, 1995) was based on the optimiza-\ntion of an objective function deﬁned by information maximization. One advantage\nof a probabilistic latent-variable formulation is that it helps to motivate and formu-\nlate generalizations of basic ICA. For instance, independent factor analysis (Attias,\n1999) considers a model in which the number of latent and observed variables can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2642, "text": "differ, the observed variables are noisy, and the individual latent variables have ﬂex-\nible distributions modelled by mixtures of Gaussians. The log likelihood for this\nmodel is maximized using EM, and the reconstruction of the latent variables is ap-\nproximated using a variational approach. Many other types of model have been\nconsidered, and there is now a huge literature on ICA and its applications (Jutten\nand Herault, 1991; Comon, Jutten, and Herault, 1991; Amari, Cichocki, and Yang,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2643, "text": "1996; Pearlmutter and Parra, 1997; Hyv ¨arinen and Oja, 1997; Hinton et al., 2001;\nMiskin and MacKay, 2001; Hojen-Sorensen, Winther, and Hansen, 2002; Choudrey\nand Roberts, 2003; Chan, Lee, and Sejnowski, 2003; Stone, 2004).\n16.2.6 Kalman ﬁlters\nSo far we have assumed that the data values are i.i.d. A common situation in\nwhich this assumption does not hold is when the data points form an ordered se-\nquence. We have seen that a hidden Markov model can be viewed as an extension"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2644, "text": "of the mixture models to allow for sequential correlations in the data. In a similarSection 15.3.1\nway, a continuous latent-variable model can be extended to handle sequential data\nby connecting the latent variables to form a Markov chain, as shown in the graph-\nical model of Figure 16.9. This is known as a linear dynamical system or Kalman\nﬁlter (Zarchan and Musoff, 2005). Note that this is the same graphical structure as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2645, "text": "a hidden Markov model. It is interesting to note that, historically, hidden MarkovSection 15.3.1\nmodels and linear dynamical systems were developed independently. Once they are\nboth expressed as graphical models, however, the deep relationship between them\n516 16. CONTINUOUS LATENT V ARIABLES\nFigure 16.9 A probabilistic graphical model for se-\nquential data, known as a linear dynami-\ncal system, or Kalman ﬁlter, in which the\nlatent variables form a Markov chain.\nz1 z2 zN\nx1 x2 xN"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2646, "text": "z1 z2 zN\nx1 x2 xN\nimmediately becomes apparent. Kalman ﬁlters are widely used in many real-time\ntracking applications, for example to track aircraft using radar reﬂections.\nIn the simplest such model, the distributionsp(xn|zn) in Figure 16.9 represent a\nlinear-Gaussian latent-variable model for that particular observation, of the kind we\nhave discussed previously for i.i.d. data. However, the latent variables {zn}are no"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2647, "text": "longer treated as independent but now form a Markov chain in which the distribution\np(zn|zn−1 ) of each latent variable is conditioned on the state of the previous latent\nvariable in the chain. Again these can be chosen to be linear-Gaussian in which\nthe distribution of zn is Gaussian with a mean given by a linear function of zn−1 .\nTypically the parameters of all the distributions p(xn|zn) are shared, and likewise"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2648, "text": "the parameters of the distributions p(zn|zn−1 ) are shared, so that the total number\nof parameters in the model is ﬁxed, independently of the length of the sequence.\nThese parameters can be learned from data by maximum likelihood with efﬁcient\nalgorithms that involve propagating messages around the graph (Bishop, 2006). For\nthe rest of this chapter, however, we will focus on i.i.d. data.\n16.3.\nEvidence Lower Bound"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2649, "text": "16.3.\nEvidence Lower Bound\nIn our discussion of models with discrete latent variables, we derived the evidence\nlower bound (ELBO) on the marginal log likelihood and showed how this forms theSection 15.4\nbasis for deriving the expectation–maximization (EM) algorithm including its gener-\nalizations such as variational inference. The same framework applies to continuous\nlatent variables as well as to models that combine both discrete and continuous vari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2650, "text": "ables. Here we present a slightly different derivation of the ELBO, and we assume\nthat the latent variables z are continuous.\nConsider a model p(x;z|w) with an observed variablex, a latent variablez, and\na learnable parameter vector w. If we introduce an arbitrary distribution q(z) over\nthe latent variable then we can write the log likelihood function ln p(x|w) as a sum\nof two terms in the formExercise 16.18\nln p(x|w) = L(w) + KL (q(z)∥p(z|x;w)) (16.57)\nwhere we have deﬁned\nL(q;w) =\n∫\nq(z) ln"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2651, "text": "where we have deﬁned\nL(q;w) =\n∫\nq(z) ln\n{ p(x;z|w)\nq(z)\n}\ndz (16.58)\nKL (q(z)∥p(z|x;w)) = −\n∫\nq(z) ln\n{ p(z|x;w)\nq(z)\n}\ndz: (16.59)\n16.3. Evidence Lower Bound 517\nSince KL (q(z)∥p(z|x;w)) is a Kullback–Leibler divergence, it satisﬁes the prop-\nerty KL (-∥-)> 0 from which it follows thatSection 2.5.5\nln p(x|w) > L(w) (16.60)\nand we therefore see that L(q;w) given by (16.58) forms a lower bound on the log\nlikelihood, known as the evidence lower bound or ELBO. We see that L(q;w) takes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2652, "text": "the same form (15.53 ) as derived for the discrete case but with summations replaced\nby integrals.\nWe can maximize the log likelihood function using a two-stage iterative proce-\ndure called the expectation maximization algorithm, or EM algorithm, in which we\nalternately maximize L(q;w) with respect to q(z) (the E step) and w (the M step).\nWe ﬁrst initialize the parameters w(old). Then in the E step we keep w ﬁxed and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2653, "text": "we maximize the lower bound with respect to q(z). This is easily done by noting\nthat the highest value for the bound is obtained by minimizing the Kullback–Leibler\ndivergence in (16.59) and hence is achieved when q(z) = p(z|x;w(old)) for which\nthe Kullback–Leibler divergence is zero. In the M step, we keep this choice of q(z)\nﬁxed and maximize L(q;w) with respect to w. Substituting for q(z) in (16.58) we\nobtain\nL(q;w) =\n∫\np(z|x;w(old)) lnp(x;z|w) dz\n−\n∫\np(z|x;w(old)) lnp(z|x;w(old)) dz: (16.61)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2654, "text": "−\n∫\np(z|x;w(old)) lnp(z|x;w(old)) dz: (16.61)\nWe now maximize this with respect to w in the M step while keeping w(old) ﬁxed.\nNote that the second term on the right-hand side of (16.61) is independent of w and\nso can be ignored during the M step. The ﬁrst term on the right-hand side is the\nexpectation of the complete data log likelihood where the expectation is taken withSection 15.3\nrespect to the posterior distribution of z computed using w(old)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2655, "text": "If we have a data set x1;:::; xN of i.i.d. observations then the likelihood func-\ntion takes the form\nln p(X|w) =\nN∑\nn=1\nln p(xn|w) (16.62)\nwhere the data matrix X comprises x1;:::; xN, and the parameters w are shared\nacross all data points. For each data point we introduce a corresponding latent vari-\nable zn with its associated distributionq(zn), and by following similar steps to those\nused to derive (16.58), we obtain the ELBO in the formExercise 16.19\nL(q;w) =\nN∑\nn=1\n∫\nq(zn) ln\n{ p(xn;zn|w)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2656, "text": "L(q;w) =\nN∑\nn=1\n∫\nq(zn) ln\n{ p(xn;zn|w)\nq(zn)\n}\ndzn: (16.63)\nWhen we discuss variational autoencoders, we will encounter a model for whichSection 19.2\nan exact solution to the E step is not feasible so instead a partial maximization is\nperformed by modelling q(z) using a deep neural network and then using the ELBO\nto learn the parameters of the network.\n518 16. CONTINUOUS LATENT V ARIABLES\n16.3.1 Expectation maximization\nWe can now use the EM algorithm, derived by iteratively maximizing the ev-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2657, "text": "idence lower bound, to learn the parameters of the probabilistic PCA model. This\nmay seem rather pointless because we have already obtained an exact closed-form\nsolution for the maximum likelihood parameter values. However, in spaces of high\ndimensionality, there may be computational advantages in using an iterative EM\nprocedure rather than working directly with the sample covariance matrix. This EM\nprocedure can also be extended to the factor analysis model, for which there is noSection 16.2.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2658, "text": "closed-form solution. Finally, it allows missing data to be handled in a principled\nway.\nWe can derive the EM algorithm for probabilistic PCA by following the general\nframework for EM. Thus, we write down the complete-data log likelihood and takeSection 15.3\nits expectation with respect to the posterior distribution of the latent distribution\nevaluated using ‘old’ parameter values. Maximization of this expected complete-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2659, "text": "data log likelihood then yields the ‘new’ parameter values. Because the data points\nare assumed independent, the complete-data log likelihood function takes the form\nln p\n(\nX;Z|\u0016;W;\u001b2)\n=\nN∑\nn=1\n{lnp(xn|zn) + lnp(zn)} (16.64)\nwhere the nth row of the matrix Z is given by zn. We already know that the exact\nmaximum likelihood solution for\u0016is given by the sample mean\nx deﬁned by (16.1),\nand it is convenient to substitute for \u0016at this stage. Making use of the expressions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2660, "text": "(16.31) and (16.32) for the latent and conditional distributions, respectively, and tak-\ning the expectation with respect to the posterior distribution over the latent variables,\nwe obtain\nE[ln p\n(\nX;Z|\u0016;W;\u001b2)\n] = −\nN∑\nn=1\n{ D\n2 ln(2\u0019\u001b2) + 1\n2Tr\n(\nE[znzT\nn]\n)\n+ 1\n2\u001b2 ∥xn −\u0016∥2 − 1\n\u001b2 E[zn]TWT(xn −\u0016)\n+ 1\n2\u001b2 Tr\n(\nE[znzT\nn]WTW\n)\n+ M\n2 ln(2\u0019)\n}\n: (16.65)\nNote that this depends on the posterior distribution only through the sufﬁcient statis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2661, "text": "tics of the Gaussian. Thus, in the E step, we use the old parameter values to evaluate\nE[zn] = M−1 WT(xn −x) (16.66)\nE[znzT\nn] = \u001b2M−1 + E[zn]E[zn]T; (16.67)\nwhich follow directly from the posterior distribution (16.43) together with the stan-\ndard result E[znzT\nn] = cov[zn] + E[zn]E[zn]T. Here M is deﬁned by (16.42).\nIn the M step, we maximize with respect to W and \u001b2, keeping the posterior\nstatistics ﬁxed. Maximization with respect to \u001b2 is straightforward. For the maxi-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2662, "text": "mization with respect to W, we make use of (A.24) to obtain the M-step equations:Exercise 16.21\n16.3. Evidence Lower Bound 519\nWnew =\n[N∑\nn=1\n(xn −x)E[zn]T\n][N∑\nn=1\nE[znzT\nn]\n]−1\n(16.68)\n\u001b2\nnew = 1\nND\nN∑\nn=1\n{\n∥xn −x∥2 −2E[zn]TWT\nnew(xn −x)\n+Tr\n(\nE[znzT\nn]WT\nnewWnew\n)}\n: (16.69)\nThe EM algorithm for probabilistic PCA proceeds by initializing the parameters\nand then alternately computing the sufﬁcient statistics of the latent space posterior"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2663, "text": "distribution using (16.66) and (16.67) in the E step and revising the parameter values\nusing (16.68) and (16.69) in the M step.\nOne of the beneﬁts of the EM algorithm for PCA is its computational efﬁciency\nfor large-scale applications (Roweis, 1998). Unlike conventional PCA based on an\neigenvector decomposition of the sample covariance matrix, the EM approach is\niterative and so might appear to be less attractive. However, each cycle of the EM"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2664, "text": "algorithm can be computationally much more efﬁcient than conventional PCA in\nspaces of high dimensionality. To see this, note that the eigendecomposition of the\ncovariance matrix requires O(D3) computation. Often we are interested only in the\nﬁrst M eigenvectors and their corresponding eigenvalues, in which case we can use\nalgorithms that are O(MD2). However, evaluating the covariance matrix requires\nO(ND2) computations, where N is the number of data points. Algorithms such"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2665, "text": "as the snapshot method (Sirovich, 1987), which assume that the eigenvectors are\nlinear combinations of the data vectors, avoid a direct evaluation of the covariance\nmatrix but are O(N3) and hence unsuited to large data sets. The EM algorithm\ndescribed here also does not construct the covariance matrix explicitly. Instead, the\nmost computationally demanding steps are those involving sums over the data set\nthat are O(NDM). For large D, and M ≪ D, this can be a signiﬁcant saving"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2666, "text": "compared to O(ND2) and can offset the iterative nature of the EM algorithm.\nNote that this EM algorithm can be implemented in an online form in which\neach D-dimensional data point is read in and processed and then discarded before\nthe next data point is considered. To see this, note that the quantities evaluated in\nthe E step (an M-dimensional vector and an M ×M matrix) can be computed for\neach data point separately, and in the M step we need to accumulate sums over data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2667, "text": "points, which we can do incrementally. This approach can be advantageous if both\nN and Dare large.\nBecause we now have a fully probabilistic model for PCA, we can deal with\nmissing data, provided that it is missing at random, in other words that the process\nthat determines which values are missing does not depend on the values of any ob-\nserved or unobserved variables. Such data sets can be handled by marginalizing over"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2668, "text": "the distribution of the unobserved variables, and the resulting likelihood function can\nbe maximized using the EM algorithm.Exercise 16.22\n16.3.2 EM for PCA\nAnother elegant feature of the EM approach is that we can take the limit\u001b2 → 0,\ncorresponding to standard PCA, and still obtain a valid EM-like algorithm (Roweis,\n520 16. CONTINUOUS LATENT V ARIABLES\n1998). From (16.67), we see that the only quantity we need to compute in the E step"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2669, "text": "is E[zn]. Furthermore, the M step is simpliﬁed because M = WTW. To emphasize\nthe simplicity of the algorithm, let us deﬁne ˜X to be a matrix of size N ×Dwhose\nnth row is given by the vector xn −x and similarly deﬁne\n to be a matrix of size\nM ×N whose nth column is given by the vector E[zn]. The E step (16.66) of the\nEM algorithm for PCA then becomes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2670, "text": "= (WT\noldWold)−1 WT\nold ˜XT (16.70)\nand the M step (16.68) takes the form\nWnew = ˜XT\nT("}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2671, "text": "T)−1 : (16.71)\nAgain these can be implemented in an online form. These equations have a simple\ninterpretation as follows. From our earlier discussion, we see that the E step involves\nan orthogonal projection of the data points onto the current estimate for the principal\nsubspace. Correspondingly, the M step represents a re-estimation of the principal\nsubspace to minimize the reconstruction error in which the projections are ﬁxed.Exercise 16.23"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2672, "text": "We can give a simple physical analogy for this EM algorithm, which is easily\nvisualized for D = 2 and M = 1. Consider a collection of data points in two\ndimensions, and let the one-dimensional principal subspace be represented by a solid\nrod. Now attach each data point to the rod via a spring obeying Hooke’s law (force\nis proportional to the length of the spring and therefore stored energy is proportional\nto the square of the spring’s length). In the E step, we keep the rod ﬁxed and allow"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2673, "text": "the attachment points to slide up and down the rod so as to minimize the energy.\nThis causes each attachment point (independently) to position itself at the orthogonal\nprojection of the corresponding data point onto the rod. In the M step, we keep the\nattachment points ﬁxed and then release the rod and allow it to move to the minimum\nenergy position. The E step and M step are then repeated until a suitable convergence\ncriterion is satisﬁed, as is illustrated in Figure 16.10."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2674, "text": "16.3.3 EM for factor analysis\nWe can determine the parameters \u0016, W, and \t in a factor analysis model bySection 16.2.4\nmaximum likelihood. The solution for \u0016is again given by the sample mean. How-\never, unlike probabilistic PCA, there is no longer a closed-form maximum likelihood\nsolution for W, which must therefore be found iteratively. Because factor analysis is\na latent-variable model, this can be done using an EM algorithm (Rubin and Thayer,Exercise 16.24"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2675, "text": "1982) that is analogous to the one used for probabilistic PCA. Speciﬁcally, the E-step\nequations are given by\nE[zn] = GWT\t−1 (xn −\nx) (16.72)\nE[znzT\nn] = G + E[zn]E[zn]T (16.73)\nwhere we have deﬁned\nG = (I + WT\t−1 W)−1 : (16.74)\n16.3. Evidence Lower Bound 521\n(a)\n−2 0 2\n−2\n0\n2\n(b)\n−2 0 2\n−2\n0\n2\n(c)\n−2 0 2\n−2\n0\n2\n(d)\n−2 0 2\n−2\n0\n2\n(e)\n−2 0 2\n−2\n0\n2\n(f)\n−2 0 2\n−2\n0\n2\nFigure 16.10 Synthetic data illustrating the EM algorithm for PCA deﬁned by (16.70) and (16.71). (a) A set"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2676, "text": "of data points shown in green, together with the true principal components (shown as eigenvectors scaled by\nthe square roots of the eigenvalues). (b) Initial conﬁguration of the principal subspace deﬁned by W, shown in\nred, together with the projections of the latent points Z into the data space, given by ZWT, shown in cyan. (c)\nAfter one M step, W has been updated with Z held ﬁxed. (d) After the successive E step, the values of Z have"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2677, "text": "been updated, giving orthogonal projections, with W held ﬁxed. (e) After the second M step. (f) The converged\nsolution.\nNote that this is expressed in a form that involves inversion of matrices of sizeM×M\nrather than D×D(except for the D×Ddiagonal matrix \t whose inverse is trivial\nto compute in O(D) steps), which is convenient because often M ≪ D. Similarly,\nthe M-step equations take the formExercise 16.25\nWnew =\n[N∑\nn=1\n(xn −x)E[zn]T\n][N∑\nn=1\nE[znzT\nn]\n]−1\n(16.75)\n\tnew = diag\n{\nS −Wnew\n1\nN\nN∑"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2678, "text": "n]\n]−1\n(16.75)\n\tnew = diag\n{\nS −Wnew\n1\nN\nN∑\nn=1\nE[zn](xn −x)T\n}\n(16.76)\nwhere the diag operator sets all the non-diagonal elements of a matrix to zero.\n522 16. CONTINUOUS LATENT V ARIABLES\n16.4. Nonlinear Latent Variable Models\nSo far in this chapter we have focused on latent variable models based on linear trans-\nformations from the latent space to the data space. It is natural to ask whether we\ncan use the ﬂexibility of deep neural networks to represent more complex transfor-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2679, "text": "mations, while exploiting the learning ability of deep networks to allow the resulting\ndistribution to be ﬁtted to a data set. Consider a simple distribution over a vector\nvariable z, for example a Gaussian of the form\npz(z) = N(z|0;I): (16.77)\nNow suppose we transform z using a function x = g(z;w) given by a deep neu-\nral network, where w represents the weights and biases. The combination of the\ndistribution over z together with the neural network deﬁnes a distribution over x."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2680, "text": "Sampling from such a model is straightforward because we can generate samples\nfrom pz(z) and then transform each of them using the neural network function to\ngive corresponding samples of x. This is an efﬁcient process since it does not in-\nvolve iteration.\nTo learn g(z;w) from data, consider how to evaluate the likelihood function\np(x|w). The distribution of x is given by the change of variables formula for densi-\nties:Section 2.4\npx(x) = pz(z(x)) |detJ(x)| (16.78)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2681, "text": "px(x) = pz(z(x)) |detJ(x)| (16.78)\nwhere J is the Jacobian matrix of partial derivatives whose elements are given by\nJij(x) = @zi\n@xj\n: (16.79)\nTo evaluate the distribution pz(z(x)) on the right-hand side of (16.78) for a given\ndata vector x and to evaluate the Jacobian matrix in (16.79) for that same value of\nx, we need the inverse z = g−1 (x;w) of the neural network function. For most\nneural networks this inverse will not be well deﬁned. For example, the network may"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2682, "text": "represent a many-to-one function in which multiple different input values map to\nthe same output value, in which case the change of variable formula does not give a\nwell-deﬁned density. Moreover, if the dimensionality of the latent space is different\nfrom that of the data space then the transformation will not be invertible.\nOne approach is to restrict our attention to functions g(z;w) that are invertible,\nwhich requires that z and x have the same dimensionality. We will explore this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2683, "text": "approach in more detail when we introduce the technique of normalizing ﬂows.Chapter 18\n16.4.1 Nonlinear manifolds\nRequiring that the latent and data spaces have the same number of dimensions\nis a signiﬁcant limitation. Consider the situation in which z has dimensionality M\nand x has dimensionality D, where M < D. In this case the distribution over x\nis conﬁned to a manifold, or subspace, of dimensionality M, as illustrated in Fig-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2684, "text": "ure 16.11. Low-dimensional manifolds arise in many machine learning applications,\n16.4. Nonlinear Latent Variable Models 523\nFigure 16.11 Illustration of a mapping\nfrom a two-dimensional la-\ntent space z = ( z1;z2)\nto a three-dimensional data\nspace x = ( x1;x2;x3) us-\ning a nonlinear function x =\ng(z;w) represented by a\nneural network with param-\neter vector w.\nz1\nz2\nx1\nx3\nx2\nx\nz\ng(z;w)\nfor example when modelling the distribution of natural images. Nonlinear latent-Section 6.1.4"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2685, "text": "variable models can be very useful in modelling such data because they express the\nstrong inductive bias that the data does not ‘ﬁll’ the data space but is conﬁned to a\nmanifold, although the shape and dimensionality of this manifold are typically not\nknown in advance.\nHowever, one problem with this framework is that it assigns zero probability\ndensity to any data vector that does not lie exactly on the manifold, which is a prob-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2686, "text": "lem for gradient-based learning since the likelihood function will be zero at each of\nthe data points and constant for small changes in w, for any realistic data set. To ad-\ndress this, we follow the approach used previously with regression and classiﬁcation\nproblems and deﬁne a conditional distribution across the entire data space, whose pa-\nrameters are given by the output of the neural network. If, for example, x comprises"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2687, "text": "a vector of continuous variables then we can choose the conditional distribution to\nbe a Gaussian:\np(x|z;w) = N(x|g(z;w);\u001b2I) (16.80)\nin which the neural network g(z;w) has linear output-unit activation functions, and\ng ∈RD. The generative model is speciﬁed by the latent distribution over z to-\ngether with the conditional distribution overx, and can be represented by the simple\ngraphical model shown in Figure 16.12.\nNote that it is straightforward, and computationally efﬁcient, to draw indepen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2688, "text": "dent samples from this distribution. We ﬁrst draw a sample from the Gaussian distri-\nbution (16.77) using standard methods. Next, we use this value as input to the neuralSection 14.1.2\nnetwork, giving an output value g(z;w). Finally, we draw a sample from a Gaus-\nsian distribution with mean g(z;w) and covariance \u001b2I, as deﬁned by (16.80). This\nthree-step process can then be repeated to generate multiple independent samples."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2689, "text": "The combination of a latent-variable distribution p(z) and a conditional distri-\nFigure 16.12 Graphical model representing the distribution given by (16.77) and\n(16.80), which together deﬁne a joint distribution p(x;z) = p(x|z)p(z).\nz\nx\n524 16. CONTINUOUS LATENT V ARIABLES\n−4 −2 0 2 4\nz\np(z)\n(a)\nz\nx1\nx2 (b)\nFigure 16.13 Illustration of a nonlinear latent-variable model for a one-dimensional latent space and a two-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2690, "text": "dimensional data space. (a) The prior distribution in latent space is given by a zero-mean unit-variance Gaussian\ndistribution. (b) The three left-most plots show examples of the Gaussian conditional distribution p(x|z) for\ndifferent values of z, whereas the right-most plot shows the marginal distribution p(x). The nonlinear function\ng(z), which deﬁnes the mean of the conditional distribution, is given by g1(z) = sin(z ), g2(z) = cos(z ), and,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2691, "text": "therefore, traces out a circle in data space. The standard deviation of the conditional distribution is given by\n\u001b= 0:3. [Based on Prince (2020) with permission.]\nbution p(x|z)deﬁnes a marginal distribution over the data space given by\np(x) =\n∫\np(z)p(x|z) dz: (16.81)\nWe illustrate this using a simple example involving a one-dimensional latent space\nand a two-dimensional data space in Figure 16.13.\n16.4.2 Likelihood function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2692, "text": "16.4.2 Likelihood function\nWe have seen that it is easy to draw samples from this nonlinear latent-variable\nmodel. Now suppose we wish to ﬁt the model to an observed data set by maximizing\nthe likelihood function. The likelihood is obtained from the product and sum rules\nof probability by integrating over z:\np(x|w) =\n∫\np(x|z;w)p(z) dz\n=\n∫\nN(x|g(z;w);\u001b2I)N(z|0;I) dz: (16.82)\nAlthough both distributions inside the integral are Gaussian, the integral is analyti-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2693, "text": "cally intractable due to the highly nonlinear function g(z;w) deﬁned by the neural\nnetwork.\n16.4. Nonlinear Latent Variable Models 525\nFigure 16.14 Three example images\nof handwritten digits, illustrating why\nsampling from the latent space to evalu-\nate the likelihood function requires large\nnumbers of samples. (a) shows the\noriginal image, (b) shows a corrupted\nimage with part of the stroke removed,\nand (c) shows the original image shifted\nby half a pixel down and half a pixel to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2694, "text": "by half a pixel down and half a pixel to\nthe right. Image (b) is closer to (a) in\nterms of likelihood, even though image\n(c) is much closer to (a) in appearance.\n[From Doersch (2016) with permission.]\nOne approach for evaluating the likelihood function would be to draw samples\nfrom the latent space distribution and use these to approximate (16.82) by\np(x|w) ≃ 1\nK\nK∑\ni=1\np(x|zi;w) (16.83)\nwhere zi ∼p(z). This expresses the distribution over z as a mixture of Gaussians"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2695, "text": "with ﬁxed mixing coefﬁcients given by1=K, and in the limit of an inﬁnite number of\nsamples, this gives the true likelihood function. However, the value ofKneeded for\neffective training will typically be far too high to be practical. To see why, consider\nthe three images of handwritten digits shown inFigure 16.14, and suppose that image\n(a) represents the vector x for which we wish to evaluate the likelihood function. If"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2696, "text": "a trained model generated image (b), we would consider this a poor model as this\nimage is not a good representation of a digit ‘2’, and so this should be assigned\na much lower likelihood. Conversely, image (c), which was obtained by shifting\nthe digit in (a) down and to the right by half a pixel, is a good example of a digit\n‘2’ and should therefore have a high likelihood. Since the distribution (16.80) is\nGaussian, the likelihood function is proportional to the exponential of the negative"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2697, "text": "squared distance between the output of the network and the data vector x. However,\nthe squared distance between (a) and (b) is 0:0387 whereas the squared distance\nbetween (a) and (c) is 0:2693. So if the variance parameter \u001b2 is set to a sufﬁciently\nsmall value that image (b) has low likelihood, then image (c) will have an even lower\nlikelihood. Even if the model is good at generating digits, we would have to consider"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2698, "text": "extremely large numbers of samples for z before seeing a digit that is sufﬁciently\nclose to (a). We therefore seek more sophisticated techniques for training nonlinear\nlatent variable models that can be used in practical applications. Before outlining\nsuch methods, we ﬁrst discuss brieﬂy some considerations regarding discrete data\nspaces.\n526 16. CONTINUOUS LATENT V ARIABLES\nFigure 16.15 Schematic illustration of de-\nquantization, showing (a)\na discrete distribution over"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2699, "text": "a discrete distribution over\na single variable and (b)\nan associated dequantized\ncontinuous distribution.\n0 1 2\n(a)\n0 1 2 (b)\n16.4.3 Discrete data\nIf the observed data set comprises independent binary variables then we can use\na conditional distribution of the form\np(x|z;w) =\nD∏\ni=1\ngi(z;w)xi (1 −gi(z;w))1−xi\n(16.84)\nwhere gi(z;w) = \u001b(ai(z;w)) represents the activation of output unit i, the activa-\ntion function \u001b(-)is given by the logistic sigmoid, and ai(z;w) is the pre-activation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2700, "text": "for output unit i. Similarly, for one-hot encoded categorical variables, we can use a\nmultinomial distribution:\np(x|z;w) =\nD∏\ni=1\ngi(z;w)xi (16.85)\nwhere\ngi(z;w) = exp(ai(z;w))∑\njexp(aj(z;w)) (16.86)\nis the softmax activation function. We can also consider combinations of discrete\nand continuous variables by forming the product of the associated conditional distri-\nbutions.\nIn practice, continuous variables are represented with discrete values, for exam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2701, "text": "ple in images, the red, green, and blue channel intensities might be expressed using\n8-bit numbers representing the values {0; :::;255}. This can cause problems when\nwe employ highly ﬂexible models based on deep neural networks, as the likelihood\nfunction can go to zero if the density collapses onto one or more of the discrete val-\nues. The problem can be resolved using a technique called dequantization, which\ninvolves adding noise to the variables, typically drawn from a uniform distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2702, "text": "over the region between successive discrete values, as shown in Figure 16.15. A\ntraining set is dequantized by replacing each observed value with a sample drawn\nrandomly from the associated continuous distribution associated with that discrete\nvalue, and this makes it less likely that the model will discover a pathological solu-\ntion.\nExercises 527\n16.4.4 Four approaches to generative modelling\nWe have seen that nonlinear latent-variable models based on deep neural net-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2703, "text": "works offer a highly ﬂexible framework for building generative models. Due to the\nuniversality of the neural network transformation, such models are capable, in prin-\nciple, of approximating essentially any desired distribution to high accuracy. More-\nover, such models offer the potential, once trained, to generate samples from the\ndistribution in using an efﬁcient, non-iterative process. However, we have also iden-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2704, "text": "tiﬁed some challenges associated with training such models that force us to develop\nmore sophisticated techniques than those needed for linear models. Many such meth-\nods have been proposed, each having their own strengths and limitations. These can\nbe broadly grouped into four approaches, as follows.\nWith generative adversarial networks, or GANs, we relax the requirement forChapter 17\nthe network mapping to be invertible, thereby allowing the latent space to have a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2705, "text": "lower dimensionality than the data space. We also abandon the concept of a likeli-\nhood function and instead introduce a second neural network whose function is to\nprovide a training signal for the generative network. Due to the absence of a well-\ndeﬁned likelihood function, the training procedure may be brittle, but once trained it\nis straightforward to generate samples from the model, and the results can be of high\nquality."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2706, "text": "quality.\nThe framework of variational autoencoders, or V AEs, also uses a second neuralChapter 19\nnetwork whose role is to approximate the posterior distribution over the latent vari-\nables, thereby allowing an approximation to the likelihood function to be evaluated.\nTraining is more robust than with GANs, and sampling from the trained model is\nstraightforward, although it can be harder to obtain the highest quality results."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2707, "text": "In normalizing ﬂows, we set the dimensionality of the latent space to be equalChapter 18\nto that of the data space and then modify the generative neural network so that it\nbecomes invertible. The requirement that the network is invertible restricts its func-\ntional form but it allows the likelihood function to be evaluated without approxima-\ntion and it also allows for efﬁcient sampling.\nFinally, diffusion models use a network that learns to transform a sample fromChapter 20"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2708, "text": "the prior distribution into a sample from the data distribution through a sequence of\ndenoising steps. This leads to state-of-the-art performance in many applications, al-\nthough the cost of sampling can be high due to the multiple denoising passes through\nthe network.\nWe explore these approaches in detail in the ﬁnal four chapters of this book.\nExercises\n16.1 (??) In this exercise, we use proof by induction to show that the linear projection"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2709, "text": "onto an M-dimensional subspace that maximizes the variance of the projected data\nis deﬁned by the M eigenvectors of the data covariance matrix S, given by (16.3),\ncorresponding to the M largest eigenvalues. In Section 16.1, this result was proven\nfor M = 1. Now suppose the result holds for some general value ofMand show that\nit consequently holds for dimensionality M + 1. To do this, ﬁrst set the derivative\nof the variance of the projected data with respect to a vector uM+1 deﬁning the new"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2710, "text": "528 16. CONTINUOUS LATENT V ARIABLES\ndirection in data space equal to zero. This should be done subject to the constraints\nthat uM+1 are orthogonal to the existing vectors u1;:::; uM, and also that it is\nnormalized to unit length. Use Lagrange multipliers to enforce these constraints.Appendix C\nThen make use of the orthonormality properties of the vectors u1;:::; uM to show\nthat the new vector uM+1 is an eigenvector of S. Finally, show that the variance is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2711, "text": "maximized if the eigenvector is chosen to be the one corresponding to eigenvalue\n\u0015M+1 where the eigenvalues have been ordered in decreasing value.\n16.2 (??) Show that the minimum value of the PCA error measure J given by (16.15)\nwith respect to the ui, subject to the orthonormality constraints (16.7), is obtained\nwhen the ui are eigenvectors of the data covariance matrixS. To do this, introduce a\nmatrix H of Lagrange multipliers, one for each constraint, so that the modiﬁed error"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2712, "text": "measure, in matrix notation reads\n˜J = Tr\n{\nˆUTS ˆU\n}\n+ Tr\n{\nH(I −ˆUT ˆU)\n}\n(16.87)\nwhere ˆU is a matrix of dimension D×(D−M) whose columns are given by ui.\nNow minimize ˜J with respect to ˆU and show that the solution satisﬁes S ˆU = ˆUH.\nClearly, one possible solution is that the columns of ˆU are eigenvectors of S, in\nwhich case H is a diagonal matrix containing the corresponding eigenvalues. To\nobtain the general solution, show that H can be assumed to be a symmetric matrix,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2713, "text": "and by using its eigenvector expansion, show that the general solution toS ˆU = ˆUH\ngives the same value for ˜J as the speciﬁc solution in which the columns of ˆU are\nthe eigenvectors of S. Because these solutions are all equivalent, it is convenient to\nchoose the eigenvector solution.\n16.3 (?) Verify that the eigenvectors deﬁned by (16.30) are normalized to unit length,\nassuming that the eigenvectors vi have unit length."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2714, "text": "16.4 (?) Suppose we replace the zero-mean, unit-covariance latent space distribution (16.31)\nin the probabilistic PCA model by a general Gaussian distribution of the formN(z|m;\u0006).\nBy redeﬁning the parameters of the model, show that this leads to an identical model\nfor the marginal distribution p(x) over the observed variables for any valid choice of\nm and \u0006.\n16.5 (??) Let x be a D-dimensional random variable having a Gaussian distribution given"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2715, "text": "by N(x|\u0016;\u0006), and consider the M-dimensional random variable given by y =\nAx+ b where A is an M×Dmatrix. Show that y also has a Gaussian distribution,\nand ﬁnd expressions for its mean and covariance. Discuss the form of this Gaussian\ndistribution for M <D, for M = D, and for M >D.\n16.6 (??) By making use of the results (2.122) and (2.123) for the mean and covariance\nof a general distribution, derive the result (16.35) for the marginal distribution p(x)\nin the probabilistic PCA model."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2716, "text": "in the probabilistic PCA model.\n16.7 (?) Draw a directed probabilistic graph for the probabilistic PCA model described in\nSection 16.2 in which the components of the observed variablex are shown explicitly\nExercises 529\nas separate nodes. Hence, verify that the probabilistic PCA model has the same\nindependence structure as the naive Bayes model discussed in Section 11.2.3.\n16.8 (??) By making use of the result (3.100), show that the posterior distributionp(z|x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2717, "text": "for the probabilistic PCA model is given by (16.43).\n16.9 (?) Verify that maximizing the log likelihood (16.44) for the probabilistic PCA model\nwith respect to the parameter \u0016gives the result \u0016ML = x where x is the mean of the\ndata vectors.\n16.10 (??) By evaluating the second derivatives of the log likelihood function (16.44) for\nthe probabilistic PCA model with respect to the parameter\u0016, show that the stationary\npoint \u0016ML = x represents the unique maximum."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2718, "text": "point \u0016ML = x represents the unique maximum.\n16.11 (??) Show that in the limit \u001b2 → 0, the posterior mean for the probabilistic PCA\nmodel becomes an orthogonal projection onto the principal subspace, as in conven-\ntional PCA.\n16.12 (??) For \u001b2 > 0 show that the posterior mean in the probabilistic PCA model is\nshifted towards the origin relative to the orthogonal projection.\n16.13 (??) Show that the optimal reconstruction of a data point under probabilistic PCA,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2719, "text": "according to the least-squares projection cost of conventional PCA, is given by\n˜x = WML(WT\nMLWML)−1 ME[z|x]: (16.88)\n16.14 (?) The number of independent parameters in the covariance matrix for a probabilis-\ntic PCA model with an M-dimensional latent space and a D-dimensional data space\nis given by (16.52). Verify that for M = D−1, the number of independent param-\neters is the same as in a general covariance Gaussian, whereas for M = 0 it is the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2720, "text": "same as for a Gaussian with an isotropic covariance.\n16.15 (?) Derive an expression for the number of independent parameters in the factor\nanalysis model described in Section 16.2.4.\n16.16 (??) Show that the factor analysis model described in Section 16.2.4 is invariant\nunder rotations of the latent space coordinates.\n16.17 (??) Consider a linear-Gaussian latent-variable model having a latent space distri-\nbution p(z) = N(x|0;I) and a conditional distribution for the observed variable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2721, "text": "p(x|z) = N(x|Wz+ \u0016;\b) where \b is an arbitrary symmetric positive-deﬁnite\nnoise covariance matrix. Now suppose that we make a non-singular linear transfor-\nmation of the data variables x → Ax, where A is a D×Dmatrix. If \u0016ML, WML,\nand \bML represent the maximum likelihood solution corresponding to the original\nun-transformed data, show that A\u0016ML, AWML, and A\bMLAT represent the corre-\nsponding maximum likelihood solution for the transformed data set. Finally, show"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2722, "text": "that the form of the model is preserved in two cases: (i) A is a diagonal matrix\nand \b is a diagonal matrix. This corresponds to factor analysis. The transformed\n\b remains diagonal, and hence factor analysis is covariant under component-wise\n530 16. CONTINUOUS LATENT V ARIABLES\nre-scaling of the data variables; (ii) A is orthogonal and \b is proportional to the unit\nmatrix so that \b = \u001b2I. This corresponds to probabilistic PCA. The transformed \b"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2723, "text": "matrix remains proportional to the unit matrix, and hence probabilistic PCA is co-\nvariant under a rotation of the axes of the data space, as is the case for conventional\nPCA.\n16.18 (?) Verify that the log likelihood function for a model with continuous latent vari-\nables can be written as the sum of two terms in the form (16.57) in which the terms\nare deﬁned by (16.58) and (16.59). This can be done by using the product rule of\nprobability in the form\np(x;z|w) = p(z|x;w)p(x|w) (16.89)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2724, "text": "p(x;z|w) = p(z|x;w)p(x|w) (16.89)\nand then substituting for p(x;z|w) in (16.58).\n16.19 (?) Show that, for a set of i.i.d. data, the evidence lower bound (ELBO) takes the\nform (16.63).\n16.20 (??) Draw a directed probabilistic graphical model representing a discrete mixture\nof probabilistic PCA models in which each PCA model has its own values of W,\n\u0016, and \u001b2. Now draw a modiﬁed graph in which these parameter values are shared\nbetween the components of the mixture."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2725, "text": "between the components of the mixture.\n16.21 (??) Derive the M-step equations (16.68) and (16.69) for the probabilistic PCA\nmodel by maximizing the expected complete-data log likelihood function given by\n(16.65).\n16.22 (???) One beneﬁt of a probabilistic formulation of principal component analysis is\nthat it can be applied to a data set in which some of the values are missing, provided\nthey are missing at random. Derive the EM algorithm for maximizing the likelihood"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2726, "text": "function for the probabilistic PCA model in this situation. Note that the{zn}, as well\nas the missing data values that are components of the vectors {xn}, are now latent\nvariables. Show that in the special case in which all the data values are observed,\nthis reduces to the EM algorithm for probabilistic PCA derived in Section 16.3.2.\n16.23 (??) Let W be a D ×M matrix whose columns deﬁne a linear subspace of di-\nmensionality M embedded within a data space of dimensionality D, and let \u0016be a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2727, "text": "D-dimensional vector. Given a data set {xn}where n= 1;:::;N , we can approx-\nimate the data points using a linear mapping from a set of M-dimensional vectors\n{zn}, so that xn is approximated by Wzn + \u0016. The associated sum-of-squares\nreconstruction cost is given by\nJ =\nN∑\nn=1\n∥xn −\u0016−Wzn∥2: (16.90)\nFirst show that minimizingJwith respect to\u0016leads to an analogous expression with\nxn and zn replaced by zero-mean variablesxn−\nx and zn−z, respectively, wherex"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2728, "text": "x and zn−z, respectively, wherex\nand z denote sample means. Then show that minimizing Jwith respect to zn, where\nExercises 531\nW is kept ﬁxed, gives rise to the PCA E step (16.70), and that minimizing J with\nrespect to W, where {zn}is kept ﬁxed, gives rise to the PCA M step (16.71).\n16.24 (??) Derive the formulae (16.72) and (16.73) for the E step of the EM algorithm for\nfactor analysis. Note that from the result of Exercise 16.26, the parameter \u0016can be\nreplaced by the sample mean x."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2729, "text": "replaced by the sample mean x.\n16.25 (??) Write down an expression for the expected complete-data log likelihood func-\ntion for the factor analysis model, and hence derive the corresponding M-step equa-\ntions (16.75) and (16.76).\n16.26 (??) By considering second derivatives, show that the only stationary point of the\nlog likelihood function for the factor analysis model discussed in Section 16.2.4\nwith respect to the parameter \u0016 is given by the sample mean deﬁned by (16.1)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2730, "text": "Furthermore, show that this stationary point is a maximum.\n17\nGenerative\nAdversarial\nNetworks\nGenerative models use machine learning algorithms to learn a distribution from a set\nof training data and then generate new examples from that distribution. For example,\na generative model might be trained on images of animals and then used to generate\nnew images of animals. We can think of such a generative model in terms of a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2731, "text": "distribution p(x|w) in which x is a vector in the data space, and w represent the\nlearnable parameters of the model. In many cases we are interested in conditional\ngenerative models of the formp(x|c;w) where c represents a vector of conditioning\nvariables. In the case of our generative model for animal images, we may wish to\nspecify that a generated image should be of a particular animal, such as a cat or a\ndog, speciﬁed by the value of c."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2732, "text": "dog, speciﬁed by the value of c.\nFor real-world applications such as image generation, the distributions are ex-\ntremely complex, and consequently the introduction of deep learning has dramati-\ncally improved the performance of generative models. We have already encountered\n533© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_17"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2733, "text": "534 17. GENERATIVE ADVERSARIAL NETWORKS\nz Generator\ng(z;w)\nsynthetic images\nreal images\nDiscriminator\nd(x;\u001e)\nt\nFigure 17.1 Schematic illustration of a GAN in which a discriminator neural network d(x;\u001e) is trained\nto distinguish between real samples from the training set, in this case images of kittens,\nand synthetic samples produced by the generator network g(z;w). The generator aims\nto maximize the error of the discriminator network by producing realistic images, whereas"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2734, "text": "the discriminator network tries to minimize the same error by becoming better at distin-\nguishing real from synthetic examples.\nan important class of deep generative models when we discussed autoregressive\nlarge language models based on transformers. We have also outlined four impor-Chapter 12\ntant classes of generative model based on nonlinear latent variable models, and inSection 16.4.4\nthis chapter we discuss the ﬁrst of these, called generative adversarial networks. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2735, "text": "other three approaches will be discussed in subsequent chapters.\n17.1.\nAdversarial Training\nConsider a generative model based on a nonlinear transformation from a latent space\nz to a data space x. We introduce a latent distribution p(z), which might take the\nform of a simple Gaussian\np(z) = N(z|0;I); (17.1)\nalong with with a nonlinear transformation x = g(z;w) deﬁned by a deep neural\nnetwork with learnable parameters w known as the generator. Together these im-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2736, "text": "plicitly deﬁne a distribution over x, and our goal is to ﬁt this distribution to a data\nset of training examples {xn}where n= 1;:::;N . However, we cannot determine\nw by optimizing the likelihood function because this cannot, in general, be evalu-\nated in closed form. The key idea of generative adversarial networks, or GANs,\n(Goodfellow et al., 2014; Ruthotto and Haber, 2021) is to introduce a second dis-\ncriminator network, which is trained jointly with the generator network and which"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2737, "text": "provides a training signal to update the weights of the generator. This is illustrated\nin Figure 17.1.\n17.1. Adversarial Training 535\nThe goal of the discriminator network is to distinguish between real examples\nfrom the data set and synthetic, or ‘fake’, examples produced by the generator net-\nwork, and it is trained by minimizing a conventional classiﬁcation error function.\nConversely, the goal of the generator network is to maximize this error by synthe-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2738, "text": "sizing examples from the same distribution as the training set. The generator and\ndiscriminator networks are therefore working against each other, hence the term ‘ad-\nversarial’. This is an example of azero-sum game in which any gain by one network\nrepresents a loss to the other. It allows the discriminator network to provide a training\nsignal, which can be used to train the generator network, and this turns the unsuper-\nvised density modelling problem into a form of supervised learning."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2739, "text": "17.1.1 Loss function\nTo make this precise, we deﬁne a binary target variable given by\nt= 1; real data; (17.2)\nt= 0; synthetic data: (17.3)\nThe discriminator network has a single output unit with a logistic-sigmoid activation\nfunction, whose output represents the probability that a data vector x is real:\nP(t= 1) = d(x;\u001e): (17.4)\nWe train the discriminator network using the standard cross-entropy error function,\nwhich takes the form\nE(w;\u001e) = −1\nN\nN∑\nn=1\n{tnln dn + (1 −tn) ln(1−dn)} (17.5)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2740, "text": "N\nN∑\nn=1\n{tnln dn + (1 −tn) ln(1−dn)} (17.5)\nwhere dn = d(xn;\u001e) is the output of the discriminator network for input vector\nn, and we have normalized by the total number of data points. The training setSection 1.2.4\ncomprises both real data examples denoted xn and synthetic examples given by the\noutput of the generator network g(zn;w) where zn is a random sample from the\nlatent space distribution p(z). Since tn = 1 for real examples and tn = 0 for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2741, "text": "synthetic examples, we can write the error function (17.5) in the form\nEGAN(w;\u001e) = − 1\nNreal\n∑\nn∈real\nln d(xn;\u001e)\n− 1\nNsynth\n∑\nn∈synth\nln(1 −d(g(zn;w);\u001e)) (17.6)\nwhere typically the number Nreal of real data points is equal to the number Nsynth\nof synthetic data points. This combination of generator and discriminator networks\ncan be trained end-to-end using stochastic gradient descent with gradients evalu-Chapter 7"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2742, "text": "ated using backpropagation. However, the unusual aspect is the adversarial training\nwhereby the error is minimized with respect to \u001ebut maximized with respect to w.\n536 17. GENERATIVE ADVERSARIAL NETWORKS\nThis maximization can be done using standard gradient-based methods with the sign\nof the gradient reversed, so that the parameter updates become\n∆\u001e = −\u0015∇\u001eEn(w;\u001e) (17.7)\n∆w = \u0015∇wEn(w;\u001e) (17.8)\nwhere En(w;\u001e) denotes the error deﬁned for data point n or more generally for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2743, "text": "a mini-batch of data points. Note that the two terms in (17.7) and (17.8) have dif-\nferent signs since the discriminator is trained to decrease the error rate whereas the\ngenerator is trained to increase it. In practice, training alternates between updating\nthe parameters of the generative network and updating those of the discriminative\nnetwork, in each case taking just one gradient descent step using a mini-batch, af-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2744, "text": "ter which a new set of synthetic samples is generated. If the generator succeeds in\nﬁnding a perfect solution, then the discriminator network will be unable to tell the\ndifference between the real and synthetic data and hence will always produce an out-\nput of 0:5. Once the GAN is trained, the discriminator network is discarded and the\ngenerator network can be used to synthesize new examples in the data space by sam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2745, "text": "pling from the latent space and propagating those samples through the trained gen-\nerator network. We can show that for generative and discriminative networks having\nunlimited ﬂexibility, a fully optimized GAN will have a generative distribution that\nmatches the data distribution exactly. Some impressive examples of synthetic faceExercise 17.1\nimages generated by a GAN are shown in Figure 1.3.\nThe GAN model discussed so far generates samples from the unconditional dis-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2746, "text": "tribution p(x). For example, it could generate synthetic images of dogs if it is trained\non dog images. We can also create conditional GANs (Mirza and Osindero, 2014),\nwhich sample from a conditional distribution p(x|c)in which the conditioning vec-\ntor c might, for example, represent different species of dog. To do this, both the\ngenerator and the discriminator network take c as an additional input, and labelled\nexamples of images, comprising pairs{xn;cn}, are used for training. Once the GAN"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2747, "text": "has been trained, images from a desired class can be generated by setting c to the\ncorresponding class vector. Compared to training separate GANs for each class, this\nhas the advantage that shared internal representations can be learned jointly across\nall classes, thereby making more efﬁcient use of the data.\n17.1.2 GAN training in practice\nAlthough GANs can produce high quality results, they are not easy to train suc-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2748, "text": "cessfully due to the adversarial learning. Also, unlike standard error function min-Exercise 17.2\nimization, there is no metric of progress because the objective can go up as well as\ndown during training.\nOne challenge that can arise is calledmode collapse, in which the generator net-\nwork weights adapt during training such that all latent-variable samplesz are mapped\nto a subset of possible valid outputs. In extreme cases the output can correspond to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2749, "text": "just one, or a small number, of the output values x. The discriminator then assigns\nthe value 0:5 to these instances, and training ceases. For example, a GAN trained on\nhandwritten digits might learn to generate only examples of the digit ‘3’, and while\n17.1. Adversarial Training 537\nx\npData(x)\npG(x)\nd(x)\n˜d(x)\nFigure 17.2 Conceptual illustration of why it can be difﬁcult to train GANs, showing a simple one-\ndimensional data space xwith the ﬁxed, but unknown, data distribution pData(x) and the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2750, "text": "initial generative distribution pG(x). The optimal discriminator function d(x) has virtually\nzero gradient in the vicinity of either the training or synthetic data points, making learn-\ning very slow. A smoothed version ed(x) of the discriminator function can lead to faster\nlearning.\nthe discriminator is unable to distinguish these from genuine examples of the digit\n‘3’, it fails to recognize that the generator is not generating the full range of digits."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2751, "text": "Insight into the difﬁculty of training GANs can be obtained by considering Fig-\nure 17.2, which shows a simple one§-dimensional data space xwith samples {xn}\ndrawn from the ﬁxed, but unknown, data distribution pData(x). Also shown is the\ninitial generative distribution pG(x) together with samples drawn from this distri-\nbution. Because the data and generative distributions are so different, the optimal\ndiscriminator function d(x) is easy to learn and has a very steep fall-off with virtu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2752, "text": "ally zero gradient in the vicinity of either the real or synthetic samples. Consider\nthe second term in the GAN error function (17.6). Because d(g(z;w);\u001e) is equal\nto zero across the region spanned by the generated samples, small changes in the\nparameters w of the generative network produce very little change in the output of\nthe discriminator and so the gradients are small and learning proceeds slowly.\nThis can be addressed by using a smoothed version ˜d(x) of the discriminator"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2753, "text": "function, illustrated in Figure 17.2, thereby providing a stronger gradient to drive\nthe training of the generator network. The least-squares GAN (Mao et al., 2016)\nachieves smoothing by modifying the discriminator to produce a real-valued output\nrather than a probability in the range (0;1) and by replacing the cross-entropy error\nfunction with a sum-of-squares error function. Alternatively, the technique of in-\nstance noise (Sønderby et al., 2016) adds Gaussian noise to both the real data and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2754, "text": "the synthetic samples, again leading to a smoother discriminator function.\nNumerous other modiﬁcations to the GAN error function and training procedure\nhave been proposed to improve training (Mescheder, Geiger, and Nowozin, 2018).\nOne change that is often used is to replace the generative network term in the original\nerror function\n538 17. GENERATIVE ADVERSARIAL NETWORKS\nFigure 17.3 Plots of −ln(d) and ln(1 −d) showing the very dif-\nferent behaviour of the gradients close tod= 0 and\nd= 1.\n0 1\nx"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2755, "text": "d= 1.\n0 1\nx\n−2\n0\n2 − ln(d)\nln(1 − d)\n− 1\nNsynth\n∑\nn∈synth\nln(1 −d(g(zn;w);\u001e)) (17.9)\nwith the modiﬁed form\n1\nNsynth\n∑\nn∈synth\nln d(g(zn;w);\u001e): (17.10)\nAlthough the ﬁrst form minimizes the probability that the image is fake, the second\nversion maximizes the probability that the image is real. The different properties\nof these two forms can be understood from Figure 17.3. When the generative dis-\ntribution pG(x) is very different from the true data distribution pData(x), the quan-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2756, "text": "tity d(g(z;w)) is close to zero, and hence the ﬁrst form has a very small gradient,\nwhereas the second form has a large gradient, leading to faster training.\nA more direct way to ensure that the generator distributionpG(x) moves towards\nthe data distribution pdata(x) is to modify the error criterion to reﬂect how far apart\nthe two distributions are in data space. This can be measured using the Wasserstein\ndistance, also known as the earth mover’s distance. Imagine the distribution pG(x)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2757, "text": "as a pile of earth that is transported in small increments to construct the distribution\npdata(x). The Wasserstein metric is the total amount of earth moved multiplied by\nthe mean distance moved. Of the many ways of rearranging the pile of earth to build\npdata(x), the one that yields the smallest mean distance is the one used to deﬁne\nthe metric. In practice, this cannot be implemented directly, and it is approximated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2758, "text": "by using a discriminator network that has real-valued outputs and then limiting the\ngradient ∇xd(x;\u001e) of the discriminator function with respect to xby using weight\nclipping, giving rise to theWasserstein GAN (Arjovsky, Chintala, and Bottou, 2017).\nAn improved approach is to introduce a penalty on the gradient, giving rise to the\ngradient penalty Wasserstein GAN (Gulrajani et al., 2017) whose error function is\ngiven by\nEWGAN-GP(w;\u001e) = − 1\nNreal\n∑\nn∈real\n[\nln d(xn;\u001e) −\u0011\n(\n∥∇xnd(xn;\u001e)∥2 −1\n)2]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2759, "text": "∑\nn∈real\n[\nln d(xn;\u001e) −\u0011\n(\n∥∇xnd(xn;\u001e)∥2 −1\n)2]\n+ 1\nNsynth\n∑\nn∈synth\nln d(g(zn;w;\u001e)) (17.11)\nwhere \u0011controls the relative importance of the penalty term.\n17.2. Image GANs 539\n17.2. Image GANs\nThe basic concept of the GAN has given rise to a huge research literature, with many\nalgorithmic developments and numerous applications. One of the most widespread\nand successful application areas for GANs is the generation of images. Early GAN"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2760, "text": "models used fully connected networks for the generator and discriminator. How-\never, there are many beneﬁts to using convolutional networks, especially for imagesChapter 10\nof higher resolution. The discriminator network takes an image as input and pro-\nvides a scalar probability as output, so a standard convolutional network is appro-\npriate. The generator network needs to map a lower-dimensional latent space into a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2761, "text": "high-resolution image, and so a network based on transpose convolutions is used, asSection 10.5.3\nillustrated in Figure 17.4.\nHigh quality images can be obtained by progressively growing both the gener-\nator network and the discriminator network starting from a low resolution and then\nsuccessively adding new layers that model increasingly ﬁne details as training pro-\ngresses (Karras et al., 2017). This speeds up the training and permits the synthesis of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2762, "text": "high-resolution images of size 1024×1024 starting from images of size4×4. As an\nexample of the scale and complexity of some GAN architectures, consider the GAN\nmodel for class-conditional image generation called BigGAN, whose architecture is\nshown in Figure 17.5.\n17.2.1 CycleGAN\nAs an example of the broad variety of GANs we consider an architecture called\na CycleGAN (Zhu et al., 2017). This also illustrates how techniques in deep learning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2763, "text": "can be adapted to solve different kinds of problems beyond traditional tasks such as\nz\n100\nproject and\nreshape conv 1\nconv 2\nconv 3\nconv 4\n4 ×4 ×1024\n8 ×8 ×512\n16 ×16 ×256\n32 ×32 ×128\n64 ×64 ×3\nFigure 17.4 Example architecture of a deep convolutional GAN showing the use of transpose convolutions to\nexpand the dimensionality in successive blocks of the network.\n540 17. GENERATIVE ADVERSARIAL NETWORKS\nz\nSplit\nLinear\nResidual Block Concat\nResidual Block Concat\nNon-Local\nResidual Block Concat\nx\nc"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2764, "text": "Non-Local\nResidual Block Concat\nx\nc\n(a)\nBatch Norm\nReLUReLU\nUp-sample\nConvolution\nBatch Norm\nReLU\nConvolution\nLinear\nLinear\nUp-sample\nConvolution\nAdd\n(b)\nFigure 17.5 (a) Architecture of the generative network in the BigGAN model, which has over 70 million param-\neters. (b) Details of each of the residual blocks in the generative network. The discriminative network, which has\n88 million parameters, has a somewhat analogous structure except that it uses average pooling layers to reduce"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2765, "text": "the dimensionality, instead of using up-sampling to increase the dimensionality. [Based on Brock, Donahue, and\nSimonyan (2018).]\nclassiﬁcation and density estimation. Consider the problem of turning a photograph\ninto a Monet painting of the same scene, or vice versa. In Figure 17.6 we show\nexamples of image pairs from a trained CycleGAN that has learned to perform such\nan image-to-image translation.\nThe aim is to learn two bijective (one-to-one) mappings, one that goes from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2766, "text": "domain Xof photographs to the domainY of Monet paintings and one in the reverse\ndirection. To achieve this, CycleGAN makes use of two conditional generators, gX\nand gY, and two discriminators, dX and dY. The generator gX(y;wX) takes as\ninput a sample painting y ∈Y and generates a corresponding synthetic photograph,\nwhereas the discriminator dX(x;\u001eX) distinguishes between synthetic and real pho-\ntographs. Similarly, the generator gY(x;wY) takes a photograph x ∈X as input"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2767, "text": "and generates a synthetic painting y, and the discriminator dY(y;\u001eY) distinguishes\nbetween synthetic paintings and real ones. The discriminator dX is therefore trained\non a combination of synthetic photographs generated by gX and real photographs,\nwhereas dY is trained on a combination of synthetic paintings generated by gY and\n17.2. Image GANs 541\nFigure 17.6 Examples of image translation\nusing a CycleGAN showing the synthesis of a\nphotographic-style image from a Monet paint-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2768, "text": "photographic-style image from a Monet paint-\ning (top row) and the synthesis of an image\nin the style of a Monet painting from a photo-\ngraph (bottom row). [From Zhu et al. (2017)\nwith permission.]\nreal paintings.\nIf we train this architecture using the standard GAN loss function, it would learn\nto generate realistic synthetic Monet paintings and realistic synthetic photographs,\nbut there would be nothing to force a generated painting to look anything like the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2769, "text": "corresponding photograph, or vice versa. We therefore introduce an additional term\nin the loss function called the cycle consistency error, containing two terms, whose\nconstruction is illustrated in Figure 17.7.\nThe goal is to ensure that when a photograph is translated into a painting and\nthen back into a photograph it should be close to the original photograph, thereby\nensuring that the generated painting retains sufﬁcient information about the photo-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2770, "text": "graph to allow the photograph to be reconstructed. Similarly, when a painting is\ntranslated into a photograph and then back into a painting it should be close to the\nFigure 17.7 Diagram showing how the cycle\nconsistency error is calculated for an example\nphotograph xn. The photograph is ﬁrst mapped\ninto the painting domain using the generatorgY,\nand the resulting vector is then mapped back\ninto the photograph domain using the genera-\ntor gX. The discrepancy between the resulting"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2771, "text": "tor gX. The discrepancy between the resulting\nphotograph and the original xn deﬁnes a contri-\nbution to the cycle consistency error. An analo-\ngous process is used to calculate the contribu-\ntion to the cycle consistency error from a paint-\ning yn by mapping it to a photograph using gX\nand then back to a painting using gY.\nX\nphotographs\nY\npaintings\nxn\ngX(gY(xn)) gY(xn)\nEcyc\n542 17. GENERATIVE ADVERSARIAL NETWORKS\nxn gY\ndY EGAN\ngX Ecyc\nyn\nyn gX\ndX EGAN\ngY Ecyc\nxn"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2772, "text": "xn gY\ndY EGAN\ngX Ecyc\nyn\nyn gX\ndX EGAN\ngY Ecyc\nxn\nFigure 17.8 Flow of information through a CycleGAN. The total error for the data points xn and yn is the sum\nof the four component errors.\noriginal painting. Applying this to all the photographs and paintings in the training\nset then gives a cycle consistency error of the form\nEcyc(wX;wY) = 1\nNX\n∑\nn∈X\n∥gX(gY(xn)) −xn∥1\n+ 1\nNY\n∑\nn∈Y\n∥gY(gX(yn)) −yn∥1 (17.12)\nwhere ∥-∥1 denotes the L1 norm. The cycle consistency error is added to the usual"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2773, "text": "GAN loss functions deﬁned by (17.6) to give a total error function:\nEGAN(wX;\u001eX) + EGAN(wY;\u001eY) + \u0011Ecyc(wX;wY) (17.13)\nwhere the coefﬁcient \u0011determines the relative importance of the GAN errors and the\ncycle consistency error. Information ﬂow through the CycleGAN when calculating\nthe error function for one image and one painting is shown in Figure 17.8.\nWe have seen that GANs can perform well as generative models, but they can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2774, "text": "also be used for representation learning in which rich statistical structure in a dataSection 6.3.3\nset is revealed through unsupervised learning. When the deep convolutional GAN\nshown in Figure 17.4 is trained on a data set of bedroom images (Radford, Metz, and\nChintala, 2015) and random samples from the latent space are propagated through\nthe trained network, the generated images also look like bedrooms, as expected. In"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2775, "text": "addition, however, the latent space has become organized in ways that are semanti-\ncally meaningful. For example, if we follow a smooth trajectory through the latent\nspace and generate the corresponding series of images, we obtain smooth transitions\nfrom one image to the next, as seen in Figure 17.9.\nMoreover, it is possible to identify directions in latent space that correspond\nto semantically meaningful transformations. For example, for faces, one direction"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2776, "text": "might correspond to changes in the orientation of the face, whereas other directions\nmight correspond to changes in lighting or the degree to which the face is smiling or\nnot. These are called disentangled representations and allow new images to be syn-\nthesized having speciﬁed properties. Figure 17.10 is an example from a GAN trained\non face images, showing that semantic attributes such as gender or the presence of\nglasses correspond to particular directions in latent space."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2777, "text": "17.2. Image GANs 543\nFigure 17.9 Samples generated by a deep convolutional GAN trained on images of bedrooms. Each row is\ngenerated by taking a smooth walk through latent space between randomly generated locations. We see smooth\ntransitions, with each image plausibly looking like a bedroom. In the bottom row, for example, we see a TV on\nthe wall gradually morph into a window. [From Radford, Metz, and Chintala (2015) with permission.]\nFigure 17.10 An example of vector"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2778, "text": "Figure 17.10 An example of vector\narithmetic in the latent space of a\ntrained GAN. In each of the three\ncolumns, the latent space vectors that\ngenerated these images are averaged\nand then vector arithmetic is applied\nto the resulting mean vectors to cre-\nate a new vector corresponding to the\ncentral image in the 3 ×3 array on the\nright. Adding noise to this vector gen-\nerates another eight sample images.\nThe four images on the bottom row\nshow that the same arithmetic applied"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2779, "text": "show that the same arithmetic applied\ndirectly in data space simply results\nin a blurred image due to misalign-\nment. [From Radford, Metz, and Chin-\ntala (2015) with permission.]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2780, "text": "544 17. GENERATIVE ADVERSARIAL NETWORKS\nExercises\n17.1 (???) We would like the GAN error function (17.6) to have the property that, given\nsufﬁciently ﬂexible neural networks, the stationary point is obtained when the gen-\nerator distribution matches the true data distribution. In this exercise we prove this\nresult for network models with inﬁnite ﬂexibility by optimizing over the full space\nof probability distributions pG(x) and over the full space of functions d(x) corre-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2781, "text": "sponding to the generative and discriminative networks, respectively. Speciﬁcally,\nwe assume that the discriminative model is optimized in an inner loop, giving rise to\nan effective outer loop error function for the generative model. First, show that, in\nthe limit of an inﬁnite number of data samples, the GAN error function (17.6) can be\nrewritten in the form\nE(pG;d) = −\n∫\npdata(x) lnd(x) dx −\n∫\npG(x) ln(1−d(x)) dx (17.14)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2782, "text": "∫\npG(x) ln(1−d(x)) dx (17.14)\nwhere pdata(x) is the ﬁxed distribution of real data points. Now consider a varia-\ntional optimization over all functions d(x). Show that, for a ﬁxed generative net-Appendix B\nwork, the solution for the discriminator d(x) that minimizes Eis given by\nd?(x) = pdata(x)\npdata(x) + pG(x): (17.15)\nHence, show that the error function E can be written as a function of the generator\nnetwork pG(x) in the form\nC(pG) = −\n∫\npdata(x) ln\n{ pdata(x)\npdata(x) + pG(x)\n}\ndx\n−\n∫"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2783, "text": "pdata(x) ln\n{ pdata(x)\npdata(x) + pG(x)\n}\ndx\n−\n∫\npG(x) ln\n{ pG(x)\npdata(x) + pG(x)\n}\ndx: (17.16)\nNow show that this can be rewritten in the form\nC(pG) = −ln(4) + KL\n(\npdata\n\npdata + pG\n2\n)\n+ KL\n(\npG\n\n\npdata + pG\n2\n)\n(17.17)\nwhere the Kullback–Leibler divergence KL(p∥q) is deﬁned by (2.100). Finally, us-\ning the property that KL(p∥q) > 0 with equality if, and only if, p(x) = q(x) forSection 2.5.5\nall x, show that the minimum of C(pG) occurs when pG(x) = pdata(x). Note"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2784, "text": "that the sum of the two Kullback–Leibler divergence terms in (17.17) is known as\nthe Jensen–Shannon divergence between pdata and pG. Like the Kullback–Leibler\ndivergence, this is a non-negative quantity that vanishes if, and only if, the two dis-\ntributions are equal, but unlike the KL divergence, it is symmetric with respect to the\ntwo distributions.\n17.2 (???) In this exercise we explore the problems that can arise from the adversarial"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2785, "text": "nature of GAN training. Consider a cost function E(a;b) = ab deﬁned over two\nparameters aand b, analogous to the parameters of a generative and discriminative\nExercises 545\nnetwork, respectively. Show that the point a = 0 ;b = 0 is a stationary point of\nthe cost function. By considering the second derivatives along the lines b = aand\nb = −a show that the point a = 0;b = 0 is a saddle point. Now suppose that we\noptimize this error function by taking inﬁnitesimal steps, so that the variables be-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2786, "text": "come functions of continuous time a(t), b(t) deﬁned by a continuous-time gradient\ndescent, in which the parameter a(t) of the generative network is updated so as to\nincrease E(a;b), whereas the parameter b(t) is updated so as to decrease E(a;b).\nShow that the evolution of the parameters is governed by the equations\nda\ndt = \u0011@E\n@a; db\ndt = −\u0011@E\n@b : (17.18)\nHence, show that a(t) satisﬁes the second-order differential equation\nd2a\ndt2 = −\u00112a(t): (17.19)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2787, "text": "d2a\ndt2 = −\u00112a(t): (17.19)\nVerify that the following expression is a solution of (17.19):\na(t) = Ccos(\u0011t) + Dsin(\u0011t) (17.20)\nwhere C and Dare arbitrary constants. If the system is initialized at t= 0 with the\nvalues a = 1, b = 0, ﬁnd the values of C and Dand hence show that the resulting\nvalues of a(t) and b(t) trace out a circle of unit radius in a;b space centred on the\norigin, and that they therefore never converge to the saddle point."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2788, "text": "17.3 (?) Consider a GAN in which the training set consists of equal numbers of cat and\ndog images and in which the generator network has learned to produce high quality\nimages of dogs. Show that, when presented with a dog image, the optimal output for\nthe discriminator network (trained to generate the probability that the image is real)\nis 1=3.\n18\nNormalizing\nFlows\nWe have seen how generative adversarial networks (GANs) extend the frameworkChapter 17"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2789, "text": "of linear latent-variable models by using deep neural networks to represent highly\nﬂexible and learnable nonlinear transformations from the latent space to the data\nspace. However, the likelihood function is generally either intractable, because the\nnetwork function cannot be inverted, or may not even be deﬁned if the latent space\nhas a lower dimensionality than the data space. In GANs, a second, discriminative\nnetwork was therefore introduced to facilitate adversarial training."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2790, "text": "Here we discuss the second of our four approaches to training nonlinear latentSection 16.4.4\nvariable models that involves restricting the form of the neural network model such\nthat the likelihood function can be evaluated without approximation while still en-\nsuring that sampling from the trained model is straightforward. Suppose we deﬁne\na distribution pz(z), sometimes also called a base distribution, over a latent variable"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2791, "text": "z along with a nonlinear function x = f(z;w), given by a deep neural network, that\n547© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_18"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2792, "text": "548 18. NORMALIZING FLOWS\ntransforms the latent space into the data space. Assuming pz(z) is a simple distribu-\ntion such as a Gaussian, sampling from such a model is easy as each latent sample\nz? ∼pz(z) is simply passed through the neural network to generate a corresponding\ndata sample x? = f(z?;w).\nTo calculate the likelihood function for this model, we need the data-space dis-\ntribution, which depends on the inverse of the neural network function. We write"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2793, "text": "this as z = g(x;w), and it satisﬁes z = g(f(z;w);w). This requires that, for every\nvalue of w, the functions f(z;w) and g(x;w) are invertible, also calledbijective, so\nthat each value of x corresponds to a unique value of z and vice versa. We can then\nuse the change of variables formula to calculate the data density:Section 2.4\npx(x|w) = pz(g(x;w)) |detJ(x)| (18.1)\nwhere J(x) is the Jacobian matrix of partial derivatives whose elements are given by\nJij(x) = @gi(x;w)\n@xj\n(18.2)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2794, "text": "Jij(x) = @gi(x;w)\n@xj\n(18.2)\nand |-|denotes the modulus or absolute value. We will continue to refer toz as a ‘la-\ntent’ variable even though the deterministic mapping means that any given data value\nx corresponds to a unique value of z whose value is therefore no longer uncertain.\nThe mapping function f(z;w) will be deﬁned in terms of a special form of neu-\nral network, whose structure we will discuss shortly. One consequence of requiring"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2795, "text": "an invertible mapping is that the dimensionality of the latent space must be the same\nas that of the data space, which can lead to large models for high-dimensional data\nsuch as images. Also, in general, the cost of evaluating the determinant of a D×D\nmatrix is O(D3), so we will seek to impose some further restrictions on the model\nin order that evaluation of the Jacobian matrix determinant is more efﬁcient.\nIf we consider a training set D= {x1;:::; xN}of independent data points, the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2796, "text": "log likelihood function is given from (18.1) by\nln p(D|w) =\nN∑\nn=1\nln px(xn|w) (18.3)\n=\nN∑\nn=1\n{\nln pz(g(xn;w)) + ln |detJ(xn)|\n}\n(18.4)\nand our goal is to use the likelihood function to train the neural network. To be able\nto model a wide range of distributions, we want the transformation function x =\nf(z;w) to be highly ﬂexible, and so we use a deep neural network architecture. We\ncan ensure that the overall function is invertible if we make each layer of the network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2797, "text": "invertible. To see this, consider three successive transformations, each correspondingExercise 18.2\nto one layer, of the form:\nx = fA(fB(fC(z))): (18.5)\nThen the inverse function is given by\nz = gC(gB(gA(x))) (18.6)\n18.1. Coupling Flows 549\nwhere gA;gB, and gC are the inverse functions of fA;fB, and fC, respectively.\nMoreover, the determinant of the Jacobian for such a layered structure is also easy\nto evaluate in terms of the Jacobian determinants for each of the individual layers by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2798, "text": "making use of the chain rule of calculus:\nJij = @zi\n@xj\n=\n∑\nk\n∑\nl\n@gC\ni\n@gB\nk\n@gB\nk\n@gA\nl\n@gA\nl\n@xj\n: (18.7)\nWe recognize the right-hand side as the product of three matrices, and the determi-\nnant of a product is the product of the determinants. Therefore, the log determinantAppendix A\nof the overall Jacobian will be the sum of the log determinants corresponding to each\nlayer.\nThis approach to modelling a ﬂexible distribution is called a normalizing ﬂow"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2799, "text": "because the transformation of a probability distribution through a sequence of map-\npings is somewhat analogous to the ﬂow of a ﬂuid. Also, the effect of the inverse\nmapping is to transform the complex data distribution into a normalized form, typ-\nically a Gaussian or normal distribution. Normalizing ﬂows have been reviewed by\nKobyzev, Prince, and Brubaker (2019) and Papamakarios et al. (2019). Here we\ndiscuss the core concepts from the two main classes of normalizing ﬂows used in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2800, "text": "practice: coupling ﬂows and autoregressive ﬂows. We also look at the use of neural\ndifferential equations to deﬁne invertible mappings, leading to continuous ﬂows.\n18.1.\nCoupling Flows\nOur goal is to design a single invertible function layer, so that we can compose many\nof them together to deﬁne a highly ﬂexible class of invertible functions. Consider\nﬁrst a linear transformation of the form\nx = az + b: (18.8)\nThis is easy to invert, giving\nz = 1\na(x −b): (18.9)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2801, "text": "z = 1\na(x −b): (18.9)\nHowever, linear transformations are closed under composition, meaning that a se-\nquence of linear transformations is equivalent to a single overall linear transforma-\ntion. Moreover, a linear transformation of a Gaussian distribution is again Gaussian.Exercise 3.6\nSo even if we have many such ‘layers’ of linear transformation, we will only ever\nhave a Gaussian distribution. The question is whether we can retain the invertability"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2802, "text": "of a linear transformation while allowing additional ﬂexibility so that the resulting\ndistribution can be non-Gaussian.\nOne solution to this problem is given by a form of normalizing ﬂow model called\nreal NVP (Dinh, Krueger, and Bengio, 2014; Dinh, Sohl-Dickstein, and Bengio,\n2016), which is short for ‘real-valued non-volume-preserving’. The idea is to par-\ntition the latent-variable vector z into two parts z = ( zA;zB), so that if z has di-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2803, "text": "mension D and zA has dimension d, then zB has dimension D−d. We similarly\n550 18. NORMALIZING FLOWS\nFigure 18.1 A single layer of the real NVP nor-\nmalizing ﬂow model. Here the network NN1\ncomputes the function exp(s(zA;w)) and the\nnetwork NN2 computes the function b(zA;w).\nThe output vector is then deﬁned by (18.10) and\n(18.11).\nzA\nzB\nNN1 NN2\nxA\nxB\n⊙ +\nexp(s(-)) b(-)\npartition the output vector x = (x A;xB) where xA has dimension d and xB has"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2804, "text": "dimension D−d. For the ﬁrst part of the output vector, we simply copy the input:\nxA = zA: (18.10)\nThe second part of the vector undergoes a linear transformation, but now the coefﬁ-\ncients in the linear transformation are given by nonlinear functions of zA:\nxB = exp(s(zA;w)) ⊙zB + b(zA;w) (18.11)\nwhere s(zA;w) and b(zA;w) are the real-valued outputs of neural networks, and\nthe exponential ensures that the multiplicative term is non-negative. Here ⊙denotes"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2805, "text": "the Hadamard product involving an element-wise multiplication of the two vectors.\nSimilarly, the exponential in (18.11) is taken element-wise. Note that we have shown\nthe same vector w in both network functions. In practice, these may be implemented\nas separate networks with their own parameters, or as one network with two sets of\noutputs.\nDue to the use of neural network functions, the value ofxB can be a very ﬂexible"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2806, "text": "function of xA. Nevertheless, the overall transformation is easily invertible: given a\nvalue for x = (xA;xB) we ﬁrst compute\nzA = xA; (18.12)\nthen we evaluate s(zA;w) and b(zA;w), and ﬁnally we compute zB using\nzB = exp(−s(zA;w)) ⊙(xB −b(zA;w)): (18.13)\nThe overall transformation is illustrated in Figure 18.1. Note that there is no re-\nquirement for the individual neural network functions s(zA;w) and b(zA;w) to be\ninvertible."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2807, "text": "invertible.\nNow consider the evaluation of the Jacobian deﬁned by (18.2) and its determi-\nnant. We can divide the Jacobian matrix into blocks, corresponding to the partition-\ning of z and x, giving\nJ =\n\n\nId 0\n@zB\n@xA\ndiag(exp(−s))\n\n: (18.14)\n18.1. Coupling Flows 551\nzA\nzB\nNN1 NN2 NN3 NN4\n⊙ +\n⊙ +\nFigure 18.2 By composing two layers of the form shown in Figure 18.1, we obtain a more ﬂexible, but still"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2808, "text": "invertible, nonlinear layer. Each sub-layer is invertible and has an easily evaluated Jacobian, and hence the\noverall double layer has the same properties.\nThe top left block corresponds to the derivatives ofzA with respect to xA and hence\nfrom (18.12) is given by thed×didentity matrix. The top right block corresponds to\nthe derivatives of zA with respect to xB and these terms vanish, again from (18.12).\nThe bottom left block corresponds to the derivatives ofzB with respect to xA. From"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2809, "text": "(18.13), these are complicated expressions involving the neural network functions.\nFinally, the bottom right block corresponds to the derivatives of zB with respect to\nxB, which from (18.13) are given by a diagonal matrix whose diagonal elements\nare given by the exponentials of the negative elements of s(zA;w). We therefore\nsee that the Jacobian matrix (18.14) is a lower triangular matrix, meaning that all\nelements above the leading diagonal are zero. For such a matrix, the determinant"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2810, "text": "is just the product of the elements along the leading diagonal, and therefore it doesAppendix A\nnot depend on the complicated expressions in the lower left block. Consequently,\nthe determinant of the Jacobian is simply given by the product of the elements of\nexp(−s(zA;w)).\nA clear limitation of this approach is that the value of zA is unchanged by the\ntransformation. This is easily resolved by adding another layer in which the roles"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2811, "text": "of zA and zB are reversed, as illustrated in Figure 18.2. This double-layer structure\ncan then be repeated multiple times to facilitate a very ﬂexible class of generative\nmodels.\nThe overall training procedure involves creating mini-batches of data points, in\nwhich the contribution of each data point to the log likelihood function is obtained\nfrom (18.4). For a latent distribution of the formN(z|0;I), the log density is simply"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2812, "text": "−∥z∥2=2 up to an additive constant. The inverse transformation z = g(x) is cal-\nculated using a sequence of inverse transformations of the form (18.13). Similarly,\nthe log of the Jacobian determinant is given by a sum of log determinants for each\nlayer where each term is itself a sum of terms of the form −si(x;w). Gradients of\nthe log likelihood can be evaluated using automatic differentiation, and the network\nparameters updated by stochastic gradient descent."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2813, "text": "The real NVP model belongs to a broad class of normalizing ﬂows called cou-\npling ﬂows, in which the linear transformation (18.11) is replaced by a more general\n552 18. NORMALIZING FLOWS\nFigure 18.3 Illustration of the real NVP\nnormalizing ﬂow model ap-\nplied to the two-moons data\nset showing (a) the Gaus-\nsian base distribution, (b)\nthe distribution after a trans-\nformation of the vertical axis\nonly, (c) the distribution after\na subsequent transformation\nof the horizontal axis, (d) the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2814, "text": "of the horizontal axis, (d) the\ndistribution after a second\ntransformation of the vertical\naxis, (e) the distribution af-\nter a second transformation\nof the horizontal axis, and\n(f) the data set on which the\nmodel was trained.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nform:\nxB = h(zB;g(zA;w)) (18.15)\nwhere h(zB;g) is a function ofzB that is efﬁciently invertible for any given value of\ng and is called the coupling function. The function g(zA;w) is called a conditioner"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2815, "text": "and is typically represented by a neural network.\nWe can illustrate the real NVP normalizing ﬂow using a simple data set, some-\ntimes known as ‘two moons’, as shown in Figure 18.3. Here a two-dimensional\nGaussian distribution is transformed into a more complex distribution by using two\nsuccessive layers each of which consists of alternate transformations on each of the\ntwo dimensions.\n18.2. Autoregressive Flows\nA related formulation of normalizing ﬂows can be motivated by noting that the joint"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2816, "text": "distribution over a set of variables can always be written as the product of conditional\ndistributions, one for each variable. We ﬁrst choose an ordering of the variables inSection 11.1\n18.2. Autoregressive Flows 553\nFigure 18.4 Illustration of two al-\nternative structures for autoregres-\nsive normalizing ﬂows. The masked\nautoregressive ﬂow shown in (a) al-\nlows efﬁcient evaluation of the like-\nlihood function, whereas the alter-\nnative inverse autoregressive ﬂow"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2817, "text": "native inverse autoregressive ﬂow\nshown in (b) allows for efﬁcient\nsampling.\nz1\nz2\nz3\nzn\nx1\nx2\nx3\nxn\n(a)\nz1\nz2\nz3\nzn\nx1\nx2\nx3\nxn\n(b)\nthe vector x, from which we can write, without loss of generality,\np(x1;:::;x D) =\nD∏\ni=1\np(xi|x1:i−1 ) (18.16)\nwhere x1:i−1 denotes x1;:::;x i−1 . This factorization can be used to construct a\nclass of normalizing ﬂow called a masked autoregressive ﬂow, or MAF (Papamakar-\nios, Pavlakou, and Murray, 2017), given by\nxi = h(zi;gi(x1:i−1 ;wi)) (18.17)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2818, "text": "xi = h(zi;gi(x1:i−1 ;wi)) (18.17)\nwhich is illustrated in Figure 18.4(a). Here h(zi;-)is the coupling function, which\nis chosen to be easily invertible with respect to zi, and gi is the conditioner, which\nis typically represented by a deep neural network. The term masked refers to the use\nof a single neural network to implement a set of equations of the form (18.17) along\nwith a binary mask (Germain et al., 2015) that force a subset of the network weights"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2819, "text": "to be zero to implement the autoregressive constraint (18.16).\nIn this case the reverse calculations needed to evaluate the likelihood function\nare given by\nzi = h−1 (xi;gi(x1:i−1 ;wi)) (18.18)\nand hence can be performed efﬁciently on modern hardware since the individual\nfunctions in (18.18) needed to evaluate z1;:::;z D can be evaluated in parallel. The\nJacobian matrix corresponding to the set of transformations (18.18) has elements"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2820, "text": "@zi=@xj, which form an upper-triangular matrix whose determinant is given by theExercise 18.4\nproduct of the diagonal elements and can therefore also be evaluated efﬁciently.\nHowever, sampling from this model must be done by evaluating (18.17), which is\nintrinsically sequential and therefore slow because the values of x1;:::;x i−1 must\nbe evaluated before xi can be computed.\nTo avoid this inefﬁcient sampling, we can instead deﬁne an inverse autoregres-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2821, "text": "sive ﬂows, or IAF (Kingma et al., 2016), given by\nxi = h(zi;˜gi(z1:i−1 ;wi)) (18.19)\n554 18. NORMALIZING FLOWS\nas illustrated in Figure 18.4(b). Sampling is now efﬁcient since, for a given choice\nof z, the evaluation of the elements x1;:::;x D using (18.19) can be performed in\nparallel. However, the inverse function, which is needed to evaluate the likelihood,\nrequires a series of calculations of the form\nzi = h−1 (xi;˜gi(z1:i−1 ;wi)); (18.20)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2822, "text": "zi = h−1 (xi;˜gi(z1:i−1 ;wi)); (18.20)\nwhich are intrinsically sequential and therefore slow. Whether a masked autoregres-\nsive ﬂow or an inverse autoregressive ﬂow is preferred will depend on the speciﬁc\napplication.\nWe see that coupling ﬂows and autoregressive ﬂows are closely related. Al-\nthough autoregressive ﬂows introduce considerable ﬂexibility, this comes with a\ncomputational cost that grows linearly in the dimensionality D of the data space"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2823, "text": "due to the need for sequential ancestral sampling. Coupling ﬂows can be viewed as\na special case of autoregressive ﬂows in which some of this generality is sacriﬁced\nfor efﬁciency by dividing the variables into two groups instead of Dgroups.\n18.3.\nContinuous Flows\nThe ﬁnal approach to normalizing ﬂows that we consider in this chapter will make\nuse of deep neural networks deﬁned in terms of an ordinary differential equation, or"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2824, "text": "ODE. This can be thought of as a deep network with an inﬁnite number of layers.\nWe ﬁrst introduce the concept of a neural ODE then we see how this can be applied\nto the formulation of a normalizing ﬂow model.\n18.3.1 Neural differential equations\nWe have seen that neural networks are especially useful when they comprise\nmany layers of processing, and so we can ask what happens if we explore the limit\nof an inﬁnitely large number of layers. Consider a residual network where each layer"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2825, "text": "of processing generates an output given by the input vector with the addition of some\nparameterized nonlinear function of that input vector:\nz(t+1) = z(t) + f(z(t);w) (18.21)\nwhere t = 1;:::;T labels the layers in the network. Note that we have used the\nsame function at each layer, with a shared parameter vector w, because this allows\nus to consider an arbitrarily large number of such layers while keeping the number"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2826, "text": "of parameters bounded. Imagine that we increase the number of layers while ensur-\ning that the changes introduced at each layer become correspondingly smaller. In\nthe limit, the hidden-unit activation vector becomes a function z(t) of a continuous\nvariable t, and we can express the evolution of this vector through the network as a\ndifferential equation:Exercise 18.5\ndz(t)\ndt = f(z(t);w) (18.22)\nwhere tis often referred to as ‘time’. The formulation in (18.22) is known as aneural"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2827, "text": "ordinary differential equation or neural ODE (Chen et al., 2018). Here ‘ordinary’\n18.3. Continuous Flows 555\nFigure 18.5 Comparison of a conventional layered network with a neural differential equation. The di-\nagram on the left corresponds to a residual network with ﬁve layers and shows trajectories\nfor several starting values of a single scalar input. The diagram on the right shows the re-\nsult of numerical integration of a continuous neural ODE, again for several starting values"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2828, "text": "of the scalar input, in which we see that the function is not evaluated at uniformly-spaced\ntime intervals, but instead the evaluation points are chosen adaptively by the numerical\nsolver and depend on the choice of input value. [From Chenet al. (2018) with permission.]\nmeans that there is a single variable t. If we denote the input to the network by\nthe vector z(0), then the output z(T) is obtained by integration of the differential\nequation\nz(T) =\n∫T\n0\nf(z(t);w) dt: (18.23)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2829, "text": "equation\nz(T) =\n∫T\n0\nf(z(t);w) dt: (18.23)\nThis integral can be evaluated using standard numerical integration packages. The\nsimplest method for solving differential equations is Euler’s forward integration\nmethod, which corresponds to the expression (18.21). In practice, more powerful\nnumerical integration algorithms can adapt their function evaluation to achieve. In\nparticular, they can adaptively choose values of t that typically are not uniformly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2830, "text": "spaced. The number of such evaluations replaces the concept of depth in a conven-\ntional layered network. A comparison of a standard layered neural network and a\nneural differential equation are shown in Figure 18.5.\n18.3.2 Neural ODE backpropagation\nWe now need to address the challenge of how to train a neural ODE, that is how\nto determine the value of w by optimizing a loss function. Let us assume that we are\ngiven a data set comprising values of the input vector z(0) along with an associated"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2831, "text": "output target vector and a loss function L(-) that depends on the output vector z(T).\nOne approach would be to use automatic differentiation to differentiate through allSection 8.2\nof the operations performed by the ODE solver during the forward pass. Although\n556 18. NORMALIZING FLOWS\nthis is straightforward to do, it is costly from a memory perspective and is not op-\ntimal in terms of controlling numerical error. Instead, Chen et al. (2018) treat the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2832, "text": "ODE solver as a black box and use a technique called the adjoint sensitivity method,\nwhich can be viewed as the continuous analogue of explicit backpropagation. Recall\nthat backpropagation involves, for each data point, three successive phases: ﬁrst aChapter 8\nforward propagation to evaluate the activation vectors at each layer of the network,\nsecond the evaluation of the derivatives of the loss with respect to the activations at"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2833, "text": "each layer starting at the output and propagating backwards through the network by\nexploiting the chain rule of calculus, and third the evaluation of the derivatives with\nrespect to network parameters by forming products of activations from the forward\npass and gradients from the backward pass. We will see that there are analogous\nsteps when computing the gradients for a neural ODE.\nTo apply backpropagation to neural ODEs, we deﬁne a quantity called the ad-\njoint given by\na(t) = dL"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2834, "text": "joint given by\na(t) = dL\ndz(t): (18.24)\nWe see that a(T) corresponds to the usual derivative of the loss with respect to the\noutput vector. The adjoint satisﬁes its own differential equation given byExercise 18.6\nda(t)\ndt = −a(t)T∇zf(z(t);w); (18.25)\nwhich is a continuous version of the chain rule of calculus. This can be solved by\nintegrating backwards starting from a(T), which again can be done using a black-\nbox ODE solver. In principle, this requires that we have stored the trajectory z(t)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2835, "text": "computed during the forward phase, which could be problematic as the inverse solver\nmight wish to evaluate z(t) at different values of tcompared to the forward solver.\nInstead we simply allow the backwards solver to recompute any required values of\nz(t) by integrating (18.22) alongside (18.25) starting with the output value z(T).\nThe third step in the backpropagation method is to evaluate derivatives of the loss\nwith respect to network parameters by forming appropriate products of activations"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2836, "text": "and gradients. When a parameter value is shared across multiple connections in a\nnetwork, the total derivative is formed from the sum of derivatives for each of the\nconnections. For our neural ODE, in which the same parameter vector w is sharedExercise 9.7\nthroughout the network, this summation becomes an integration over t, which takes\nthe formExercise 18.7\n∇wL= −\n∫T\n0\na(t)T∇wf(z(t);w) dt: (18.26)\nThe derivatives ∇zf in (18.25) and ∇wf in (18.26) can be evaluated efﬁciently"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2837, "text": "using automatic differentiation. Note that the above results can equally be applied toSection 8.2\na more general neural network function f(z(t);t; w) that has an explicit dependence\non tin addition to the implicit dependence through z(t).\nOne beneﬁt of neural ODEs trained using the adjoint method, compared to con-\nventional layered networks, is that there is no need to store the intermediate results\nof the forward propagation, and hence the memory cost is constant. Furthermore,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2838, "text": "18.3. Continuous Flows 557\nneural ODEs can naturally handle continuous-time data in which observations occur\nat arbitrary times. If the error function Ldepends on values of z(t) other than the\noutput value, then multiple runs of the reverse-model solver are required, with one\nrun for each consecutive pair of outputs, so that the single solution is broken down\ninto multiple consecutive solutions in order to access the intermediate states (Chen"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2839, "text": "et al., 2018). Note that a high level of accuracy in the solver can be used during train-\ning, with a lower accuracy, and hence fewer function evaluations, during inference\nin applications for which compute resources are limited.\n18.3.3 Neural ODE ﬂows\nWe can make use of a neural ordinary differential equation to deﬁne an alter-\nnative approach to the construction of tractable normalizing ﬂow models. A neural\nODE deﬁnes a highly ﬂexible transformation from an input vector z(0) to an output"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2840, "text": "vector z(T) in terms of a differential equation of the form\ndz(t)\ndt = f(z(t);w): (18.27)\nIf we deﬁne a base distribution over the input vector p(z(0)) then the neural ODE\npropagates this forward through time to give a distribution p(z(t)) for each value\nof t, leading to a distribution over the output vector p(z(T)). Chen et al. (2018)\nshowed that for neural ODEs, the transformation of the density can be evaluated by\nintegrating a differential equation given byExercise 18.8\nd lnp(z(t))\ndt = −Tr"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2841, "text": "d lnp(z(t))\ndt = −Tr\n( @f\n@z(t)\n)\n(18.28)\nwhere @f=@z represents the Jacobian matrix with elements @fi=@zj. This integra-\ntion can be performed using standard ODE solvers. Likewise, samples from this\ndensity can be obtained by sampling from the base density p(z(0)), which is chosen\nto be a simple distribution such as a Gaussian, and propagating the values to the out-\nput by integrating (18.27) again using the ODE solver. The resulting framework is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2842, "text": "known as acontinuous normalizing ﬂow and is illustrated inFigure 18.6. ContinuousExercise 18.9\nnormalizing ﬂows can be trained using the adjoint sensitivity method used for neuralSection 18.3.1\nODEs, which can be viewed as the continuous time equivalent of backpropagation.\nSince (18.28) involves the trace of the Jacobian rather than the determinant,\nwhich arises in discrete normalizing ﬂows, it might appear to be more computation-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2843, "text": "ally efﬁcient. In general, evaluating the determinant of a D ×D matrix requires\nO(D3) operations, whereas evaluating the trace requires O(D) operations. How-\never, if the determinant is lower diagonal, as in many forms of normalizing ﬂow,\nthen the determinant is the product of the diagonal terms and therefore also involves\nO(D) operations. Since evaluating the individual elements of the Jacobian matrix\nrequires a separate forward propagation, which itself requiresO(D) operations, eval-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2844, "text": "uating the trace or the determinant (for a lower triangular matrix) takes O(D2) op-\nerations overall. However, the cost of evaluating the trace can be reduced to O(D)\nby using Hutchinson’s trace estimator (Grathwohl et al., 2018), which for a matrix\n558 18. NORMALIZING FLOWS\nFigure 18.6 Illustration of a continu-\nous normalizing ﬂow showing a simple\nGaussian distribution at t = 0 that is\ncontinuously transformed into a multi-\nmodal distribution at t = T. The ﬂow"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2845, "text": "modal distribution at t = T. The ﬂow\nlines show how points along the z-axis\nevolve as a function of t. Where the\nﬂow lines spread apart the density is re-\nduced, and where they move together\nthe density is increased.\np(z(T))\n0\nT\nt\nz\np(z(0))\nA takes the form\nTr(A) = E\u000f\n[\n\u000fTA\u000f\n]\n(18.29)\nwhere \u000fis a random vector whose distribution has zero mean and unit covariance, for\nexample, a Gaussian N(0;I). For a speciﬁc \u000f, the matrix-vector product A\u000fcan be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2846, "text": "evaluated efﬁciently in a single pass using reverse-mode automatic differentiation.\nWe can then approximate the trace using a ﬁnite number of samples in the form\nTr(A) ≃ 1\nM\nM∑\nm=1\n\u000fT\nmA\u000fm: (18.30)\nIn practice we can set M = 1 and just use a single sample, which is refreshed for\neach new data point. Although this is a noisy estimate, this might not be too signiﬁ-\ncant since it forms part of a noisy stochastic gradient descent procedure. Importantly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2847, "text": "it is unbiased, meaning that the expectation of the estimator is equal to the true value.Exercise 18.11\nSigniﬁcant improvements in training efﬁciency for continuous normalizing ﬂows\ncan be achieved using a technique called ﬂow matching (Lipman et al., 2022). This\nbrings normalizing ﬂows closer to diffusion models and avoids the need for back-Chapter 20\npropagation through the integrator while signiﬁcantly reducing memory require-\nments and enabling faster inference and more stable training."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2848, "text": "Exercises 559\nExercises\n18.1 (??) Consider a transformation x = f(z) along with its inverse z = g(x). By\ndifferentiating x = f(g(x)), show that\nJK = I (18.31)\nwhere I is the identity matrix, and J and K are matrices with elements\nJij = @gi\n@xj\n; K ij = @fi\n@zj\n: (18.32)\nUsing the result that the determinant of a product of matrices is the product of their\ndeterminants, show that\ndet(J) = 1\ndet(K): (18.33)\nHence, show that the formula (18.1) for the transformation of a density under a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2849, "text": "change of variables can be rewritten as\npx(x) = pz(g(x)) |detK|−1 (18.34)\nwhere K is evaluated at z = g(x).\n18.2 (?) Consider a sequence of invertible transformations of the form\nx = f1(f2(---fM−1 (fM(z)) ---)): (18.35)\nShow that the inverse function is given by\nz = f−1\nM (f−1\nM−1 (---f−1\n2 (f−1\n1 (x)) ---)): (18.36)\n18.3 (?) Consider a linear change of variables of the form\nx = z + b: (18.37)\nShow that the Jacobian of this transformation is the identity matrix. Interpret this"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2850, "text": "result by comparing the volume of a small region of z-space with the volume of the\ncorresponding region of x-space.\n18.4 (??) Show that the Jacobian of the autoregressive normalizing ﬂow transformation\ngiven by (18.18) is a lower triangular matrix. The determinant of such a matrix is\ngiven by the product of the terms on the leading diagonal and is therefore easily\nevaluated.\n18.5 (?) Consider the forward propagation equation for a residual network given by (18.21)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2851, "text": "in which we consider a small increment \u000fin the ‘time’ variablet:\nz(t+\u000f) = z(t) + \u000ff(z(t);w): (18.38)\nHere the additive contribution from the neural network is scaled by \u000f. Note that\n(18.21) corresponds to the case \u000f= 1. By taking the limit \u000f→ 0, derive the forward\npropagation differential equation given by (18.22).\n560 18. NORMALIZING FLOWS\n18.6 (??) In this exercise and the next we provide an informal derivation of the backpropa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2852, "text": "gation and gradient evaluation equations for a neural ODE. A more formal derivation\nof these results can be found in Chen et al. (2018). Write down the backpropagation\nequation corresponding to the forward equation (18.38). By taking the limit \u000f→ 0,\nderive the backward propagation equation (18.25), where a(t) is deﬁned by (18.24).\n18.7 (??) By making use of the result (8.10), write down an expression for the gradient"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2853, "text": "of a loss function L(z(T)) for a multilayered residual network deﬁned by (18.38)\nin which all layers share the same parameter vector w. By taking the limit \u000f → 0,\nderive the equation (18.26) for the derivative of the loss function.\n18.8 (???) In this exercise we give an informal derivation of (18.28) for one-dimensional\ndistributions. Consider a distribution q(z) at time t that is transformed to a new\ndistribution p(x) at time t+ \u000et as a result of a transformation from z to x. Also"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2854, "text": "consider nearby values z and z+ ∆z along with corresponding values xand x+\n∆x as shown in Figure 18.7. First, write down an equation that expresses that the\nprobability mass in the interval ∆z is the same as that in the interval ∆x. Second,\nwrite down an equation that shows how the probability density changes in going\nfrom tto t+\u000et, expressed in terms of the derivative dq(t)=dt. Third, write down an\nequation for ∆x in terms of ∆z by introducing the function f(z) = d z=dt. Finally,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2855, "text": "by combining these three equations and taking the limit \u000et→ 0, show that\nd\ndtln q(z) = −f′(z); (18.39)\nwhich is the one-dimensional version of (18.28).\nFigure 18.7 Schematic illustration of the\ntransformation of probability den-\nsities used to derive the equation\nfor continuous normalizing ﬂows\nin one dimension.\nq(z)\nt\np(x)\nt+ \u000et\nz\nx\nz+ \u0001z\nx+ \u0001x\n18.9 (??) The ﬂow lines in Figure 18.6 were plotted by taking a set of equally spaced"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2856, "text": "values and using the inverse of the cumulative distribution function at each value oft\nto plot the corresponding points in z-space. Show that this is equivalent to using the\ndifferential equation (18.27) to compute the ﬂow lines wheref is deﬁned by (18.28).\n18.10 (??) Using the differential equation (18.27) write down an expression for the base\ndensity of a continuous normalizing ﬂow in terms of the output density, expressed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2857, "text": "as an integral over t. Hence, by making use of the fact that changing the sign of a\ndeﬁnite integral is equivalent to swapping the limits on that integral, show that the\ncomputational cost of inverting a continuous normalizing ﬂow is the same as that\nneeded to evaluate the forward ﬂow.\nExercises 561\n18.11 (?) Show that the expectation of the right-hand side in the Hutchinson trace estimator\n(18.30) is equal to Tr(A) for any value of M. This shows that the estimator is\nunbiased.\n19\nAutoencoders"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2858, "text": "unbiased.\n19\nAutoencoders\nA central goal of deep learning is to discover representations of data that are useful\nfor one or more subsequent applications. One well-established approach to learn-\ning internal representations is called the auto-associative neural network or autoen-\ncoder. This consists of a neural network having the same number of output units as\ninputs and which is trained to generate an output y that is close to the input x. Once"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2859, "text": "trained, an internal layer within the neural network gives a representation z(x) for\neach new input. Such a network can be viewed as having two parts. The ﬁrst is an\nencoder, which maps the input x into a hidden representation z(x), and the second\nis a decoder, which maps the hidden representation onto the output y(z).\nIf an autoencoder is to ﬁnd non-trivial solutions, it is necessary to introduce\nsome form of constraint, otherwise the network can simply copy the input values"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2860, "text": "to the outputs. This constraint might be achieved, for example, by restricting the\ndimensionality of z relative to that of x or by requiring z to have a sparse represen-\n563© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_19"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2861, "text": "564 19. AUTOENCODERS\ntation. Alternatively, the network can be forced to discover non-trivial solutions by\nmodifying the training process such that the network has to learn to undo corrup-\ntions to the input vectors such as additive noise or missing values. These kinds of\nconstraint encourage the network to discover interesting structure within the data to\nachieve good training performance.\nIn this chapter, we start with deterministic autoencoders and then later gener-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2862, "text": "alize to stochastic models that learn an encoder distribution p(z|x)together with a\ndecoder distribution p(y|z). These probabilistic models are known as variational\nautoencoders and represent the third of our four approaches to learning nonlinear\nlatent variable models.Section 16.4.4\n19.1.\nDeterministic Autoencoders\nWe encountered a simple form of autoencoder when we studied principal compo-\nnent analysis (PCA). This is a model that makes a linear transformation of an inputSection 16.1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2863, "text": "vector onto a lower dimensional manifold, and the resulting projection can be ap-\nproximately reconstructed back in the original data space, again through a linear\ntransformation. We can make use of the nonlinearity of neural networks to deﬁne a\nform of nonlinear PCA in which the latent manifold is no longer a linear subspace\nof the data space. This is achieved by using a network having the same number of\noutputs as inputs and by optimizing the weights so as to minimize some measure of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2864, "text": "the reconstruction error between inputs and outputs with respect to a set of training\ndata.\nSimple autoencoders are rarely used directly in modern deep learning, as they\ndo not provide semantically meaningful representations in the latent space and they\nare not able directly to generate new examples from the data distribution. However,\nthey provide an important conceptual foundation for some of the more powerful deep\ngenerative models such as variational autoencoders.Section 19.2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2865, "text": "19.1.1 Linear autoencoders\nConsider ﬁrst a multilayer perceptron of the form shown in Figure 19.1, having\nD inputs, D output units, and M hidden units, with M < D. The targets used\nto train the network are simply the input vectors themselves, so that the network\nattempts to map each input vector onto itself. Such a network is said to form anauto-\nassociative mapping. Since the number of hidden units is smaller than the number"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2866, "text": "of inputs, a perfect reconstruction of all input vectors is not in general possible. We\ntherefore determine the network parameters w by minimizing an error function that\ncaptures the degree of mismatch between the input vectors and their reconstructions.\nIn particular, we choose a sum-of-squares error of the form\nE(w) = 1\n2\nN∑\nn=1\n∥y(xn;w) −xn∥2: (19.1)\nIf the hidden units have linear activation functions, then it can be shown that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2867, "text": "error function has a unique global minimum and that at this minimum the network\n19.1. Deterministic Autoencoders 565\nFigure 19.1 An autoencoder neural network having two\nlayers of weights. Such a network is trained\nto map input vectors onto themselves by\nminimizing a sum-of-squares error. Even\nwith nonlinear units in the hidden layer,\nsuch a network is equivalent to linear prin-\ncipal component analysis. Links represent-\ning bias parameters have been omitted for\nclarity.\nxd\ninputs\n …\nx1\nzm …\nz1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2868, "text": "clarity.\nxd\ninputs\n …\nx1\nzm …\nz1\nyd\noutputs\n …\ny1\nperforms a projection onto the M-dimensional subspace that is spanned by the ﬁrst\nM principal components of the data (Bourlard and Kamp, 1988; Baldi and Hornik,\n1989). Thus, the vectors of weights that lead into the hidden units in Figure 19.1\nform a basis set that spans the principal subspace. Note, however, that these vectors\nneed not be orthogonal or normalized. This result is unsurprising, since both PCA"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2869, "text": "and neural networks rely on linear dimensionality reduction and minimize the same\nsum-of-squares error function.\nIt might be thought that the limitations of a linear manifold could be overcome\nby using nonlinear activation functions for the hidden units in the network in Fig-\nure 19.1. However, even with nonlinear hidden units, the minimum error solution\nis again given by the projection onto the principal component subspace (Bourlard"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2870, "text": "and Kamp, 1988). There is therefore no advantage in using two-layer neural net-\nworks to perform dimensionality reduction. Standard techniques for PCA, based on\nsingular-value decomposition (SVD), are guaranteed to give the correct solution in\nﬁnite time, and they also generate an ordered set of eigenvalues with corresponding\northonormal eigenvectors.\n19.1.2 Deep autoencoders\nThe situation is different, however, if additional nonlinear layers are included in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2871, "text": "the network. Consider the four-layer auto-associative network shown inFigure 19.2.\nAgain, the output units are linear, and the M units in the second layer can also\nbe linear. However, the ﬁrst and third layers have sigmoidal nonlinear activation\nfunctions. The network is again trained by minimizing the error function (19.1). We\ncan view this network as two successive functional mappingsF1 and F2, as indicated\nin Figure 19.2. The ﬁrst mapping F1 projects the original D-dimensional data onto"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2872, "text": "an M-dimensional subspace Sdeﬁned by the activations of the units in the second\nlayer. Because of the ﬁrst layer of nonlinear units, this mapping is very general and\nis not restricted to being linear. Similarly, the second half of the network deﬁnes\nan arbitrary functional mapping from the M-dimensional hidden space back into the\noriginal D-dimensional input space. This has a simple geometrical interpretation, as\nindicated for D= 3 and M = 2 in Figure 19.3."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2873, "text": "indicated for D= 3 and M = 2 in Figure 19.3.\nSuch a network effectively performs a nonlinear form of PCA. It has the ad-\nvantage of not being limited to linear transformations, although it contains standard\n566 19. AUTOENCODERS\nFigure 19.2 Adding extra hidden layers\nof nonlinear units produces an auto-\nassociative network, which can perform a\nnonlinear dimensionality reduction. xD\ninputs\n …\nx1\n …\nzM …\nz1\n …\nyD\noutputs\n …\ny1\nnonlinear\nF1 F2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2874, "text": "x1\n …\nzM …\nz1\n …\nyD\noutputs\n …\ny1\nnonlinear\nF1 F2\nPCA as a special case. However, training the network now involves a nonlinear op-\ntimization, since the error function (19.1) is no longer a quadratic function of the\nnetwork parameters. Computationally intensive nonlinear optimization techniques\nmust be used, and there is the risk of ﬁnding a sub-optimal local minimum of the\nerror function. Also, the dimensionality of the subspace must be speciﬁed before\ntraining the network."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2875, "text": "training the network.\n19.1.3 Sparse autoencoders\nInstead of limiting the number of nodes in one of the hidden layers in the net-\nwork, an alternative way to constrain the internal representation is to use a regularizer\nto encourage a sparse representation, leading to a lower effective dimensionality.\nA simple choice is the L1 regularizer since this encourages sparseness, giving aSection 9.2.2\nx1\nx2\nx3\nz1\nz2\ny1\ny2\ny3\ny\nzx\nF2\nF1\nS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2876, "text": "x1\nx2\nx3\nz1\nz2\ny1\ny2\ny3\ny\nzx\nF2\nF1\nS\nFigure 19.3 Geometrical interpretation of the mappings performed by the network in Figure 19.2 for a model\nwith D = 3 inputs and M = 2 units in the second layer. The function F2 from the latent space deﬁnes the way\nin which the manifold Sis embedded within the higher-dimensional data space. Since F2 can be nonlinear, the\nembedding of Scan be non-planar, as indicated in the ﬁgure. The functionF1 then deﬁnes a projection from the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2877, "text": "original D-dimensional data space into the M-dimensional latent space.\n19.1. Deterministic Autoencoders 567\nregularized error function of the form\n˜E(w) = E(w) + \u0015\nK∑\nk=1\n|zk| (19.2)\nwhere E(w) is the unregularized error, and the sum over k is taken over the acti-\nvation values of all the units in one of the hidden layers. Note that regularization\nis usually applied to the parameters of a network, whereas here it is being used on"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2878, "text": "the unit activations. The derivatives required for gradient descent training can be\nevaluated using automatic differentiation, as usual.\n19.1.4 Denoising autoencoders\nWe have seen the importance of constraining the dimensionality of the latent\nspace layer in a simple autoencoder to avoid the model simply learning the identity\nmapping. An alternative approach, that also forces the model to discover interesting"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2879, "text": "internal structure in the data, is to use adenoising autoencoder (Vincent et al., 2008).\nThe idea is to take each input vectorxnand to corrupt it with noise to give a modiﬁed\nvector ˜xn which is then input to an autoencoder to give an output y(˜xn;w). The\nnetwork is trained to reconstruct the original noise-free input vector by minimizing\nan error function such as the sum-of squares given by\nE(w) =\nN∑\nn=1\n∥y(˜xn;w) −xn∥2: (19.3)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2880, "text": "E(w) =\nN∑\nn=1\n∥y(˜xn;w) −xn∥2: (19.3)\nOne form of noise involves setting a randomly chosen subset of the input variables\nto zero. The fraction \u0017of such inputs represents the noise level, and lies in the range\n0 6 \u0017 6 1. An alternative approach is to add independent zero-mean Gaussian\nnoise to every input variable, where the scale of the noise is set by the variance of\nthe Gaussian. By learning to denoise the input data, the network is forced to learn"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2881, "text": "aspects of the structure of that data. For example, if the data comprises images,\nthen learning that nearby pixel values are strongly correlated allows noise-corrupted\npixels to be corrected.\nMore formally, the training of denoising autoencoders is related to score match-\ning (Vincent, 2011) where the score is deﬁned bys(x) = ∇x ln p(x). Some intuition\nfor this relationship is given in Figure 19.4. The autoencoder learns to reverse the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2882, "text": "distortion vector ˜xn −xn and therefore learns a vector for each point in data space\nthat points towards the manifold and therefore towards the region of high data den-\nsity. The score vector ∇ln p(x) is similarly a vector pointing towards the region of\nhigh data density. We will explore the relationship between score matching and de-\nnoising in more depth when we discuss diffusion models which also learn to remove\nnoise from noise-corrupted inputs.Section 20.3\n19.1.5 Masked autoencoders"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2883, "text": "19.1.5 Masked autoencoders\nWe have seen that transformer models such as BERT can learn rich internal\nrepresentations of natural languages through self-supervision by masking random\n568 19. AUTOENCODERS\nFigure 19.4 In a denoising autoencoder, data\npoints, which are assumed to\nlive on a lower-dimensional man-\nifold in data space, are corrupted\nwith additive noise. The autoen-\ncoder learns to map corrupted\ndata points back to their original\nvalues and therefore learns a vec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2884, "text": "values and therefore learns a vec-\ntor for each point in data space\nthat points towards the manifold.\ncorrupted data point\noriginal\ndata point\ndata manifold\nsubsets of the inputs, and it is natural to ask if a similar approach can be applied toSection 12.2\nnatural images. In a masked autoencoder (He et al., 2021), a deep network is used\nto reconstruct an image given a corrupted version of that image as input, similar to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2885, "text": "denoising autoencoders. However in this case, the form of corruption is masking, or\ndropping out, part of the input image. This technique is generally used in combina-\ntion with a vision transformer architecture, as in this case, masking part of the inputSection 12.4.1\ncan be easily implemented by passing only a subset of randomly selected input patch\ntokens to the encoder. The overall algorithm is summarized in Figure 19.5."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2886, "text": "Compared to language, images have much more redundancy along with strong\nlocal correlations. Omitting a single word from a sentence can greatly increase am-\nbiguity whereas removing a random patch from an image typically has little impact\non the semantics of the image. Unsurprisingly, the best internal representations are\nlearned when a relatively high proportion of the input image is masked, typically\n75% compared with the 15% masking for BERT. In BERT the masked inputs are"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2887, "text": "replaced by a ﬁxed mask token, whereas in the masked autoencoder the masked\npatches are simply omitted. By omitting a large fraction of the input patches, we can\nsave signiﬁcant computation, particularly as the computation required for a training\ninstance of a transformer scales poorly with input sequence length, thus making the\nmasked autoencoder a good choice for pre-training large transformer encoders.\nAs the decoder layer is also a transformer, it needs to work in the dimensionality"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2888, "text": "of the original image. Since the output of a transformer has the same dimensionality\nas the input, we need to restore the image dimensionality between the output of the\nencoder and the input of the decoder. This is achieved by reinstating the masked\npatches, represented by a ﬁxed mask token vector, with each patch token augmented\nby positional encoding information. Due to the much higher dimensionality of the\ndecoder representation, the decoder transformer has far fewer learnable parameters"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2889, "text": "than the encoder. The output of the decoder is followed by a learnable linear layer\n19.2. Variational Autoencoders 569\ninputs\nencoder\n:::\ndecoder\noutputs\n:::\npredictions\ntargets\nFigure 19.5 Architecture of a masked autoencoder during the training phase. Note that the target is the com-\nplement of the input as the loss is only applied on masked patches. After training, the decoder is discarded and\nthe encoder is used to map images to an internal representation for use in downstream tasks."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2890, "text": "that maps the output representation into the space of pixel values, and the training\nerror function is simply the mean squared error averaged over the missing patches\nfor each image. Examples of images reconstructed by a trained masked autoencoder\nare shown in Figure 19.6 and demonstrate the ability of a trained autoencoder to\ngenerate semantically plausible reconstructions. However, the ultimate goal is to\nlearn useful internal representations for subsequent downstream tasks, for which the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2891, "text": "decoder is discarded and the encoder is applied to the full image with no masking\nand with a fresh set of output layers that are ﬁne-tuned for the required application.\nNote also that although this algorithm was initially designed for image data, it can in\ntheory be applied to any modality.\n19.2.\nVariational Autoencoders\nWe have already seen that the likelihood function for a latent-variable model given\nby\np(x|w) =\n∫\np(x|z;w)p(z) dz; (19.4)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2892, "text": "by\np(x|w) =\n∫\np(x|z;w)p(z) dz; (19.4)\nin which p(x|z;w) is deﬁned by a deep neural network, is intractable because the\nintegral over z cannot be evaluated analytically. The variational autoencoder, or\n570 19. AUTOENCODERS\nFigure 19.6 Four examples of images reconstructed using a trained masked autoencoder, in which 80% of\nthe input patches are masked. In each case the masked image is on the left, the reconstructed image is in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2893, "text": "centre, and the original image is on the right. [From He et al. (2021) with permission.]\nVAE (Kingma and Welling, 2013; Rezende, Mohamed, and Wierstra, 2014; Doer-\nsch, 2016; Kingma and Welling, 2019) instead works with an approximation to this\nlikelihood when training the model. There are three key ideas in the V AE: (i) use of\nthe evidence lower bound (ELBO) to approximate the likelihood function, leading to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2894, "text": "a close relationship to the EM algorithm, (ii) amortized inference in which a secondSection 15.3\nmodel, the encoder network, is used to approximate the posterior distributions over\nlatent variables in the E step, rather than evaluating the posterior distribution for each\ndata point exactly, and (iii) making the training of the encoder model tractable using\nthe reparameterization trick.\nConsider a generative model with a conditional distribution p(x|z;w) over the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2895, "text": "D-dimensional data variable x governed by the output of a deep neural network\ng(z;w). For example, g(z;w) might represent the mean of a Gaussian conditional\ndistribution. Also, consider a distribution over the M-dimensional latent variable z\nthat is given by a zero-mean unit-variance Gaussian:\np(z) = N(z|0;I): (19.5)\nTo derive the V AE approximation, ﬁrst recall that, for an arbitrary probability distri-\nbution q(z) over a space described by the latent variablez, the following relationship"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2896, "text": "holds:Section 15.4\nln p(x|w) = L(w) + KL (q(z)∥p(z|x;w)) (19.6)\nwhere Lis the evidence lower bound, or ELBO, also known as thevariational lower\nbound, given by\nL(w) =\n∫\nq(z) ln\n{ p(x|z;w)p(z)\nq(z)\n}\ndz (19.7)\n19.2. Variational Autoencoders 571\nand the Kullback–Leibler divergence KL (-∥-)is deﬁned by\nKL (q(z)∥p(z|x;w)) = −\n∫\nq(z) ln\n{ p(z|x;w)\nq(z)\n}\ndz: (19.8)\nBecause the Kullback–Leibler divergence satisﬁes KL (q∥p)> 0, it follows that\nln p(x|w) > L (19.9)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2897, "text": "ln p(x|w) > L (19.9)\nand so Lis a lower bound on ln p(x|w). Although the log likelihood ln p(x|w) is\nintractable, we will see how the lower bound can be evaluated using a Monte Carlo\nestimate. Hence it provides an approximation to the true log likelihood.\nNow consider a set of training data points D= {x1;:::; xN}, which are as-\nsumed to be drawn independently from the model distribution p(x). The log likeli-\nhood function for this data set is given by\nln p(D|w) =\nN∑\nn=1\nLn +\nN∑\nn=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2898, "text": "ln p(D|w) =\nN∑\nn=1\nLn +\nN∑\nn=1\nKL (qn(zn)∥p(zn|xn;w)) (19.10)\nwhere\nLn =\n∫\nqn(zn) ln\n{ p(xn|zn;w)p(zn)\nqn(zn)\n}\ndzn: (19.11)\nNote that this introduces a separate latent variable zn corresponding to each data\nvector xn, as we saw with mixture models and with the probabilistic PCA model.Section 15.2\nSection 16.2 Consequently, each latent variable has its own independent distributionqn(zn), each\nof which can be optimized separately."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2899, "text": "of which can be optimized separately.\nSince (19.10) holds for any choice of the distributions qn(z), we can choose the\ndistributions that maximize the bound Ln, or equivalently the distributions that min-\nimize the Kullback–Leibler divergences KL (qn(zn)∥p(zn|xn;w)). For the simple\nGaussian mixture and probabilistic PCA models considered previously, we were able\nto evaluate these posterior distributions exactly in the E step of the EM algorithm,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2900, "text": "which corresponds to setting each qn(zn) equal to the corresponding posterior dis-\ntribution p(zn|xn;w). This gives zero Kullback–Leibler divergence, and hence the\nlower bound is equal to the true log likelihood. The interpretation of the posterior\ndistribution is illustrated in Figure 19.7 using the simple example introduced earlier\nin the context of generative adversarial networks.Section 16.4.1\nThe exact posterior distribution of zn is given from Bayes’ theorem by"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2901, "text": "p(zn|xn;w) = p(xn|zn;w)p(zn)\np(xn|w) : (19.12)\nThe numerator is straightforward to evaluate for our deep generative model. How-\never, we see that the denominator is given by the likelihood function, which as we\nhave already noted, is intractable. We therefore need to ﬁnd an approximation to\nthe posterior distribution. In principle, we could consider a separate parameterized\nmodel for each of the distributions qn(zn) and optimize each model numerically,\n572 19. AUTOENCODERS\n−4 −2 0 2 4\nz"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2902, "text": "572 19. AUTOENCODERS\n−4 −2 0 2 4\nz\np(z) p(z|x⋆)\n(a)\nz\nx1\nx2\nx⋆ (b)\nFigure 19.7 Evaluation of the posterior distribution for the same model as shown inFigure 16.13. The marginal\ndistribution p(x), shown in the right-most plot in (b), has a banana shape, and the speciﬁc data pointx? is closer\nto the horns of the shape than to the middle. Consequently the posterior distribution p(z|x?), shown in (a), is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2903, "text": "bimodal, even though the prior distribution p(z) is unimodal. [Based on Prince (2020) with permission.]\nbut this would be computationally very expensive, especially for large data sets, and\nmoreover we would have to re-evaluate the distributions after every update of w.\nInstead, we turn now to a different, more efﬁcient approximation framework based\non the introduction of a second neural network.\n19.2.1 Amortized inference"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2904, "text": "19.2.1 Amortized inference\nIn the variational autoencoder, instead of trying to evaluate a separate posterior\ndistribution p(zn|xn;w) for each of the data pointsxnindividually, we train a single\nneural network, called the encoder network, to approximate all these distributions.\nThis technique is called amortized inference and requires an encoder that produces\na single distribution q(z|x;\u001e) that is conditioned on x, where \u001erepresents the pa-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2905, "text": "rameters of the network. The objective function, given by the evidence lower bound,\nnow has a dependence on\u001eas well as onw, and we use gradient-based optimization\nmethods to maximize the bound jointly with respect to both sets of parameters.\nA V AE therefore comprises two neural networks that have independent param-\neters but which are trained jointly: an encoder network that takes a data vector and\nmaps it to a latent space, and the original network that takes a latent space vector and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2906, "text": "maps it back to the data space and which we can therefore interpret as adecoder net-\nwork. This like the simple neural network autoencoder model, except that we nowSection 19.1\ndeﬁne a probability distribution over the latent space. We will see that the encoder\ncalculates an approximate probabilistic inverse of the decoder according to Bayes’\ntheorem.\nA typical choice for the encoder is a Gaussian distribution with a diagonal co-\nvariance matrix whose mean and variance parameters, \u0016j and \u001b2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2907, "text": "j, are given by the\n19.2. Variational Autoencoders 573\nw0\nw\nL(w0, φ0)\nL(w0, φ1)\nln p(x|w)\n(a)\nw0 w1\nw\nL(w0, φ1)\nL(w1, φ1)\nln p(x|w)\n (b)\nFigure 19.8 Illustration of the optimization of the ELBO (evidence lower bound). (a) For a given value w0 of\nthe decoder network parameters w, we can increase the bound by optimizing the parameters \u001eof the encoder\nnetwork. (b) For a given value of \u001e, we can increase the value of the ELBO function by optimizing w. Note that"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2908, "text": "the ELBO function, shown by the blue curves, always lies somewhat below the log likelihood function, shown in\nred, because the encoder network is generally not able to match the true posterior distribution exactly.\noutputs of a neural network that takes x as input:\nq(z|x;\u001e) =\nM∏\nj=1\nN\n(\nzj|\u0016j(x;\u001e);\u001b2\nj(x;\u001e)\n)\n: (19.13)\nNote that the means \u0016j(x;\u001e) lie in the range (−∞; ∞), and so the corresponding\noutput-unit activation functions can be linear, whereas the variances \u001b2\nj(x;\u001e) must"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2909, "text": "j(x;\u001e) must\nbe non-negative and so the associated output units typically use exp(-)as their acti-\nvation function.\nThe goal is to use gradient-based optimization to maximize the bound with re-\nspect to both sets of parameters \u001e and w, typically by using stochastic gradient\ndescent based on mini-batches. Although we optimize the parameters jointly, con-\nceptually we could imagine alternating between optimizing \u001eand optimizing w, in\nthe spirit of the EM algorithm, as illustrated in Figure 19.8."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2910, "text": "A key difference compared to EM is that, for a given value ofw, optimizing with\nrespect to the parameters \u001eof the encoder does not in general reduce the Kullback-\nLeibler divergence to zero, because the encoder network is not a perfect predictor\nof the posterior latent distribution and so there is a residual gap between the lower\nbound and the true log likelihood. Although the encoder is very ﬂexible, since it\nis based on a deep neural network, it is not expected to model the true posterior"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2911, "text": "distribution exactly because (i) the true conditional posterior distribution will not be\n574 19. AUTOENCODERS\nln p(x|w)\nE\nM\n(a)\nln p(x|w)\n (b)\nFigure 19.9 Comparison of the EM algorithm with ELBO optimization in a VAE. (a) In the EM algorithm we\nalternate between updating the variational posterior distribution in the E step, and the model parameters in the\nM step. When the E step is exact, the gap between the lower bound and the log likelihood is reduced to zero"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2912, "text": "after each E step. (b) In the VAE we perform joint optimization of the encoder network parameters\u001e(analogous\nto the E step) and the decoder network parameters w (analogous to the M step).\na factorized Gaussian, (ii) even a large neural network has limited ﬂexibility, and (iii)\nthe training process is only an approximate optimization. The relation between the\nEM algorithm and ELBO optimization is summarized in Figure 19.9.\n19.2.2 The reparameterization trick"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2913, "text": "19.2.2 The reparameterization trick\nUnfortunately, as it stands, the lower bound (19.11) is still intractable to compute\nbecause it involves integrals over the latent variables{zn}in which the integrand has\na complicated dependence on the latent variables because of the decoder network.\nFor data point xn we can write the contribution to the lower bound in the form\nLn(w;\u001e) =\n∫\nq(zn|xn;\u001e) ln\n{ p(xn|zn;w)p(zn)\nq(zn|xn;\u001e)\n}\ndzn\n=\n∫\nq(zn|xn;\u001e) lnp(xn|zn;w) dzn −KL(q(zn|xn;\u001e)∥p(zn)): (19.14)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2914, "text": "The second term on the right-hand side is a Kullback–Leibler divergence between\ntwo Gaussian distributions and can be evaluated analytically:Exercise 2.27\nKL (q(zn|xn;\u001e)∥p(zn)) = 1\n2\nM∑\nj=1\n{\n1 + ln\u001b2\nj(xn) −\u00162\nj(xn) −\u001b2\nj(xn)\n}\n: (19.15)\n19.2. Variational Autoencoders 575\nx \u001e\n\u0016i\n\u001b2\ni\nz w p(x|z;w)\nFigure 19.10 When the ELBO is estimated by ﬁxing the latent variable z to a sampled value this blocks\nbackpropagation of the error signal to the encoder network."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2915, "text": "For the ﬁrst term in (19.14), we could try to approximate the integral over zn with a\nsimple Monte Carlo estimator:\n∫\nq(zn|xn;\u001e) lnp(xn|zn;w) dzn ≃ 1\nL\nL∑\nl=1\nln p(xn|z(l)\nn ;w) (19.16)\nwhere {z(l)\nn }are samples drawn from the encoder distribution q(zn|xn;\u001e). This is\neasily differentiated with respect tow, but the gradient with respect to\u001eis problem-\natic because changes to \u001ewill change the distribution q(zn|xn;\u001e) from which the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2916, "text": "samples are drawn and yet these samples are ﬁxed values so that we do not have a\nway to obtain the derivatives of these samples with respect to \u001e. Conceptually, we\ncan think of the process of ﬁxing zn to a speciﬁc sample value as blocking the back-\npropagation of the error signal to the encoder network, as illustrated inFigure 19.10.\nWe can resolve this by making use of the reparameterization trick in which we\nreformulate the Monte Carlo sampling procedure such that derivatives with respect"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2917, "text": "to \u001ecan be calculated explicitly. First, note that if \u000fis a Gaussian random variable\nwith zero mean and unit variance, then the quantity\nz= \u001b\u000f+ \u0016 (19.17)\nwill have a Gaussian distribution, with mean \u0016and variance \u001b2. We now apply thisExercise 19.2\nto the samples in (19.16) in which \u0016 and \u001b are deﬁned by the outputs \u0016j(xn;\u001e)\nand \u001b2\nj(xn;\u001e) of the encoder network, which represent the means and variances in\ndistribution (19.13). Instead of drawing samples of zn directly, we draw samples for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2918, "text": "\u000fand use (19.17) to evaluate corresponding samples for zn:\nz(l)\nnj = \u0016j(xn;\u001e)\u000f(l)\nnj + \u001b2\nj(xn;\u001e) (19.18)\nwhere l = 1;:::;L indexes the samples. This makes the dependence on \u001eexplicit\nand allows gradients with respect to \u001eto be evaluated, as illustrated inFigure 19.11.\nThe reparameterization trick can be extended to other distributions but is limited to\ncontinuous variables. There are techniques to evaluate gradients directly without the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2919, "text": "reparameterization trick (Williams, 1992), but these estimators have high variance,\nand so reparameterization can also be viewed as a variance reduction technique.\nThe full error function for the V AE, using our speciﬁc modelling assumptions,\ntherefore becomes\nL=\n∑\nn\n\n\n\n1\n2\nM∑\nj=1\n{\n1 + ln\u001b2\nnj −\u00162\nnj −\u001b2\nnj\n}\n+ 1\nL\nL∑\nl=1\nln p(xn|z(l)\nn ;w)\n\n\n (19.19)\n576 19. AUTOENCODERS\nx \u001e\n\u0016i\n\u001b2\ni\nz\n\u000f\nw p(x|z;w)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2920, "text": "576 19. AUTOENCODERS\nx \u001e\n\u0016i\n\u001b2\ni\nz\n\u000f\nw p(x|z;w)\nFigure 19.11 The reparameterization trick replaces a direct sample ofz by one that is calculated from a\nsample of an independent random variable\u000f, thereby allowing the error signal to be back-\npropagated to the encoder network. The resulting model can be trained using gradient-\nbased optimization to learn the parameters of both the encoder and decoder networks.\nwhere z(l)\nn has components z(l)\nnj = \u001bnj\u000f(l) + \u0016nj, in which \u0016nj = \u0016j(xn;\u001e) and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2921, "text": "nj = \u001bnj\u000f(l) + \u0016nj, in which \u0016nj = \u0016j(xn;\u001e) and\n\u001bnj = \u001bj(xn;\u001e), and the summation over nin (19.19) is over the data points in a\nmini-batch. The number of samples L, for each data pointxn, is typically set to1, so\nthat only a single sample is used. Although this gives a noisy estimate of the bound,\nit forms part of the stochastic gradient optimization step, which is already noisy, and\noverall leads to more efﬁcient optimization."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2922, "text": "overall leads to more efﬁcient optimization.\nWe can summarize V AE training as follows. For each data point in a mini-\nbatch, forward propagate through the encoder network to evaluate the means and\nvariances of the approximate latent distribution, sample from this distribution using\nthe reparameterization trick, and then propagate these samples through the decoder\nnetwork to evaluate the ELBO (19.19). The gradients with respect to w and \u001eare"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2923, "text": "then evaluated using automatic differentiation. V AE training is summarized in Al-\ngorithm 19.1, where, for clarity, we have omitted that this would generally be done\nusing mini-batches. Once the model is trained, the encoder network is discarded and\nnew data points are generated by sampling from the prior p(z) and forward propa-\ngating through the decoder network to obtain samples in the data space.\nAfter training we might want to assess how well the model represents a new test"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2924, "text": "point ˆx. Since the log likelihood is intractable, we can use the lower bound Las an\napproximation. To estimate this we can sample from q(z|ˆx;\u001e) as this gives more\naccurate estimates than sampling from p(z).\nThere are many variants of V AEs. When applied to image data, the encoder is\ntypically based on convolutions and the decoder based on transpose convolutions.Section 10.5.3\nIn a conditional VAE both the encoder and decoder take a conditioning variable c"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2925, "text": "as an additional input. For example, we might want to generate images of objects,\nin which c represents the object class. The latent-space prior distribution p(z) can\nagain be a simple Gaussian, or it can be extended to a conditional distributionp(z|c)\ngiven by another neural network. Training and testing proceed as before.\nNote that the ﬁrst term in the ELBO (19.14) encourages the encoder distribution\nq(z|x;\u001e) to be close to the prior p(z), and so the decoder model is encouraged to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2926, "text": "produce realistic outputs when the trained model is run generatively by sampling\nfrom p(z). When training V AEs, a problem can arise in which the variational dis-\ntribution q(z|x;\u001e) converges to the prior distribution p(z) and therefore becomes\nuninformative because it no longer depends on x. In effect the latent code is ig-\n19.2. Variational Autoencoders 577\nAlgorithm 19.1: Variational autoencoder training\nInput: Training data set D= {x1;:::; xN}\nEncoder network {\u0016j(xn;\u001e);\u001b2"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2927, "text": "Encoder network {\u0016j(xn;\u001e);\u001b2\nj(xn;\u001e)}; j ∈{1;:::;M }\nDecoder network g(z;w)\nInitial weight vectors w;\u001e\nLearning rate \u0011\nOutput: Final weight vectors w;\u001e\nrepeat\nL← 0\nfor j ∈{1;:::;M }do\n\u000fnj ∼N(0;1)\nznj ← \u0016j(xn;\u001e)\u000fnj + \u001b2\nj(xn;\u001e)\nL←L + 1\n2\n{\n1 + ln\u001b2\nnj −\u00162\nnj −\u001b2\nnj\n}\nend for\nL←L + lnp(xn|zn;w)\nw ← w + \u0011∇wL // Update decoder weights\n\u001e← \u001e+ \u0011∇\u001eL // Update encoder weights\nuntil converged\nreturn w;\u001e\nnored. This is known as posterior collapse. A symptom of this is that if we take"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2928, "text": "an input and encode it and then decode it, we get a poor reconstruction that looks\nblurry. In this case the Kullback–Leibler divergence KL(q(z|x;\u001e)∥p(z))is close to\nzero.\nA different problem occurs when the latent code is not compressed, which is\ncharacterized by highly accurate reconstructions, but such that outputs generated by\nsampling p(z) and passing the samples through the decoder network have poor qual-\nity and do not resemble the training data. In this case the Kullback–Leibler diver-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2929, "text": "gence is relatively large, and because the trained system has a variational distribution\nthat is very different from the prior, samples from the prior do not generate realistic\noutputs.\nBoth problems can be addressed by introducing a coefﬁcient\fin front of the ﬁrst\nterm in (19.14) to control the regularization effectiveness of the Kullback–Leibler\ndivergence, where typically \f > 1 (Higgins et al., 2017). If the reconstructions"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2930, "text": "look poor then \f can be increased, whereas if the samples look poor then \f can be\ndecreased. The value of \f can also be set to follow an annealing schedule in which\nit starts with a small value and is gradually increased during training.\nFinally, note that we have considered a decoder networkg(z;w) that represents\n578 19. AUTOENCODERS\nthe mean of a Gaussian output distribution. We can extend the V AE to include out-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2931, "text": "puts representing the variance of the Gaussian or, more generally, the parameters that\ncharacterize other more complex distributions.Section 6.5\nExercises\n19.1 (??) Show that, for any distribution q(z|\u001e) and any function G(z), the following\nrelation holds:\n∇\u001e\n∫\nq(z|\u001e)G(z) dz =\n∫\nq(z|\u001e)G(z)∇\u001eln q(z|\u001e) dz: (19.20)\nHence, show that the left-hand side of (19.20) can be approximated by the following\nMonte Carlo estimator:\n∇\u001e\n∫\nq(z|\u001e)G(z) dz ≃\n∑\ni\nG(z(i))∇\u001eln q(z(i)|\u001e) (19.21)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2932, "text": "q(z|\u001e)G(z) dz ≃\n∑\ni\nG(z(i))∇\u001eln q(z(i)|\u001e) (19.21)\nwhere the samples {z(i)}are drawn independently from the distributionq(z|\u001e). Ver-\nify that this estimator is unbiased, i.e., that the average value of the right-hand side\nof (19.21), averaged over the distribution of the samples, is equal to the left-hand\nside. In principle, by setting G(z) = p(x|z;w), this result would allow the gradient\nof the second term on the right-hand side of (19.14) with respect to \u001eto be evalu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2933, "text": "ated without making use of the reparameterization trick. Also, because this method\nis unbiased, it will give the exact answer in the limit of an inﬁnite number of sam-\nples. However, the reparameterization trick is more efﬁcient, meaning that fewer\nsamples are needed to get good accuracy, because it directly computes the change of\np(x|z;w) due to the change in z that results from a change in \u001e.\n19.2 (?) Verify that if \u000f has a zero-mean unit-variance Gaussian distribution, then the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2934, "text": "variable zin (19.17) will have a Gaussian distribution with mean \u0016and variance \u001b2.\n19.3 (??) In this exercise we extend the diagonal covariance V AE encoder network (19.13)\nto one with a general covariance matrix. Consider a K-dimensional random vector\ndrawn from a simple Gaussian:\n\u000f∼N(z|0;I); (19.22)\nwhich is then linearly transformed using the relation\nz = \u0016+ L\u000f (19.23)\nwhere L is a lower-triangular matrix (i.e., a K×K matrix with all elements above"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2935, "text": "the leading diagonal being zero). Show that z has a distribution N(z|\u0016;\u0006), and\nwrite down an expression for\u0006 in terms of L. Explain why the diagonal elements of\nL must be non-negative. Describe how \u0016and L can be expressed as the outputs of a\nneural network, and discuss suitable choices for output-unit activation functions.\nExercises 579\n19.4 (??) Evaluate the Kullback–Leibler divergence term in (19.14). Hence, show how"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2936, "text": "the gradients of this term with respect to w and \u001ecan be evaluated for training the\nencoder and decoder networks.\n19.5 (?) We have seen that the ELBO given by (19.11) can be written in the form (19.14).\nShow that it can also be written as\nLn(w;\u001e) =\n∫\nq(zn|xn;\u001e) ln{p(xn|zn;w)p(zn)}dzn\n−\n∫\nq(zn|xn;\u001e) lnq(zn|xn;\u001e) dzn: (19.24)\n19.6 (?) Show that the ELBO given by (19.11) can be written in the form\nLn(w;\u001e) =\n∫\nq(zn|xn;\u001e) lnp(zn) dzn\n+\n∫\nq(zn|xn;\u001e) ln\n{ p(xn|zn;w)\nq(zn|xn;\u001e)\n}\ndzn: (19.25)\n20"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2937, "text": "{ p(xn|zn;w)\nq(zn|xn;\u001e)\n}\ndzn: (19.25)\n20\nDiffusion\nModels\nWe have seen that a powerful way to construct rich generative models is to introduce\na distribution p(z) over a latent variablez, and then to transformz into the data space\nx using a deep neural network. It is sufﬁcient to use a simple, ﬁxed distribution\nfor p(z), such as a Gaussian N(z|0;I), since the generality of the neural network\ntransforms this into a highly ﬂexible family of distributions over x. In previous"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2938, "text": "chapters we have explored several models which ﬁt within this framework but which\ntake different approaches to deﬁning and training the deep neural network, based on\ngenerative adversarial networks, variational autoencoders, and normalizing ﬂows.\nIn this chapter we discuss a fourth class of models within this general frame-Section 16.4.4\nwork, known as diffusion models, also called denoising diffusion probabilistic mod-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2939, "text": "els, or DDPMs (Sohl-Dickstein et al., 2015; Ho, Jain, and Abbeel, 2020), which\nhave emerged as the state of the art for many applications. For illustration we will\nfocus on models of image data although the framework has much broader applicabil-\n581© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4_20"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2940, "text": "582 20. DIFFUSION MODELS\nx\n z1\n z2\n zT\nFigure 20.1 Illustration of the encoding process in a diffusion model showing an image x that is gradually\ncorrupted with multiple stages of additive Gaussian noise giving a sequence of increasingly noisy images. After\na large number T of steps the result is indistinguishable from a sample drawn from a Gaussian distribution. A\ndeep neural network is then trained to reverse this process."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2941, "text": "ity. The central idea is to take each training image and to corrupt it using a multi-step\nnoise process to transform it into a sample from a Gaussian distribution. This is il-\nlustrated in Figure 20.1. A deep neural network is then trained to invert this process,\nand once trained the network can then generate new images starting with samples\nfrom a Gaussian as input.\nDiffusion models can be viewed as a form of hierarchical variational autoen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2942, "text": "coder in which the encoder distribution is ﬁxed, and deﬁned by the noise process,Section 19.2\nand only the generative distribution is learned (Luo, 2022). They are easy to train,\nthey scale well on parallel hardware, and they avoid the challenges and instabilities\nof adversarial training while producing results that have quality comparable to, or\nbetter than, generative adversarial networks. However, generating new samples can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2943, "text": "be computationally expensive due to the need for multiple forward passes through\nthe decoder network (Dhariwal and Nichol, 2021).\n20.1.\nForward Encoder\nSuppose we take an image from the training set, which we will denote by x, and\nblend it with Gaussian noise independently for each pixel to give a noise-corrupted\nimage z1 deﬁned by\nz1 =\n√\n1 −\f1x +\n√\n\f1\u000f1 (20.1)\nwhere \u000f1 ∼N (\u000f1|0;I) and \f1 < 1 is the variance of the noise distribution. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2944, "text": "choice of coefﬁcients √1 −\f1 and √\f1 in (20.1) and (20.3) ensures that the mean ofExercise 20.1\nthe distribution ofztis closer to zero than the mean ofzt−1 and that the variance ofzt\nis closer to the unit matrix than the variance ofzt−1. We can write the transformation\n(20.1) in the formExercise 20.2\nq(z1|x) = N(z1|\n√\n1 −\f1x;\f1I): (20.2)\nWe then repeat the process with additional independent Gaussian noise steps to give"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2945, "text": "a sequence of increasingly noisy images z2;:::; zT. Note that in the literature on\ndiffusion models, these latent variables are sometimes denoted x1;:::; xT and the\nobserved variable is denoted x0. We use the notation of z for latent variables and x\n20.1. Forward Encoder 583\nx zt−1 zt zT\nq(zt|zt−1)\nq(zt−1|zt;x)\np(zt−1|zt;w)\nFigure 20.2 A diffusion process represented as a probabilistic graphical model. The original imagex is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2946, "text": "shown by the shaded node, since it is an observed variable, whereas the noise-corrupted\nimages z1;:::; zT are considered to be latent variables. The noise process is deﬁned by\nthe forward distributionq(zt|zt−1) and can be viewed as an encoder. Our goal is to learn a\nmodel p(zt−1|zt;w) that tries to reverse this noise process and which can be viewed as a\ndecoder. As we will see later, the conditional distribution q(zt−1|zt;x) plays an important\nrole in deﬁning the training procedure."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2947, "text": "role in deﬁning the training procedure.\nfor the observed variable for consistency with the rest of the book. Each successive\nimage is given by\nzt =\n√\n1 −\ftzt−1 +\n√\n\ft\u000ft (20.3)\nwhere \u000ft ∼N(\u000ft|0;I). Again, we can write (20.3) in the form\nq(zt|zt−1 ) = N(zt|\n√\n1 −\ftzt−1 ;\ftI): (20.4)\nThe sequence of conditional distributions (20.4) forms a Markov chain and can beSection 11.3\nexpressed as a probabilistic graphical model as shown in Figure 20.2. The values of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2948, "text": "the variance parameters \ft ∈(0;1) are set by hand and are typically chosen such\nthat the variance values increase through the chain according to a prescribed schedule\nsuch that \f1 <\f2 <:::<\f T.\n20.1.1 Diffusion kernel\nThe joint distribution of the latent variables, conditioned on the observed data\nvector x, is given by\nq(z1;:::; zt|x) =q(z1|x)\nt∏\n\u001c=2\nq(z\u001c|z\u001c−1 ): (20.5)\nIf we now marginalize over the intermediate variables z1;:::; zt−1 , we obtain the\ndiffusion kernel:Exercise 20.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2949, "text": "diffusion kernel:Exercise 20.3\nq(zt|x) =N(zt|√\n\u000btx;(1 −\u000bt)I) (20.6)\nwhere we have deﬁned\n\u000bt =\nt∏\n\u001c=1\n(1 −\f\u001c): (20.7)\n584 20. DIFFUSION MODELS\nWe see that each intermediate distribution has a simple closed-form Gaussian ex-\npression from which we can directly sample, which will prove useful when training\nDDPMs as it allows efﬁcient stochastic gradient descent using randomly chosen in-\ntermediate terms in the Markov chain without having to run the whole chain. We can\nalso write (20.6) in the form"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2950, "text": "also write (20.6) in the form\nzt = √\n\u000btx +\n√\n1 −\u000bt\u000ft (20.8)\nwhere again \u000ft ∼N(\u000ft|0;I). Note that that \u000fnow represents the total noise added to\nthe original image instead of the incremental noise added at this step of the Markov\nchain.\nAfter many steps the image becomes indistinguishable from Gaussian noise, and\nin the limit T →∞ we haveExercise 20.4\nq(zT|x) =N(zT|0;I) (20.9)\nand therefore all information about the original image is lost. The choice of coef-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2951, "text": "ﬁcients √1 −\ft and √\ft in (20.3) ensures that once the Markov chain converges\nto a distribution with zero mean and unit covariance, further updates will leave this\nunchanged.Exercise 20.5\nSince the right-hand side of (20.9) is independent ofx, it follows that the marginal\ndistribution of zT is given by\nq(zT) = N(zT|0;I): (20.10)\nIt is common to refer to the Markov chain (20.4) as the forward process, and it is\nanalogous to the encoder in a V AE, except that here it is ﬁxed rather than learned."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2952, "text": "Note, however, that the usual terminology in the literature is the opposite of that\ntypically used in the literature regarding normalizing ﬂows, where the mapping from\nlatent space to data space is considered the forward process.\n20.1.2 Conditional distribution\nOur goal is to learn to undo the noise process, and so it is natural to consider\nthe reverse of the conditional distribution q(zt|zt−1 ), which we can express using\nBayes’ theorem in the form\nq(zt−1 |zt) = q(zt|zt−1 )q(zt−1 )"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2953, "text": "q(zt−1 |zt) = q(zt|zt−1 )q(zt−1 )\nq(zt) : (20.11)\nWe can write the marginal distribution q(zt−1 ) in the form\nq(zt−1 ) =\n∫\nq(zt−1 |x)p(x) dx (20.12)\nwhere q(zt−1 |x)is given by the conditional Gaussian (20.6). This distribution is\nintractable, however, because we must integrate over the unknown data densityp(x).\nIf we approximate the integration using samples from the training data set, we obtain\na complicated distribution expressed as a mixture of Gaussians.\n20.2. Reverse Decoder 585"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2954, "text": "20.2. Reverse Decoder 585\nInstead, we consider the conditional version of the reverse distribution, condi-\ntioned on the data vector x, deﬁned by q(zt−1 |zt;x), which as we will see shortly\nturns out to be a simple Gaussian distribution. Intuitively this is reasonable since,\ngiven a noisy image, it is difﬁcult to guess which lower-noise image gave rise to it,\nwhereas if we also know the starting image then the problem becomes much easier."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2955, "text": "We can calculate this conditional distribution using Bayes’ theorem:\nq(zt−1 |zt;x) = q(zt|zt−1 ;x)q(zt−1 |x)\nq(zt|x) : (20.13)\nWe now make use of the Markov property of the forward process to write\nq(zt|zt−1 ;x) = q(zt|zt−1 ) (20.14)\nwhere the right-hand side is given by (20.4). As a function of zt−1 , this takes the\nform of an exponential of a quadratic form. The term q(zt−1 |x)in the numerator of\n(20.13) is the diffusion kernel given by (20.6), which again involves the exponential"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2956, "text": "of a quadratic form with respect to zt−1 . We can ignore the denominator in (20.13)\nsince as a function of zt−1 it is constant. Thus, we see that the right-hand side of\n(20.13) takes the form of a Gaussian distribution, and we can identify its mean and\ncovariance using the technique of ‘completing the square’ to giveExercise 20.6\nq(zt−1 |zt;x) = N\n(\nzt−1 |mt(x;zt);\u001b2\ntI\n)\n(20.15)\nwhere\nmt(x;zt) = (1 −\u000bt−1 )√\n1 −\ftzt + √\u000bt−1 \ftx\n1 −\u000bt\n(20.16)\n\u001b2\nt = \ft(1 −\u000bt−1 )\n1 −\u000bt\n(20.17)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2957, "text": "1 −\u000bt\n(20.16)\n\u001b2\nt = \ft(1 −\u000bt−1 )\n1 −\u000bt\n(20.17)\nand we have made use of (20.7).\n20.2. Reverse Decoder\nWe have seen that the forward encoder model is deﬁned by a sequence of Gaussian\nconditional distributions q(zt|zt−1 ) but that inverting this directly leads to a distri-\nbution q(zt−1 |zt) that is intractable, as it would require integrating over all possible\nvalues of the starting vector x whose distribution is the unknown data distribution"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2958, "text": "p(x) that we wish to model. Instead, we will learn an approximation to the reverse\ndistribution by using a distributionp(zt−1 |zt;w) governed by a deep neural network,\nwhere w represents the network weights and biases. This reverse step is analogous\nto the decoder in a variational autoencoder and is illustrated in Figure 20.2. OnceChapter 19\nthe network is trained, we can sample from the simple Gaussian distribution overzT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2959, "text": "and transform it into a sample from the data distribution p(x) through a sequence of\nreverse sampling steps by repeated application of the trained network.\n586 20. DIFFUSION MODELS\nzt−1\nzt\nq(zt|zt−1)\nzt\nzt−1\nq(zt−1|zt)\nq(zt−1)\nFigure 20.3 Illustration of the evaluation of the reverse distribution q(zt−1|zt) using Bayes’ theorem (20.13) for\nscalar variables. The red curve on the right-hand plot shows the marginal distribution q(zt−1) illustrated using a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2960, "text": "mixture of three Gaussians, whereas the left-hand plot shows the Gaussian forward noise process q(zt|zt−1) as\na distribution over zt centred on zt−1. By multiplying these together and normalizing, we obtain the distribution\nq(zt−1|zt) shown for a particular choice of zt by the blue curve. Because the distribution on the left is relatively\nbroad, corresponding to a large variance \ft, the distribution q(zt−1|zt) has a complex multimodal structure."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2961, "text": "Intuitively, if we keep the variances small so that \ft ≪ 1 then the change in the\nlatent vector between steps will be relatively small, and hence it should be easier to\nlearn to invert the transformation. More speciﬁcally, if \ft ≪ 1 then the distribution\nq(zt−1 |zt) will be approximately a Gaussian distribution over zt−1 . This can be\nseen from (20.11) since the right-hand side depends on zt−1 through q(zt|zt−1 ) and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2962, "text": "q(zt−1 ). If q(zt|zt−1 ) is a sufﬁciently narrow Gaussian then q(zt−1 ) will vary only\na small amount over the region in which q(zt|zt−1 ) has signiﬁcant mass, and hence\nq(zt−1 |zt) will also be approximately Gaussian. This intuition can be conﬁrmed\nusing a simple example as shown in Figures 20.3 and 20.4. However, since the\nvariances at each step are small, we must use a large number of steps to ensure that\nthe distribution over the ﬁnal latent variable zT obtained from the forward noising"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2963, "text": "process will still be close to a Gaussian, and this increases the cost of generating new\nsamples. In practice, T may be several thousand.\nWe can see more formally that q(zt−1 |zt) will be approximately Gaussian by\nzt−1\nzt\nq(zt|zt−1)\nzt\nzt−1\nq(zt−1|zt)\nq(zt−1)\nFigure 20.4 As in Figure 20.3 but in which the Gaussian distribution q(zt|zt−1) in the left-hand plot has a much\nsmaller variance \ft. We see that the corresponding distribution q(zt−1|zt) shown in blue on the right-hand plot"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2964, "text": "is close to being Gaussian, with a similar variance to q(zt|zt−1).\n20.2. Reverse Decoder 587\nmaking a Taylor series expansion of ln q(zt−1 |zt) around the point zt as a function\nof zt−1 . This also shows that for small variance, the reverse distribution q(zt|zt−1 )Exercise 20.7\nwill have a covariance that is close to the covariance\ftI of the forward noise process\nq(zt−1 |zt). We therefore model the reverse process using a Gaussian distribution of\nthe form"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2965, "text": "the form\np(zt−1 |zt;w) = N(zt−1 |\u0016(zt;w;t);\f tI) (20.18)\nwhere \u0016(zt;w;t) is a deep neural network governed by a set of parametersw. Note\nthat the network takes the step index texplicitly as an input so that it can account\nfor the variation of the variance \ft across different steps of the chain. This allows us\nto use a single network to invert all the steps in the Markov chain, instead of having\nto learn a separate network for each step. It is also possible to learn the covariances"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2966, "text": "of the denoising process by incorporating further outputs in the network to account\nfor the curvature in the distribution q(zt−1 ) in the neighbourhood of zt (Nichol and\nDhariwal, 2021). There considerable ﬂexibility in the choice of architecture for the\nneural network used to model \u0016(zt;w;t) provided the output has the same dimen-\nsionality as the input. Given this restriction, a U-net architecture is a common choiceSection 10.5.4\nfor image processing applications."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2967, "text": "for image processing applications.\nThe overall reverse denoising process then takes the form of a Markov chain\ngiven by\np(x;z1;:::; zT|w) = p(zT)\n{ T∏\nt=2\np(zt−1 |zt;w)\n}\np(x|z1;w): (20.19)\nHere p(zT) is assumed to be the same as the distribution of q(zT) and hence is\ngiven by N(zT|0;I). Once the model has been trained, sampling is straightforward\nbecause we ﬁrst sample from the simple Gaussian p(zT) and then we sample se-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2968, "text": "quentially from each of the conditional distributions p(zt−1 |zt;w) in turn, ﬁnally\nsampling from p(x|z1;w) to obtain a sample x in the data space.\n20.2.1 Training the decoder\nWe next have to decide on an objective function for training the neural network.\nThe obvious choice is the likelihood function, which for data point x is given by\np(x|w) =\n∫\n---\n∫\np(x;z1;:::; zT|w) dz1 ::: dzT (20.20)\nin which p(x;z1;:::; zT|w) is deﬁned by (20.19). This is an instance of the general"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2969, "text": "latent-variable model (16.81) in which the latent variables comprisez = (z1;:::; zT)\nand the observed variable isx. Note that the latent variables all have the same dimen-\nsionality as the data space, as was the case for normalizing ﬂows but not for varia-\ntional autoencoders or generative adversarial networks. We see from (20.20) that the\nlikelihood involves integrating over all possible trajectories by which noise samples"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2970, "text": "could give rise to the observed data point. The integrals in (20.20) are intractable as\nthey involve integrating over the highly complex neural network functions.\n588 20. DIFFUSION MODELS\n20.2.2 Evidence lower bound\nSince the exact likelihood is intractable, we can adopt a similar approach to that\nused with variational autoencoders and maximize a lower bound on the log likelihood\ncalled the evidence lower bound (ELBO), which we re-derive here in the context ofSection 16.3"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2971, "text": "diffusion models. For any choice of distribution q(z), the following relation always\nholds:\nln p(x|w) = L(w) + KL (q(z)∥p(z|x;w)) (20.21)\nwhere Lis the evidence lower bound, also known as the variational lower bound,\ngiven by\nL(w) =\n∫\nq(z) ln\n{ p(x;z|w)\nq(z)\n}\ndz (20.22)\nand the Kullback–Leibler divergence KL (f∥g) between two probability densities\nf(z) and g(z) is deﬁned bySection 2.5.7\nKL (f(z)∥g(z)) = −\n∫\nf(z) ln\n{ g(z)\nf(z)\n}\ndz: (20.23)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2972, "text": "∫\nf(z) ln\n{ g(z)\nf(z)\n}\ndz: (20.23)\nTo verify the relation (20.21) ﬁrst note that, from the product rule of probability, we\nhave\np(x;z|w) = p(z|x;w)p(x|w): (20.24)\nSubstituting (20.24) into (20.22) and making use of (20.23) gives (20.21). TheExercise 20.8\nKullback–Leibler divergence has the property KL (-∥-)> 0 from which it followsSection 2.5.7\nthat\nln p(x|w) > L(w): (20.25)\nSince the log likelihood function is intractable, we train the neural network by max-\nimizing the lower bound L(w)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2973, "text": "imizing the lower bound L(w).\nTo do this, we ﬁrst derive an explicit form for the lower bound of the diffusion\nmodel. In deﬁning the lower bound we are free to choose any form we like forq(z) as\nlong as it is a valid probability distribution, i.e., that it is non-negative and integrates\nto 1. With many applications of the ELBO, such as the variational autoencoder, we\nchose a form for q(z) that has adjustable parameters, often in the form of a deep"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2974, "text": "neural network, and then we maximize the ELBO with respect to those parameters\nas well as with respect to the parameters of the distribution p(x;z|w). Optimizing\nthe distribution q(z) encourages the bound to be tight, which brings the optimization\nof the parameters in p(x;z|w) closer to that of maximum likelihood. With diffusion\nmodels, however, we choseq(z) to be given by theﬁxed distribution q(z1;:::; zT|x)\ndeﬁned by the Markov chain (20.5), and so the only adjustable parameters are those"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2975, "text": "in the model p(x;z1;:::; zT|w) for the reverse Markov chain. Note that we are\nusing the ﬂexibility in the choice of q(z) to select a form that depends on x.\nWe therefore substitute forq(z1;:::; zT|x)in (20.21) using (20.5), and likewise\nwe substitute for p(x;z1;:::; zT|w) using (20.19), which allows us to write the\n20.2. Reverse Decoder 589\nELBO in the form\nL(w) = Eq\n\nln\np(zT)\n{∏T\nt=2 p(zt−1 |zt;w)\n}\np(x|z1;w)\nq(z1|x)∏T\nt=2 q(zt|zt−1 ;x)\n\n\n= Eq\n[\nln p(zT) +\nT∑\nt=2\nln p(zt−1 |zt;w)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2976, "text": "\n\n= Eq\n[\nln p(zT) +\nT∑\nt=2\nln p(zt−1 |zt;w)\nq(zt|zt−1 ;x) −ln q(z1|x) + lnp(x|z1;w)\n]\n(20.26)\nwhere we have deﬁned\nEq[ -] ≡\n∫\n---\n∫\nq(z1|x)\nT∏\nt=2\nq(zt|zt−1 ) [-] dz1 ::: dzT: (20.27)\nThe ﬁrst term ln p(zT) on the right-hand side of (20.26) is just the ﬁxed distri-\nbution N(zT|0;I). This has no trainable parameters and can therefore be omitted\nfrom the ELBO since it represents a ﬁxed additive constant. Similarly, the third term\n−ln q(z1|x)is independent of w and so again can be omitted."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2977, "text": "The fourth term on the right-hand side of (20.26) corresponds to the reconstruc-\ntion term from the variational autoencoder. It can be evaluated by approximating the\nexpectation Eq[ -] by a Monte Carlo estimate obtained by drawing samples from the\ndistribution over z1 deﬁned by (20.2) so that\nEq[ln p(x|z1;w)] ≃\nL∑\nl=1\nln p(x|z(l)\n1 ;w) (20.28)\nwhere z(l)\n1 ∼N (z1|√1 −\f1x;\f1I). Unlike with V AEs we do not need to back-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2978, "text": "propagate an error signal through the sampled value because the q-distribution is\nﬁxed and so there is no need here for the reparameterization trick.Section 19.2.2\nThis leaves the second term on the right-hand side of (20.26), which comprises a\nsum of terms each of which is dependent on a pair of adjacent latent-variable values\nzt−1 and zt. We saw earlier when we derived the diffusion kernel (20.6) that we can\nsample from q(zt−1 |x)directly as a Gaussian distribution and we could then obtain"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2979, "text": "a corresponding sample of zt using (20.4), which is also a Gaussian. Although this\nwould be a correct procedure in the limit of an inﬁnite number of samples, the use of\npairs of sampled values creates very noisy estimates with high variance, so that an\nunnecessarily large numbers of samples is required. Instead, we rewrite the ELBO\nin a form that can be estimated by sampling just one value per term.\n20.2.3 Rewriting the ELBO"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2980, "text": "20.2.3 Rewriting the ELBO\nFollowing our discussion of the ELBO for the variational autoencoder, our goal\nhere is to write the ELBO in terms of Kullback–Leibler divergences, which we can\nthen subsequently express in closed form. The neural network is a model of the\ndistribution in the reverse direction p(zt−1 |zt;w) whereas the q-distribution is ex-\npressed in the forward direction q(zt|zt−1 ;x), and so we use Bayes’ theorem to\n590 20. DIFFUSION MODELS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2981, "text": "590 20. DIFFUSION MODELS\nreverse the conditional distribution by writing\nq(zt|zt−1 ;x) = q(zt−1 |zt;x)q(zt|x)\nq(zt−1 |x) : (20.29)\nThis allows us to write the second term in (20.26) in the form\nln p(zt−1 |zt;w)\nq(zt|zt−1 ;x) = ln p(zt−1 |zt;w)\nq(zt−1 |zt;x) + ln q(zt−1 |x)\nq(zt|x) : (20.30)\nThe second term on the right-hand side of (20.30) is independent ofw and so can be\nomitted. Substituting (20.30) into (20.26), we then obtain\nL(w) = Eq\n[T∑\nt=2\nln p(zt−1 |zt;w)\nq(zt−1 |zt;x) + lnp(x|z1;w)\n]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2982, "text": "ln p(zt−1 |zt;w)\nq(zt−1 |zt;x) + lnp(x|z1;w)\n]\n: (20.31)\nFinally, we can rewrite (20.31) in the formExercise 20.9\nL(w) =\n∫\nq(z1|x) lnp(x|z1;w) dz1\n  \nreconstruction term\n−\nT∑\nt=2\n∫\nKL(q(zt−1 |zt;x)∥p(zt−1 |zt;w))q(zt|x) dzt\n  \nconsistency terms\n(20.32)\nwhere we have simpliﬁed the expectation overq(z1;:::; zT|x)in the ﬁrst term since\nz1 is the only latent variable appearing in the integrand. Therefore in the expectation"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2983, "text": "deﬁned by (20.27), all the conditional distributions integrate to unity leaving only\nthe integral over z1. Likewise, in the second term, each integral involves only two\nadjacent latent variables zt−1 and zt, and all remaining variables can be integrated\nout.\nThe bound (20.32) is now very similar to the ELBO for the variational autoen-\ncoder given by (19.14), except that there are now multiple encoder and decoder\nstages. The reconstruction term rewards high probability for the observed data sam-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2984, "text": "ple and can be trained in the same way as the corresponding term in the V AE by using\nthe sampling approximation (20.28). The consistency terms in (20.32) are deﬁnedChapter 19\nbetween pairs of Gaussian distributions and therefore can be expressed in closed\nform, as follows. The distribution q(zt−1 |zt;x) is given by (20.15) whereas the dis-\ntribution p(zt−1 |zt;w) is given by (20.18) and so the Kullback–Leibler divergence\nbecomesExercise 20.11\nKL(q(zt−1 |zt;x)∥p(zt−1 |zt;w))\n= 1\n2\ft"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2985, "text": "KL(q(zt−1 |zt;x)∥p(zt−1 |zt;w))\n= 1\n2\ft\n∥mt(x;zt) −\u0016(zt;w;t)∥2 + const (20.33)\n20.2. Reverse Decoder 591\nwhere mt(x;zt) is deﬁned by (20.16) and where any additive terms that are inde-\npendent of the network parameters w have been absorbed into the constant term,\nwhich plays no role in training. Each of the consistency terms in (20.32) has one\nremaining integral overzt, weighted by q(zt|x). This can be approximated by draw-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2986, "text": "ing a sample from q(zt|x), which can be done efﬁciently using the diffusion kernel\n(20.6).\nWe see that the KL divergence (20.33) takes the form of a simple squared-loss\nfunction. Since we adjust the network parameters to maximize the lower bound in\n(20.32), we will be minimizing this squared error because there is a minus sign in\nfront of the Kullback–Leibler divergence terms in the ELBO.\n20.2.4 Predicting the noise"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2987, "text": "20.2.4 Predicting the noise\nOne modiﬁcation that leads to higher quality results is to change the role of the\nneural network so that instead of predicting the denoised image at each step of the\nMarkov chain it predicts the total noise component that was added to the original\nimage to create the noisy image at that step (Ho, Jain, and Abbeel, 2020). To do this\nwe ﬁrst take (20.8) and rearrange to give\nx = 1\n√\u000bt\nzt −\n√1 −\u000bt\n√\u000bt\n\u000ft: (20.34)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2988, "text": "x = 1\n√\u000bt\nzt −\n√1 −\u000bt\n√\u000bt\n\u000ft: (20.34)\nIf we now substitute this into (20.16) we can rewrite the mean mt(x;zt) of the\nreverse conditional distribution q(zt−1 |zt;x) in terms of the original data vector x\nand the noise \u000fto giveExercise 20.12\nmt(x;zt) = 1√1 −\ft\n{\nzt − \ft√1 −\u000bt\n\u000ft\n}\n: (20.35)\nSimilarly, instead of a neural network \u0016(zt;w;t) that predicts the denoised image,\nwe introduce a neural networkg(zt;w;t) that aims to predict the total noise that was"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2989, "text": "added to x to generate zt. Following the same steps that led to (20.35) shows that\nthese two network functions are related by\n\u0016(zt;w;t) = 1√1 −\ft\n{\nzt − \ft√1 −\u000bt\ng(zt;w;t)\n}\n: (20.36)\nWe can now substitute (20.35) and (20.36) into (20.33) to give\nKL(q(zt−1 |zt;x)∥p(zt−1 |zt;w))\n= \ft\n2(1 −\u000bt)(1 −\ft) ∥g(zt;w;t) −\u000ft∥2 + const\n= \ft\n2(1 −\u000bt)(1 −\ft)\ng(√\u000btx +\n√\n1 −\u000bt\u000ft;w;t) −\u000ft\n\n2\n+ const (20.37)\nwhere in the ﬁnal line we have substituted for zt using (20.8).\n592 20. DIFFUSION MODELS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2990, "text": "592 20. DIFFUSION MODELS\nThe reconstruction term in the ELBO (20.32) can be approximated using (20.28)\nwith a sampled value of z1. Using the form (20.18) for p(x|z;w) we have\nln p(x|z1;w) = − 1\n2\f1\n∥x−\u0016(z1;w;1)∥2 + const: (20.38)\nIf we substitute for \u0016(z1;w;1) using (20.36) and we substitute for x using (20.1)\nand then make use of \u000b1 = (1 −\f1), which follows from (20.7), we obtainExercise 20.13\nln p(x|z1;w) = − 1\n2(1 −\ft)∥g(z1;w;1) −\u000f1∥2 + const: (20.39)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2991, "text": "2(1 −\ft)∥g(z1;w;1) −\u000f1∥2 + const: (20.39)\nThis is precisely the same form as (20.37) for the special case t = 1, and so the\nreconstruction and consistency terms can be combined.\nHo, Jain, and Abbeel (2020) found empirically that performance is further im-\nproved simply by omitting the factor \ft=2(1 −\u000bt)(1 −\ft) in front of (20.37), so\nthat all steps in the Markov chain have equal weighting. Substituting this simpliﬁed"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2992, "text": "version of (20.37) into (20.33) gives a training objective function in the form\nL(w) = −\nT∑\nt=1\ng(√\u000btx +\n√\n1 −\u000bt\u000ft;w;t) −\u000ft\n\n2\n: (20.40)\nThe squared error on the right-hand side of (20.40) has a very simple interpretation:\nfor a given steptin the Markov chain and for a given training data pointx, we sample\na noise vector \u000ft and use this to create the corresponding noisy latent vector zt for\nthat step. The loss function is then the squared difference between the predicted"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2993, "text": "noise and the actual noise. Note that the networkg(-;-;-)is predicting the total noise\nadded to the original data vector x, not just the incremental noise added in step t.\nWhen we use stochastic gradient descent, we evaluate the gradient vector of the\nloss function with respect to the network parameters for a randomly selected data\npoint x from the training set. Also, for each such data point we randomly select a\nstep talong the Markov chain, rather than evaluate the error for every term in the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2994, "text": "summation over tin (20.40). These gradients are accumulated over mini-batches of\ndata samples and then used to update the weights.\nAlso note that this loss function automatically builds in a form of data augmen-\ntation, because every time a particular training samplex is used it is combined with a\nfresh sample \u000ft of noise. All the above relates to a single data pointx from the train-\ning set. The corresponding computation of the gradient is shown in Algorithm 20.1.\n20.2.5 Generating new samples"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2995, "text": "20.2.5 Generating new samples\nOnce the network has been trained we can generate new samples in the data\nspace by ﬁrst sampling from the Gaussian distribution p(zT) and then denoising\nsuccessively through each step of the Markov chain. Given a denoised sample zt at\nstep t, we generate a sample zt−1 in three steps. First we evaluate the output of the\nneural network given byg(zt;w;t). From this we evaluate \u0016(zt;w;t) using (20.36).\n20.2. Reverse Decoder 593"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2996, "text": "20.2. Reverse Decoder 593\nAlgorithm 20.1: Training a denoising diffusion probabilistic model\nInput: Training data D= {xn}\nNoise schedule {\f1;:::;\f T}\nOutput: Network parameters w\nfor t∈{1;:::;T }do\n\u000bt ← ∏t\n\u001c=1(1 −\f\u001c) // Calculate alphas from betas\nend for\nrepeat\nx ∼D // Sample a data point\nt∼{1;:::;T }// Sample a point along the Markov chain\n\u000f∼N(\u000f|0;I) // Sample a noise vector\nzt ← √\u000btx + √1 −\u000bt\u000f // Evaluate noisy latent variable\nL(w) ←∥g (zt;w;t) −\u000f∥2 // Compute loss term\nTake optimizer step"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2997, "text": "Take optimizer step\nuntil converged\nreturn w\nFinally we generate a sample zt−1 from p(zt−1 |zt;w) = N(zt−1 |\u0016(zt;w;t);\ftI)\nby adding noise scaled by the variance so that\nzt−1 = \u0016(zt;w;t) +\n√\n\ft\u000f (20.41)\nwhere \u000f∼N (\u000f|0;I). Note that the network g(-;-;-)predicts the total noise added\nto the original data vector x to obtain zt, but in the sampling step, we subtract off\nonly a fraction \ft=√1 −\u000bt of this noise from zt−1 and then add additional noise"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2998, "text": "with variance \ft to generate zt−1 . At the ﬁnal step when we calculate a synthetic\ndata sample x, we do not add additional noise since we are aiming to generate a\nnoise-free output. The sampling procedure is summarized in Algorithm 20.2.\nThe main drawback of diffusion models for generating data is that they require\nmultiple sequential inference passes through the trained network, which can be com-\nputationally expensive. One way to speed up the sampling process is ﬁrst to convert"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 2999, "text": "the denoising process to a differential equation over continuous time and then to use\nalternative efﬁcient discretization methods to solve the equation efﬁciently.Section 20.3.4\nWe have assumed in this chapter that the data and latent variables are continuous\nand that we can therefore use Gaussian noise models. Diffusion models can also\nbe deﬁned for discrete spaces (Austin et al., 2021), for example, to generate new"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3000, "text": "candidate drug molecules in which part of the generation process involves choosing\natom types from a subset of chemical elements.\nWe have seen that diffusion models can be computationally intensive because\n594 20. DIFFUSION MODELS\nAlgorithm 20.2: Sampling from a denoising diffusion probabilistic model\nInput: Trained denoising network g(z;w;t)\nNoise schedule {\f1;:::;\f T}\nOutput: Sample vector x in data space\nzT ∼N(z|0;I) // Sample from final latent space\nfor t∈T;:::; 2 do\n\u000bt ← ∏t"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3001, "text": "for t∈T;:::; 2 do\n\u000bt ← ∏t\n\u001c=1(1 −\f\u001c) // Calculate alpha\n// Evaluate network output\n\u0016(zt;w;t) ← 1√1−\ft\n{\nzt − \ft√1−\u000bt\ng(zt;w;t)\n}\n\u000f∼N(\u000f|0;I) // Sample a noise vector\nzt−1 ← \u0016(zt;w;t) + √\ft\u000f // Add scaled noise\nend for\nx = 1√1−\f1\n{\nz1 − \f1√1−\u000b1\ng(z1;w;t)\n}\n// Final denoising step\nreturn x\nthey sequentially reverse a noise process that can have hundreds or thousands of\nsteps. Song, Meng, and Ermon (2020) introduced a related technique called de-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3002, "text": "noising diffusion implicit models that relax the Markovian assumption on the noise\nprocess while retaining the same objective function for training. This thereby allows\none or two orders of magnitude speed-up during sampling without degrading the\nquality of the generated samples.\n20.3.\nScore Matching\nThe denoising diffusion models discussed so far in this chapter are closely related to\nanother class of deep generative models that were developed relatively independently"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3003, "text": "and which are based on score matching (Hyv¨arinen, 2005; Song and Ermon, 2019).\nThese make use of the score function or Stein score, which is deﬁned as the gradient\nof the log likelihood with respect to the data vector x and is given by\ns(x) = ∇x ln p(x): (20.42)\nHere it is important to emphasize that the gradient is with respect to the data vector,\nnot with respect to any parameter vector. Note that s(x) is a vector-valued function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3004, "text": "of the same dimensionality as x and that each element si(x) = @ln p(x)=@xi is\nassociated with a corresponding element xi of x. For example, if x is an image then\ns(x) can also be represented as an image of the same dimensions with corresponding\n20.3. Score Matching 595\nFigure 20.5 Illustration of the score func-\ntion, showing a distribution in\ntwo dimensions comprising a\nmixture of Gaussians repre-\nsented as a heat map and the\ncorresponding score function\ndeﬁned by (20.42) plotted as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3005, "text": "deﬁned by (20.42) plotted as\nvectors on a regular grid of x-\nvalues.\npixels. Figure 20.5 shows an example of a probability density in two dimensions,\nalong with the corresponding score function.\nTo see why the score function is useful, consider two functions q(x) and p(x)\nthat have the property that their scores are equal, so that ∇x ln q(x) = ∇x ln p(x)\nfor all values of x. If we integrate both sides of the equation with respect to x and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3006, "text": "take exponentials, we obtain q(x) = Kp(x) where K is a constant independent of\nx. So if we are able to learn a model s(x;w) of the score function then we have\nmodelled the original data density, up to a multiplicative constant.\n20.3.1 Score loss function\nTo train such a model we need to deﬁne a loss function that aims to match the\nmodel score function s(x;w) to the score function ∇x ln p(x) of the distribution\np(x) that generated the data. An example of such a loss function is the expected"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3007, "text": "squared error between the model score and the true score, given by\nJ(w) = 1\n2\n∫\n∥s(x;w) −∇x ln p(x)∥2 p(x) dx: (20.43)\nAs we saw in the discussion of energy-based models, the score function doesSection 14.3.1\nnot require the associated probability density to be normalized, because the normal-\nization constant is removed by the gradient operator, and so there is considerable\nﬂexibility in the choice of model. There are broadly two ways to represent the score"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3008, "text": "function s(x;w) using a deep neural network. Each element si of s corresponds to\none of the elements xi of x, so the ﬁrst approach is to have a network with the same\nnumber of outputs as inputs. However, the score function is deﬁned to be the gradient\nof a scalar function (the log probability density), which is a more restricted class of\nfunctions. So an alternative approach is to have a network with a single output \u001e(x)Exercise 20.14\n596 20. DIFFUSION MODELS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3009, "text": "596 20. DIFFUSION MODELS\nand then to compute ∇x\u001e(x) using automatic differentiation. This second approach,\nhowever, requires two backpropagation steps and is therefore computationally more\nexpensive. For this reason, most applications simply adopt the ﬁrst approach.Exercise 20.15\n20.3.2 Modiﬁed score loss\nOne problem with the loss function (20.43) is that we cannot minimize it directly\nbecause we do not know the true data score∇x ln p(x). All we have is the ﬁnite data"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3010, "text": "set D= (x1;:::; xN) from which we can construct an empirical distribution:\npD(x) = 1\nN\nN∑\nn=1\n\u000e(x −xn): (20.44)\nHere \u000e(x) is the Dirac delta function, which can be thought of informally as an\ninﬁnitely tall ‘spike’ atx = 0 with the properties\n\u000e(x) = 0; x ̸=0 (20.45)∫\n\u000e(x) dx = 1: (20.46)\nSince (20.44) is not a differentiable function ofx, we cannot compute its score func-\ntion. We can address this by introducing a noise model to ‘smear out’ the data points"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3011, "text": "and give a smooth, differentiable representation of the density. This is known as a\nParzen estimator or kernel density estimator and is deﬁned bySection 3.5.2\nq\u001b(z) =\n∫\nq(z|x;\u001b)p(x) dx (20.47)\nwhere q(z|x;\u001b) is the noise kernel. A common choice of kernel is the Gaussian\nq(z|x;\u001b) = N(z|x;\u001b2I): (20.48)\nInstead of minimizing the loss function (20.43), we then use the corresponding\nloss with respect to the smoothed Parzen density in the form\nJ(w) = 1\n2\n∫\n∥s(z;w) −∇z ln q\u001b(z)∥2 q\u001b(z) dz: (20.49)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3012, "text": "2\n∫\n∥s(z;w) −∇z ln q\u001b(z)∥2 q\u001b(z) dz: (20.49)\nA key result is that by substituting (20.47) into (20.49) we can rewrite this loss\nfunction in an equivalent form given by (Vincent, 2011)Exercise 20.17\nJ(w) = 1\n2\n∫∫\n∥s(z;w) −∇z ln q(z|x;\u001b)∥2 q(z|x;\u001b)p(x) dz dx + const:\n(20.50)\nIf we substitute for p(x) using the empirical density (20.44), we obtain\nJ(w) = 1\n2N\nN∑\nn=1\n∫\n∥s(z;w) −∇z ln q(z|xn;\u001b)∥2 q(z|xn;\u001b) dz+const: (20.51)\n20.3. Score Matching 597\nFigure 20.6 Examples of sampling trajec-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3013, "text": "Figure 20.6 Examples of sampling trajec-\ntories obtained using Langevin\ndynamics deﬁned by (14.61) for\nthe distribution shown in Fig-\nure 20.5, showing three trajec-\ntories all starting at the centre of\nthe plot.\nFor the Gaussian Parzen kernel (20.48), the score function becomes\n∇z ln q(z|x;\u001b) = −1\n\u001b\u000f (20.52)\nwhere \u000f= z −x is drawn from N(z|0;I). If we consider the speciﬁc noise model\n(20.6) then we obtain\n∇z ln q(z|x;\u001b) = − 1√1 −\u000bt\n\u000f: (20.53)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3014, "text": "∇z ln q(z|x;\u001b) = − 1√1 −\u000bt\n\u000f: (20.53)\nWe therefore see that the score loss (20.50) measures the difference between the\nneural network prediction and the noise \u000f. Therefore, this loss function has the same\nminimum as the form (20.37) used in the denoising diffusion model, with the score\nfunction s(z;w) playing the same role as the noise prediction network g(z;w) up\nto a constant scaling −1=√1 −\u000bt (Song and Ermon, 2019). Minimizing (20.50) is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3015, "text": "known as denoising score matching, and we see the close connection to denoising\ndiffusion models. There remains the question of how to choose the noise variance\n\u001b2, and we will return to this shortly.\nHaving trained a score-based model we then need to draw new samples. Langevin\ndynamics is well-suited to score-based models because it is based on the score func-\ntion and therefore does not require a normalized probability distribution, and is il-Section 14.3\nlustrated in Figure 20.6."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3016, "text": "lustrated in Figure 20.6.\n20.3.3 Noise variance\nWe have seen how to learn the score function from a set of training data and how\nto generate new samples from the learned distribution using Langevin sampling.\nHowever, we can identify three potential problems with this approach (Song and\n598 20. DIFFUSION MODELS\nErmon, 2019; Luo, 2022). First, if the data distribution lies on a manifold of lower\ndimensionality than the data space, the probability density will be zero at points offChapter 16"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3017, "text": "the manifold and here the score function is undeﬁned since ln p(x) is undeﬁned.\nSecond, in regions of low data density, the estimate of the score function may be\ninaccurate since the loss function (20.43) is weighted by the density. An inaccurate\nscore function can lead to poor trajectories when using Langevin sampling. Third,\neven with an accurate model of the score function, the Langevin procedure may not"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3018, "text": "sample correctly if the data distribution comprises a mixture of disjoint distributions.Exercise 20.18\nAll three problems can be addressed by choosing a sufﬁciently large value for\nthe noise variance \u001b2 used in the kernel function (20.48), because this smears out the\ndata distribution. However, too large a variance will introduce a signiﬁcant distortion\nof the original distribution and this itself introduces inaccuracies in the modelling of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3019, "text": "the score function. This trade-off can be addressed by considering a sequence of\nvariance values \u001b2\n1 < \u001b2\n2 < ::: < \u001b2\nT (Song and Ermon, 2019), in which \u001b2\n1 is\nsufﬁciently small that the data distribution is accurately represented whereas \u001b2\nT is\nsufﬁciently large that the aforementioned problems are avoided. The score network\nis then modiﬁed to take the variance as an additional inputs(x;w;\u001b2) and is trained\nby using a loss function that is a weighted sum of the loss functions of the form"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3020, "text": "(20.51) in which each term represents the error between the associated network and\nthe corresponding perturbed data set. For a data vector xn, the loss function then\ntakes the form\n1\n2\nL∑\ni=1\n\u0015(i)\n∫s(z;w;\u001b2\ni) −∇z ln q(z|xn;\u001bi)\n\n2\nq(z|xn;\u001bi) dz (20.54)\nwhere \u0015(i) are weighting coefﬁcients. We see that this training procedure precisely\nmirrors that used to train hierarchical denoising networks.Section 20.2.1\nOnce trained, samples can be generated by running a few steps of Langevin"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3021, "text": "sampling from each of the models for i= L;L −1;:::; 2;1 in turn. This technique\nis called annealed Langevin dynamics, and is analogous to Algorithm 20.2 used to\nsample from denoising diffusion models.\n20.3.4 Stochastic differential equations\nWe have seen that it is helpful to use a large number of steps, often several\nthousand, when constructing the noise process for a diffusion model. It is therefore\nnatural to ask what happens if we consider the limit of an inﬁnite number of steps,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3022, "text": "much as we did for inﬁnitely deep neural networks when we introduced neural dif-\nferential equations. In taking such a limit, we need to ensure that the noise varianceSection 18.3.1\n\ft at each step becomes smaller in keeping with the step size. This leads to a formu-\nlation of diffusion models for continuous time as stochastic differential equations or\nSDEs (Song et al., 2020). Both denoising diffusion probabilistic models and score"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3023, "text": "matching models can then be viewed as a discretization of a continuous-time SDE.\nWe can write a general SDE as an inﬁnitesimal update to the vectorz in the form\ndz = f(z;t) dt\n\ndrift\n+ g(t) dv\ndiffusion\n(20.55)\n20.4. Guided Diffusion 599\nwhere the drift term is deterministic, as in an ODE, but the diffusion term is stochas-\ntic, for example given by inﬁnitesimal Gaussian steps. Here the parameter tis often\ncalled ‘time’ by analogy with physical systems. The forward noise process (20.3)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3024, "text": "for a diffusion model can be written as an SDE of the form (20.55) by taking the\ncontinuous-time limit.Exercise 20.19\nFor the SDE (20.55), there is a corresponding reverse SDE (Song et al., 2020)\ngiven by\ndz =\n{\nf(z;t) −g2(t)∇z ln p(z)\n}\ndt+ g(t) dv (20.56)\nwhere we recognize ∇z ln p(z) as the score function. The SDE given by (20.55) is\nto be solved in reverse from t= T to t= 0.\nTo solve an SDE numerically, we need to discretize the time variable. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3025, "text": "simplest approach is to use ﬁxed, equally spaced time steps, which is known as\nthe Euler–Maruyama solver. For the reverse SDE, we then recover a form of the\nLangevin equation. However, more sophisticated solvers can be employed that useSection 14.3\nmore ﬂexible forms of discretization (Kloeden and Platen, 2013).\nFor all diffusion processes governed by an SDE, there exists a corresponding de-\nterministic process described by an ODE whose trajectories have the same marginal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3026, "text": "probability densities p(z|t)as the SDE (Song et al., 2020). For an SDE of the form\n(20.56), the corresponding ODE is given by\ndz\ndt = f(z;t) −1\n2g2(t)∇z ln p(z): (20.57)\nThe ODE formulation allows the use of efﬁcient adaptive-step solvers to reduce the\nnumber of function evaluations dramatically. Moreover, it allows probabilistic dif-\nfusion models to be related to normalizing ﬂow models, from which the change-Chapter 18"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3027, "text": "of-variables formula (18.1) can be used to provide an exact evaluation of the log\nlikelihood.\n20.4. Guided Diffusion\nSo far, we have considered diffusion models as a way to represent the unconditional\ndensity p(x) learned from a set of training examples x1;:::; xN drawn indepen-\ndently from p(x). Once the model has been trained, we can generate new samples\nfrom this distribution. We have already seen an example of unconditional sampling"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3028, "text": "from a deep generative model for face images inFigure 1.3, in that case from a GAN\nmodel.\nIn many applications, however, we want to sample from a conditional distribu-\ntion p(x|c)where the conditioning variable c could, for example, be a class label or\na textual description of the desired content for an image. This also forms the basis\nfor applications such as image super-resolution, image inpainting, video generation,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3029, "text": "and many others. The simplest approach to achieving this would be to treat c as an\nadditional input into the denoising neural network g(z;w;t; c) and then to train the\nnetwork using matched pairs {xn;cn}. The main limitation of this approach is that\n600 20. DIFFUSION MODELS\nthe network can give insufﬁcient weight to, or even ignore, the conditioning vari-\nables, so we need a way to control how much weight is given to the conditioning"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3030, "text": "information and to trade this off against sample diversity. This additional pressure\nto match the conditioning information is called guidance. There are two main ap-\nproaches to guidance depending on whether or not a separate classiﬁer model is\nused.\n20.4.1 Classiﬁer guidance\nSuppose that a trained classiﬁer p(c|x)is available, and consider a diffusion\nmodel from the perspective of the score function. Using Bayes’ theorem we can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3031, "text": "write the score function for the conditional diffusion model in the form\n∇x ln p(x|c) =∇x ln\n{ p(c|x)p(x)\np(c)\n}\n= ∇x ln p(x) + ∇x ln p(c|x) (20.58)\nwhere we have used ∇x ln p(c) = 0 since p(c) is independent of x. The ﬁrst term\non the right-hand side of (20.58) is the usual unconditional score function, whereas\nthe second term pushes the denoising process towards the direction that maximizes\nthe probability of the given label c under the classiﬁer model (Dhariwal and Nichol,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3032, "text": "2021). The inﬂuence of the classiﬁer can be controlled by introducing a hyperpa-\nrameter \u0015, called the guidance scale, which controls the weight given to the classiﬁer\ngradient. The score function used for sampling then becomes\nscore(x;c;\u0015) = ∇x ln p(x) + \u0015∇x ln p(c|x): (20.59)\nIf \u0015= 0 we recover the original unconditional diffusion model, whereas if\u0015= 1 we\nobtain the score corresponding to the conditional distribution p(x|c). For\u0015> 1 the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3033, "text": "model is strongly encouraged to respect the conditioning label, and values of \u0015≫ 1\nmay be used, for example\u0015= 10. However, this comes at the expense of diversity in\nthe samples as the model prefers ‘easy’ examples that the classiﬁer is able to classify\ncorrectly.\nOne problem with the classiﬁer-based approach to guidance is that a separate\nclassiﬁer must be trained. Furthermore, this classiﬁer needs to be able to classify"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3034, "text": "examples with varying degrees of noise, whereas standard classiﬁers are trained on\nclean examples. We therefore turn to an alternative approach that avoids the use of a\nseparate classiﬁer.\n20.4.2 Classiﬁer-free guidance\nIf we use (20.58) to replace ∇x ln p(c|x)in (20.59), we can write the score\nfunction in the formExercise 20.20\nscore(x;c;\u0015) = \u0015∇x ln p(x|c) + (1−\u0015)∇x ln p(x); (20.60)\nwhich for 0 <\u0015< 1 represents a convex combination of the conditional log density"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3035, "text": "ln p(x|c)and the unconditional log density ln p(x). For \u0015> 1 the contribution from\n20.4. Guided Diffusion 601\nthe unconditional score becomes negative, meaning the model actively reduces the\nprobability of generating samples that ignore the conditioning information in favour\nof samples that do.\nFurthermore, we can avoid training separate networks to modelp(x|c)and p(x)\nby training a single conditional model in which the conditioning variable c is set to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3036, "text": "a null value, for example c = 0, with some probability during training, typically\naround 10–20%. Then p(x) is represented by p(x|c= 0). This is somewhat anal-\nogous to dropout in which the conditioning inputs are collectively set to zero for aSection 9.6.1\nrandom subset of training vectors.\nOnce trained, the score function (20.60) is then used to encourage a strong\nweighting of the conditional information. In practice, classiﬁer-free guidance gives"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3037, "text": "much higher quality results than classiﬁer guidance (Nichol et al., 2021; Saharia et\nal., 2022). The reason is that a classiﬁer p(c|x)can ignore most of the input vector\nx as long as it makes a good prediction ofc whereas classiﬁer-free guidance is based\non the conditional densityp(x|c), which must assign a high probability to all aspects\nof x.\nText-guided diffusion models can leverage techniques from large language mod-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3038, "text": "els to allow the conditioning input to be a general text sequence, known as aprompt,Chapter 12\nand not simply a selection from a predeﬁned set of class labels. This allows the text\ninput to inﬂuence the denoising process in two ways, ﬁrst by concatenating the inter-\nnal representation from a transformer-based language model with the input to the de-\nnoising network and second by allowing cross-attention layers within the denoising"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3039, "text": "network to attend to the text token sequence. Classiﬁer-free guidance, conditioned\non a text prompt, is illustrated in Figure 20.7.\nAnother application for conditional diffusion models is image super-resolution\nin which a low-resolution image is transformed into a corresponding high-resolution\nimage. This is intrinsically an inverse problem, and multiple high-resolution im-\nages will be consistent with a given low-resolution image. Super-resolution can"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3040, "text": "be achieved by denoising a high-resolution sample from a Gaussian using the low-\nresolution image as a conditioning variable (Saharia, Ho, et al., 2021). Examples of\nthis method are shown in Figure 20.8. Such models can be cascaded to achieve very\nhigh resolution (Ho et al., 2021), for example going from 64 ×64 to 256 ×256,\nand then from 256 ×256 to 1024 ×1024. Each stage is typically represented by a"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3041, "text": "U-net architecture, with each U-net conditioned on the ﬁnal denoised output of theSection 10.5.4\nprevious one.\nThis type of cascade can also be used with image-generation diffusion models,\nin which the image denoising is performed at a lower resolution and the result is sub-\nsequently up-sampled using a separate network (which may also take a text prompt\nas input) to give a ﬁnal high-resolution output (Nichol et al., 2021; Saharia et al.,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3042, "text": "2022). This can signiﬁcantly reduce the computational cost compared to working\ndirectly in a high-dimensional space since the denoising process may involve hun-\ndreds of passes through the denoising network. Note that these approaches still work\nwithin the image space directly but at lower resolution.\nA different approach to addressing the high computational cost of applying dif-\nfusion models directly in the space of high-resolution images is called latent diffu-\n602 20. DIFFUSION MODELS"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3043, "text": "602 20. DIFFUSION MODELS\nFigure 20.7 Illustration of classiﬁer-free guidance of diffusion models, generated from a model called GLIDE\nusing the conditioning text A stained glass window of a panda eating bamboo. Examples on the left were\ngenerated with \u0015 = 0 (no guidance, just the plain conditional model) whereas examples on the right were\ngenerated with \u0015= 3. [From Nichol et al. (2021) with permission.]"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3044, "text": "sion models (Rombach et al., 2021). Here an autoencoder is ﬁrst trained on noise-Section 19.1\nfree images to obtain a lower-dimensional representation of the images and is then\nﬁxed. A U-net architecture is then trained to perform the denoising within the lower-\ndimensional space, which itself is not directly interpretable as an image. Finally,\nthe denoised representation is mapped into the high-resolution image space using"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3045, "text": "the output half of the ﬁxed autoencoder network. This approach makes more efﬁ-\ncient use of the low-dimensional space, which can then focus on image semantics,\nleaving the decoder to create a corresponding sharp, high-resolution image from the\ndenoised low-dimensional representation.\nThere are many other applications of conditional image generation including\ninpainting, un-cropping, restoration, image morphing, style transfer, colourization,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3046, "text": "de-blurring, and video generation (Yang, Srivastava, and Mandt, 2022). An example\nof inpainting is shown in Figure 20.9.\nExercises 603\nFigure 20.8 Two examples of low-\nresolution images along with associ-\nated samples of corresponding high-\nresolution images generated by a\ndiffusion model. The top row shows\na 16 ×16 input image and the cor-\nresponding 128 ×128 output image\nalong with the original image from\nwhich the input image was gener-\nated. The bottom row shows a 64 ×"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3047, "text": "ated. The bottom row shows a 64 ×\n64 input image with a 256 ×256 out-\nput image, again with the original im-\nage for comparison. [From Saharia,\nHo, et al. (2021) with permission.]\nOriginalInput Output\nFigure 20.9 Example of inpaint-\ning showing the original image on\nthe left, an image with sections re-\nmoved in the middle, and the image\nwith inpainting on the right. [From\nSaharia, Chan, Chang, et al. (2021)\nwith permission.]\nExercises"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3048, "text": "with permission.]\nExercises\n20.1 (?) Using (20.3) write down expressions for the mean and covariance of zt in terms\nof the mean and covariance of zt−1. Hence, show that for 0 < \ft < 1 the mean of\nthe distribution of zt is closer to zero than the mean of zt−1 and that the covariance\nof zt is closer to the unit matrix I than the covariance of zt−1.\n20.2 (?) Show that the transformation (20.1) can be written in the equivalent form (20.2)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3049, "text": "20.3 (??? ) In this exercise we use proof by induction to show that the marginal distri-\nbution of xt for the forward process of the diffusion model, as deﬁned by (20.4), is\ngiven by (20.6) where \u000bt is deﬁned by (20.7). First verify that (20.6) holds when\nt = 1. Now assume that (20.6) is true for some particular value of tand derive the\ncorresponding result for the value t+ 1. To do this, it is easiest to write the for-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3050, "text": "ward process using the representation (20.3) and to make use of the result (3.212),\nwhich shows that the sum of two independent Gaussian random variables is itself a\nGaussian in which the means and covariances are additive.\n20.4 (?) By using the result (20.6), where \u000bt is deﬁned by (20.7), show that in the limit\nT →∞ we obtain (20.9).\n604 20. DIFFUSION MODELS\n20.5 (??) Consider two independent random variables a and b along with a ﬁxed scalar\n\u0015. Show that\ncov[a + b] = cov[a] + cov[b] (20.61)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3051, "text": "\u0015. Show that\ncov[a + b] = cov[a] + cov[b] (20.61)\ncov[\u0015a] = \u00152cov[a]: (20.62)\nUse these results to show that if the distribution of zt−1 has zero mean and unit\ncovariance, then the distribution of zt, deﬁned by (20.3), will also have zero mean\nand unit covariance, irrespective of the value of \ft.\n20.6 (???) In this exercise we will use the technique of completing the square to derive\nthe result (20.15) starting from Bayes’ theorem (20.13). First note that the two terms"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3052, "text": "in the numerator on the right-hand side of (20.13), given by (20.4) and (20.6), both\ntake the form of exponentials of quadratic functions of zt−1 . The required distribu-\ntion is therefore a Gaussian, and so we need only to ﬁnd its mean and covariance. To\ndo this, consider only the terms in the exponentials that depend onzt−1 and note that\nthe product of two exponentials is the exponential of the sum of the two exponents."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3053, "text": "Gather together all the terms that are quadratic inzt−1 as well as those that are linear\nin zt−1 and then rearrange them in the form (zt−1 −mt)TS−1\nt (zt−1 −mt). Then,\nby inspection, ﬁnd expressions for mt(x;zt) and St. Note that additive terms that\nare independent of zt−1 can be ignored.\n20.7 (???) In this exercise we show that the reverse of the conditional distributionq(zt|zt−1 )\nfor the forward noise process in a diffusion model can be approximated by a Gaus-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3054, "text": "sian when the noise variance is small. Consider the inverse conditional distribution\nq(zt−1 |zt) given by Bayes’ theorem in the form (20.11) where the forward distribu-\ntion q(zt|zt−1 ) is given by (20.4). By taking the logarithm of both sides of (20.11)\nand then making a Taylor expansion of q(zt−1 ) centred on the value zt, show that,\nfor small values of the noise variance\ft, the distributionq(zt−1 |zt) is approximately"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3055, "text": "a Gaussian with mean zt and covariance \ftI. Find expressions for the lowest-order\ncorrections to the mean and to the covariance as expansions in powers of \ft.\n20.8 (??) By substituting the product rule of probability in the form (20.24) into the deﬁ-\nnition (20.22) of the ELBO for the diffusion model and making use of the deﬁnition\n(20.23) of the Kullback–Leibler divergence, verify that the log likelihood function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3056, "text": "can be written as the sum of a lower bound and a Kullback–Leibler divergence in the\nform (20.21).\n20.9 (??) Verify that the ELBO for the diffusion model given by (20.31) can be written\nin the form (20.32) where the Kullback–Leibler divergence is deﬁned by (20.23).\n20.10 (??) When we derived the ELBO for the diffusion model given by (20.32), we omit-\nted the ﬁrst and third terms in (20.26) because they are independent of w. Similarly"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3057, "text": "we omitted the second term in the right-hand side of (20.30) because this is also in-\ndependent of w. Show that if all of these omitted terms are retained they lead to an\nadditional term in the ELBO L(x) given by\nKL (q(zT|x)∥p(zT)) : (20.63)\nExercises 605\nNote that the noise process is constructed in such a way that the distributionq(zT|x)\nis equal to the Gaussian N(x|0;I). Similarly, the distribution p(zT) is deﬁned to be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3058, "text": "equal to N(x|0;I), and hence the two distributions in (20.63) are equal and so the\nKullback–Leibler divergence vanishes.\n20.11 (??) By making use of (20.15) for the distribution q(zt−1 |zt;x) and (20.18) for the\ndistribution p(zt−1 |zt;w), show that the Kullback–Leibler divergence appearing in\nthe consistency terms in (20.32) is given by (20.33).\n20.12 (??) By substituting (20.34) into (20.16) rewrite the mean mt(x;zt) in terms of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3059, "text": "original data vector x and the noise \u000fin the form (20.35), where \u000bt is deﬁned by\n(20.7).\n20.13 (??) Show that the reconstruction term (20.38) in the ELBO for diffusion models can\nbe written in the form (20.39). To do this, substitute for \u0016(z1;w;1) using (20.36)\nand substitute for x using (20.1), and then make use of\u000b1 = (1−\f1), which follows\nfrom (20.7).\n20.14 (?) The score function is deﬁned by s(x) = ∇xp(x|w) and is therefore a vector of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3060, "text": "the same dimensionality as the input vector x. Consider a matrix whose elements\nare given by\nMij = @si\n@xj\n−@sj\n@xi\n: (20.64)\nShow that if the score function is deﬁned by taking the gradient s = ∇x\u001e(x) of\nthe output of a neural network with a single output variable\u001e(x), then all the matrix\nelements Mij = 0 for all pairs i;j. Note that if the score functions(x) = ∇xp(x|w)\nis instead represented directly by a deep neural network with the same number of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3061, "text": "outputs as inputs, then only the diagonal matrix elementsMii = 0, and so the output\nof the network does not in general correspond to the gradient of any scalar function.\n20.15 (??) Consider a deep neural network representation s(x;w) for the score function\ndeﬁned by (20.42), where x and s have dimensionality D. Compare the computa-\ntional complexity of evaluating the score for a network withDoutputs that represents"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3062, "text": "the score function directly with one that computes a single scalar function \u001e(x;w)\nin which the score function is computed indirectly through automatic differentiation.\nShow that the latter approach is typically more computationally expensive.\n20.16 (???) We cannot minimize the score function (20.43) directly because we do not\nknow the functional form of the true data density p(x), and therefore we cannot\nwrite down an expression for the score function ∇x ln p(x). However, by using"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3063, "text": "integration by parts (Hyv¨arinen, 2005), we can rewrite (20.43) in the form\nJ(w) =\n∫{\n∇-s(x;w) + 1\n2 ∥s(x;w)∥2\n}\np(x) dx + const (20.65)\n606 20. DIFFUSION MODELS\nwhere the constant term is independent of the network parameters w, and the diver-\ngence ∇-s(x;w) is deﬁned by\n∇-s =\nD∑\ni=1\n@si\n@xi\n=\nD∑\ni=1\n@2 ln p(x)\n@x2\ni\n(20.66)\nin which Dis the dimensionality ofx. Derive the result (20.65) by ﬁrst expanding the\nsquare in (20.43) and noting that the term involving ∥s(x;w)∥2 already appears in"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3064, "text": "(20.43) whereas the term involving∥sD∥2 can be absorbed into the additive constant,\nwhere we have deﬁned sD= ∇ln pD(x). Now consider the formula\nd\ndx{p(x)g(x)}= dp(x)\ndx g(x) + p(x) dg(x)\ndx (20.67)\nfor the derivative of the product of two functions. Integrate both sides of this formula\nwith respect to xand rearrange to obtain the integration-by-parts formula:\n∫∞\n−∞\ndp(x)\ndx g(x) dx= −\n∫∞\n−∞\ndg(x)\ndx p(x) dx (20.68)\nwhere we have assumed that p(∞) = p(−∞) = 0. Apply this result together"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3065, "text": "with the deﬁnition sD = ∇ln p(x) to the term involving s(x;w)TsDto complete\nthe proof. Note that the evaluation of the second derivatives in (20.66) requires a\nseparate backward propagation pass for each derivative and hence has an overall\ncomputational cost that grows quadratically with the dimensionality D of the data\nspace (Martens, Sutskever, and Swersky, 2012). This precludes the direct application\nof this loss function to spaces of high dimensionality, and so techniques such as"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3066, "text": "sliced score matching (Song et al., 2019) have been developed to help address this\ninefﬁciency.\n20.17 (??) In this exercise we show that the score function loss (20.50) is equivalent, up\nto an additive constant, to the form (20.49). To do this, ﬁrst expand the square in\n(20.49) and by using (20.47) show that the term insTs from (20.49) is the same as the\ncorresponding term obtained by expanding the square in (20.50). Next note that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3067, "text": "term in ∥∇z ln q∥2 in (20.49) is independent ofw and likewise that the corresponding\nterm in (20.50) is also independent of w, and so these can be viewed as additive\nconstants in the loss function and play no role in training. Finally, consider the cross-\nterm in (20.49). By substituting for q(z) using (20.47), show that this is equal to the\ncorresponding cross-term from (20.50). Hence, show that the two loss functions are\nequal up to an additive constant."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3068, "text": "equal up to an additive constant.\n20.18 (?) Consider a probability distribution that consists of a mixture of two disjoint dis-\ntributions (i.e., distributions with the property that when one of them is non-zero the\nother must be zero) of the form\np(x) = \u0015pA(x) + (1−\u0015)pB(x): (20.69)\nExercises 607\nShow that when the score function, deﬁned by (20.42), is evaluated for any given\npoint x, the mixing coefﬁcient \u0015does not appear. From this it follows that Langevin"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3069, "text": "dynamics deﬁned by (14.61) will not sample from the two component distributions\nwith the correct proportions. This problem is resolved by adding noise from a broad\ndistribution, as discussed in the text.\n20.19 (??) For discrete steps, the forward noise process in a diffusion model is deﬁned by\n(20.3). Here we take the continuous-time limit and convert this to an SDE. We ﬁrst\nintroduce a continuously-changing variance function \f(t) such that \ft = \f(t)∆t."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3070, "text": "By making a Taylor expansion of the square root in the ﬁrst term on the right-hand-\nside of (20.3), show that the inﬁnitesimal update can be written in the form\ndz = −1\n2\f(t)z dt+\n√\n\f(t) dv: (20.70)\nWe see that this is a special case of the general SDE (20.55).\n20.20 (?) By using (20.58) to replace ∇x ln p(c|x), show that the score function in (20.59)\ncan be written in the form (20.60).\nAppendix A. Linear Algebra"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3071, "text": "Appendix A. Linear Algebra\nIn this appendix, we gather together some useful properties and identities involving\nmatrices and determinants. This is not intended to be an introductory tutorial, and\nit is assumed that the reader is already familiar with basic linear algebra. For some\nresults, we indicate how to prove them, whereas in more complex cases we leave\nthe interested reader to refer to standard textbooks on the subject. In all cases, we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3072, "text": "assume that inverses exist and that matrix dimensions are such that the formulae\nare correctly deﬁned. A comprehensive discussion of linear algebra can be found in\nGolub and Van Loan (1996), and an extensive collection of matrix properties is given\nby L ¨utkepohl (1996). Matrix derivatives are discussed in Magnus and Neudecker\n(1999).\nA.1.\nMatrix Identities\nA matrix A has elements Aij where iindexes the rows and j indexes the columns."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3073, "text": "We use IN to denote the N ×N identity matrix (also called the unit matrix), and\nif there is no ambiguity over dimensionality, we simply use I. The transpose matrix\nAT has elements (AT)ij = Aji. From the deﬁnition of a transpose, we have\n(AB)T = BTAT; (A.1)\nwhich can be veriﬁed by writing out the indices. The inverse of A, denoted A−1,\nsatisﬁes\nAA−1 = A−1A = I: (A.2)\nBecause ABB−1A−1 = I, we have\n(AB)−1 = B−1A−1: (A.3)\nAlso we have (\nAT)−1\n=\n(\nA−1)T\n; (A.4)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3074, "text": "Also we have (\nAT)−1\n=\n(\nA−1)T\n; (A.4)\n609© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4\n610 A. LINEAR ALGEBRA\nwhich is easily proven by taking the transpose of (A.2) and applying (A.1).\nA useful identity involving matrix inverses is the following:\n(P−1 + BTR−1 B)−1 BTR−1 = PBT(BPBT + R)−1 ; (A.5)\nwhich is easily veriﬁed by right-multiplying both sides by (BPBT + R). Suppose"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3075, "text": "that P has dimensionality N×N and that R has dimensionality M×M, so that B\nis M×N. Then if M ≪ N, it will be much cheaper to evaluate the right-hand side\nof (A.5) than the left-hand side. A special case that sometimes arises is\n(I + AB)−1 A = A(I + BA)−1 : (A.6)\nAnother useful identity involving inverses is the following:\n(A + BD−1 C)−1 = A−1 −A−1 B(D + CA−1 B)−1 CA−1 ; (A.7)\nwhich is known as theWoodbury identity. It can be veriﬁed by multiplying both sides"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3076, "text": "by (A + BD−1 C). This is useful, for instance, when A is large and diagonal and\nhence easy to invert, and when B has many rows but few columns (and conversely\nfor C), so that the right-hand side is much cheaper to evaluate than the left-hand\nside.\nA set of vectors {a1;:::; aN}is said to be linearly independent if the relation∑\nn\u000bnan = 0 holds only if all \u000bn = 0. This implies that none of the vectors\ncan be expressed as a linear combination of the remainder. The rank of a matrix is"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3077, "text": "the maximum number of linearly independent rows (or equivalently the maximum\nnumber of linearly independent columns).\nA.2.\nTraces and Determinants\nSquare matrices have traces and determinants. The trace Tr(A) of a matrix A is\ndeﬁned as the sum of the elements on the leading diagonal. By writing out the\nindices, we see that\nTr(AB) = Tr(BA): (A.8)\nBy applying this formula multiple times to the product of three matrices, we see that\nTr(ABC) = Tr(CAB) = Tr(BCA); (A.9)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3078, "text": "Tr(ABC) = Tr(CAB) = Tr(BCA); (A.9)\nwhich is known as the cyclic property of the trace operator. It clearly extends to the\nproduct of any number of matrices. The determinant |A|of an N ×N matrix A is\ndeﬁned by\n|A|=\n∑\n(±1)A1i1 A2i2 ---ANiN (A.10)\nin which the sum is taken over all products consisting of precisely one element from\neach row and one element from each column, with a coefﬁcient +1 or −1 according\nto whether the permutation i1i2 :::i N is even or odd, respectively. Note that|I|= 1,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3079, "text": "A.3. Matrix Derivatives 611\nand that the determinant of a diagonal matrix is given by the product of the elements\non the leading diagonal. Thus, for a 2 ×2 matrix, the determinant takes the form\n|A|=\n⏐⏐⏐⏐\na11 a12\na21 a22\n⏐\n⏐⏐⏐= a11a22 −a12a21: (A.11)\nThe determinant of a product of two matrices is given by\n|AB|= |A||B| (A.12)\nas can be shown from (A.10). Also, the determinant of an inverse matrix is given by\n⏐\n⏐A−1 ⏐\n⏐= 1\n|A|; (A.13)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3080, "text": "⏐\n⏐A−1 ⏐\n⏐= 1\n|A|; (A.13)\nwhich can be shown by taking the determinant of (A.2) and applying (A.12).\nIf A and B are matrices of size N ×M, then\n⏐⏐IN + ABT⏐\n⏐=\n⏐\n⏐IM + ATB\n⏐\n⏐: (A.14)\nA useful special case is ⏐\n⏐IN + abT⏐\n⏐= 1 + aTb (A.15)\nwhere a and b are N-dimensional column vectors.\nA.3.\nMatrix Derivatives\nSometimes we need to consider derivatives of vectors and matrices with respect to\nscalars. The derivative of a vector a with respect to a scalar x is a vector whose"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3081, "text": "components are given by (@a\n@x\n)\ni\n= @ai\n@x (A.16)\nwith an analogous deﬁnition for the derivative of a matrix. Derivatives with respect\nto vectors and matrices can also be deﬁned, for instance\n(@x\n@a\n)\ni\n= @x\n@ai\n(A.17)\nand similarly (@a\n@b\n)\nij\n= @ai\n@bj\n: (A.18)\nThe following is easily proven by writing out the components:\n@\n@x\n(\nxTa\n)\n= @\n@x\n(\naTx\n)\n= a: (A.19)\n612 A. LINEAR ALGEBRA\nSimilarly\n@\n@x (AB) = @A\n@xB + A@B\n@x: (A.20)\nThe derivative of the inverse of a matrix can be expressed as\n@"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3082, "text": "@\n@x\n(\nA−1 )\n= −A−1 @A\n@xA−1 (A.21)\nas can be shown by differentiating the equation A−1 A = I using (A.20) and then\nright-multiplying by A−1 . Also\n@\n@x ln |A|= Tr\n(\nA−1 @A\n@x\n)\n; (A.22)\nwhich we shall prove later. If we choose xto be one of the elements of A, we have\n@\n@Aij\nTr (AB) = Bji (A.23)\nas can be seen by writing out the matrices using index notation. We can write this\nresult more compactly in the form\n@\n@ATr (AB) = BT: (A.24)\nWith this notation, we have the following properties:\n@\n@ATr"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3083, "text": "@\n@ATr\n(\nATB\n)\n= B; (A.25)\n@\n@ATr(A) = I; (A.26)\n@\n@ATr(ABAT) = A(B + BT); (A.27)\nwhich can again be proven by writing out the matrix indices. We also have\n@\n@A ln |A|=\n(\nA−1 )T\n; (A.28)\nwhich follows from (A.22) and (A.24).\nA.4. Eigenvectors\nFor a square matrix A of size M ×M, the eigenvector equation is deﬁned by\nAui = \u0015iui (A.29)\nA.4. Eigenvectors 613\nfor i= 1;:::;M , where uiis an eigenvector and \u0015iis the correspondingeigenvalue."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3084, "text": "This can be viewed as a set of M simultaneous homogeneous linear equations, and\nthe condition for a solution is that\n|A−\u0015iI|= 0; (A.30)\nwhich is known as thecharacteristic equation. Because this is a polynomial of order\nM in \u0015i, it must have M solutions (though these need not all be distinct). The rank\nof A is equal to the number of non-zero eigenvalues.\nOf particular interest are symmetric matrices, which arise as covariance ma-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3085, "text": "trices, kernel matrices, and Hessians. Symmetric matrices have the property that\nAij = Aji, or equivalentlyAT = A. The inverse of a symmetric matrix is also sym-\nmetric, as can be seen by taking the transpose of A−1 A = I and using AA−1 = I\ntogether with the symmetry of I.\nIn general, the eigenvalues of a matrix are complex numbers, but for symmetric\nmatrices, the eigenvalues\u0015iare real. This can be seen by ﬁrst left-multiplying (A.29)\nby (u?\ni)T, where ?denotes the complex conjugate, to give"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3086, "text": "(u?\ni)T Aui = \u0015i(u?\ni)T ui: (A.31)\nNext we take the complex conjugate of (A.29) and left-multiply by uT\ni to give\nuT\ni Au?\ni = \u0015?\niuT\ni u?\ni (A.32)\nwhere we have used A? = A because we are considering only real matrices A.\nTaking the transpose of the second of these equations and using AT = A, we see\nthat the left-hand sides of the two equations are equal and hence that \u0015?\ni = \u0015i, and\nso \u0015i must be real.\nThe eigenvectors ui of a real symmetric matrix can be chosen to be orthonormal"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3087, "text": "(i.e., orthogonal and of unit length) so that\nuT\ni uj = Iij (A.33)\nwhere Iij are the elements of the identity matrixI. To show this, we ﬁrst left-multiply\n(A.29) by uT\nj to give\nuT\njAui = \u0015iuT\njui (A.34)\nand hence, by exchanging the indices, we have\nuT\ni Auj = \u0015juT\ni uj: (A.35)\nWe now take the transpose of the second equation and make use of the symmetry\nproperty AT = A, and then subtract the two equations to give\n(\u0015i −\u0015j) uT\ni uj = 0: (A.36)\nHence, for \u0015i ̸=\u0015j, we have uT"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3088, "text": "i uj = 0: (A.36)\nHence, for \u0015i ̸=\u0015j, we have uT\ni uj = 0 so that ui and uj are orthogonal. If the two\neigenvalues are equal, then any linear combination\u000bui+ \fuj is also an eigenvector\n614 A. LINEAR ALGEBRA\nwith the same eigenvalue, so we can select one linear combination arbitrarily, and\nthen choose the second to be orthogonal to the ﬁrst (it can be shown that the de-\ngenerate eigenvectors are never linearly dependent). Hence, the eigenvectors can be"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3089, "text": "chosen to be orthogonal, and by normalizing can be set to unit length. Because there\nare M eigenvalues, the corresponding M orthogonal eigenvectors form a complete\nset and so any M-dimensional vector can be expressed as a linear combination of\nthe eigenvectors.\nWe can take the eigenvectors ui to be the columns of an M ×M matrix U,\nwhich from orthonormality satisﬁes\nUTU = I: (A.37)\nSuch a matrix is said to be orthogonal. Interestingly, the rows of this matrix are also"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3090, "text": "orthogonal, so that UUT = I. To show this, note that (A.37) implies UTUU−1 =\nU−1 = UT and so UU−1 = UUT = I. Using (A.12), it also follows that |U|= 1.\nThe eigenvector equation (A.29) can be expressed in terms of U in the form\nAU = U\u0003 (A.38)\nwhere \u0003 is an M ×M diagonal matrix whose diagonal elements are given by the\neigenvalues \u0015i.\nIf we consider a column vector x that is transformed by an orthogonal matrix U\nto give a new vector\n˜x = Ux (A.39)\nthen the length of the vector is preserved because"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3091, "text": "˜xT˜x = xTUTUx = xTx (A.40)\nand similarly the angle between any two such vectors is preserved because\n˜xT˜y = xTUTUy = xTy: (A.41)\nThus, multiplication by U can be interpreted as a rigid rotation of the coordinate\nsystem.\nFrom (A.38), it follows that\nUTAU = \u0003 (A.42)\nand because \u0003 is a diagonal matrix, we say that the matrix A is diagonalized by the\nmatrix U. If we left-multiply by U and right-multiply by UT, we obtain\nA = U\u0003UT: (A.43)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3092, "text": "A = U\u0003UT: (A.43)\nTaking the inverse of this equation and using (A.3) together with U−1 = UT, we\nhave\nA−1 = U\u0003−1 UT: (A.44)\nA.4. Eigenvectors 615\nThese last two equations can also be written in the form\nA =\nM∑\ni=1\n\u0015iuiuT\ni (A.45)\nA−1 =\nM∑\ni=1\n1\n\u0015i\nuiuT\ni : (A.46)\nIf we take the determinant of (A.43) and use (A.12), we obtain\n|A|=\nM∏\ni=1\n\u0015i: (A.47)\nSimilarly, taking the trace of (A.43), and using the cyclic property (A.8) of the trace\noperator together with UTU = I, we have\nTr(A) =\nM∑\ni=1"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3093, "text": "Tr(A) =\nM∑\ni=1\n\u0015i: (A.48)\nWe leave it as an exercise for the reader to verify (A.22) by making use of the results\n(A.33), (A.45), (A.46), and (A.47).\nA matrix A is said to be positive deﬁnite, denoted by A ≻0, if wTAw >0 for\nall non-zero values of the vectorw. Equivalently, a positive deﬁnite matrix has\u0015i >\n0 for all of its eigenvalues (as can be seen by settingw to each of the eigenvectors in\nturn and noting that an arbitrary vector can be expanded as a linear combination of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3094, "text": "the eigenvectors). Note that having all positive elements does not necessarily mean\nthat a matrix is that positive deﬁnite. For example, the matrix\n(\n1 2\n3 4\n)\n(A.49)\nhas eigenvalues \u00151 ≃5:37 and \u00152 ≃−0:37. A matrix is said to be positive semidef-\ninite if wTAw > 0 holds for all values of w, which is denoted A ⪰ 0 and is\nequivalent to \u0015i > 0.\nThe condition number of a matrix is given by\nCN =\n(\u0015max\n\u0015min\n)1=2\n(A.50)\nwhere \u0015max is the largest eigenvalue and \u0015min is the smallest eigenvalue."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3095, "text": "Appendix B. Calculus of Variations\nWe can think of a function y(x) as being an operator that, for any input value x,\nreturns an output value y. In the same way, we can deﬁne a functional F[y] to be\nan operator that takes a function y(x) and returns an output value F. An example\nof a functional is the length of a curve drawn in a two-dimensional plane in which\nthe path of the curve is deﬁned in terms of a function. In the context of machine"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3096, "text": "learning, a widely used functional is the entropy H[x] for a continuous variable x\nbecause, for any choice of probability density function p(x), it returns a scalar value\nrepresenting the entropy of x under that density. Thus, the entropy of p(x) could\nequally well have been written as H[p].\nA common problem in conventional calculus is to ﬁnd a value of x that max-\nimizes (or minimizes) a function y(x). Similarly, in the calculus of variations we"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3097, "text": "seek a function y(x) that maximizes (or minimizes) a functional F[y]. That is, of all\npossible functions y(x), we wish to ﬁnd the particular function for which the func-\ntional F[y] is a maximum (or minimum). The calculus of variations can be used, for\ninstance, to show that the shortest path between two points is a straight line or that\nthe maximum entropy distribution is a Gaussian.\nIf we were not familiar with the rules of ordinary calculus, we could evaluate"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3098, "text": "a conventional derivative dy=dxby making a small change \u000fto the variable xand\nthen expanding in powers of \u000f, so that\ny(x+ \u000f) = y(x) + dy\ndx\u000f+ O(\u000f2) (B.1)\nand ﬁnally taking the limit \u000f → 0. Similarly, for a function of several variables\ny(x1;:::;x D), the corresponding partial derivatives are deﬁned by\ny(x1 + \u000f1;:::;x D + \u000fD) = y(x1;:::;x D) +\nD∑\ni=1\n@y\n@xi\n\u000fi + O(\u000f2): (B.2)\nThe analogous deﬁnition of a functional derivative arises when we consider how"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3099, "text": "much a functional F[y] changes when we make a small change\u000f\u0011(x) to the function\n617© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4\n618 B. CALCULUS OF V ARIATIONS\nFigure B.1 A functional derivative can be deﬁned by\nconsidering how the value of a functional\nF[y] changes when the function y(x) is\nchanged to y(x) + \u000f\u0011(x) where \u0011(x) is an\narbitrary function of x.\ny(x)\ny(x) + \u000f\u0011(x)\nx"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3100, "text": "arbitrary function of x.\ny(x)\ny(x) + \u000f\u0011(x)\nx\ny(x), where \u0011(x) is an arbitrary function ofx, as illustrated inFigure B.1. We denote\nthe functional derivative of F[y] with respect to y(x) by \u000eF=\u000ey(x) and deﬁne it by\nthe following relation:\nF[y(x) + \u000f\u0011(x)] = F[y(x)] + \u000f\n∫ \u000eF\n\u000ey(x)\u0011(x) dx+ O(\u000f2): (B.3)\nThis can be seen as a natural extension of (B.2) in which F[y] now depends on a\ncontinuous set of variables, namely the values ofyat all points x. Requiring that the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3101, "text": "functional be stationary with respect to small variations in the function y(x) gives\n∫ \u000eF\n\u000ey(x)\u0011(x) dx= 0: (B.4)\nBecause this must hold for an arbitrary choice of \u0011(x), it follows that the functional\nderivative must vanish. To see this, imagine choosing a perturbation\u0011(x) that is zero\neverywhere except in the neighbourhood of a point ˆx, in which case the functional\nderivative must be zero at x = ˆx. However, because this must be true for every"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3102, "text": "choice of ˆx, the functional derivative must vanish for all values ofx.\nConsider a functional that is deﬁned by an integral over a function G(y;y′;x),\nwhich depends on both y(x) and its derivative y′(x) and has a direct dependence on\nx:\nF[y] =\n∫\nG(y(x);y′(x);x) dx (B.5)\nwhere the value of y(x) is assumed to be ﬁxed at the boundary of the region of\nintegration (which might be at inﬁnity). If we now consider variations in the function\ny(x), we obtain\nF[y(x) + \u000f\u0011(x)] = F[y(x)] + \u000f\n∫{ @G"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3103, "text": "F[y(x) + \u000f\u0011(x)] = F[y(x)] + \u000f\n∫{ @G\n@y\u0011(x) + @G\n@y′\u0011′(x)\n}\ndx+ O(\u000f2): (B.6)\nWe now have to cast this in the form (B.3). To do so, we integrate the second term\nby parts and note that\u0011(x) must vanish at the boundary of the integral (becausey(x)\nis ﬁxed at the boundary). This gives\nF[y(x) + \u000f\u0011(x)] = F[y(x)] + \u000f\n∫{ @G\n@y − d\ndx\n(@G\n@y′\n)}\n\u0011(x) dx+ O(\u000f2) (B.7)\nB. CALCULUS OF V ARIATIONS 619\nfrom which we can read off the functional derivative by comparison with (B.3)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3104, "text": "Requiring that the functional derivative vanishes then gives\n@G\n@y − d\ndx\n(@G\n@y′\n)\n= 0; (B.8)\nwhich are known as the Euler–Lagrange equations. For example, if\nG= y(x)2 + (y′(x))\n2\n(B.9)\nthen the Euler–Lagrange equations take the form\ny(x) − d2y\ndx2 = 0: (B.10)\nThis second-order differential equation can be solved for y(x) by making use of the\nboundary conditions on y(x).\nOften, we consider functionals deﬁned by integrals whose integrands take the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3105, "text": "form G(y;x) and that do not depend on the derivatives ofy(x). In this case, station-\narity simply requires that @G=@y(x) = 0 for all values of x.\nIf we are optimizing a functional with respect to a probability distribution, then\nwe need to maintain the normalization constraint on the probabilities. This is often\nmost conveniently done using a Lagrange multiplier, which then allows an uncon-Appendix C\nstrained optimization to be performed."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3106, "text": "strained optimization to be performed.\nThe extension of the above results to a multi-dimensional variable x is straight-\nforward. For a more comprehensive discussion of the calculus of variations, see\nSagan (1969).\nAppendix C. Lagrange Multipliers\nLagrange multipliers, also sometimes called undetermined multipliers, are used to\nﬁnd the stationary points of a function of several variables subject to one or more\nconstraints."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3107, "text": "constraints.\nConsider the problem of ﬁnding the maximum of a functionf(x1;x2) subject to\na constraint relating x1 and x2, which we write in the form\ng(x1;x2) = 0: (C.1)\nOne approach would be to solve the constraint equation (C.1) and thus expressx2 as\na function of x1 in the form x2 = h(x1). This can then be substituted into f(x1;x2)\nto give a function of x1 alone of the form f(x1;h(x1)). The maximum with respect"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3108, "text": "to x1 could then be found by differentiation in the usual way, to give the stationary\nvalue x?\n1, with the corresponding value of x2 given by x?\n2 = h(x?\n1).\nOne problem with this approach is that it may be difﬁcult to ﬁnd an analytic\nsolution of the constraint equation that allowsx2 to be expressed as an explicit func-\ntion of x1. Also, this approach treats x1 and x2 differently and so spoils the natural\nsymmetry between these variables."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3109, "text": "symmetry between these variables.\nA more elegant, and often simpler, approach introduces a parameter \u0015called a\nLagrange multiplier. We shall motivate this technique from a geometrical perspec-\ntive. Consider a D-dimensional variable x with components x1;:::;x D. The con-\nstraint equation g(x) = 0 then represents a (D−1)-dimensional surface in x-space\nas indicated in Figure C.1.\nFirst note that at any point on the constraint surface, the gradient ∇g(x) of the"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3110, "text": "constraint function is orthogonal to the surface. To see this, consider a point x that\nlies on the constraint surface along with a nearby point x + \u000fthat also lies on the\nsurface. If we make a Taylor expansion around x, we have\ng(x + \u000f) ≃g(x) + \u000fT∇g(x): (C.2)\nBecause both x and x+\u000flie on the constraint surface, we haveg(x) = g(x+\u000f) and\nhence \u000fT∇g(x) ≃0. In the limit ∥\u000f∥→ 0, we have \u000fT∇g(x) = 0, and because \u000fis\n621© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3111, "text": "C. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4\n622 C. LAGRANGE MULTIPLIERS\nFigure C.1 A geometrical picture of the technique of Lagrange\nmultipliers in which we seek to maximize a func-\ntion f(x), subject to the constraint g(x) = 0. If\nx is D dimensional, the constraint g(x) = 0 cor-\nresponds to a subspace of dimensionality D−1,\nas indicated by the red curve. The problem can\nbe solved by optimizing the Lagrangian function\nL(x;\u0015) = f(x) + \u0015g(x).\nrf(x)\nrg(x)\nxA"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3112, "text": "L(x;\u0015) = f(x) + \u0015g(x).\nrf(x)\nrg(x)\nxA\ng(x) = 0\nthen parallel to the constraint surface g(x) = 0, we see that the vector ∇g is normal\nto the surface.\nNext we seek a point x? on the constraint surface such that f(x) is maximized.\nSuch a point must have the property that the vector ∇f(x) is also orthogonal to the\nconstraint surface, as illustrated in Figure C.1, because otherwise we could increase\nthe value of f(x) by moving a short distance along the constraint surface. Thus, ∇f"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3113, "text": "and ∇g are parallel (or anti-parallel) vectors, and so there must exist a parameter \u0015\nsuch that\n∇f + \u0015∇g = 0 (C.3)\nwhere \u0015̸= 0is known as a Lagrange multiplier. Note that \u0015can have either sign.\nAt this point, it is convenient to introduce the Lagrangian function deﬁned by\nL(x;\u0015) ≡f(x) + \u0015g(x): (C.4)\nThe constrained stationarity condition (C.3) is obtained by setting ∇xL = 0. Fur-\nthermore, the condition @L=@\u0015 = 0 leads to the constraint equation g(x) = 0."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3114, "text": "Thus, to ﬁnd the maximum of a functionf(x) subject to the constraintg(x) = 0,\nwe deﬁne the Lagrangian function given by (C.4) and we then ﬁnd the stationary\npoint of L(x;\u0015) with respect to both x and \u0015. For a D-dimensional vector x, this\ngives D+ 1equations that determine both the stationary pointx? and the value of \u0015.\nIf we are interested only in x?, then we can eliminate \u0015from the stationarity equa-\ntions without needing to ﬁnd its value (hence, the term ‘undetermined multiplier’)."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3115, "text": "As a simple example, suppose we wish to ﬁnd the stationary point of the function\nf(x1;x2) = 1 −x2\n1 −x2\n2 subject to the constraint g(x1;x2) = x1 + x2 −1 = 0, as\nillustrated in Figure C.2. The corresponding Lagrangian function is given by\nL(x;\u0015) = 1 −x2\n1 −x2\n2 + \u0015(x1 + x2 −1): (C.5)\nThe conditions for this Lagrangian to be stationary with respect tox1, x2, and \u0015give\nthe following coupled equations:\n−2x1 + \u0015 = 0 (C.6)\n−2x2 + \u0015 = 0 (C.7)\nx1 + x2 −1 = 0: (C.8)\nC. LAGRANGE MULTIPLIERS 623"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3116, "text": "x1 + x2 −1 = 0: (C.8)\nC. LAGRANGE MULTIPLIERS 623\nFigure C.2 A simple example of the use of Lagrange multipliers\nin which the aim is to maximize f(x1;x2) = 1 −\nx2\n1−x2\n2 subject to the constraintg(x1;x2) = 0 where\ng(x1;x2) = x1+x2−1. The circles show contours of\nthe function f(x1;x2), and the diagonal line shows\nthe constraint surface g(x1;x2) = 0.\ng(x1; x2) = 0\nx1\nx2\n(x?\n1; x?\n2)\nSolving these equations then gives the stationary point as(x?\n1;x?\n2) = (1=2;1=2), and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3117, "text": "1;x?\n2) = (1=2;1=2), and\nthe corresponding value for the Lagrange multiplier is \u0015= 1.\nSo far, we have considered the problem of maximizing a function subject to an\nequality constraint of the form g(x) = 0 . We now consider the problem of maxi-\nmizing f(x) subject to an inequality constraint of the form g(x) > 0, as illustrated\nin Figure C.3.\nThere are now two kinds of solution possible, according to whether the con-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3118, "text": "strained stationary point lies in the region where g(x) > 0, in which case the con-\nstraint is inactive, or whether it lies on the boundary g(x) = 0 , in which case the\nconstraint is said to be active. In the former case, the function g(x) plays no role\nand so the stationary condition is simply ∇f(x) = 0. This again corresponds to\na stationary point of the Lagrange function (C.4) but this time with \u0015 = 0. The"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3119, "text": "latter case, where the solution lies on the boundary, is analogous to the equality con-\nstraint discussed previously and corresponds to a stationary point of the Lagrange\nfunction (C.4) with \u0015 ̸= 0. Now, however, the sign of the Lagrange multiplier is\ncrucial, because the function f(x) is at a maximum only if its gradient is oriented\naway from the region g(x) > 0, as illustrated in Figure C.3. We therefore have\n∇f(x) = −\u0015∇g(x) for some value of \u0015> 0."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3120, "text": "∇f(x) = −\u0015∇g(x) for some value of \u0015> 0.\nFor either of these two cases, the product \u0015g(x) = 0 . Thus, the solution to\nFigure C.3 Illustration of the problem of maximizing\nf(x) subject to the inequality constraint\ng(x) > 0.\nrf(x)\nrg(x)\nxA\nxB\ng(x) = 0g(x) > 0\n624 C. LAGRANGE MULTIPLIERS\nthe problem of maximizing f(x) subject to g(x) > 0 is obtained by optimizing the\nLagrange function (C.4) with respect to x and \u0015subject to the conditions\ng(x) > 0 (C.9)\n\u0015 > 0 (C.10)\n\u0015g(x) = 0 : (C.11)"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3121, "text": "g(x) > 0 (C.9)\n\u0015 > 0 (C.10)\n\u0015g(x) = 0 : (C.11)\nThese are known as the Karush–Kuhn–Tucker (KKT) conditions (Karush, 1939;\nKuhn and Tucker, 1951).\nNote that if we wish to minimize (rather than maximize) the function f(x) sub-\nject to an inequality constraint g(x) > 0, then we minimize the Lagrangian function\nL(x;\u0015) = f(x) −\u0015g(x) with respect to x, again subject to \u0015> 0.\nFinally, it is straightforward to extend the technique of Lagrange multipliers to"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3122, "text": "cases with multiple equality and inequality constraints. Suppose we wish to maxi-\nmize f(x) subject to gj(x) = 0 for j = 1;:::;J , and hk(x) > 0 for k= 1;:::;K .\nWe then introduce Lagrange multipliers {\u0015j}and {\u0016k}, and then optimize the La-\ngrangian function given by\nL(x;{\u0015j};{\u0016k}) =f(x) +\nJ∑\nj=1\n\u0015jgj(x) +\nK∑\nk=1\n\u0016khk(x) (C.12)\nsubject to \u0016k > 0 and \u0016khk(x) = 0 for k = 1;:::;K . Extensions to constrained"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3123, "text": "functional derivatives are similarly straightforward. For a more detailed discussionAppendix B\nof the technique of Lagrange multipliers, see Nocedal and Wright (1999).\nBibliography\nAbramowitz, M., and I. A. Stegun. 1965.Handbook\nof Mathematical Functions. Dover.\nAdler, S. L. 1981. “Over-relaxation method for the\nMonte Carlo evaluation of the partition func-\ntion for multiquadratic actions.” Physical Re-\nview D 23:2901–2904.\nAghajanyan, Armen, Bernie Huang, Candace Ross,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3124, "text": "Aghajanyan, Armen, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, et al. 2022. CM3: A Causal\nMasked Multimodal Model of the Internet.\nTechnical report. arXiv:2201.07520.\nAghajanyan, Armen, Luke Zettlemoyer, and\nSonal Gupta. 2020. Intrinsic Dimension-\nality Explains the Effectiveness of Lan-\nguage Model Fine-Tuning. Technical report.\narXiv:2012.13255.\nAhn, J. H., and J. H. Oh. 2003. “A constrained EM\nalgorithm for principal component analysis.”"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3125, "text": "algorithm for principal component analysis.”\nNeural Computation 15 (1): 57–65.\nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, et al. 2022. Flamingo: a Visual Lan-\nguage Model for Few-Shot Learning. Techni-\ncal report. arXiv:2204.14198.\nAmari, S., A. Cichocki, and H. H. Yang. 1996. “A\nnew learning algorithm for blind signal sep-\naration.” In Advances in Neural Information\nProcessing Systems, edited by D. S. Touretzky,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3126, "text": "Processing Systems, edited by D. S. Touretzky,\nM. C. Mozer, and M. E. Hasselmo, 8:757–763.\nMIT Press.\nAnderson, J. A., and E. Rosenfeld. 1988. Neu-\nrocomputing: Foundations of Research. MIT\nPress.\nAnderson, T. W. 1963. “Asymptotic Theory for\nPrincipal Component Analysis.” Annals of\nMathematical Statistics 34:122–148.\nArjovsky, M., S. Chintala, and L. Bottou.\n2017. Wasserstein GAN. Technical report.\narXiv:1701.07875.\nAttias, H. 1999. “Independent factor analysis.”Neu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3127, "text": "ral Computation 11 (4): 803–851.\nAustin, Jacob, Daniel D. Johnson, Jonathan Ho,\nDaniel Tarlow, and Rianne van den Berg. 2021.\n“Structured Denoising Diffusion Models in\nDiscrete State-Spaces.” In Advances in Neural\nInformation Processing Systems, 34:17981–\n17993.\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer Normalization. Technical\nreport. arXiv:1607.06450.\nBach, F. R., and M. I. Jordan. 2002. “Kernel Inde-\npendent Component Analysis.”Journal of Ma-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3128, "text": "pendent Component Analysis.”Journal of Ma-\nchine Learning Research 3:1–48.\nBadrinarayanan, Vijay, Alex Kendall, and Roberto\nCipolla. 2015. SegNet: A Deep Convolu-\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4\n625\n626 BIBLIOGRAPHY\ntional Encoder-Decoder Architecture for\nImage Segmentation. Technical report.\narXiv:1511.00561.\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3129, "text": "Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural Machine Translation by\nJointly Learning to Align and Translate. Tech-\nnical report. arXiv:1409.0473.\nBaldi, P., and K. Hornik. 1989. “Neural networks\nand principal component analysis: learning\nfrom examples without local minima.” Neural\nNetworks 2 (1): 53–58.\nBalduzzi, David, Marcus Frean, Lennox Leary,\nJP Lewis, Kurt Wan-Duo Ma, and Brian\nMcWilliams. 2017. The Shattered Gradi-\nents Problem: If resnets are the answer,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3130, "text": "ents Problem: If resnets are the answer,\nthen what is the question? Technical report.\narXiv:1702.08591.\nBartholomew, D J. 1987. Latent Variable Models\nand Factor Analysis. Charles Grifﬁn.\nBasilevsky, Alexander. 1994. Statistical Factor\nAnalysis and Related Methods: Theory and\nApplications. Wiley.\nBather, J. 2000. Decision Theory: An Introduction\nto Dynamic Programming and Sequential De-\ncisions. Wiley.\nBattaglia, Peter W., Jessica B. Hamrick, Vic-\ntor Bapst, Alvaro Sanchez-Gonzalez, Vinicius"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3131, "text": "tor Bapst, Alvaro Sanchez-Gonzalez, Vinicius\nZambaldi, Mateusz Malinowski, Andrea Tac-\nchetti, et al. 2018. Relational inductive biases,\ndeep learning, and graph networks. Technical\nreport. arXiv:1806.01261.\nBaydin, A. G., B. A. Pearlmutter, A. A. Radul, and\nJ. M. Siskind. 2018. “Automatic differentiation\nin machine learning: a survey.”Journal of Ma-\nchine Learning Research 18:1–43.\nBecker, S., and Y . LeCun. 1989. “Improving\nthe convergence of back-propagation learn-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3132, "text": "the convergence of back-propagation learn-\ning with second order methods.” In Proceed-\nings of the 1988 Connectionist Models Sum-\nmer School, edited by D. Touretzky, G. E. Hin-\nton, and T. J. Sejnowski, 29–37. Morgan Kauf-\nmann.\nBelkin, Mikhail, Daniel Hsu, Siyuan Ma, and\nSoumik Mandal. 2019. “Reconciling mod-\nern machine-learning practice and the clas-\nsical bias-variance trade-off.” Proceedings of\nthe National Academy of Sciences 116 (32):\n15849–15854."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3133, "text": "15849–15854.\nBell, A. J., and T. J. Sejnowski. 1995. “An infor-\nmation maximization approach to blind sepa-\nration and blind deconvolution.” Neural Com-\nputation 7 (6): 1129–1159.\nBellman, R. 1961. Adaptive Control Processes: A\nGuided Tour. Princeton University Press.\nBengio, Yoshua, Aaron Courville, and Pascal Vin-\ncent. 2012. Representation Learning: A Re-\nview and New Perspectives. Technical report.\narXiv:1206.5538.\nBengio, Yoshua, Nicholas L ´eonard, and Aaron"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3134, "text": "Bengio, Yoshua, Nicholas L ´eonard, and Aaron\nCourville. 2013. Estimating or Propagating\nGradients Through Stochastic Neurons for\nConditional Computation. Technical report.\narXiv:1308.3432.\nBerger, J. O. 1985. Statistical Decision Theory and\nBayesian Analysis. Second. Springer.\nBernardo, J. M., and A. F. M. Smith. 1994.Bayesian\nTheory. Wiley.\nBishop, C. M. 1995a. “Regularization and Com-\nplexity Control in Feed-forward Networks.” In\nProceedings International Conference on Ar-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3135, "text": "Proceedings International Conference on Ar-\ntiﬁcial Neural Networks ICANN’95, edited by\nF. Fougelman-Soulie and P. Gallinari, 1:141–\n148. EC2 et Cie.\nBishop, Christopher M. 1992. “Exact Calculation of\nthe Hessian Matrix for the Multilayer Percep-\ntron.”Neural Computation 4 (4): 494–501.\nBishop, Christopher M. 1994. “Novelty Detection\nand Neural Network Validation.”IEE Proceed-\nings: Vision, Image and Signal Processing141\n(4): 217–222.\nBishop, Christopher M. 1995b.Neural Networks for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3136, "text": "Bishop, Christopher M. 1995b.Neural Networks for\nPattern Recognition. Oxford University Press.\nBishop, Christopher M. 1995c. “Training with noise\nis equivalent to Tikhonov regularization.”Neu-\nral Computation 7 (1): 108–116.\nBishop, Christopher M. 2006. Pattern Recognition\nand Machine Learning. Springer.\nBIBLIOGRAPHY 627\nBommasani, Rishi, Drew A. Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, et al. 2021. On the Op-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3137, "text": "Michael S. Bernstein, et al. 2021. On the Op-\nportunities and Risks of Foundation Models.\nTechnical report. arXiv:2108.07258.\nBottou, L. 2010. “Large-scale machine learning\nwith stochastic gradient descent.” In Proceed-\nings COMPSTAT 2010, 177–186. Springer.\nBourlard, H., and Y . Kamp. 1988. “Auto-\nassociation by multilayer perceptrons and sin-\ngular value decomposition.”Biological Cyber-\nnetics 59:291–294.\nBreiman, L. 1996. “Bagging predictors.” Machine\nLearning 26:123–140."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3138, "text": "Learning 26:123–140.\nBrinker, T. J., A. Hekler, A. H. Enk, C. Berking, S\nHaferkamp, A. Hauschild, M. Weichenthal, et\nal. 2019. “Deep neural networks are superior\nto dermatologists in melanoma image classiﬁ-\ncation.” European Journal of Cancer 119:11–\n17.\nBrock, Andrew, Jeff Donahue, and Karen Si-\nmonyan. 2018. “Large-Scale GAN Train-\ning for High Fidelity Natural Image Syn-\nthesis.” In Proceedings of the International\nConference Learning Representations (ICLR).\nArXiv:1809.11096."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3139, "text": "ArXiv:1809.11096.\nBronstein, Michael M., Joan Bruna, Taco Co-\nhen, and Petar Velickovic. 2021. Geomet-\nric Deep Learning: Grids, Groups, Graphs,\nGeodesics, and Gauges. Technical report.\narXiv:2104.13478.\nBronstein, Michael M., Joan Bruna, Yann Le-\nCun, Arthur Szlam, and Pierre Vandergheynst.\n2017. “Geometric Deep Learning: Going Be-\nyond Eulcidean Data.” In IEEE Signal Pro-\ncessing Magazine, vol. 34. 4. IEEE, July.\nBroomhead, D. S., and D. Lowe. 1988. “Multivari-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3140, "text": "Broomhead, D. S., and D. Lowe. 1988. “Multivari-\nable functional interpolation and adaptive net-\nworks.”Complex Systems 2:321–355.\nBrown, Tom B., Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, et al.\n2020. Language Models are Few-Shot Learn-\ners. Technical report. arXiv:2005.14165.\nBubeck, S ´ebastien, Varun Chandrasekaran, Ronen\nEldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, et al. 2023. Sparks of Artiﬁcial"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3141, "text": "mar, Peter Lee, et al. 2023. Sparks of Artiﬁcial\nGeneral Intelligence: Early experiments with\nGPT-4. Technical report. arXiv:2303.12712.\nCardoso, J-F. 1998. “Blind signal separation: sta-\ntistical principles.” Proceedings of the IEEE 9\n(10): 2009–2025.\nCaruana, R. 1997. “Multitask learning.” Machine\nLearning 28:41–75.\nCasella, G., and R. L. Berger. 2002. Statistical In-\nference. Second. Duxbury.\nChan, K., T. Lee, and T. J. Sejnowski. 2003. “Vari-\national Bayesian learning of ICA with missing"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3142, "text": "ational Bayesian learning of ICA with missing\ndata.”Neural Computation 15 (8): 1991–2011.\nChen, A. M., H. Lu, and R. Hecht-Nielsen. 1993.\n“On the geometry of feedforward neural net-\nwork error surfaces.” Neural Computation 5\n(6): 910–927.\nChen, Mark, Alec Radford, Rewon Child, Jef-\nfrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. 2020. “Generative Pretraining From\nPixels.”Proceedings of Machine Learning Re-\nsearch 119:1691–1703.\nChen, R. T. Q., Rubanova Y, J. Bettencourt,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3143, "text": "Chen, R. T. Q., Rubanova Y, J. Bettencourt,\nand D. Duvenaud. 2018. Neural Ordinary\nDifferential Equations. Technical report.\narXiv:1806.07366.\nChen, Ting, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A Simple\nFramework for Contrastive Learning of\nVisual Representations. Technical report.\narXiv:2002.05709.\nCho, Kyunghyun, Bart van Merrienboer, C ¸ aglar\nG¨ulc ¸ehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. 2014. Learning Phrase\nRepresentations using RNN Encoder-Decoder"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3144, "text": "Representations using RNN Encoder-Decoder\nfor Statistical Machine Translations.Technical\nreport. arXiv:1406.1078.\nChoudrey, R. A., and S. J. Roberts. 2003. “Varia-\ntional mixture of Bayesian independent com-\nponent analyzers.”Neural Computation 15 (1):\n213–252.\n628 BIBLIOGRAPHY\nChristiano, Paul, Jan Leike, Tom B. Brown, Miljan\nMartic, Shane Legg, and Dario Amodei. 2017.\nDeep reinforcement learning from human pref-\nerences. Technical report. arXiv:1706.03741."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3145, "text": "erences. Technical report. arXiv:1706.03741.\nCollobert, R. 2004. “Large Scale Machine Learn-\ning.” PhD diss., Universit´e Paris VI.\nComon, P., C. Jutten, and J. Herault. 1991. “Blind\nsource separation, 2: problems statement.”Sig-\nnal Processing 24 (1): 11–20.\nCover, T., and P. Hart. 1967. “Nearest neighbor pat-\ntern classiﬁcation.” IEEE Transactions on In-\nformation Theory IT-11:21–27.\nCover, T. M., and J. A. Thomas. 1991. Elements of\nInformation Theory. Wiley."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3146, "text": "Information Theory. Wiley.\nCox, R. T. 1946. “Probability, frequency and rea-\nsonable expectation.” American Journal of\nPhysics 14 (1): 1–13.\nCybenko, G. 1989. “Approximation by superposi-\ntions of a sigmoidal function.” Mathematics of\nControl, Signals and Systems 2:304–314.\nDawid, A. P. 1979. “Conditional Independence in\nStatistical Theory (with discussion).” Journal\nof the Royal Statistical Society, Series B 4:1–\n31.\nDawid, A. P. 1980. “Conditional Independence"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3147, "text": "31.\nDawid, A. P. 1980. “Conditional Independence\nfor Statistical Operations.” Annals of Statistics\n8:598–617.\nDeisenroth, M. P., A. A. Faisal, and C. S. Ong.\n2020. Mathematics for Machine Learning.\nCambridge University Press.\nDempster, A. P., N. M. Laird, and D. B. Rubin.\n1977. “Maximum likelihood from incomplete\ndata via the EM algorithm.” Journal of the\nRoyal Statistical Society, B 39 (1): 1–38.\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. 2009. “ImageNet: A large-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3148, "text": "Li, and Li Fei-Fei. 2009. “ImageNet: A large-\nscale hierarchical image database.” In IEEE\nConference on Computer Vision and Pattern\nRecognition.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2018. BERT: Pre-\ntraining of Deep Bidirectional Transformers\nfor Language Understanding. Technical re-\nport. arXiv:1810.04805.\nDhariwal, Prafulla, and Alex Nichol. 2021. Diffu-\nsion Models Beat GANs on Image Synthesis.\nTechnical report. arXiv:2105.05233."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3149, "text": "Technical report. arXiv:2105.05233.\nDinh, Laurent, David Krueger, and Yoshua Ben-\ngio. 2014. NICE: Non-linear Independent\nComponents Estimation. Technical report.\narXiv:1410.8516.\nDinh, Laurent, Jascha Sohl-Dickstein, and Samy\nBengio. 2016. Density estimation using Real\nNVP. Technical report. arXiv:1605.08803.\nDodge, Samuel, and Lina Karam. 2017. A Study\nand Comparison of Human and Deep Learning\nRecognition Performance Under Visual Dis-\ntortions. Technical report. arXiv:1705.02498."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3150, "text": "tortions. Technical report. arXiv:1705.02498.\nDoersch, C. 2016. Tutorial on Variational Autoen-\ncoders. Technical report. arXiv:1606.05908.\nDosovitskiy, Alexey, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, et al.\n2020. An Image is Worth16×16 Words: Trans-\nformers for Image Recognition at Scale. Tech-\nnical report. arXiv:2010.11929.\nDuane, S., A. D. Kennedy, B. J. Pendleton, and D.\nRoweth. 1987. “Hybrid Monte Carlo.”Physics"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3151, "text": "Roweth. 1987. “Hybrid Monte Carlo.”Physics\nLetters B 195 (2): 216–222.\nDuchi, J., E. Hazan, and Y . Singer. 2011. “Adaptive\nSubgradient Methods for Online Learning and\nStochastic Optimization.” Journal of Machine\nLearning Research 12:2121–2159.\nDuda, R. O., and P. E. Hart. 1973. Pattern Classiﬁ-\ncation and Scene Analysis. Wiley.\nDufter, Philipp, Martin Schmitt, and Hinrich\nSch¨utze. 2021. Position Information in Trans-\nformers: An Overview. Technical report.\narXiv:2102.11090."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3152, "text": "arXiv:2102.11090.\nDumoulin, Vincent, and Francesco Visin. 2016. A\nguide to convolution arithmetic for deep learn-\ning. Technical report. arXiv:1603.07285.\nElliott, R. J., L. Aggoun, and J. B. Moore. 1995.\nHidden Markov Models: Estimation and Con-\ntrol. Springer.\nBIBLIOGRAPHY 629\nEsser, Patrick, Robin Rombach, and Bj ¨orn Om-\nmer. 2020. Taming Transformers for High-\nResolution Image Synthesis. Technical report.\narXiv:2012.09841.\nEsteva, A., B. Kuprel, R. A. Novoa, J. Ko,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3153, "text": "Esteva, A., B. Kuprel, R. A. Novoa, J. Ko,\nS. M. Swetter, H. M. Blau, and S. Thrun.\n2017. “Dermatologist-level classiﬁcation of\nskin cancer with deep neural networks.” Na-\nture 542:115–118.\nEveritt, B. S. 1984. An Introduction to Latent Vari-\nable Models. Chapman / Hall.\nEykholt, Kevin, Ivan Evtimov, Earlence Fernan-\ndes, Bo Li, Amir Rahmati, Chaowei Xiao,\nAtul Prakash, Tadayoshi Kohno, and Dawn\nSong. 2018. “Robust Physical-World Attacks\non Deep Learning Visual Classiﬁcation.” In"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3154, "text": "on Deep Learning Visual Classiﬁcation.” In\nProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nFawcett, T. 2006. “An introduction to ROC analy-\nsis.”Pattern Recognition Letters 27:861–874.\nFeller, W. 1966.An Introduction to Probability The-\nory and its Applications.Second. V ol. 2. Wiley.\nFletcher, R. 1987. Practical Methods of Optimiza-\ntion. Second. Wiley.\nForsyth, D. A., and J. Ponce. 2003. Computer Vi-\nsion: A Modern Approach. Prentice Hall."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3155, "text": "sion: A Modern Approach. Prentice Hall.\nFreund, Y ., and R. E. Schapire. 1996. “Experi-\nments with a new boosting algorithm.” InThir-\nteenth International Conference on Machine\nLearning, edited by L. Saitta, 148–156. Mor-\ngan Kaufmann.\nFukushima, K. 1980. “Neocognitron: A Self-\norganizing Neural Network Model for a Mech-\nanism of Pattern Recognition Unaffected by\nShift in Position.” Biological Cybernetics\n36:193–202.\nFunahashi, K. 1989. “On the approximate realiza-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3156, "text": "Funahashi, K. 1989. “On the approximate realiza-\ntion of continuous mappings by neural net-\nworks.”Neural Networks 2 (3): 183–192.\nFung, R., and K. C. Chang. 1990. “Weighting and\nIntegrating Evidence for Stochastic Simulation\nin Bayesian Networks.” In Uncertainty in Ar-\ntiﬁcial Intelligence, edited by P. P. Bonissone,\nM. Henrion, L. N. Kanal, and J. F. Lemmer,\n5:208–219. Elsevier.\nGatys, Leon A., Alexander S. Ecker, and Matthias\nBethge. 2015. A Neural Algorithm of Artistic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3157, "text": "Bethge. 2015. A Neural Algorithm of Artistic\nStyle. Technical report. arXiv:1508.06576.\nGeman, S., and D. Geman. 1984. “Stochastic re-\nlaxation, Gibbs distributions, and the Bayesian\nrestoration of images.”IEEE PAMI 6 (1): 721–\n741.\nGemmeke, Jort F., Daniel P. W. Ellis, Dylan Freed-\nman, Aren Jansen, Wade Lawrence, R. Chan-\nning Moore, Manoj Plakal, and Marvin Ritter.\n2017. “Audio Set: An ontology and human-\nlabeled dataset for audio events.” In Proc.\nIEEE ICASSP 2017. New Orleans, LA."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3158, "text": "IEEE ICASSP 2017. New Orleans, LA.\nGermain, Mathieu, Karol Gregor, Iain Murray, and\nHugo Larochelle. 2015. MADE: Masked Au-\ntoencoder for Distribution Estimation. Techni-\ncal report. arXiv:1502.03509.\nGilks, W. R. 1992. “Derivative-free adaptive re-\njection sampling for Gibbs sampling.” In\nBayesian Statistics, edited by J. Bernardo, J.\nBerger, A. P. Dawid, and A. F. M. Smith, vol. 4.\nOxford University Press.\nGilks, W. R., N. G. Best, and K. K. C. Tan. 1995."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3159, "text": "Gilks, W. R., N. G. Best, and K. K. C. Tan. 1995.\n“Adaptive rejection Metropolis sampling.”Ap-\nplied Statistics 44:455–472.\nGilks, W. R., S. Richardson, and D. J. Spiegelhalter.\n1996. Markov Chain Monte Carlo in Practice.\nChapman / Hall.\nGilks, W. R., and P. Wild. 1992. “Adaptive rejection\nsampling for Gibbs sampling.” Applied Statis-\ntics 41:337–348.\nGilmer, Justin, Samuel S. Schoenholz, Patrick F. Ri-\nley, Oriol Vinyals, and George E. Dahl. 2017.\nNeural Message Passing for Quantum Chem-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3160, "text": "Neural Message Passing for Quantum Chem-\nistry. Technical report. arXiv:1704.01212.\nGirshick, Ross B. 2015. Fast R-CNN. Technical re-\nport. arXiv:1504.08083.\nGolub, G. H., and C. F. Van Loan. 1996. Matrix\nComputations. Third. John Hopkins University\nPress.\n630 BIBLIOGRAPHY\nGong, Yuan, Yu-An Chung, and James R. Glass.\n2021. AST: Audio Spectrogram Transformer.\nTechnical report. arXiv:2104.01778.\nGoodfellow, Ian, Yoshua Bengio, and Aaron\nCourville. 2016. Deep Learning. MIT Press."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3161, "text": "Courville. 2016. Deep Learning. MIT Press.\nGoodfellow, Ian J., Jean Pouget-Abadie, Mehdi\nMirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio.\n2014. Generative Adversarial Networks. Tech-\nnical report. arXiv:1406.2661.\nGoodfellow, Ian J., Jonathon Shlens, and Chris-\ntian Szegedy. 2014. Explaining and Harness-\ning Adversarial Examples. Technical report.\narXiv:1412.6572.\nGrathwohl, Will, Ricky T. Q. Chen, Jesse Bet-\ntencourt, Ilya Sutskever, and David Duve-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3162, "text": "tencourt, Ilya Sutskever, and David Duve-\nnaud. 2018. FFJORD: Free-form Continuous\nDynamics for Scalable Reversible Generative\nModels. Technical report. arXiv:1810.01367.\nGriewank, A., and A Walther. 2008. Evaluating\nDerivatives: Principles and Techniques of Al-\ngorithmic Differentiation. Second. SIAM.\nGrosse, R. 2018. Automatic Differentiation.\nCSC321 Lecture 10. University of Toronto.\nGulrajani, I., F. Ahmed, M. Arjovsky, V . Du-\nmoulin, and A. Courville. 2017. Improved"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3163, "text": "moulin, and A. Courville. 2017. Improved\ntraining of Wasserstein GANs. Technical re-\nport. arXiv:1704.00028.\nGutmann, Michael, and Aapo Hyv ¨arinen. 2010.\n“Noise-contrastive estimation: A new esti-\nmation principle for unnormalized statistical\nmodels.” Journal of Machine Learning Re-\nsearch 9:297–304.\nHamilton, W. L. 2020. Graph Representation\nLearning. Morgan / Claypool.\nHartley, R., and A. Zisserman. 2004. Multiple View\nGeometry in Computer Vision. Second. Cam-\nbridge University Press."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3164, "text": "bridge University Press.\nHassibi, B., and D. G. Stork. 1993. “Second order\nderivatives for network pruning: optimal brain\nsurgeon.” In Proceedings International Con-\nference on Neural Information Processing Sys-\ntems (NeurIPS), edited by S. J. Hanson, J. D.\nCowan, and C. L. Giles, 5:164–171. Morgan\nKaufmann.\nHastie, T., R. Tibshirani, and J. Friedman. 2009.\nThe Elements of Statistical Learning. Second.\nSpringer.\nHastings, W. K. 1970. “Monte Carlo sampling"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3165, "text": "Hastings, W. K. 1970. “Monte Carlo sampling\nmethods using Markov chains and their appli-\ncations.”Biometrika 57:97–109.\nHe, Kaiming, Xinlei Chen, Saining Xie, Yanghao\nLi, Piotr Doll ´ar, and Ross B. Girshick. 2021.\nMasked Autoencoders Are Scalable Vision\nLearners. Technical report. arXiv:2111.06377.\nHe, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie,\nand Ross Girshick. 2019. Momentum Con-\ntrast for Unsupervised Visual Representation\nLearning. Technical report. arXiv:1911.05722."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3166, "text": "Learning. Technical report. arXiv:1911.05722.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren,\nand Jian Sun. 2015a. Deep Residual Learn-\ning for Image Recognition. Technical report.\narXiv:1512.03385.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2015b. Delving Deep into Recti-\nﬁers: Surpassing Human-Level Performance\non ImageNet Classiﬁcation. Technical report.\narXiv:1502.01852.\nHenrion, M. 1988. “Propagation of Uncertainty by\nLogic Sampling in Bayes’ Networks.” In Un-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3167, "text": "Logic Sampling in Bayes’ Networks.” In Un-\ncertainty in Artiﬁcial Intelligence, edited by\nJ. F. Lemmer and L. N. Kanal, 2:149–164.\nNorth Holland.\nHiggins, I., L. Matthey, A. Pal, C. Burgess, X. Glo-\nrot, M. Botvinik, S. Mohamed, and A. Ler-\nchner. 2017. “\f -V AE: learning basic visual\nconcepts with a constrained variational frame-\nwork.” In Proceedings of the International\nConference Learning Representations (ICLR).\nHinton, G. E. 2012. Neural Networks for Machine"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3168, "text": "Hinton, G. E. 2012. Neural Networks for Machine\nLearning. Lecture 6.5. Coursera Lectures.\nHinton, G. E., M. Welling, Y . W. Teh, and S Osin-\ndero. 2001. “A new view of ICA.” InProceed-\nings of the International Conference on Inde-\npendent Component Analysis and Blind Signal\nSeparation, vol. 3.\nBIBLIOGRAPHY 631\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean.\n2015. Distilling the Knowledge in a Neural\nNetwork. Technical report. arXiv:1503.02531.\nHinton, Geoffrey E. 2002. “Training products of ex-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3169, "text": "perts by minimizing contrastive divergence.”\nNeural Computation 14:1771–1800.\nHo, Jonathan, Ajay Jain, and Pieter Abbeel.\n2020. Denoising Diffusion Probabilistic Mod-\nels. Technical report. arXiv:2006.11239.\nHo, Jonathan, Chitwan Saharia, William Chan,\nDavid J. Fleet, Mohammad Norouzi, and Tim\nSalimans. 2021. Cascaded Diffusion Models\nfor High Fidelity Image Generation. Technical\nreport. arXiv:2106.15282.\nHochreiter, S., and J. Schmidhuber. 1997. “Long\nshort-term Memory.” Neural Computation 9"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3170, "text": "short-term Memory.” Neural Computation 9\n(8): 1735–1780.\nHojen-Sorensen, P. A., O. Winther, and L. K.\nHansen. 2002. “Mean ﬁeld approaches to in-\ndependent component analysis.” Neural Com-\nputation 14 (4): 889–918.\nHoltzman, Ari, Jan Buys, Maxwell Forbes, and\nYejin Choi. 2019. The Curious Case of\nNeural Text Degeneration. Technical report.\narXiv:1904.09751.\nHornik, K., M. Stinchcombe, and H. White. 1989.\n“Multilayer feedforward networks are univer-\nsal approximators.” Neural Networks 2 (5):"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3171, "text": "sal approximators.” Neural Networks 2 (5):\n359–366.\nHospedales, Timothy, Antreas Antoniou, Paul Mi-\ncaelli, and Amos Storkey. 2021. “Meta-\nlearning in neural networks: A survey.” IEEE\nTransactions on Pattern Analysis and Machine\nIntelligence 44 (9): 5149–5169.\nHotelling, H. 1933. “Analysis of a complex of sta-\ntistical variables into principal components.”\nJournal of Educational Psychology 24:417–\n441.\nHotelling, H. 1936. “Relations between two sets of\nvariables.”Biometrika 28:321–377."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3172, "text": "variables.”Biometrika 28:321–377.\nHu, Anthony, Lloyd Russell, Hudson Yeo, Zak\nMurez, George Fedoseev, Alex Kendall, Jamie\nShotton, and Gianluca Corrado. 2023. GAIA-\n1: A Generative World Model for Autonomous\nDriving. Technical report. arXiv:2309.17080.\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu\nWang, and Weizhu Chen. 2021. LoRA: Low-\nRank Adaptation of Large Language Models.\nTechnical report. arXiv:2106.09685."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3173, "text": "Technical report. arXiv:2106.09685.\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive\nﬁelds of single neurons in the cat’s striate cor-\ntex.”Journal of Physiology 148:574–591.\nHyv¨arinen, A. 2005. “Estimation of Non-\nNormalized Statistical Models by Score\nMatching.” Journal of Machine Learning Re-\nsearch 6:695–709.\nHyv¨arinen, A., and E. Oja. 1997. “A fast ﬁxed-point\nalgorithm for independent component analy-\nsis.”Neural Computation 9 (7): 1483–1492.\nHyv¨arinen, Aapo, Jarmo Hurri, and Patrick O."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3174, "text": "Hyv¨arinen, Aapo, Jarmo Hurri, and Patrick O.\nHoyer. 2009. Natural Image Statistics: A Prob-\nabilistic Approach to Early Computational Vi-\nsion. Springer.\nIoffe, S., and C. Szegedy. 2015. “Batch normaliza-\ntion.” InProceedings of the International Con-\nference on Machine Learning (ICML), 448–\n456.\nJacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E.\nHinton. 1991. “Adaptive mixtures of local ex-\nperts.”Neural Computation 3 (1): 79–87.\nJebara, T. 2004. Machine Learning: Discriminative"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3175, "text": "Jebara, T. 2004. Machine Learning: Discriminative\nand Generative. Kluwer.\nJensen, C., A. Kong, and U. Kjaerulff. 1995.\n“Blocking Gibbs sampling in very large proba-\nbilistic expert systems.” International Journal\nof Human Computer Studies. Special Issue on\nReal-World Applications of Uncertain Reason-\ning. 42:647–666.\nJolliffe, I. T. 2002. Principal Component Analysis.\nSecond. Springer.\nJumper, John, Richard Evans, Alexander Pritzel,\nTim Green, Michael Figurnov, and Olaf"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3176, "text": "Tim Green, Michael Figurnov, and Olaf\nRonneberger. 2021. “Highly accurate protein\nstructure prediction with AlphaFold.” Nature\n596:583–589.\n632 BIBLIOGRAPHY\nJutten, C., and J. Herault. 1991. “Blind separation\nof sources, 1: An adaptive algorithm based on\nneuromimetic architecture.”Signal Processing\n24 (1): 1–10.\nKaplan, Jared, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu,\nand Dario Amodei. 2020. Scaling Laws for"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3177, "text": "and Dario Amodei. 2020. Scaling Laws for\nNeural Language Models. Technical report.\narXiv:2001.08361.\nKarras, Tero, Timo Aila, Samuli Laine, and Jaakko\nLehtinen. 2017. Progressive Growing of GANs\nfor Improved Quality, Stability, and Variation.\nTechnical report. arXiv:1710.10196.\nKarush, W. 1939. “Minima of functions of several\nvariables with inequalities as side constraints.”\nMaster’s thesis, Department of Mathematics,\nUniversity of Chicago.\nKhosla, Prannay, Piotr Teterwak, Chen Wang,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3178, "text": "Khosla, Prannay, Piotr Teterwak, Chen Wang,\nAaron Sarna, Yonglong Tian, Phillip Isola,\nAaron Maschinot, Ce Liu, and Dilip Krishnan.\n2020. Supervised Contrastive Learning. Tech-\nnical report. arXiv:2004.11362.\nKingma, D., and J. Ba. 2014. Adam: A method\nfor stochastic optimization. Technical report.\narXiv:1412.6980.\nKingma, D. P., and M. Welling. 2013. “Auto-\nencoding variational Bayes.” In Proceedings\nof the International Conference on Machine\nLearning (ICML). ArXiv:1312.6114."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3179, "text": "Learning (ICML). ArXiv:1312.6114.\nKingma, Diederik P., and Max Welling. 2019.\nAn Introduction to Variational Autoencoders.\nTechnical report. arXiv:1906.02691.\nKingma, Durk P, Tim Salimans, Rafal Jozefowicz,\nXi Chen, Ilya Sutskever, and Max Welling.\n2016. “Improved variational inference with in-\nverse autoregressive ﬂow.”Advances in Neural\nInformation Processing Systems 29.\nKipf, Thomas N., and Max Welling. 2016.\nSemi-Supervised Classiﬁcation with Graph\nConvolutional Networks. Technical report."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3180, "text": "Convolutional Networks. Technical report.\narXiv:1609.02907.\nKloeden, Peter E, and Eckhard Platen. 2013.\nNumerical solution of stochastic differential\nequations. V ol. 23. Stochastic Modelling and\nApplied Probability. Springer.\nKobyzev, I., S. J. D. Prince, and M. A. Brubaker.\n2019. “Normalizing ﬂows: an introduction and\nreview of current methods.” IEEE Transac-\ntions on Pattern Analysis and Machine Intel-\nligence 43 (11): 3964–3979.\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3181, "text": "Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E.\nHinton. 2012. “Imagenet classiﬁcation with\ndeep convolutional neural networks.” In Ad-\nvances in Neural Information Processing Sys-\ntems, vol. 25.\nKuhn, H. W., and A. W. Tucker. 1951. “Nonlin-\near programming.” In Proceedings of the 2nd\nBerkeley Symposium on Mathematical Statis-\ntics and Probabilities, 481–492. University of\nCalifornia Press.\nKullback, S., and R. A. Leibler. 1951. “On informa-\ntion and sufﬁciency.” Annals of Mathematical"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3182, "text": "tion and sufﬁciency.” Annals of Mathematical\nStatistics 22 (1): 79–86.\nKurkov´a, V ., and P. C. Kainen. 1994. “Functionally\nEquivalent Feed-forward Neural Networks.”\nNeural Computation 6 (3): 543–558.\nLasserre, J., Christopher M. Bishop, and T. Minka.\n2006. “Principled hybrids of generative and\ndiscriminative models.” In Proceedings 2006\nIEEE Conference on Computer Vision and Pat-\ntern Recognition, New York.\nLauritzen, S. L. 1996. Graphical Models. Oxford\nUniversity Press."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3183, "text": "University Press.\nLawley, D. N. 1953. “A Modiﬁed Method of Es-\ntimation in Factor Analysis and Some Large\nSample Results.” In Uppsala Symposium on\nPsychological Factor Analysis, 35–42. Num-\nber 3 in Nordisk Psykologi Monograph Series.\nUppsala: Almqvist / Wiksell.\nLazarsfeld, P. F., and N. W. Henry. 1968. Latent\nStructure Analysis. Houghton Mifﬂin.\nLeCun, Y ., B. Boser, J. S. Denker, D. Henderson,\nR. E. Howard, W. Hubbard, and L. D. Jackel.\n1989. “Backpropagation Applied to Handwrit-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3184, "text": "1989. “Backpropagation Applied to Handwrit-\nten ZIP Code Recognition.” Neural Computa-\ntion 1 (4): 541–551.\nBIBLIOGRAPHY 633\nLeCun, Y ., L. Bottou, Y . Bengio, and P. Haffner.\n1998. “Gradient-Based Learning Applied to\nDocument Recognition.” Proceedings of the\nIEEE 86:2278–2324.\nLeCun, Y ., J. S. Denker, and S. A. Solla. 1990.\n“Optimal Brain Damage.” In Proceedings In-\nternational Conference on Neural Information\nProcessing Systems (NeurIPS), edited by D. S."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3185, "text": "Processing Systems (NeurIPS), edited by D. S.\nTouretzky, 2:598–605. Morgan Kaufmann.\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton.\n2015. “Deep Learning.”Nature 512:436–444.\nLeCun, Yann, Sumit Chopra, Raia Hadsell,\nMarc’Aurelio Ranzato, and Fu-Jie Huang.\n2006. “A Tutorial on Energy-Based Learn-\ning.” In Predicting Structured Data, edited by\nG. Bakir, T. Hofman, B. Sch¨olkopf, A. Smola,\nand B. Taskar. MIT Press.\nLeen, T. K. 1995. “From data distributions to regu-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3186, "text": "larization in invariant learning.” Neural Com-\nputation 7:974–981.\nLeshno, M., V . Y . Lin, A. Pinkus, and S. Schocken.\n1993. “Multilayer feedforward networks with\na polynomial activation function can approx-\nimate any function.” Neural Networks 6:861–\n867.\nLi, Hao, Zheng Xu, Gavin Taylor, Christoph Studer,\nand Tom Goldstein. 2017. Visualizing the Loss\nLandscape of Neural Nets. Technical report.\narXiv:1712.09913.\nLi, Junnan, Dongxu Li, Caiming Xiong, and\nSteven Hoi. 2022. BLIP: Bootstrapping"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3187, "text": "Steven Hoi. 2022. BLIP: Bootstrapping\nLanguage-Image Pre-training for Uniﬁed\nVision-Language Understanding and Gener-\nation. Technical report. arXiv:2201.12086.\nLin, Min, Qiang Chen, and Shuicheng Yan.\n2013. Network in Network. Technical report.\narXiv:1312.4400.\nLin, Tianyang, Yuxin Wang, Xiangyang Liu, and\nXipeng Qiu. 2021. A Survey of Transformers.\nTechnical report. arXiv:2106.04554.\nLipman, Yaron, Ricky T. Q. Chen, Heli Ben-Hamu,\nMaximilian Nickel, and Matt Le. 2022. Flow"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3188, "text": "Maximilian Nickel, and Matt Le. 2022. Flow\nMatching for Generative Modeling. Technical\nreport arXiv:2210.02747. https://arxiv.org/.\nLiu, Pengfei, Weizhe Yuan, Jinlan Fu, Zhengbao\nJiang, Hiroaki Hayashi, and Graham Neubig.\n2021. Pre-train, Prompt, and Predict: A Sys-\ntematic Survey of Prompting Methods in Nat-\nural Language Processing. Technical report.\narXiv:2107.13586.\nLloyd, S. P. 1982. “Least squares quantization\nin PCM.” IEEE Transactions on Information\nTheory 28 (2): 129–137."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3189, "text": "Theory 28 (2): 129–137.\nLong, Jonathan, Evan Shelhamer, and Trevor\nDarrell. 2014. Fully Convolutional Networks\nfor Semantic Segmentation. Technical report.\narXiv:1411.4038.\nLuo, Calvin. 2022. Understanding Diffusion Mod-\nels: A Uniﬁed Perspective. Technical report.\narXiv:2208.11970.\nL¨utkepohl, H. 1996. Handbook of Matrices. Wiley.\nMacKay, D. J. C. 1992. “A Practical Bayesian\nFramework for Back-propagation Networks.”\nNeural Computation 4 (3): 448–472."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3190, "text": "Neural Computation 4 (3): 448–472.\nMacKay, D. J. C. 2003. Information Theory, In-\nference and Learning Algorithms. Cambridge\nUniversity Press.\nMacQueen, J. 1967. “Some methods for classiﬁca-\ntion and analysis of multivariate observations.”\nIn Proceedings of the Fifth Berkeley Sympo-\nsium on Mathematical Statistics and Probabil-\nity, edited by L. M. LeCam and J. Neyman,\nI:281–297. University of California Press.\nMagnus, J. R., and H. Neudecker. 1999.Matrix Dif-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3191, "text": "Magnus, J. R., and H. Neudecker. 1999.Matrix Dif-\nferential Calculus with Applications in Statis-\ntics and Econometrics. Wiley.\nMallat, S. 1999. A Wavelet Tour of Signal Process-\ning. Second. Academic Press.\nMao, X., Q. Li, H. Xie, R. Lau, Z. Wang, and\nS. Smolley. 2016. Least Squares Genera-\ntive Adversarial Networks. Technical report.\narXiv:1611.04076.\nMardia, K. V ., and P. E. Jupp. 2000. Directional\nStatistics. Wiley.\nMartens, James, Ilya Sutskever, and Kevin Swer-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3192, "text": "Martens, James, Ilya Sutskever, and Kevin Swer-\nsky. 2012. “Estimating the Hessian by Back-\npropagating Curvature.” In Proceedings of the\n634 BIBLIOGRAPHY\nInternational Conference on Machine Learn-\ning (ICML). ArXiv:1206.6464.\nMcCullagh, P., and J. A. Nelder. 1989. Generalized\nLinear Models. Second. Chapman / Hall.\nMcCulloch, W. S., and W. Pitts. 1943. “A Logi-\ncal Calculus of the Ideas Immanent in Nervous\nActivity.” Reprinted in Anderson and Rosen-\nfeld (1988), Bulletin of Mathematical Bio-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3193, "text": "feld (1988), Bulletin of Mathematical Bio-\nphysics 5:115–133.\nMcLachlan, G. J., and T. Krishnan. 1997. The EM\nAlgorithm and its Extensions. Wiley.\nMcLachlan, G. J., and D. Peel. 2000.Finite Mixture\nModels. Wiley.\nMeng, X. L., and D. B. Rubin. 1993. “Maximum\nlikelihood estimation via the ECM algorithm:\na general framework.”Biometrika 80:267–278.\nMescheder, L., A. Geiger, and S. Nowozin.\n2018. Which Training Methods for GANs\ndo actually Converge? Technical report.\narXiv:1801.04406."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3194, "text": "arXiv:1801.04406.\nMetropolis, N., A. W. Rosenbluth, M. N. Rosen-\nbluth, A. H. Teller, and E. Teller. 1953. “Equa-\ntion of State Calculations by Fast Computing\nMachines.” Journal of Chemical Physics 21\n(6): 1087–1092.\nMetropolis, N., and S. Ulam. 1949. “The Monte\nCarlo method.” Journal of the American Sta-\ntistical Association 44 (247): 335–341.\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient Estimation of Word\nRepresentations in Vector Space.Technical re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3195, "text": "Representations in Vector Space.Technical re-\nport. arXiv:1301.3781.\nMinsky, M. L., and S. A. Papert. 1969.Perceptrons.\nExpanded edition 1990. MIT Press.\nMirza, M., and S. Osindero. 2014. Conditional\nGenerative Adversarial Nets. Technical report.\narXiv:1411.1784.\nMiskin, J. W., and D. J. C. MacKay. 2001. “En-\nsemble learning for blind source separation.”\nIn Independent Component Analysis: Princi-\nples and Practice, edited by S. J. Roberts and\nR. M. Everson. Cambridge University Press."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3196, "text": "R. M. Everson. Cambridge University Press.\nMøller, M. 1993. “Efﬁcient Training of Feed-\nForward Neural Networks.” PhD diss., Aarhus\nUniversity, Denmark.\nMont´ufar, G. F., R. Pascanu, K. Cho, and Y .\nBengio. 2014. “On the number of linear re-\ngions of deep neural networks.” In Proceed-\nings of the International Conference on Neural\nInformation Processing Systems (NeurIPS).\nArXiv:1402.1869.\nMordvintsev, Alexander, Christopher Olah, and\nMike Tyka. 2015.Inceptionism: Going Deeper"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3197, "text": "Mike Tyka. 2015.Inceptionism: Going Deeper\ninto Neural Networks. Google AI blog.\nMurphy, Kevin P. 2022. Probabilistic Machine\nLearning: An introduction.MIT Press. probml.\nai.\nMurphy, Kevin P. 2023. Probabilistic Machine\nLearning: Advanced Topics. MIT Press. http:\n//probml.github.io/book2.\nNakkiran, Preetum, Gal Kaplun, Yamini Bansal,\nTristan Yang, Boaz Barak, and Ilya Sutskever.\n2019. Deep Double Descent: Where Bigger\nModels and More Data Hurt.Technical report.\narXiv:1912.02292."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3198, "text": "arXiv:1912.02292.\nNeal, R. M. 1993. Probabilistic inference using\nMarkov chain Monte Carlo methods. Techni-\ncal report CRG-TR-93-1. Department of Com-\nputer Science, University of Toronto, Canada.\nNeal, R. M. 1999. “Suppressing random walks in\nMarkov chain Monte Carlo using ordered over-\nrelaxation.” In Learning in Graphical Models,\nedited by Michael I. Jordan, 205–228. MIT\nPress.\nNeal, R. M., and G. E. Hinton. 1999. “A new view\nof the EM algorithm that justiﬁes incremental"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3199, "text": "of the EM algorithm that justiﬁes incremental\nand other variants.” In Learning in Graphical\nModels, edited by M. I. Jordan, 355–368. MIT\nPress.\nNelder, J. A., and R. W. M. Wedderburn. 1972.\n“Generalized linear models.” Journal of the\nRoyal Statistical Society, A 135:370–384.\nNesterov, Y . 2004.Introductory Lectures on Convex\nOptimization: A Basic Course. Kluwer.\nBIBLIOGRAPHY 635\nNichol, Alex, and Prafulla Dhariwal. 2021. Im-\nproved Denoising Diffusion Probabilistic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3200, "text": "proved Denoising Diffusion Probabilistic\nModels. Technical report. arXiv:2102.09672.\nNichol, Alex, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob Mc-\nGrew, Ilya Sutskever, and Mark Chen. 2021.\nGLIDE: Towards Photorealistic Image Gener-\nation and Editing with Text-Guided Diffusion\nModels. Technical report. arXiv:2112.10741.\nNocedal, J., and S. J. Wright. 1999. Numerical Op-\ntimization. Springer.\nNoh, Hyeonwoo, Seunghoon Hong, and Bohyung"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3201, "text": "Noh, Hyeonwoo, Seunghoon Hong, and Bohyung\nHan. 2015. Learning Deconvolution Network\nfor Semantic Segmentation. Technical report.\narXiv:1505.04366.\nNowlan, S. J., and G. E. Hinton. 1992. “Simplifying\nneural networks by soft weight sharing.” Neu-\nral Computation 4 (4): 473–493.\nOgden, R. T. 1997.Essential Wavelets for Statistical\nApplications and Data Analysis. Birkh¨auser.\nOord, Aaron van den, Nal Kalchbrenner, and\nKoray Kavukcuoglu. 2016. Pixel Recur-\nrent Neural Networks. Technical report."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3202, "text": "rent Neural Networks. Technical report.\narXiv:1601.06759.\nOord, Aaron van den, Nal Kalchbrenner, Oriol\nVinyals, Lasse Espeholt, Alex Graves, and Ko-\nray Kavukcuoglu. 2016. Conditional Image\nGeneration with PixelCNN Decoders. Techni-\ncal report. arXiv:1606.05328.\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals.\n2018. Representation Learning with Con-\ntrastive Predictive Coding. Technical report.\narXiv:1807.03748.\nOord, Aaron van den, Oriol Vinyals, and Ko-\nray Kavukcuoglu. 2017. Neural Discrete"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3203, "text": "ray Kavukcuoglu. 2017. Neural Discrete\nRepresentation Learning. Technical report.\narXiv:1711.00937.\nOpenAI. 2023. GPT-4 Technical Report. Technical\nreport. arXiv:2303.08774.\nOpper, M., and O. Winther. 2000. “Gaussian pro-\ncesses and SVM: mean ﬁeld theory and leave-\none-out.” In Advances in Large Margin Clas-\nsiﬁers, edited by A. J. Smola, P. L. Bartlett, B.\nSch¨olkopf, and D. Shuurmans, 311–326. MIT\nPress.\nPapamakarios, G., T. Pavlakou, and Iain Murray."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3204, "text": "Papamakarios, G., T. Pavlakou, and Iain Murray.\n2017. “Masked Autoregressive Flow for Den-\nsity Estimation.” In Proceedings of the Inter-\nnational Conference on Neural Information\nProcessing Systems (NeurIPS), vol. 30.\nPapamakarios, George, Eric Nalisnick, Danilo\nJimenez Rezende, Shakir Mohamed, and\nBalaji Lakshminarayanan. 2019. Normalizing\nFlows for Probabilistic Modeling and Infer-\nence. Technical report. arXiv:1912.02762.\nParisi, Giorgio. 1981. “Correlation functions and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3205, "text": "Parisi, Giorgio. 1981. “Correlation functions and\ncomputer simulations.” Nuclear Physics B\n180:378–384.\nPearl, J. 1988. Probabilistic Reasoning in Intelli-\ngent Systems. Morgan Kaufmann.\nPearlmutter, B. A. 1994. “Fast exact multiplication\nby the Hessian.” Neural Computation 6 (1):\n147–160.\nPearlmutter, B. A., and L. C. Parra. 1997. “Maxi-\nmum likelihood source separation: a context-\nsensitive generalization of ICA.” In Advances\nin Neural Information Processing Systems,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3206, "text": "in Neural Information Processing Systems,\nedited by M. C. Mozer, M. I. Jordan, and T.\nPetsche, 9:613–619. MIT Press.\nPearson, Karl. 1901. “On lines and planes of clos-\nest ﬁt to systems of points in space.” The\nLondon, Edinburgh and Dublin Philosophical\nMagazine and Journal of Science, Sixth Series\n2:559–572.\nPhuong, Mary, and Marcus Hutter. 2022. Formal\nAlgorithms for Transformers.Technical report.\narXiv:2207.09238.\nPrince, Simon J.D. 2020. Variational autoen-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3207, "text": "Prince, Simon J.D. 2020. Variational autoen-\ncoders. Https://www.borealisai.com/research-\nblogs/tutorial-5-variational-auto-encoders.\nPrince, Simon J.D. 2023. Understanding Deep\nLearning. MIT Press. http://udlbook.com.\nRadford, A., L. Metz, and S. Chintala. 2015.\nUnsupervised representation learning with\ndeep convolutional generative adversarial net-\nworks. Technical report. arXiv:1511.06434.\n636 BIBLIOGRAPHY\nRadford, Alec, Jong Wook Kim, Chris Hal-\nlacy, Aditya Ramesh, Gabriel Goh, Sandhini"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3208, "text": "lacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, et al. 2021. Learn-\ning Transferable Visual Models From Natu-\nral Language Supervision. Technical report.\narXiv:2103.00020.\nRadford, Alec, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage Models are Unsupervised Multitask\nLearners. Technical report. OpenAI.\nRakhimov, Ruslan, Denis V olkhonskiy, Alexey\nArtemov, Denis Zorin, and Evgeny Burnaev.\n2020. Latent Video Transformer.Technical re-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3209, "text": "2020. Latent Video Transformer.Technical re-\nport. arXiv:2006.10704.\nRamachandran, P., B. Zoph, and Q. V . Le. 2017.\nSearching for Activation Functions. Technical\nreport. arXiv:1710.05941v2.\nRao, C. R., and S. K. Mitra. 1971. Generalized In-\nverse of Matrices and Its Applications. Wiley.\nRedmon, Joseph, Santosh Kumar Divvala, Ross B.\nGirshick, and Ali Farhadi. 2015. You Only\nLook Once: Uniﬁed, Real-Time Object Detec-\ntion. Technical report. arxiv:1506.02640."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3210, "text": "tion. Technical report. arxiv:1506.02640.\nRen, Shaoqing, Kaiming He, Ross B. Girshick, and\nJian Sun. 2015. Faster R-CNN: Towards Real-\nTime Object Detection with Region Proposal\nNetworks. Technical report. arxiv:1506.01497.\nRezende, Danilo J, Shakir Mohamed, and Daan\nWierstra. 2014. “Stochastic backpropagation\nand approximate inference in deep genera-\ntive models.” In Proceedings of the 31st In-\nternational Conference on Machine Learning\n(ICML-14), 1278–1286."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3211, "text": "(ICML-14), 1278–1286.\nRicotti, L. P., S. Ragazzini, and G. Martinelli. 1988.\n“Learning of word stress in a sub-optimal sec-\nond order backpropagation neural network.” In\nProceedings of the IEEE International Confer-\nence on Neural Networks, 1:355–361. IEEE.\nRobert, C. P., and G. Casella. 1999. Monte Carlo\nStatistical Methods. Springer.\nRombach, Robin, Andreas Blattmann, Dominik\nLorenz, Patrick Esser, and Bj ¨orn Ommer.\n2021. High-Resolution Image Synthesis with"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3212, "text": "2021. High-Resolution Image Synthesis with\nLatent Diffusion Models. Technical report.\narXiv:2112.10752.\nRonneberger, Olaf, Philipp Fischer, and Thomas\nBrox. 2015. “U-Net: Convolutional Networks\nfor Biomedical Image Segmentation.” InMed-\nical Image Computing and Computer-Assisted\nIntervention – MICCAI, edited by N. Navab, J.\nHornegger, W. Wells, and A. Frangi. Springer.\nRosenblatt, F. 1962. Principles of Neurodynamics:\nPerceptrons and the Theory of Brain Mecha-\nnisms. Spartan."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3213, "text": "nisms. Spartan.\nRoweis, S. 1998. “EM algorithms for PCA and\nSPCA.” In Advances in Neural Information\nProcessing Systems, edited by M. I. Jordan,\nM. J. Kearns, and S. A. Solla, 10:626–632.\nMIT Press.\nRoweis, S., and Z. Ghahramani. 1999. “A unify-\ning review of linear Gaussian models.” Neural\nComputation 11 (2): 305–345.\nRubin, D. B., and D. T. Thayer. 1982. “EM algo-\nrithms for ML factor analysis.” Psychometrika\n47 (1): 69–76.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3214, "text": "1986. “Learning internal representations by er-\nror propagation.” In Parallel Distributed Pro-\ncessing: Explorations in the Microstructure of\nCognition, edited by D. E. Rumelhart, J. L.\nMcClelland, and the PDP Research Group,\nvol. 1: Foundations, 318–362. Reprinted in\nAnderson and Rosenfeld (1988). MIT Press.\nRuthotto, L., and E. Haber. 2021. An introduction\nto deep generative modeling. Technical report.\narXiv:2103.05180.\nSagan, H. 1969. Introduction to the Calculus of\nVariations. Dover."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3215, "text": "Variations. Dover.\nSaharia, Chitwan, William Chan, Huiwen Chang,\nChris A. Lee, Jonathan Ho, Tim Salimans,\nDavid J. Fleet, and Mohammad Norouzi.\n2021. Palette: Image-to-Image Diffusion Mod-\nels. Technical report. arXiv:2111.05826.\nSaharia, Chitwan, William Chan, Saurabh Saxena,\nLala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, et al. 2022.\nPhotorealistic Text-to-Image Diffusion Models\nBIBLIOGRAPHY 637\nwith Deep Language Understanding. Techni-\ncal report. arXiv:2205.11487."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3216, "text": "cal report. arXiv:2205.11487.\nSaharia, Chitwan, Jonathan Ho, William Chan,\nTim Salimans, David J. Fleet, and Moham-\nmad Norouzi. 2021. Image Super-Resolution\nvia Iterative Reﬁnement. Technical report.\narXiv:2104.07636.\nSanturkar, S., D. Tsipras, A. Ilyas, and A. Madry.\n2018. How does batch normalization help opti-\nmization? Technical report. arXiv:1805.11604.\nSatorras, Victor Garcia, Emiel Hoogeboom,\nand Max Welling. 2021. E(n) Equivariant\nGraph Neural Networks. Technical report."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3217, "text": "Graph Neural Networks. Technical report.\narXiv:2102.09844.\nSch¨olkopf, B., and A. J. Smola. 2002.Learning with\nKernels. MIT Press.\nSchuhmann, Christoph, Richard Vencu, Romain\nBeaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia\nJitsev, and Aran Komatsuzaki. 2021. LAION-\n400M: Open Dataset of CLIP-Filtered 400\nMillion Image-Text Pairs. Technical report.\narXiv:2111.02114.\nSchuster, Mike, and Kaisuke Nakajima. 2012.\n“Japanese and Korean voice search.” In 2012"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3218, "text": "“Japanese and Korean voice search.” In 2012\nIEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP),\n5149–5152.\nSelvaraju, Ramprasaath R., Abhishek Das, Ra-\nmakrishna Vedantam, Michael Cogswell, Devi\nParikh, and Dhruv Batra. 2016. Grad-CAM:\nVisual Explanations from Deep Networks via\nGradient-based Localization.Technical report.\narXiv:1610.02391.\nSennrich, Rico, Barry Haddow, and Alexandra\nBirch. 2015. Neural Machine Translation of"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3219, "text": "Birch. 2015. Neural Machine Translation of\nRare Words with Subword Units.Technical re-\nport. arXiv:1508.07909.\nSermanet, Pierre, David Eigen, Xiang Zhang,\nMichael Mathieu, Rob Fergus, and Yann\nLeCun. 2013. OverFeat: Integrated Recog-\nnition, Localization and Detection using\nConvolutional Networks. Technical report.\narXiv:1312.6229.\nShachter, R. D., and M. Peot. 1990. “Simulation\nApproaches to General Probabilistic Inference\non Belief Networks.” In Uncertainty in Artiﬁ-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3220, "text": "on Belief Networks.” In Uncertainty in Artiﬁ-\ncial Intelligence, edited by P. P. Bonissone, M.\nHenrion, L. N. Kanal, and J. F. Lemmer, vol. 5.\nElsevier.\nShannon, C. E. 1948. “A mathematical theory of\ncommunication.” The Bell System Technical\nJournal 27 (3): 379–423 and 623–656.\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma,\nZhewei Yao, Amir Gholami, Michael W. Ma-\nhoney, and Kurt Keutzer. 2019.Q-BERT: Hes-\nsian Based Ultra Low Precision Quantization\nof BERT. Technical report. arXiv:1909.05840."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3221, "text": "of BERT. Technical report. arXiv:1909.05840.\nSimard, P., B. Victorri, Y . LeCun, and J. Denker.\n1992. “Tangent prop – a formalism for spec-\nifying selected invariances in an adaptive net-\nwork.” InAdvances in Neural Information Pro-\ncessing Systems, edited by J. E. Moody, S. J.\nHanson, and R. P. Lippmann, 4:895–903. Mor-\ngan Kaufmann.\nSimard, P. Y ., D. Steinkraus, and J. Platt. 2003.\n“Best practice for convolutional neural net-\nworks applied to visual document analysis.”"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3222, "text": "works applied to visual document analysis.”\nIn Proceedings International Conference on\nDocument Analysis and Recognition (ICDAR),\n958–962. IEEE Computer Society.\nSimonyan, Karen, Andrea Vedaldi, and Andrew\nZisserman. 2013. “Deep Inside Convolu-\ntional Networks: Visualising Image Clas-\nsiﬁcation Models and Saliency Maps.” In\nComputer Vision and Pattern Recognition.\nArXiv:1312.6034.\nSimonyan, Karen, and Andrew Zisserman. 2014.\nVery Deep Convolutional Networks for Large-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3223, "text": "Very Deep Convolutional Networks for Large-\nScale Image Recognition. Technical report.\narXiv:1409.1556.\nSirovich, L. 1987. “Turbulence and the Dynam-\nics of Coherent Structures.”Quarterly Applied\nMathematics 45 (3): 561–590.\nSohl-Dickstein, Jascha, Eric A. Weiss, Niru Ma-\nheswaranathan, and Surya Ganguli. 2015.\nDeep Unsupervised Learning using Nonequi-\n638 BIBLIOGRAPHY\nlibrium Thermodynamics. Technical report.\narXiv:1503.03585.\nSønderby, C., J. Caballero, L. Theis, W. Shi, and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3224, "text": "Sønderby, C., J. Caballero, L. Theis, W. Shi, and\nF. Husz ´ar. 2016. Amortised MAP inference\nfor image super-resolution. Technical report.\narXiv:1610.04490.\nSong, Jiaming, Chenlin Meng, and Stefano Ermon.\n2020. Denoising Diffusion Implicit Models.\nTechnical report. arXiv:2010.02502.\nSong, Yang, and Stefano Ermon. 2019. “Genera-\ntive Modeling by Estimating Gradients of the\nData Distribution.” In Advances in Neural In-\nformation Processing Systems, 11895–11907.\nArXiv:1907.05600."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3225, "text": "ArXiv:1907.05600.\nSong, Yang, Sahaj Garg, Jiaxin Shi, and Stefano\nErmon. 2019. “Sliced score matching: A scal-\nable approach to density and score estimation.”\nIn Uncertainty in Artiﬁcial Intelligence, 204.\nArXiv:1905.07088.\nSong, Yang, and Diederik P. Kingma. 2021. How\nto Train Your Energy-Based Models.Technical\nreport. arXiv:2101.03288.\nSong, Yang, Jascha Sohl-Dickstein, Diederik\nP. Kingma, Abhishek Kumar, Stefano Er-\nmon, and Ben Poole. 2020. Score-Based\nGenerative Modeling through Stochastic"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3226, "text": "Generative Modeling through Stochastic\nDifferential Equations. Technical report.\narXiv:2011.13456.\nSrivastava, N., G. Hinton, A. Krizhevsky, I.\nSutskever, and R. Salakhutdinov. 2014.\n“Dropout: A Simple Way to Prevent Neural\nNetworks from Overﬁtting.” Journal of Ma-\nchine Learning Research 15:1929–1958.\nStone, J. V . 2004. Independent Component Analy-\nsis: A Tutorial Introduction. MIT Press.\nSutskever, I., J. Martens, G. Dahl, and G. E. Hin-\nton. 2013. “On the importance of initialization"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3227, "text": "ton. 2013. “On the importance of initialization\nand momentum in deep learning.” In Proceed-\nings of the International Conference on Ma-\nchine Learning (ICML).\nSutton, R. 2019. The Bitter Lesson. URL: incom-\npleteideas.net/IncIdeas/BitterLesson.html.\nSzegedy, Christian, Wojciech Zaremba, Ilya\nSutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus. 2013. Intrigu-\ning properties of neural networks. Technical\nreport. arXiv:1312.6199.\nSzeliski, R. 2022. Computer Vision: Algorithms and"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3228, "text": "Applications. Second. Springer.\nTarassenko, L. 1995. “Novelty detection for the\nidentiﬁcation of masses in mamograms.” In\nProceedings of the Fourth IEE International\nConference on Artiﬁcial Neural Networks,\n4:442–447. IEE.\nTay, Yi, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient Transformers: A Sur-\nvey. Technical report. arXiv:2009.06732.\nTibshirani, R. 1996. “Regression shrinkage and se-\nlection via the lasso.”Journal of the Royal Sta-\ntistical Society, B 58:267–288."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3229, "text": "tistical Society, B 58:267–288.\nTipping, M. E., and Christopher M. Bishop. 1997.\nProbabilistic Principal Component Analysis.\nTechnical report NCRG/97/010. Neural Com-\nputing Research Group, Aston University.\nTipping, M. E., and Christopher M. Bishop. 1999.\n“Probabilistic Principal Component Analysis.”\nJournal of the Royal Statistical Society, Series\nB 21 (3): 611–622.\nVapnik, V . N. 1995.The nature of statistical learn-\ning theory. Springer.\nVaswani, Ashish, Noam Shazeer, Niki Parmar,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3230, "text": "Vaswani, Ashish, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin.\n2017. Attention Is All You Need. Technical re-\nport. arXiv:1706.03762.\nVeliˇckovi´c, Petar. 2023. Everything is Connected:\nGraph Neural Networks. Technical report.\narXiv:2301.08210.\nVeliˇckovi´c, Petar, Guillem Cucurull, Arantxa\nCasanova, Adriana Romero, Pietro Li `o, and\nYoshua Bengio. 2017. Graph Attention Net-\nworks. Technical report. arXiv:1710.10903."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3231, "text": "works. Technical report. arXiv:1710.10903.\nVidakovic, B. 1999. Statistical Modelling by\nWavelets. Wiley.\nBIBLIOGRAPHY 639\nVig, Jesse, Ali Madani, Lav R. Varshney, Caiming\nXiong, Richard Socher, and Nazneen Fatema\nRajani. 2020. BERTology Meets Biology: In-\nterpreting Attention in Protein Language Mod-\nels. Technical report. arXiv:2006.15222.\nVincent, P. 2011. “A connection between score\nmatching and denoising autoencoders.”Neural\nComputation 23:1661–1674."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3232, "text": "Computation 23:1661–1674.\nVincent, Pascal, Hugo Larochelle, Yoshua Bengio,\nand Pierre-Antoine Manzagol. 2008. “Extract-\ning and Composing Robust Features with De-\nnoising Autoencoders.” In Proceedings of the\nInternational Conference on Machine Learn-\ning (ICML).\nWalker, A. M. 1969. “On the asymptotic behaviour\nof posterior distributions.”Journal of the Royal\nStatistical Society, B 31 (1): 80–88.\nWang, Chengyi, Sanyuan Chen, Yu Wu, Ziqiang\nZhang, Long Zhou, Shujie Liu, Zhuo Chen, et"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3233, "text": "Zhang, Long Zhou, Shujie Liu, Zhuo Chen, et\nal. 2023. Neural Codec Language Models are\nZero-Shot Text to Speech Synthesizers.Techni-\ncal report. arXiv:2301.02111.\nWeisstein, E. W. 1999. CRC Concise Encyclopedia\nof Mathematics. Chapman / Hall, / CRC.\nWelling, Max, and Yee Whye Teh. 2011. “Bayesian\nLearning via Stochastic Gradient Langevin\nDynamics.” In Proceedings of the Inter-\nnational Conference on Machine Learning\n(ICML).\nWilliams, P. M. 1996. “Using neural networks"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3234, "text": "Williams, P. M. 1996. “Using neural networks\nto model conditional multivariate densities.”\nNeural Computation 8 (4): 843–854.\nWilliams, R J. 1992. “Simple statistical gradient-\nfollowing algorithms for connectionist re-\ninforcement learning.” Machine Learning\n8:229–256.\nWinn, J., C. M. Bishop, T. Diethe, J. Guiver, and Y .\nZaykov. 2023. Model-Based Machine Learn-\ning. Www.mbmlbook.com. Chapman / Hall.\nWolpert, D. H. 1996. “The lack of a-priori dis-\ntinctions between learning algorithms.”Neural"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3235, "text": "tinctions between learning algorithms.”Neural\nComputation 8:1341–1390.\nWu, Zhirong, Yuanjun Xiong, Stella Yu, and Dahua\nLin. 2018. Unsupervised Feature Learning\nvia Non-Parametric Instance-level Discrimi-\nnation. Technical report. arXiv:1805.01978.\nWu, Zonghan, Shirui Pan, Fengwen Chen,\nGuodong Long, Chengqi Zhang, and Philip\nS. Yu. 2019. A Comprehensive Survey on\nGraph Neural Networks. Technical report.\narXiv:1901.00596.\nYan, Wilson, Yunzhi Zhang, Pieter Abbeel, and Ar-"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3236, "text": "Yan, Wilson, Yunzhi Zhang, Pieter Abbeel, and Ar-\navind Srinivas. 2021.VideoGPT: Video Gener-\nation using VQ-VAE and Transformers. Tech-\nnical report. arXiv:2104.10157.\nYang, Ruihan, Prakhar Srivastava, and Stephan\nMandt. 2022. Diffusion Probabilistic Model-\ning for Video Generation. Technical report.\narXiv:2203.09481.\nYilmaz, Fatih Furkan, and Reinhard Heckel. 2022.\nRegularization-wise double descent: Why it\noccurs and how to eliminate it. Technical re-\nport. arXiv:2206.01378."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3237, "text": "port. arXiv:2206.01378.\nYosinski, Jason, Jeff Clune, Anh Mai Nguyen,\nThomas J. Fuchs, and Hod Lipson. 2015.\nUnderstanding Neural Networks Through\nDeep Visualization. Technical report.\narXiv:1506.06579.\nYu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang,\nRuoming Pang, James Qin, Alexander Ku,\nYuanzhong Xu, Jason Baldridge, and Yonghui\nWu. 2021. Vector-quantized Image Model-\ning with Improved VQGAN. Technical report.\narXiv:2110.04627.\nYu, Jiahui, Yuanzhong Xu, Jing Yu Koh, Thang"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3238, "text": "Yu, Jiahui, Yuanzhong Xu, Jing Yu Koh, Thang\nLuong, Gunjan Baid, Zirui Wang, Vijay Va-\nsudevan, et al. 2022. Scaling Autoregressive\nModels for Content-Rich Text-to-Image Gener-\nation. Technical report. arXiv:2206.10789.\nYu, Lili, Bowen Shi, Ramakanth Pasunuru, Ben-\njamin Muller, Olga Golovneva, Tianlu Wang,\nArun Babu, et al. 2023. Scaling Autore-\ngressive Multi-Modal Models: Pretraining\nand Instruction Tuning. Technical report.\narXiv:2309.02591.\n640 BIBLIOGRAPHY"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3239, "text": "arXiv:2309.02591.\n640 BIBLIOGRAPHY\nZaheer, Manzil, Satwik Kottur, Siamak Ravan-\nbakhsh, Barnabas Poczos, Ruslan Salakhutdi-\nnov, and Alexander Smola. 2017. Deep Sets.\nTechnical report. arXiv:1703.06114.\nZarchan, P., and H. Musoff. 2005. Fundamentals of\nKalman Filtering: A Practical Approach. Sec-\nond. AIAA.\nZeiler, Matthew D., and Rob Fergus. 2013. Visu-\nalizing and Understanding Convolutional Net-\nworks. Technical report. arXiv:1311.2901.\nZhang, Chiyuan, Samy Bengio, Moritz Hardt,"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3240, "text": "Zhang, Chiyuan, Samy Bengio, Moritz Hardt,\nBenjamin Recht, and Oriol Vinyals. 2016.\nUnderstanding deep learning requires re-\nthinking generalization. Technical report.\narXiv:1611.03530.\nZhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, et\nal. 2023. A Survey of Large Language Models.\nTechnical report. arXiv:2303.18223.\nZhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan\nZhang, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun."}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3241, "text": "Wang, Changcheng Li, and Maosong Sun.\n2018. Graph Neural Networks: A Review of\nMethods and Applications. Technical report.\narXiv:1812.08434.\nZhou, Y ., and R. Chellappa. 1988. “Computation of\noptic ﬂow using a neural network.” InInterna-\ntional Conference on Neural Networks,71–78.\nIEEE.\nZhu, J-Y, T. Park, P. Isola, and A. Efros. 2017.\nUnpaired Image-to-Image Translation using\nCycle-Consistent Adversarial Networks. Tech-\nnical report. arXiv:1703.10593.\nIndex"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3242, "text": "nical report. arXiv:1703.10593.\nIndex\nPage numbers in bold indicate the primary source of information for the corresponding topic.\n1 ×1 convolution, 296\n1-of-Kcoding, 68, 135, 460\nacceptance criterion, 441, 445, 448\nactivation, 17\nactivation function, 17, 158, 180, 182\nactive constraint, 623\nAdaGrad, 223\nAdam optimization, 224\nadaptive rejection sampling, 435\nadjacency matrix, 410\nadjoint sensitivity method, 556\nadversarial attack, 306\naggregation, 415\naleatoric uncertainty, 23\nAlexNet, 300"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3243, "text": "aleatoric uncertainty, 23\nAlexNet, 300\nalpha family, 61\namortized inference, 572\nancestral sampling, 450\nanchor, 191\nannealed Langevin dynamics, 598\nAR model, see autoregressive model\narea under the ROC curve, 149\nartiﬁcial intelligence, 1\nattention, 358\nattention head, 366\naudio data, 399\nauto-associative neural network, see autoencoder\nautoencoder, 188, 563\nautomatic differentiation, 22, 233, 244\nautoregressive ﬂow, 552\nautoregressive model, 5, 350, 379\naverage pooling, 297"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3244, "text": "average pooling, 297\nbackpropagation, 19, 233\nbackpropagation through time, 381\nbag of words, 378\nbagging, 278\nbase distribution, 547\nbasis function, 112, 158, 172, 172\nbatch gradient descent, 214\nbatch learning, 117\nbatch normalization, 227\nBayes net, 326\nBayes’ theorem, 28\nBayesian network, 326\nBayesian probability, 54\nbeam search, 386\nBernoulli distribution, 66, 94\nBernoulli mixture model, 481\nBERT, 388\nbi-gram model, 379\nbias, 39, 125\nbias parameter, 112, 132, 180"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3245, "text": "bias, 39, 125\nbias parameter, 112, 132, 180\nbias–variance trade-off, 123\nBigGAN, 539\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024\nC. M. Bishop, H. Bishop, Deep Learning, https://doi.org/10.1007/978-3-031-45468-4\n641\n642 INDEX\nbijective function, 548\nbinomial distribution, 67\nbits, 46\nblind source separation, 514\nblocked path, 339, 343\nboosting, 279\nbootstrap, 278\nbottleneck, 382\nbounding box, 309\nBox–Muller method, 432\nbyte pair encoding, 377"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3246, "text": "Box–Muller method, 432\nbyte pair encoding, 377\ncanonical correlation analysis, 501\ncanonical link function, 164\nCauchy distribution, 432\ncausal attention, 384\ncausality, 347\ncentral differences, 239\ncentral limit theorem, 71\nChatGPT, 394\nchild node, 249, 327\nCholesky decomposition, 433\ncircular normal distribution, 89\nclassical probability, 54\nclassiﬁcation, 3\nCLIP, 192\nco-parents, 348\ncodebook vector, 398, 465\ncollider node, 341\ncombining models, 146\ncommittee, 277\ncomplete data set, 476"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3247, "text": "committee, 277\ncomplete data set, 476\ncompleting the square, 77\ncomputer vision, 288\nconcave function, 52\nconcentration parameter, 92\ncondition number, 220\nconditional entropy, 53\nconditional expectation, 35\nconditional independence, 146, 337\nconditional mixture model, 199\nconditional probability, 27\nconditional V AE, 576\nconditioner, 552\nconfusion matrix, 147\ncontinuous bag of words, 375\ncontinuous normalizing ﬂow, 557\ncontrastive divergence, 455\ncontrastive learning, 191\nconvex function, 51"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3248, "text": "contrastive learning, 191\nconvex function, 51\nconvolution, 290, 322\nconvolutional network, 287\ncorrelation matrix, 503\ncost function, 140\ncoupling ﬂow, 549\ncoupling function, 552\ncovariance, 35\nCox’s axioms, 54\ncross attention, 390\ncross-correlation, 292, 322\ncross-entropy error function, 160, 162, 196\ncross-validation, 14\ncumulative distribution function, 32\ncurse of dimensionality, 172\ncurve ﬁtting, 6\nCycleGAN, 539\nd-separation, 338, 343, 479\nDAG, see directed acyclic graph"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3249, "text": "DAG, see directed acyclic graph\ndata augmentation, 192, 257\ndata compression, 465\nDDIM, 594\nDDPM, 581\ndecision, 120\ndecision boundary, 131, 139\ndecision region, 131, 139\ndecision surface, see decision boundary\ndecision theory, 120, 138\ndecoder, 563\ndeep double descent, 268\ndeep learning, 20\ndeep neural networks, 20\ndeep sets, 417\nDeepDream, 308\ndegrees of freedom, 495\ndenoising, 581\ndenoising autoencoder, 567\ndenoising diffusion implicit model, 594\ndenoising diffusion probabilistic model, 581"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3250, "text": "denoising diffusion probabilistic model, 581\ndenoising score matching, 597\nINDEX 643\ndensity estimation, 37, 65\ndequantization, 526\ndescendant node, 341\ndesign matrix, 116\ndevelopment set, 14\ndiagonal covariance matrix, 75\ndifferential entropy, 50\ndiffusion kernel, 583\ndiffusion model, 581\nDirac delta function, 34\ndirected acyclic graph, 329\ndirected cycle, 329\ndirected factorization, 349\ndirected graph, 326\ndirected graphical model, 326\ndiscriminant function, 132, 143"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3251, "text": "discriminant function, 132, 143\ndiscriminative model, 144, 157, 346\ndisentangled representations, 542\ndistributed representation, 187\ndot-product attention, 363\ndouble descent, 268\ndropout, 279\nE step, 472, 476\nearly stopping, 266\nearth mover’s distance, 538\nECM, see expectation conditional maximization\nedge, 326, 410\nedge detection, 292\nELBO, see evidence lower bound\nEM, see expectation maximization\nembedding space, 188\nembedding vector, 409\nencoder, 563\nenergy function, 452"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3252, "text": "encoder, 563\nenergy function, 452\nenergy-based models, 452\nensemble methods, 277\nentropy, 46\nepistemic uncertainty, 23\nepoch, 215\nequality constraint, 623\nequivariance, 259, 292, 296, 371, 412\nerf function, 164\nerror backpropagation, see backpropagation\nerror function, 8, 55, 194, 210\nEuler–Lagrange equations, 619\nevaluation trace, 247\nevidence lower bound, 485, 516, 570, 588\nexpectation, 34\nexpectation conditional maximization, 489\nexpectation maximization, 470, 474, 517, 519"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3253, "text": "expectation maximization, 470, 474, 517, 519\nexpectation step, see E step\nexpectations, 430\nexplaining away, 343\nexploding gradient, 227, 382\nexponential distribution, 34, 431\nexponential family, 94, 156, 329\nexpression swell, 245\nfactor analysis, 513\nfactor graph, 327\nfactor loading, 513\nfalse negative, 25\nfalse positive, 25\nfast gradient sign method, 306\nfast R-CNN, 314\nfeature extraction, 20, 113\nfeature map, 291\nfeatures, 179\nfeed-forward network, 172, 193\nfeed-forward networks, 19"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3254, "text": "feed-forward networks, 19\nfew-shot learning, 191, 394\nﬁlter, 291\nﬁne-tuning, 3, 22, 189, 392\nﬂow matching, 558\nforward kinematics, 199\nforward problem, 198\nforward propagation, 235\nfoundation model, 22, 358, 392, 409\nfrequentist probability, 54\nfuel system, 341\nfully connected graphical model, 328\nfully convolutional network, 318\nfunctional, 617\nGabor ﬁlters, 302\ngamma distribution, 434\nGAN, see generative adversarial network\ngated recurrent unit, 382\nGaussian, 36, 70"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3255, "text": "gated recurrent unit, 382\nGaussian, 36, 70\nGaussian mixture, 86, 200, 271, 466\n644 INDEX\nGEM, see generalized EM algorithm\ngeneralization, 6\ngeneralized EM algorithm, 489\ngeneralized linear model, 158, 165\ngenerative adversarial network, 533\ngenerative AI, 4\ngenerative model, 4, 144, 346, 533\ngenerative pre-trained transformer, 6, 383\ngeometric deep learning, 424\nGibbs sampling, 446\nglobal minimum, 211\nGNN, see graph neural network\nGPT, see generative pre-trained transformer"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3256, "text": "GPT, see generative pre-trained transformer\nGPU, see graphics processing unit\ngradient descent, 209\ngraph attention network, 421\ngraph convolutional network, 414\ngraph neural network, 407\ngraph representation learning, 409\ngraphical model, 326\ngraphical model factorization, 329\ngraphics processing unit, 20, 358\ngroup theory, 256\nguidance, 600\nHadamard product, 550\nHamiltonian Monte Carlo, 451\nhandwritten digit, 501\nHe initialization, 216\nhead-to-head path, 341\nhead-to-tail path, 340"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3257, "text": "head-to-head path, 341\nhead-to-tail path, 340\nHeaviside step function, 161\nHessian matrix, 211, 242\nHessian outer product approximation, 243\nheteroscedastic, 200\nhidden Markov model, 380, 480\nhidden unit, 19, 180\nhidden variable, see latent variable\nhierarchical representation, 187\nhistogram density estimation, 98\nhistory of machine learning, 16\nhold-out set, 14\nhomogeneous Markov chain, 443\nHooke’s law, 520\nHutchinson’s trace estimator, 557\nhybrid Monte Carlo, 451\nhyperparameter, 14"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3258, "text": "hybrid Monte Carlo, 451\nhyperparameter, 14\nIAF, see inverse autoregressive ﬂow\nICA, see independent component analysis\nidentiﬁability, 470\nIID, see independent and identically distributed\nimage segmentation, 315\nImageNet data set, 299\nimportance sampling, 437, 450\nimportance weight, 437\nimproper distribution, 33\nimproper prior, 263\ninactive constraint, 623\nincomplete data set, 476\nindependent and identically distributed, 37, 344\nindependent component analysis, 514"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3259, "text": "independent component analysis, 514\nindependent factor analysis, 515\nindependent variables, 31\ninductive bias, 19, 254\ninductive learning, 409, 420\ninequality constraint, 623\ninference, 120, 138, 143, 336\nInfoNCE, 191\ninformation theory, 46\ninstance discrimination, 192\ninternal covariate shift, 229\ninternal representation, 308\nintersection-over-union, 310\nintrinsic dimensionality, 496\ninvariance, 256, 256, 297, 412\ninverse autoregressive ﬂow, 553\ninverse kinematics, 199"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3260, "text": "inverse kinematics, 199\ninverse problem, 123, 198, 254, 346\nIris data, 173\nIRLS, see iterative reweighted least squares\nisotropic covariance matrix, 75\niterative reweighted least squares, 160\nJacobian matrix, 44, 240\nJensen’s inequality, 52\nJensen–Shannon divergence, 544\nKnearest neighbours, 103\nK-means clustering algorithm, 460, 480\nKalman ﬁlter, 353, 515\nINDEX 645\nKarush–Kuhn–Tucker conditions, 624\nkernel density estimator, 100, 596\nkernel function, 101\nkernel image, 291"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3261, "text": "kernel function, 101\nkernel image, 291\nKKT, see Karush-Kuhn-Tucker conditions\nKL divergence, see Kullback–Leibler divergence\nKosambi–Karhunen–Lo`eve transform, 497\nKullback–Leibler divergence, 51, 486\nLagrange multiplier, 621\nLagrangian, 622\nLangevin dynamics, 454\nLangevin sampling, 455\nlanguage model, 382\nLaplace distribution, 34\nlarge language model, 5, 382, 390\nlasso, 264\nlatent class analysis, 481\nlatent diffusion model, 601\nlatent variable, 76, 335, 459, 495\nlayer normalization, 229, 369"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3262, "text": "layer normalization, 229, 369\nLDM, see latent diffusion model\nLDS, see linear dynamical system\nleaky ReLU, 185\nlearning curve, 223, 266\nlearning rate parameter, 214\nlearning to learn, 190\nleast-mean-squares algorithm, 118\nleast-squares GAN, 537\nleave-one-out, 15\nLeNet convolutional network, 299\nLevenberg–Marquardt approximation, 244\nlikelihood function, 38, 468\nlikelihood weighted sampling, 451\nlinear discriminant, 132\nlinear dynamical system, 515\nlinear independence, 610"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3263, "text": "linear independence, 610\nlinear regression, 6, 112\nlinear-Gaussian model, 79, 332, 332\nlinearly separable, 132\nlink, see edge\nlink function, 158, 165\nLLM, see large language model\nLMS, see least-mean-squares algorithm\nlocal minimum, 211\nlog odds, 151\nlogic sampling, 450\nlogistic regression, 159\nlogistic sigmoid, 95, 113, 151, 159\nlogit function, 151\nlong short-term memory, 382\nLoRA, see low-rank adaptation\nloss function, 120, 140\nloss matrix, 142\nlossless data compression, 465"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3264, "text": "loss matrix, 142\nlossless data compression, 465\nlossy data compression, 465\nlow-rank adaptation, 392\nLSGAN, see least-squares GAN\nLSTM, see long short-term memory\nM step, 472, 477\nmacrostate, 48\nMAE, see masked autoencoder\nMAF, see masked autoregressive ﬂow\nMahalanobis distance, 71\nmanifold, 177, 522\nMAP, see maximum a posteriori\nmarginal probability, 27\nMarkov blanket, 347, 449\nMarkov boundary, see Markov blanket\nMarkov chain, 351, 442\nMarkov chain Monte Carlo, 440\nMarkov model, 351"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3265, "text": "Markov chain Monte Carlo, 440\nMarkov model, 351\nMarkov random ﬁeld, 327\nmasked attention, 384\nmasked autoencoder, 567\nmasked autoregressive ﬂow, 553\nmax-pooling, 297\nmax-unpooling, 317\nmaximization step, see M step\nmaximum a posteriori, 56, 477\nmaximum likelihood, 38, 84, 115, 153\nMCMC, see Markov chain Monte Carlo\nMDN, see mixture density network\nmean, 36\nmean value theorem, 49\nmeasure theory, 33\nmel spectrogram, 399\nmessage-passing, 414\nmessage-passing neural network, 415\n646 INDEX"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3266, "text": "message-passing neural network, 415\n646 INDEX\nmeta-learning, 190\nMetropolis algorithm, 441\nMetropolis–Hastings algorithm, 445\nmicrostate, 48\nmini-batches, 216\nminimum risk, 145\nMinkowski loss, 122\nmissing at random, 477, 519\nmissing data, 519\nmixing coefﬁcient, 87\nmixture component, 87\nmixture density network, 198\nmixture distribution, 459\nmixture model, 459\nmixture of Gaussians, 86, 200, 271, 466\nMLP, see multilayer perceptron\nMNIST data, 495\nmode collapse, 536\nmodel averaging, 277"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3267, "text": "mode collapse, 536\nmodel averaging, 277\nmodel comparison, 9\nmodel selection, 14\nmoment, 37\nmomentum, 220\nMonte Carlo dropout, 280\nMonte Carlo sampling, 429\nMoore-Penrose pseudo-inverse, see pseudo-inverse\nMRF, see Markov random ﬁeld\nmulti-class logistic regression, 161\nmulti-head attention, 366\nmultilayer perceptron, 18, 172\nmultimodal transformer, 394\nmultimodality, 199\nmultinomial distribution, 70, 95\nmultiplicity, 48\nmultitask learning, 190\nmutual information, 54\nn-gram model, 379"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3268, "text": "mutual information, 54\nn-gram model, 379\nnaive Bayes model, 147, 344, 378\nnats, 47\nnatural language processing, 374\nnatural parameter, 94\nnearest-neighbours, 103\nneocognitron, 302\nNesterov momentum, 221\nneural ordinary differential equation, 554\nneuroscience, 302\nNLP, see natural language processing\nno free lunch theorem, 255\nnode, 326, 410\nnoise, 23\nnoiseless coding theorem, 47\nnoisy-OR, 354\nnon-identiﬁability, 513\nnon-max suppression, 314\nnonparametric methods, 66, 98"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3269, "text": "nonparametric methods, 66, 98\nnormal distribution, see Gaussian\nnormal equations, 116\nnormalized exponential, see softmax function\nnovelty detection, 144\nobject detection, 308\nobserved variable, 335\nOld Faithful data, 86\non-hot encoding, see 1-of-Kencoding\none-shot learning, 191\none-versus-one classiﬁer, 134\none-versus-the-rest classiﬁer, 134\nonline gradient descent, 215\nonline learning, 117\nordered over-relaxation, 449\nouter product approximation, 244\noutlier, 137, 144, 164"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3270, "text": "outlier, 137, 144, 164\nover-ﬁtting, 10, 123, 470\nover-relaxation, 449\nover-smoothing, 422\npadding, 294\nparameter sharing, 270, 331\nparameter shrinkage, 118\nparameter tying, see parameter sharing\nparent node, 247, 327\npartition function, 452\nParzen estimator, see kernel density estimator\nParzen window, 101\nPCA, see principal component analysis\nperceptron, 17\nperiodic variables, 89\npermutation matrix, 411\nPixelCNN, 397\nPixelRNN, 397\nINDEX 647\nplate, 334\npolynomial curve ﬁtting, 6\npooling, 296"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3271, "text": "polynomial curve ﬁtting, 6\npooling, 296\npositional encoding, 371\npositive deﬁnite covariance, 72\npositive deﬁnite matrix, 615\nposterior collapse, 577\nposterior probability, 31\npower method, 498\npre-activation, 17\npre-processing, 20\npre-training, 189, 392\nprecision matrix, 77\nprecision parameter, 36\npredictive distribution, 42, 120\npreﬁx prompt, 394\nprincipal component analysis, 497, 506, 565\nprincipal subspace, 497\nprior, 263\nprior knowledge, 19, 255\nprior probability, 31, 145"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3272, "text": "prior probability, 31, 145\nprobabilistic graphical model, see graphical model\nprobabilistic PCA, 506\nprobability, 25\nprobability density, 32\nprobability theory, 23\nprobit function, 164\nprobit regression, 163\nproduct rule of probability, 26, 28, 326\nprompt, 394, 601\nprompt engineering, 394\nproposal distribution, 433, 437, 441\npseudo-inverse, 116, 136\npseudo-random numbers, 430\nquadratic discriminant, 153\nradial basis functions, 179\nrandom variable, 26\nraster scan, 397\nreadout layer, 419"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3273, "text": "raster scan, 397\nreadout layer, 419\nreal NVP normalizing ﬂow, 549\nreceiver operating characteristic, see ROC curve\nreceptive ﬁeld, 290, 416\nrecurrent neural network, 380\nregression, 3\nregression function, 121\nregularization, 12, 253\nregularized least squares, 118\nreject option, 142, 145\nrejection sampling, 433\nrelative entropy, 51\nreparameterization trick, 574\nrepresentation learning, 22, 188\nresidual block, 275\nresidual connection, 22, 274\nresidual network, 275\nresnet, see residual network"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3274, "text": "resnet, see residual network\nresponsibility, 88, 468\nRLHF, 394\nRMS error, see root-mean-square error\nRMSProp, 223\nRNN, see recurrent neural network\nrobot arm, 198\nrobustness, 137\nROC curve, 148\nroot-mean-square error, 10\nsaliency map, 305\nsame convolution, 294\nsample mean, 39\nsample variance, 39\nsampling, 429\nsampling-importance-resampling, 439\nscale invariance, 256\nscaled self-attention, 366\nscaling hypothesis, 358\nSchur complement, 79\nscore function, 455, 594\nscore matching, 594"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3275, "text": "score function, 455, 594\nscore matching, 594\nself-attention, 362\nself-supervised learning, 5, 375\nsemi-supervised learning, 420\nsequential estimation, 85\nsequential gradient descent, 118\nsequential learning, 117\nSGD, see stochastic gradient descent\nshared parameters, see parameter sharing\nshared weights, 292\nshattered gradients, 274\nshrinkage, 13\n648 INDEX\nsigmoid, see logistic sigmoid\nsingular value decomposition, 117\nSIR, see sampling-importance-resampling\nskip-grams, 375"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3276, "text": "skip-grams, 375\nskip-layer connections, 274\nsliding window, 311\nsmoothing parameter, 100\nsoft ReLU, 185\nsoft weight sharing, 271\nsoftmax function, 96, 152, 197, 201, 363\nsoftplus activation function, 185\nsparse autoencoders, 566\nsparse connections, 292\nsparsity, 264\nsphering, 504\nstandard deviation, 36\nstandardizing, 462, 503\nstate-space model, 352\nstatistical bias, see bias\nstatistical independence, see independent variables\nsteepest descent, 214\nStein score, see score function"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3277, "text": "Stein score, see score function\nStirling’s approximation, 48\nstochastic, 8\nstochastic differential equation, 598\nstochastic gradient descent, 19, 214, 215\nstochastic variable, 26\nstrided convolution, 294\nstrides, 311\nstructured data, 287, 407\nstyle transfer, 320\nsufﬁcient statistics, 67, 69, 84, 97\nsum rule of probability, 26, 28, 326\nsum-of-squares error, 8, 41, 136\nsupervised learning, 3, 420\nsupport vector machine, 179\nSVD, see singular value decomposition\nSVM, see support vector machine"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3278, "text": "SVM, see support vector machine\nswish activation function, 205\nsymmetry, 256\nsymmetry breaking, 216\ntail-to-tail path, 339\ntangent propagation, 258\ntemperature, 387\ntensor, 194, 295\ntest set, 10, 14\ntext-to-speech, 400\ntied parameters, see parameter sharing\ntoken, 360\ntokenization, 377\ntraining set, 3\ntransductive, 409, 419\ntransductive learning, 420\ntransfer learning, 3, 189, 218, 388\ntransformers, 357\ntransition probability, 443\ntranslation invariance, 256\ntranspose convolution, 318"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3279, "text": "transpose convolution, 318\ntri-gram model, 379\nTTS, see text-to-speech\nU-net, 319\nundetermined multiplier, see Lagrange multiplier\nundirected graphical model, 327\nuniquenesses, 513\nuniversal approximation theorems, 182\nunobserved variable, see latent variable\nunsupervised learning, 4, 188\nutility function, 140\nV AE,see variational autoencoder\nvalid convolution, 294\nvalidation set, 14\nvanishing gradient, 227, 382\nvariance, 35, 36, 125\nvariational autoencoder, 569\nvariational inference, 485"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3280, "text": "variational inference, 485\nvariational lower bound, see evidence lower bound\nvector quantization, 398, 465\nvertex, see node\nvision transformer, 395\nvon Mises distribution, 89\nvoxel, 289\nWasserstein distance, 538\nWasserstein GAN, 538\nwavelets, 114\nweakly supervised, 192\nweight decay, 13, 260\nweight parameter, 17, 180\nINDEX 649\nweight sharing, see parameter sharing\nweight vector, 132\nweight-space symmetry, 185\nWGAN, see Wasserstein GAN\nwhitening, 502\nWoodbury identity, 610\nword embedding, 375"}
{"source": "/Users/kanishk/Downloads/Mahesh/DL.pdf", "chunk_id": 3281, "text": "Woodbury identity, 610\nword embedding, 375\nword2vec, 375\nwrapped distribution, 94\nYellowstone National Park, 86"}
